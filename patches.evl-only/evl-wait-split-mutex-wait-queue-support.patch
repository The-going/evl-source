From bd1647e749be6c3c07e8b0cb727d087d576ef7c4 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Wed, 30 Jan 2019 12:15:09 +0100
Subject: [PATCH] evl/wait: split mutex / wait queue support

---
 drivers/evenless/hectic.c       |   5 +-
 drivers/evenless/latmus.c       |   8 +-
 include/evenless/clock.h        |  20 +-
 include/evenless/factory.h      |  20 +-
 include/evenless/file.h         |   2 +-
 include/evenless/flag.h         |  91 ++++
 include/evenless/ksem.h         |   8 +-
 include/evenless/memory.h       |   2 +-
 include/evenless/monitor.h      |   9 +-
 include/evenless/mutex.h        | 121 +++++-
 include/evenless/poller.h       |   4 +-
 include/evenless/sched.h        |   3 +-
 include/evenless/sched/queue.h  |   6 +-
 include/evenless/sched/quota.h  |  14 +-
 include/evenless/sched/rt.h     |  10 +-
 include/evenless/sched/weak.h   |   2 +-
 include/evenless/stat.h         |  11 +-
 include/evenless/synch.h        | 116 ------
 include/evenless/thread.h       |  79 ++--
 include/evenless/tick.h         |   2 +-
 include/evenless/timer.h        |  54 +--
 include/evenless/wait.h         | 141 +++----
 include/evenless/xbuf.h         |  10 +-
 include/trace/events/evenless.h |  80 ++--
 include/uapi/evenless/mutex.h   |  43 ++
 include/uapi/evenless/sem.h     |   1 -
 include/uapi/evenless/signal.h  |   4 +-
 include/uapi/evenless/synch.h   |  65 ---
 include/uapi/evenless/types.h   |   6 +-
 kernel/evenless/Kconfig         |  22 +-
 kernel/evenless/Makefile        |   2 +-
 kernel/evenless/clock.c         |  66 +--
 kernel/evenless/control.c       |  20 +-
 kernel/evenless/factory.c       |  20 +-
 kernel/evenless/file.c          |  14 +-
 kernel/evenless/init.c          |   6 +-
 kernel/evenless/ksem.c          |  13 +-
 kernel/evenless/lock.c          |   8 +-
 kernel/evenless/logger.c        |   8 +-
 kernel/evenless/mapper.c        |   2 +-
 kernel/evenless/memory.c        |  28 +-
 kernel/evenless/monitor.c       | 105 +++--
 kernel/evenless/mutex.c         | 761 ++++++++++++++++++++++++++++++++--
 kernel/evenless/poller.c        |  34 +-
 kernel/evenless/sched/core.c    |  72 ++--
 kernel/evenless/sched/idle.c    |   6 +-
 kernel/evenless/sched/quota.c   |  40 +-
 kernel/evenless/sched/rt.c      |  16 +-
 kernel/evenless/sched/weak.c    |  12 +-
 kernel/evenless/sem.c           |  53 ++-
 kernel/evenless/synch.c         | 898 ----------------------------------------
 kernel/evenless/syscall.c       |  40 +-
 kernel/evenless/thread.c        | 264 ++++++------
 kernel/evenless/tick.c          |  14 +-
 kernel/evenless/timer.c         |  22 +-
 kernel/evenless/timerfd.c       |  36 +-
 kernel/evenless/trace.c         |   8 +-
 kernel/evenless/wait.c          | 143 +++++++
 kernel/evenless/xbuf.c          |  50 +--
 59 files changed, 1856 insertions(+), 1864 deletions(-)
 create mode 100644 include/evenless/flag.h
 delete mode 100644 include/evenless/synch.h
 create mode 100644 include/uapi/evenless/mutex.h
 delete mode 100644 include/uapi/evenless/synch.h
 delete mode 100644 kernel/evenless/synch.c
 create mode 100644 kernel/evenless/wait.c

diff --git a/drivers/evenless/hectic.c b/drivers/evenless/hectic.c
index 11cf34c28c2..b085544c4d3 100644
--- a/drivers/evenless/hectic.c
+++ b/drivers/evenless/hectic.c
@@ -14,9 +14,8 @@
 #include <linux/uaccess.h>
 #include <linux/semaphore.h>
 #include <linux/irq_work.h>
-#include <evenless/synch.h>
 #include <evenless/thread.h>
-#include <evenless/wait.h>
+#include <evenless/flag.h>
 #include <evenless/file.h>
 #include <asm/evenless/fptest.h>
 #include <uapi/evenless/devices/hectic.h>
@@ -29,7 +28,7 @@ struct rtswitch_context;
 
 struct rtswitch_task {
 	struct hectic_task_index base;
-	struct evl_wait_flag rt_synch;
+	struct evl_flag rt_synch;
 	struct semaphore nrt_synch;
 	struct evl_kthread kthread; /* For kernel-space real-time tasks. */
 	unsigned int last_switch;
diff --git a/drivers/evenless/latmus.c b/drivers/evenless/latmus.c
index 35cf283fcd7..30444297d63 100644
--- a/drivers/evenless/latmus.c
+++ b/drivers/evenless/latmus.c
@@ -16,7 +16,7 @@
 #include <linux/fcntl.h>
 #include <linux/uaccess.h>
 #include <evenless/file.h>
-#include <evenless/wait.h>
+#include <evenless/flag.h>
 #include <evenless/clock.h>
 #include <evenless/thread.h>
 #include <evenless/xbuf.h>
@@ -67,7 +67,7 @@ struct latmus_runner {
 	int (*run)(struct latmus_runner *runner, struct latmus_result *result);
 	void (*cleanup)(struct latmus_runner *runner);
 	struct runner_state state;
-	struct evl_wait_flag done;
+	struct evl_flag done;
 	int status;
 	int verbosity;
 	ktime_t period;
@@ -94,14 +94,14 @@ struct irq_runner {
 
 struct kthread_runner {
 	struct evl_kthread kthread;
-	struct evl_wait_flag barrier;
+	struct evl_flag barrier;
 	ktime_t start_time;
 	struct latmus_runner runner;
 };
 
 struct uthread_runner {
 	struct evl_timer timer;
-	struct evl_wait_flag pulse;
+	struct evl_flag pulse;
 	struct latmus_runner runner;
 };
 
diff --git a/include/evenless/clock.h b/include/evenless/clock.h
index 346c4d17423..c341ea4bdbc 100644
--- a/include/evenless/clock.h
+++ b/include/evenless/clock.h
@@ -41,13 +41,13 @@ struct evl_clock {
 				const struct timespec *ts);
 		void (*program_local_shot)(struct evl_clock *clock);
 		void (*program_remote_shot)(struct evl_clock *clock,
-					    struct evl_rq *rq);
+					struct evl_rq *rq);
 		int (*set_gravity)(struct evl_clock *clock,
-				   const struct evl_clock_gravity *p);
+				const struct evl_clock_gravity *p);
 		void (*reset_gravity)(struct evl_clock *clock);
 		void (*adjust)(struct evl_clock *clock);
 		int (*adjust_time)(struct evl_clock *clock,
-				   struct timex *tx);
+				struct timex *tx);
 	} ops;
 	struct evl_timerbase *timerdata;
 	struct evl_clock *master;
@@ -67,15 +67,15 @@ extern struct evl_clock evl_mono_clock;
 extern struct evl_clock evl_realtime_clock;
 
 int evl_init_clock(struct evl_clock *clock,
-		   const struct cpumask *affinity);
+		const struct cpumask *affinity);
 
 int evl_init_slave_clock(struct evl_clock *clock,
-			 struct evl_clock *master);
+			struct evl_clock *master);
 
 void evl_announce_tick(struct evl_clock *clock);
 
 void evl_adjust_timers(struct evl_clock *clock,
-		       ktime_t delta);
+		ktime_t delta);
 
 void evl_stop_timers(struct evl_clock *clock);
 
@@ -105,7 +105,7 @@ static inline ktime_t evl_read_clock(struct evl_clock *clock)
 
 static inline int
 evl_set_clock_time(struct evl_clock *clock,
-		   const struct timespec *ts)
+		const struct timespec *ts)
 {
 	if (clock->ops.set_time)
 		return clock->ops.set_time(clock, ts);
@@ -121,14 +121,14 @@ ktime_t evl_get_clock_resolution(struct evl_clock *clock)
 
 static inline
 void evl_set_clock_resolution(struct evl_clock *clock,
-			      ktime_t resolution)
+			ktime_t resolution)
 {
 	clock->resolution = resolution;
 }
 
 static inline
 int evl_set_clock_gravity(struct evl_clock *clock,
-			  const struct evl_clock_gravity *gravity)
+			const struct evl_clock_gravity *gravity)
 {
 	if (clock->ops.set_gravity)
 		return clock->ops.set_gravity(clock, gravity);
@@ -158,7 +158,7 @@ int evl_clock_init(void);
 void evl_clock_cleanup(void);
 
 int evl_register_clock(struct evl_clock *clock,
-		       const struct cpumask *affinity);
+		const struct cpumask *affinity);
 
 void evl_unregister_clock(struct evl_clock *clock);
 
diff --git a/include/evenless/factory.h b/include/evenless/factory.h
index 3aa7c56eb4e..956fbd92441 100644
--- a/include/evenless/factory.h
+++ b/include/evenless/factory.h
@@ -38,9 +38,9 @@ struct evl_factory {
 	const struct file_operations *fops;
 	unsigned int nrdev;
 	struct evl_element *(*build)(struct evl_factory *fac,
-				     const char *name,
-				     void __user *u_attrs,
-				     u32 *state_offp);
+				const char *name,
+				void __user *u_attrs,
+				u32 *state_offp);
 	void (*dispose)(struct evl_element *e);
 	const struct attribute_group **attrs;
 	int flags;
@@ -80,7 +80,7 @@ evl_element_name(struct evl_element *e)
 }
 
 int evl_init_element(struct evl_element *e,
-		     struct evl_factory *fac);
+		struct evl_factory *fac);
 
 void evl_destroy_element(struct evl_element *e);
 
@@ -88,7 +88,7 @@ void evl_get_element(struct evl_element *e);
 
 struct evl_element *
 __evl_get_element_by_fundle(struct evl_factory *fac,
-			    fundle_t fundle);
+			fundle_t fundle);
 
 #define evl_get_element_by_fundle(__fac, __fundle, __type)		\
 	({								\
@@ -111,21 +111,21 @@ __evl_get_element_by_fundle(struct evl_factory *fac,
 void evl_put_element(struct evl_element *e);
 
 int evl_open_element(struct inode *inode,
-		     struct file *filp);
+		struct file *filp);
 
 int evl_close_element(struct inode *inode,
-		      struct file *filp);
+		struct file *filp);
 
 int evl_create_element_device(struct evl_element *e,
-			      struct evl_factory *fac,
-			      const char *devname);
+			struct evl_factory *fac,
+			const char *devname);
 
 void evl_remove_element_device(struct evl_element *e);
 
 void evl_index_element(struct evl_element *e);
 
 int evl_index_element_at(struct evl_element *e,
-			 fundle_t fundle);
+			fundle_t fundle);
 
 void evl_unindex_element(struct evl_element *e);
 
diff --git a/include/evenless/file.h b/include/evenless/file.h
index 74755b13505..265a67e8302 100644
--- a/include/evenless/file.h
+++ b/include/evenless/file.h
@@ -37,7 +37,7 @@ struct evl_file_binding {
 };
 
 int evl_open_file(struct evl_file *sfilp,
-		  struct file *filp);
+		struct file *filp);
 
 void evl_release_file(struct evl_file *sfilp);
 
diff --git a/include/evenless/flag.h b/include/evenless/flag.h
new file mode 100644
index 00000000000..9e188abc554
--- /dev/null
+++ b/include/evenless/flag.h
@@ -0,0 +1,91 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2017 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_FLAG_H
+#define _EVENLESS_FLAG_H
+
+#include <evenless/wait.h>
+#include <evenless/sched.h>
+
+struct evl_flag {
+	struct evl_wait_queue wait;
+	bool signaled;
+};
+
+#define DEFINE_EVL_FLAG(__name)					\
+	struct evl_flag __name = {				\
+		.wait = EVL_WAIT_INITIALIZER((__name).wait),	\
+		.signaled = false,				\
+	}
+
+static inline void evl_init_flag(struct evl_flag *wf)
+{
+	wf->wait = (struct evl_wait_queue)EVL_WAIT_INITIALIZER(wf->wait);
+	wf->signaled = false;
+}
+
+static inline void evl_destroy_flag(struct evl_flag *wf)
+{
+	evl_destroy_wait(&wf->wait);
+}
+
+static inline
+int evl_wait_flag_timeout(struct evl_flag *wf,
+			ktime_t timeout, enum evl_tmode timeout_mode)
+{
+	unsigned long flags;
+	int ret = 0;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	while (!wf->signaled) {
+		ret = evl_wait_timeout(&wf->wait, timeout, timeout_mode);
+		if (ret & T_BREAK)
+			ret = -EINTR;
+		if (ret & T_TIMEO)
+			ret = -ETIMEDOUT;
+		if (ret & T_RMID)
+			ret = -EIDRM;
+		if (ret)
+			break;
+	}
+
+	if (ret == 0)
+		wf->signaled = false;
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+
+static inline int evl_wait_flag(struct evl_flag *wf)
+{
+	return evl_wait_flag_timeout(wf, EVL_INFINITE, EVL_REL);
+}
+
+static inline			/* nklock held. */
+struct evl_thread *evl_wait_flag_head(struct evl_flag *wf)
+{
+	return evl_wait_head(&wf->wait);
+}
+
+static inline bool evl_raise_flag(struct evl_flag *wf)
+{
+	struct evl_thread *waiter;
+	unsigned long flags;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	wf->signaled = true;
+	waiter = evl_wake_up_head(&wf->wait);
+	evl_schedule();
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return waiter != NULL;
+}
+
+#endif /* _EVENLESS_FLAG_H */
diff --git a/include/evenless/ksem.h b/include/evenless/ksem.h
index 726c263be9e..3f661e54f2e 100644
--- a/include/evenless/ksem.h
+++ b/include/evenless/ksem.h
@@ -8,20 +8,20 @@
 #define _EVENLESS_KSEM_H
 
 #include <linux/ktime.h>
-#include <evenless/synch.h>
+#include <evenless/wait.h>
 
 struct evl_ksem {
 	unsigned int value;
-	struct evl_syn wait_queue;
+	struct evl_wait_queue wait_queue;
 };
 
 void evl_init_sem(struct evl_ksem *sem,
-		  unsigned int value);
+		unsigned int value);
 
 void evl_destroy_sem(struct evl_ksem *sem);
 
 int evl_down_timeout(struct evl_ksem *sem,
-		     ktime_t timeout);
+		ktime_t timeout);
 
 int evl_down(struct evl_ksem *sem);
 
diff --git a/include/evenless/memory.h b/include/evenless/memory.h
index 8a005e344a7..f9bd280169a 100644
--- a/include/evenless/memory.h
+++ b/include/evenless/memory.h
@@ -95,7 +95,7 @@ static inline void *evl_get_heap_base(const struct evl_heap *heap)
 }
 
 int evl_init_heap(struct evl_heap *heap, void *membase,
-		  size_t size);
+		size_t size);
 
 void evl_destroy_heap(struct evl_heap *heap);
 
diff --git a/include/evenless/monitor.h b/include/evenless/monitor.h
index c442df28666..fc9dd31e1d1 100644
--- a/include/evenless/monitor.h
+++ b/include/evenless/monitor.h
@@ -9,16 +9,19 @@
 
 #include <evenless/factory.h>
 #include <evenless/thread.h>
+#include <evenless/sched.h>
 
 int evl_signal_monitor_targeted(struct evl_thread *target,
 				int monfd);
 
-void __evl_commit_monitor_ceiling(struct evl_thread *curr);
+void __evl_commit_monitor_ceiling(void);
 
-static inline void evl_commit_monitor_ceiling(struct evl_thread *curr)
+static inline void evl_commit_monitor_ceiling(void)
 {
+	struct evl_thread *curr = evl_current_thread();
+
 	if (curr->u_window->pp_pending != EVL_NO_HANDLE)
-		__evl_commit_monitor_ceiling(curr);
+		__evl_commit_monitor_ceiling();
 }
 
 #endif /* !_EVENLESS_MONITOR_H */
diff --git a/include/evenless/mutex.h b/include/evenless/mutex.h
index 08438468dcf..79ee496ca15 100644
--- a/include/evenless/mutex.h
+++ b/include/evenless/mutex.h
@@ -7,13 +7,130 @@
 #ifndef _EVENLESS_MUTEX_H
 #define _EVENLESS_MUTEX_H
 
+#include <linux/types.h>
 #include <linux/ktime.h>
 #include <linux/atomic.h>
-#include <evenless/synch.h>
+#include <evenless/list.h>
+#include <evenless/assert.h>
+#include <evenless/timer.h>
+#include <evenless/thread.h>
+#include <uapi/evenless/mutex.h>
+
+struct evl_clock;
+struct evl_thread;
+
+#define EVL_MUTEX_PI      	BIT(0)
+#define EVL_MUTEX_PP      	BIT(1)
+#define EVL_MUTEX_CLAIMED	BIT(2)
+#define EVL_MUTEX_CEILING	BIT(3)
 
 struct evl_mutex {
-	struct evl_syn wait_queue;
+	int wprio;
+	int flags;
+	struct evl_thread *owner;
+	struct evl_clock *clock;
+	atomic_t *fastlock;
+	u32 *ceiling_ref;
+	struct evl_wait_channel wchan;
+	struct list_head wait_list;
+	struct list_head next;	/* thread->boosters */
+};
+
+#define evl_for_each_mutex_waiter(__pos, __mutex)			\
+	list_for_each_entry(__pos, &(__mutex)->wait_list, wait_next)
+
+void evl_init_mutex_pi(struct evl_mutex *mutex,
+		struct evl_clock *clock,
+		atomic_t *fastlock);
+
+void evl_init_mutex_pp(struct evl_mutex *mutex,
+		struct evl_clock *clock,
+		atomic_t *fastlock,
+		u32 *ceiling_ref);
+
+bool evl_destroy_mutex(struct evl_mutex *mutex);
+
+int evl_trylock_mutex(struct evl_mutex *mutex);
+
+int evl_lock_mutex_timeout(struct evl_mutex *mutex, ktime_t timeout,
+			enum evl_tmode timeout_mode);
+
+static inline int evl_lock_mutex(struct evl_mutex *mutex)
+{
+	return evl_lock_mutex_timeout(mutex, EVL_INFINITE, EVL_REL);
+}
+
+void __evl_unlock_mutex(struct evl_mutex *mutex);
+
+void evl_unlock_mutex(struct evl_mutex *mutex);
+
+void evl_commit_mutex_ceiling(struct evl_mutex *mutex);
+
+#ifdef CONFIG_EVENLESS_DEBUG_MUTEX_INBAND
+void evl_detect_boost_drop(struct evl_thread *owner);
+#else
+static inline
+void evl_detect_boost_drop(struct evl_thread *owner) { }
+#endif
+
+void evl_abort_mutex_wait(struct evl_thread *thread);
+
+void evl_reorder_mutex_wait(struct evl_thread *thread);
+
+struct evl_kmutex {
+	struct evl_mutex mutex;
 	atomic_t fastlock;
 };
 
+#define EVL_KMUTEX_INITIALIZER(__name) {				\
+		.mutex = {						\
+			.fastlock = &(__name).fastlock,			\
+			.flags = EVL_MUTEX_PI,				\
+			.owner = NULL,					\
+			.wprio = -1,					\
+			.ceiling_ref = NULL,				\
+			.clock = &evl_mono_clock,			\
+			.wait_list = LIST_HEAD_INIT((__name).mutex.wait_list), \
+			.wchan = {					\
+				.abort_wait = evl_abort_mutex_wait,	\
+				.reorder_wait = evl_reorder_mutex_wait,	\
+				.lock = __HARD_SPIN_LOCK_INITIALIZER((__name).wchan.lock), \
+			},						\
+		},							\
+			.fastlock = ATOMIC_INIT(0),			\
+	}
+
+#define DEFINE_EVL_MUTEX(__name)					\
+	struct evl_kmutex __name = EVL_KMUTEX_INITIALIZER(__name)
+
+static inline
+void evl_init_kmutex(struct evl_kmutex *kmutex)
+{
+	*kmutex = (struct evl_kmutex)EVL_KMUTEX_INITIALIZER(*kmutex);
+}
+
+static inline
+void evl_destroy_kmutex(struct evl_kmutex *kmutex)
+{
+	evl_destroy_mutex(&kmutex->mutex);
+}
+
+static inline
+int evl_trylock_kmutex(struct evl_kmutex *kmutex)
+{
+	return evl_trylock_mutex(&kmutex->mutex);
+}
+
+static inline
+int evl_lock_kmutex(struct evl_kmutex *kmutex)
+{
+	return evl_lock_mutex(&kmutex->mutex);
+}
+
+static inline
+void evl_unlock_kmutex(struct evl_kmutex *kmutex)
+{
+	return evl_unlock_mutex(&kmutex->mutex);
+}
+
 #endif /* !_EVENLESS_MUTEX_H */
diff --git a/include/evenless/poller.h b/include/evenless/poller.h
index b78910daee0..0e9ca03a0ac 100644
--- a/include/evenless/poller.h
+++ b/include/evenless/poller.h
@@ -12,13 +12,13 @@
 #include <linux/rbtree.h>
 #include <linux/spinlock.h>
 #include <linux/poll.h>
-#include <evenless/synch.h>
+#include <evenless/wait.h>
 #include <evenless/factory.h>
 #include <uapi/evenless/poller.h>
 
 #define EVL_POLLHEAD_INITIALIZER(__name) {				\
 		.watchpoints = LIST_HEAD_INIT((__name).watchpoints),	\
-		lock = __HARD_SPIN_LOCK_INITIALIZER(__name),		\
+		lock = __HARD_SPIN_LOCK_INITIALIZER((__name).lock),	\
 	}
 
 struct evl_poll_head {
diff --git a/include/evenless/sched.h b/include/evenless/sched.h
index 8de66f8cbb0..1a05a6af8a8 100644
--- a/include/evenless/sched.h
+++ b/include/evenless/sched.h
@@ -12,6 +12,7 @@
 #include <linux/list.h>
 #include <evenless/lock.h>
 #include <evenless/thread.h>
+#include <evenless/wait.h>
 #include <evenless/sched/queue.h>
 #include <evenless/sched/weak.h>
 #include <evenless/sched/quota.h>
@@ -102,7 +103,7 @@ struct evl_rq {
 	/* Currently active account */
 	struct evl_account *current_account;
 #endif
-	struct evl_syn yield_sync;
+	struct evl_wait_queue yield_sync;
 };
 
 DECLARE_PER_CPU(struct evl_rq, evl_runqueues);
diff --git a/include/evenless/sched/queue.h b/include/evenless/sched/queue.h
index d3763cbe37c..7a9f638a462 100644
--- a/include/evenless/sched/queue.h
+++ b/include/evenless/sched/queue.h
@@ -32,13 +32,13 @@ struct evl_thread;
 void evl_init_schedq(struct evl_multilevel_queue *q);
 
 void evl_add_schedq(struct evl_multilevel_queue *q,
-		    struct evl_thread *thread);
+		struct evl_thread *thread);
 
 void evl_add_schedq_tail(struct evl_multilevel_queue *q,
-			 struct evl_thread *thread);
+			struct evl_thread *thread);
 
 void evl_del_schedq(struct evl_multilevel_queue *q,
-		    struct evl_thread *thread);
+		struct evl_thread *thread);
 
 struct evl_thread *evl_get_schedq(struct evl_multilevel_queue *q);
 
diff --git a/include/evenless/sched/quota.h b/include/evenless/sched/quota.h
index 601a6c9ccc6..763ebe53747 100644
--- a/include/evenless/sched/quota.h
+++ b/include/evenless/sched/quota.h
@@ -16,7 +16,7 @@
 
 #define EVL_QUOTA_MIN_PRIO	1
 #define EVL_QUOTA_MAX_PRIO	255
-#define EVL_QUOTA_NR_PRIO					\
+#define EVL_QUOTA_NR_PRIO	\
 	(EVL_QUOTA_MAX_PRIO - EVL_QUOTA_MIN_PRIO + 1)
 
 extern struct evl_sched_class evl_sched_quota;
@@ -54,16 +54,16 @@ static inline int evl_quota_init_thread(struct evl_thread *thread)
 }
 
 int evl_quota_create_group(struct evl_quota_group *tg,
-			   struct evl_rq *rq,
-			   int *quota_sum_r);
+			struct evl_rq *rq,
+			int *quota_sum_r);
 
 int evl_quota_destroy_group(struct evl_quota_group *tg,
-			    int force,
-			    int *quota_sum_r);
+			int force,
+			int *quota_sum_r);
 
 void evl_quota_set_limit(struct evl_quota_group *tg,
-			 int quota_percent, int quota_peak_percent,
-			 int *quota_sum_r);
+			int quota_percent, int quota_peak_percent,
+			int *quota_sum_r);
 
 struct evl_quota_group *
 evl_quota_find_group(struct evl_rq *rq, int tgid);
diff --git a/include/evenless/sched/rt.h b/include/evenless/sched/rt.h
index da5ed06fa25..f1eecc9383f 100644
--- a/include/evenless/sched/rt.h
+++ b/include/evenless/sched/rt.h
@@ -48,10 +48,10 @@ static inline void __evl_dequeue_rt_thread(struct evl_thread *thread)
 
 static inline
 int __evl_chk_rt_schedparam(struct evl_thread *thread,
-			    const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	if (p->rt.prio < EVL_CORE_MIN_PRIO ||
-	    p->rt.prio > EVL_CORE_MAX_PRIO)
+		p->rt.prio > EVL_CORE_MAX_PRIO)
 		return -EINVAL;
 
 	return 0;
@@ -59,7 +59,7 @@ int __evl_chk_rt_schedparam(struct evl_thread *thread,
 
 static inline
 bool __evl_set_rt_schedparam(struct evl_thread *thread,
-			     const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	bool ret = evl_set_effective_thread_priority(thread, p->rt.prio);
 
@@ -71,14 +71,14 @@ bool __evl_set_rt_schedparam(struct evl_thread *thread,
 
 static inline
 void __evl_get_rt_schedparam(struct evl_thread *thread,
-			     union evl_sched_param *p)
+			union evl_sched_param *p)
 {
 	p->rt.prio = thread->cprio;
 }
 
 static inline
 void __evl_track_rt_priority(struct evl_thread *thread,
-			     const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	if (p)
 		thread->cprio = p->rt.prio; /* Force update. */
diff --git a/include/evenless/sched/weak.h b/include/evenless/sched/weak.h
index 2a38e33927e..2cbedab7c32 100644
--- a/include/evenless/sched/weak.h
+++ b/include/evenless/sched/weak.h
@@ -17,7 +17,7 @@
 #define EVL_WEAK_NR_PRIO   (EVL_WEAK_MAX_PRIO - EVL_WEAK_MIN_PRIO + 1)
 
 #if EVL_WEAK_NR_PRIO > EVL_CLASS_WEIGHT_FACTOR ||	\
-	 EVL_WEAK_NR_PRIO > EVL_MLQ_LEVELS
+	EVL_WEAK_NR_PRIO > EVL_MLQ_LEVELS
 #error "WEAK class has too many priority levels"
 #endif
 
diff --git a/include/evenless/stat.h b/include/evenless/stat.h
index 6016242a6b4..47da6d91e33 100644
--- a/include/evenless/stat.h
+++ b/include/evenless/stat.h
@@ -70,7 +70,7 @@ static inline void evl_reset_account(struct evl_account *account)
 		struct evl_account *__prev;				\
 		__prev = (struct evl_account *)				\
 			atomic_long_xchg(&(__rq)->current_account,	\
-					 (long)(__new_account));	\
+					(long)(__new_account));		\
 		__prev;							\
 	})
 
@@ -85,9 +85,9 @@ static inline void evl_reset_account(struct evl_account *account)
 		(__rq)->current_account = (__new_account);	\
 	} while (0)
 
-	struct evl_counter {
-		unsigned long counter;
-	};
+struct evl_counter {
+	unsigned long counter;
+};
 
 static inline unsigned long evl_inc_counter(struct evl_counter *c)
 {
@@ -99,7 +99,8 @@ static inline unsigned long evl_get_counter(struct evl_counter *c)
 	return c->counter;
 }
 
-static inline void evl_set_counter(struct evl_counter *c, unsigned long value)
+static inline
+void evl_set_counter(struct evl_counter *c, unsigned long value)
 {
 	c->counter = value;
 }
diff --git a/include/evenless/synch.h b/include/evenless/synch.h
deleted file mode 100644
index 95080e91073..00000000000
--- a/include/evenless/synch.h
+++ /dev/null
@@ -1,116 +0,0 @@
-/*
- * SPDX-License-Identifier: GPL-2.0
- *
- * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
- * Copyright (C) 2001, 2018 Philippe Gerum  <rpm@xenomai.org>
- */
-
-#ifndef _EVENLESS_SYN_H
-#define _EVENLESS_SYN_H
-
-#include <linux/types.h>
-#include <evenless/list.h>
-#include <evenless/assert.h>
-#include <evenless/timer.h>
-#include <uapi/evenless/synch.h>
-#include <uapi/evenless/thread.h>
-
-#define EVL_SYN_CLAIMED  0x100	/* Claimed by other thread(s) (PI) */
-#define EVL_SYN_CEILING  0x200	/* Actively boosting (PP) */
-
-struct evl_thread;
-struct evl_syn;
-struct evl_clock;
-
-struct evl_syn {
-	/* wait (weighted) prio in thread->boosters */
-	int wprio;
-	/* thread->boosters */
-	struct list_head next;
-	/*
-	 *  &variable holding the current priority ceiling value
-	 *  (evl_sched_rt-based, [1..255], EVL_SYN_PP).
-	 */
-	u32 *ceiling_ref;
-	/* Status word */
-	unsigned long status;
-	/* Waiting threads */
-	struct list_head wait_list;
-	/* Thread which owns the resource */
-	struct evl_thread *owner;
-	/* Pointer to fast lock word */
-	atomic_t *fastlock;
-	/* Reference clock. */
-	struct evl_clock *clock;
-};
-
-#define EVL_SYN_INITIALIZER(__name, __type)				\
-	{								\
-		.status = __type,					\
-			.wprio = -1,					\
-			.owner = NULL,					\
-			.fastlock = NULL,				\
-			.ceiling_ref = NULL,				\
-			.wait_list = LIST_HEAD_INIT((__name).wait_list), \
-			.next = LIST_HEAD_INIT((__name).next),		\
-			}
-
-#define evl_for_each_syn_waiter(__pos, __synch)				\
-	list_for_each_entry(__pos, &(__synch)->wait_list, syn_next)
-
-#define evl_for_each_syn_waiter_safe(__pos, __tmp, __synch)		\
-	list_for_each_entry_safe(__pos, __tmp, &(__synch)->wait_list, syn_next)
-
-static inline int evl_syn_has_waiter(struct evl_syn *synch)
-{
-	return !list_empty(&synch->wait_list);
-}
-
-struct evl_thread *evl_syn_wait_head(struct evl_syn *synch);
-
-#ifdef CONFIG_EVENLESS_DEBUG_MONITOR_INBAND
-void evl_detect_boost_drop(struct evl_thread *owner);
-#else
-static inline
-void evl_detect_boost_drop(struct evl_thread *owner) { }
-#endif
-
-void evl_init_syn(struct evl_syn *synch, int flags,
-		  struct evl_clock *clock,
-		  atomic_t *fastlock);
-
-void evl_init_syn_protect(struct evl_syn *synch,
-			  struct evl_clock *clock,
-			  atomic_t *fastlock, u32 *ceiling_ref);
-
-bool evl_destroy_syn(struct evl_syn *synch);
-
-int __must_check evl_sleep_on_syn(struct evl_syn *synch,
-				  ktime_t timeout,
-				  enum evl_tmode timeout_mode);
-
-struct evl_thread *evl_wake_up_syn(struct evl_syn *synch);
-
-int evl_wake_up_nr_syn(struct evl_syn *synch, int nr);
-
-void evl_wake_up_targeted_syn(struct evl_syn *synch,
-			      struct evl_thread *waiter);
-
-int __must_check evl_acquire_syn(struct evl_syn *synch,
-				 ktime_t timeout,
-				 enum evl_tmode timeout_mode);
-
-int __must_check evl_try_acquire_syn(struct evl_syn *synch);
-
-bool evl_release_syn(struct evl_syn *synch);
-
-bool evl_flush_syn(struct evl_syn *synch, int reason);
-
-void evl_requeue_syn_waiter(struct evl_thread *thread);
-
-void evl_forget_syn_waiter(struct evl_thread *thread);
-
-void evl_commit_syn_ceiling(struct evl_syn *synch,
-			    struct evl_thread *curr);
-
-#endif /* !_EVENLESS_SYN_H_ */
diff --git a/include/evenless/thread.h b/include/evenless/thread.h
index b5a79270b7b..77d293d8306 100644
--- a/include/evenless/thread.h
+++ b/include/evenless/thread.h
@@ -10,18 +10,15 @@
 #define _EVENLESS_THREAD_H
 
 #include <linux/types.h>
-#include <linux/wait.h>
-#include <linux/mutex.h>
-#include <linux/sched.h>
 #include <linux/dovetail.h>
-#include <linux/sched/rt.h>
 #include <linux/completion.h>
 #include <linux/irq_work.h>
+#include <linux/atomic.h>
+#include <linux/spinlock.h>
 #include <evenless/list.h>
 #include <evenless/stat.h>
 #include <evenless/timer.h>
 #include <evenless/sched/param.h>
-#include <evenless/synch.h>
 #include <evenless/factory.h>
 #include <uapi/evenless/thread.h>
 #include <uapi/evenless/signal.h>
@@ -43,6 +40,12 @@ struct evl_init_thread_attr {
 	union evl_sched_param sched_param;
 };
 
+struct evl_wait_channel {
+	void (*abort_wait)(struct evl_thread *thread);
+	void (*reorder_wait)(struct evl_thread *thread);
+	hard_spinlock_t lock;
+};
+
 struct evl_thread {
 	struct evl_element element;
 
@@ -72,27 +75,26 @@ struct evl_thread {
 	int wprio;
 
 	struct list_head rq_next;	/* evl_rq->policy.runqueue */
-	struct list_head syn_next;	/* evl_syn->wait_list */
-	struct list_head next;	/* evl_thread_list */
+	struct list_head wait_next;	/* in wchan's wait_list */
+	struct list_head next;		/* evl_thread_list */
 
 	/*
-	 * List of evl_syn owned by this thread causing a priority
+	 * List of mutexes owned by this thread causing a priority
 	 * boost due to one of the following reasons:
 	 *
 	 * - they are currently claimed by other thread(s) when
-	 * enforcing the priority inheritance protocol (EVL_SYN_PI).
+	 * enforcing the priority inheritance protocol (EVL_MUTEX_PI).
 	 *
-	 * - they require immediate priority ceiling (EVL_SYN_PP).
+	 * - they require immediate priority ceiling (EVL_MUTEX_PP).
 	 *
 	 * This list is ordered by decreasing (weighted) thread
 	 * priorities.
 	 */
 	struct list_head boosters;
 
-	struct evl_syn *wchan;		/* Resource the thread pends on */
-	struct evl_syn *wwake;		/* Wait channel the thread was resumed from */
-
-	int res_count;			/* Held resources count */
+	struct evl_wait_channel *wchan;		/* Wchan the thread pends on */
+	struct evl_wait_channel *wwake;		/* Wchan the thread was resumed from */
+	atomic_t inband_disable_count;
 
 	struct evl_timer rtimer;		/* Resource timer */
 	struct evl_timer ptimer;		/* Periodic timer */
@@ -138,7 +140,7 @@ struct evl_kthread {
 #define for_each_evl_booster(__pos, __thread)			\
 	list_for_each_entry(__pos, &(__thread)->boosters, next)
 
-#define for_each_evl_booster_safe(__pos, __tmp, __thread)	\
+#define for_each_evl_booster_safe(__pos, __tmp, __thread)		\
 	list_for_each_entry_safe(__pos, __tmp, &(__thread)->boosters, next)
 
 static inline void evl_sync_uwindow(struct evl_thread *curr)
@@ -206,43 +208,44 @@ ktime_t evl_get_thread_timeout(struct evl_thread *thread);
 ktime_t evl_get_thread_period(struct evl_thread *thread);
 
 int evl_init_thread(struct evl_thread *thread,
-		    const struct evl_init_thread_attr *attr,
-		    struct evl_rq *rq,
-		    const char *fmt, ...);
+		const struct evl_init_thread_attr *attr,
+		struct evl_rq *rq,
+		const char *fmt, ...);
 
 void evl_start_thread(struct evl_thread *thread);
 
 void evl_block_thread_timeout(struct evl_thread *thread, int mask,
-			      ktime_t timeout,
-			      enum evl_tmode timeout_mode,
-			      struct evl_clock *clock,
-			      struct evl_syn *wchan);
+			ktime_t timeout,
+			enum evl_tmode timeout_mode,
+			struct evl_clock *clock,
+			struct evl_wait_channel *wchan);
 
 void evl_resume_thread(struct evl_thread *thread,
-		       int mask);
+		int mask);
 
 int evl_unblock_thread(struct evl_thread *thread);
 
 void evl_block_thread(struct evl_thread *thread,
-		      int mask);
+		int mask);
 
 ktime_t evl_delay_thread(ktime_t timeout,
-			 enum evl_tmode timeout_mode,
-			 struct evl_clock *clock);
+			enum evl_tmode timeout_mode,
+			struct evl_clock *clock);
 
 int evl_sleep_until(ktime_t timeout);
 
 int evl_sleep(ktime_t delay);
 
 int evl_set_thread_period(struct evl_clock *clock,
-			  ktime_t idate, /* abs */
-			  ktime_t period);
+			ktime_t idate, /* abs */
+			ktime_t period);
 
 int evl_wait_thread_period(unsigned long *overruns_r);
 
 void evl_cancel_thread(struct evl_thread *thread);
 
-int evl_join_thread(struct evl_thread *thread, bool uninterruptible);
+int evl_join_thread(struct evl_thread *thread,
+		bool uninterruptible);
 
 int evl_switch_oob(void);
 
@@ -259,7 +262,7 @@ void __evl_demote_thread(struct evl_thread *thread);
 void evl_demote_thread(struct evl_thread *thread);
 
 void evl_signal_thread(struct evl_thread *thread,
-		       int sig, int arg);
+		int sig, int arg);
 
 void evl_call_mayday(struct evl_thread *thread, int reason);
 
@@ -268,7 +271,7 @@ void evl_migrate_thread(struct evl_thread *thread,
 			struct evl_rq *rq);
 #else
 static inline void evl_migrate_thread(struct evl_thread *thread,
-				      struct evl_rq *rq)
+				struct evl_rq *rq)
 { }
 #endif
 
@@ -277,8 +280,8 @@ int __evl_set_thread_schedparam(struct evl_thread *thread,
 				const union evl_sched_param *sched_param);
 
 int evl_set_thread_schedparam(struct evl_thread *thread,
-			      struct evl_sched_class *sched_class,
-			      const union evl_sched_param *sched_param);
+			struct evl_sched_class *sched_class,
+			const union evl_sched_param *sched_param);
 
 int evl_killall(int mask);
 
@@ -293,7 +296,7 @@ static inline void evl_propagate_schedparam_change(struct evl_thread *curr)
 int __evl_run_kthread(struct evl_kthread *kthread);
 
 #define _evl_run_kthread(__kthread, __affinity, __fn, __priority,	\
-			 __fmt, __args...)				\
+			__fmt, __args...)				\
 	({								\
 		int __ret;						\
 		struct evl_init_thread_attr __iattr = {			\
@@ -315,12 +318,12 @@ int __evl_run_kthread(struct evl_kthread *kthread);
 #define evl_run_kthread(__kthread, __fn, __priority,			\
 			__fmt, __args...)				\
 	_evl_run_kthread(__kthread, CPU_MASK_ALL, __fn, __priority,	\
-			 __fmt, ##__args)
+			__fmt, ##__args)
 
 #define evl_run_kthread_on_cpu(__kthread, __cpu, __fn, __priority,	\
-			       __fmt, __args...)			\
+			__fmt, __args...)				\
 	_evl_run_kthread(__kthread, *cpumask_of(__cpu), __fn, __priority, \
-			 __fmt, ##__args)
+			__fmt, ##__args)
 
 static inline void evl_cancel_kthread(struct evl_kthread *kthread)
 {
@@ -334,7 +337,7 @@ static inline int evl_kthread_should_stop(void)
 }
 
 void evl_set_kthread_priority(struct evl_kthread *thread,
-			      int priority);
+			int priority);
 
 int evl_unblock_kthread(struct evl_kthread *thread);
 
diff --git a/include/evenless/tick.h b/include/evenless/tick.h
index c485ca2da9e..db529b4542d 100644
--- a/include/evenless/tick.h
+++ b/include/evenless/tick.h
@@ -24,7 +24,7 @@ static inline void evl_program_local_tick(struct evl_clock *clock)
 }
 
 static inline void evl_program_remote_tick(struct evl_clock *clock,
-					   struct evl_rq *rq)
+					struct evl_rq *rq)
 {
 #ifdef CONFIG_SMP
 	struct evl_clock *master = clock->master;
diff --git a/include/evenless/timer.h b/include/evenless/timer.h
index 87a3fd7167a..9fe240c80fe 100644
--- a/include/evenless/timer.h
+++ b/include/evenless/timer.h
@@ -100,7 +100,7 @@ struct evl_tnode *evl_get_tqueue_head(struct evl_tqueue *tq)
 
 static inline
 struct evl_tnode *evl_get_tqueue_next(struct evl_tqueue *tq,
-				      struct evl_tnode *node)
+				struct evl_tnode *node)
 {
 	struct rb_node *_node = rb_next(&node->rb);
 	return _node ? container_of(_node, struct evl_tnode, rb) : NULL;
@@ -120,7 +120,7 @@ void evl_remove_tnode(struct evl_tqueue *tq, struct evl_tnode *node)
 	     (__node) = evl_get_tqueue_next(__tq, __node))
 
 void evl_insert_tnode(struct evl_tqueue *tq,
-		      struct evl_tnode *node);
+		struct evl_tnode *node);
 
 struct evl_rq;
 
@@ -179,8 +179,8 @@ struct evl_timer {
 #define evl_tdate(__timer)	((__timer)->node.date)
 
 void evl_start_timer(struct evl_timer *timer,
-		     ktime_t value,
-		     ktime_t interval);
+		ktime_t value,
+		ktime_t interval);
 
 void __evl_stop_timer(struct evl_timer *timer);
 
@@ -203,7 +203,7 @@ static inline void evl_stop_timer(struct evl_timer *timer)
 void evl_destroy_timer(struct evl_timer *timer);
 
 static inline ktime_t evl_abs_timeout(struct evl_timer *timer,
-				      ktime_t delta)
+				ktime_t delta)
 {
 	return ktime_add(evl_read_clock(timer->clock), delta);
 }
@@ -246,7 +246,7 @@ static inline
 ktime_t evl_get_timer_next_date(struct evl_timer *timer)
 {
 	return ktime_add_ns(timer->start_date,
-			    timer->pexpect_ticks * ktime_to_ns(timer->interval));
+			timer->pexpect_ticks * ktime_to_ns(timer->interval));
 }
 
 static inline
@@ -256,13 +256,13 @@ void evl_set_timer_priority(struct evl_timer *timer, int prio)
 }
 
 void __evl_init_timer(struct evl_timer *timer,
-		      struct evl_clock *clock,
-		      void (*handler)(struct evl_timer *timer),
-		      struct evl_rq *rq,
-		      int flags);
+		struct evl_clock *clock,
+		void (*handler)(struct evl_timer *timer),
+		struct evl_rq *rq,
+		int flags);
 
 void evl_set_timer_gravity(struct evl_timer *timer,
-			   int gravity);
+			int gravity);
 
 #ifdef CONFIG_EVENLESS_STATS
 
@@ -317,13 +317,13 @@ void evl_set_timer_name(struct evl_timer *timer, const char *name) { }
 
 #define evl_init_core_timer(__timer, __handler)				\
 	evl_init_timer(__timer, &evl_mono_clock, __handler, NULL,	\
-		       EVL_TIMER_IGRAVITY)
+		EVL_TIMER_IGRAVITY)
 
 #define evl_init_timer_on_cpu(__timer, __cpu, __handler)		\
 	do {								\
 		struct evl_rq *__rq = evl_cpu_rq(__cpu);		\
 		evl_init_timer(__timer, &evl_mono_clock, __handler,	\
-			       __rq, EVL_TIMER_IGRAVITY);		\
+			__rq, EVL_TIMER_IGRAVITY);			\
 	} while (0)
 
 bool evl_timer_deactivate(struct evl_timer *timer);
@@ -333,7 +333,7 @@ static inline ktime_t evl_get_timer_expiry(struct evl_timer *timer)
 {
 	/* Ideal expiry date without anticipation (no gravity) */
 	return ktime_add(evl_tdate(timer),
-			 evl_get_timer_gravity(timer));
+			evl_get_timer_gravity(timer));
 }
 
 static inline
@@ -378,7 +378,7 @@ ktime_t evl_get_stopped_timer_delta(struct evl_timer *timer)
 }
 
 static inline void evl_dequeue_timer(struct evl_timer *timer,
-				     struct evl_tqueue *tq)
+				struct evl_tqueue *tq)
 {
 	evl_remove_tnode(tq, &timer->node);
 	timer->status |= EVL_TIMER_DEQUEUED;
@@ -387,7 +387,7 @@ static inline void evl_dequeue_timer(struct evl_timer *timer,
 /* timer base locked. */
 static inline
 void evl_enqueue_timer(struct evl_timer *timer,
-		       struct evl_tqueue *tq)
+		struct evl_tqueue *tq)
 {
 	evl_insert_tnode(tq, &timer->node);
 	timer->status &= ~EVL_TIMER_DEQUEUED;
@@ -395,13 +395,13 @@ void evl_enqueue_timer(struct evl_timer *timer,
 }
 
 void evl_enqueue_timer(struct evl_timer *timer,
-		       struct evl_tqueue *tq);
+		struct evl_tqueue *tq);
 
 unsigned long evl_get_timer_overruns(struct evl_timer *timer);
 
 void evl_bolt_timer(struct evl_timer *timer,
-		    struct evl_clock *clock,
-		    struct evl_rq *rq);
+		struct evl_clock *clock,
+		struct evl_rq *rq);
 
 #ifdef CONFIG_SMP
 
@@ -410,15 +410,15 @@ void __evl_set_timer_rq(struct evl_timer *timer,
 			struct evl_rq *rq);
 
 static inline void evl_set_timer_rq(struct evl_timer *timer,
-				    struct evl_rq *rq)
+				struct evl_rq *rq)
 {
 	if (rq != timer->rq)
 		__evl_set_timer_rq(timer, timer->clock, rq);
 }
 
 static inline void evl_prepare_timer_wait(struct evl_timer *timer,
-					  struct evl_clock *clock,
-					  struct evl_rq *rq)
+					struct evl_clock *clock,
+					struct evl_rq *rq)
 {
 	/* We may change the reference clock before waiting. */
 	if (rq != timer->rq || clock != timer->clock)
@@ -426,7 +426,7 @@ static inline void evl_prepare_timer_wait(struct evl_timer *timer,
 }
 
 static inline bool evl_timer_on_rq(struct evl_timer *timer,
-				   struct evl_rq *rq)
+				struct evl_rq *rq)
 {
 	return timer->rq == rq;
 }
@@ -434,19 +434,19 @@ static inline bool evl_timer_on_rq(struct evl_timer *timer,
 #else /* ! CONFIG_SMP */
 
 static inline void evl_set_timer_rq(struct evl_timer *timer,
-				    struct evl_rq *rq)
+				struct evl_rq *rq)
 { }
 
 static inline void evl_prepare_timer_wait(struct evl_timer *timer,
-					  struct evl_clock *clock,
-					  struct evl_rq *rq)
+					struct evl_clock *clock,
+					struct evl_rq *rq)
 {
 	if (clock != timer->clock)
 		evl_bolt_timer(timer, clock, rq);
 }
 
 static inline bool evl_timer_on_rq(struct evl_timer *timer,
-				   struct evl_rq *rq)
+				struct evl_rq *rq)
 {
 	return true;
 }
diff --git a/include/evenless/wait.h b/include/evenless/wait.h
index 328e5800661..f4d9dabe6b2 100644
--- a/include/evenless/wait.h
+++ b/include/evenless/wait.h
@@ -1,123 +1,92 @@
 /*
  * SPDX-License-Identifier: GPL-2.0
  *
- * Copyright (C) 2017 Philippe Gerum  <rpm@xenomai.org>
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2001, 2018 Philippe Gerum  <rpm@xenomai.org>
  */
 
 #ifndef _EVENLESS_WAIT_H
 #define _EVENLESS_WAIT_H
 
+#include <linux/types.h>
 #include <linux/spinlock.h>
-#include <evenless/synch.h>
-#include <evenless/sched.h>
+#include <evenless/list.h>
+#include <evenless/timer.h>
+#include <evenless/clock.h>
+#include <evenless/thread.h>
+#include <uapi/evenless/thread.h>
 
-/*
- * FIXME: general rework pending. Maybe merge that with synch.h. as
- * evenless/wait.h, provided the superlock is gone and synch.c
- * serializes with a local per-synch lock, which would allow to turn
- * evl_syn into evl_wait_queue.
- */
+#define EVL_WAIT_FIFO    0
+#define EVL_WAIT_PRIO    BIT(0)
 
 struct evl_wait_queue {
-	struct evl_syn wait;
-	hard_spinlock_t lock;
+	int flags;
+	struct list_head wait_list;
+	struct evl_clock *clock;
+	struct evl_wait_channel wchan;
 };
 
-#define EVL_WAIT_INITIALIZER(__name) {	\
-		.wait = EVL_SYN_INITIALIZER((__name).wait, EVL_SYN_PRIO), \
-		.lock = __HARD_SPIN_LOCK_INITIALIZER((__name).lock),	  \
+#define EVL_WAIT_INITIALIZER(__name) {					\
+		.flags = EVL_WAIT_PRIO,					\
+		.wait_list = LIST_HEAD_INIT((__name).wait_list), 	\
+		.clock = &evl_mono_clock,				\
+		.wchan = {						\
+			.abort_wait = evl_abort_wait,			\
+			.reorder_wait = evl_reorder_wait,		\
+			.lock = __HARD_SPIN_LOCK_INITIALIZER((__name).wchan.lock), \
+		},							\
 	}
 
-#define DEFINE_EVL_WAIT(__name)	\
-	struct evl_wait_queue __name = EVL_WAIT_INITIALIZER(__name)
-
-#define DEFINE_EVL_WAIT_ONSTACK(__name)  DEFINE_EVL_WAIT(__name)
-
-static inline void evl_init_wait(struct evl_wait_queue *wq)
-{
-	*wq = (struct evl_wait_queue)EVL_WAIT_INITIALIZER(*wq);
-}
-
-static inline void evl_destroy_wait(struct evl_wait_queue *wq)
-{
-	evl_destroy_syn(&wq->wait);
-}
+#define evl_head_waiter(__wq)				\
+	list_first_entry_or_null(&(__wq)->wait_list,	\
+				struct evl_thread, wait_next)
 
-struct evl_wait_flag {
-	struct evl_wait_queue wq;
-	bool signaled;
-};
+#define evl_for_each_waiter(__pos, __wq)	\
+	list_for_each_entry(__pos, &(__wq)->wait_list, wait_next)
 
-#define DEFINE_EVL_WAIT_FLAG(__name)					\
-	struct evl_wait_flag __name = {					\
-		.wq = EVL_WAIT_INITIALIZER((__name).wq),		\
-		.signaled = false,					\
-	}
+#define evl_for_each_waiter_safe(__pos, __tmp, __wq)		\
+	list_for_each_entry_safe(__pos, __tmp, &(__wq)->wait_list, wait_next)
 
-static inline void evl_init_flag(struct evl_wait_flag *wf)
+static inline bool evl_wait_active(struct evl_wait_queue *wq)
 {
-	evl_init_wait(&wf->wq);
-	wf->signaled = false;
+	return !list_empty(&wq->wait_list);
 }
 
-static inline void evl_destroy_flag(struct evl_wait_flag *wf)
+static inline
+struct evl_thread *evl_wait_head(struct evl_wait_queue *wq)
 {
-	evl_destroy_wait(&wf->wq);
+	return list_first_entry_or_null(&wq->wait_list,
+					struct evl_thread, wait_next);
 }
 
-static inline int evl_wait_flag_timeout(struct evl_wait_flag *wf,
-					ktime_t timeout, enum evl_tmode timeout_mode)
-{
-	unsigned long flags;
-	int ret = 0;
-
-	xnlock_get_irqsave(&nklock, flags);
-
-	while (!wf->signaled) {
-		ret = evl_sleep_on_syn(&wf->wq.wait, timeout, timeout_mode);
-		if (ret & T_BREAK)
-			ret = -EINTR;
-		if (ret & T_TIMEO)
-			ret = -ETIMEDOUT;
-		if (ret & T_RMID)
-			ret = -EIDRM;
-		if (ret)
-			break;
-	}
+void evl_init_wait(struct evl_wait_queue *wq,
+		struct evl_clock *clock,
+		int flags);
 
-	if (ret == 0)
-		wf->signaled = false;
+void evl_destroy_wait(struct evl_wait_queue *wq);
 
-	xnlock_put_irqrestore(&nklock, flags);
-
-	return ret;
-}
+int __must_check evl_wait_timeout(struct evl_wait_queue *wq,
+				ktime_t timeout,
+				enum evl_tmode timeout_mode);
 
-static inline int evl_wait_flag(struct evl_wait_flag *wf)
+static inline int evl_wait(struct evl_wait_queue *wq)
 {
-	return evl_wait_flag_timeout(wf, EVL_INFINITE, EVL_REL);
+	return evl_wait_timeout(wq, EVL_INFINITE, EVL_REL);
 }
 
-static inline			/* nklock held. */
-struct evl_thread *evl_wait_flag_head(struct evl_wait_flag *wf)
-{
-	return evl_syn_wait_head(&wf->wq.wait);
-}
+struct evl_thread *evl_wake_up(struct evl_wait_queue *wq,
+			struct evl_thread *waiter);
 
-static inline bool evl_raise_flag(struct evl_wait_flag *wf)
+static inline
+struct evl_thread *evl_wake_up_head(struct evl_wait_queue *wq)
 {
-	struct evl_thread *waiter;
-	unsigned long flags;
-
-	xnlock_get_irqsave(&nklock, flags);
+	return evl_wake_up(wq, NULL);
+}
 
-	wf->signaled = true;
-	waiter = evl_wake_up_syn(&wf->wq.wait);
-	evl_schedule();
+void evl_flush_wait(struct evl_wait_queue *wq, int reason);
 
-	xnlock_put_irqrestore(&nklock, flags);
+void evl_abort_wait(struct evl_thread *thread);
 
-	return waiter != NULL;
-}
+void evl_reorder_wait(struct evl_thread *thread);
 
-#endif /* _EVENLESS_WAIT_H */
+#endif /* !_EVENLESS_WAIT_H_ */
diff --git a/include/evenless/xbuf.h b/include/evenless/xbuf.h
index 776f917eada..2097508ff28 100644
--- a/include/evenless/xbuf.h
+++ b/include/evenless/xbuf.h
@@ -13,16 +13,16 @@ struct evl_file;
 struct evl_xbuf;
 
 struct evl_xbuf *evl_get_xbuf(int efd,
-			      struct evl_file **sfilpp);
+			struct evl_file **sfilpp);
 
 void evl_put_xbuf(struct evl_file *sfilp);
 
 ssize_t evl_read_xbuf(struct evl_xbuf *xbuf,
-		      void *buf, size_t count,
-		      int f_flags);
+		void *buf, size_t count,
+		int f_flags);
 
 ssize_t evl_write_xbuf(struct evl_xbuf *xbuf,
-		       const void *buf, size_t count,
-		       int f_flags);
+		const void *buf, size_t count,
+		int f_flags);
 
 #endif /* !_EVENLESS_XBUF_H */
diff --git a/include/trace/events/evenless.h b/include/trace/events/evenless.h
index e0a3cf40b6a..da4bfea6dda 100644
--- a/include/trace/events/evenless.h
+++ b/include/trace/events/evenless.h
@@ -23,7 +23,9 @@ struct evl_rq;
 struct evl_thread;
 struct evl_sched_attrs;
 struct evl_init_thread_attr;
-struct evl_syn;
+struct evl_wait_channel;
+struct evl_wait_queue;
+struct evl_mutex;
 struct evl_clock;
 
 DECLARE_EVENT_CLASS(thread_event,
@@ -65,34 +67,34 @@ DECLARE_EVENT_CLASS(curr_thread_event,
 		  __entry->state, __entry->info)
 );
 
-DECLARE_EVENT_CLASS(synch_wait_event,
-	TP_PROTO(struct evl_syn *synch),
-	TP_ARGS(synch),
+DECLARE_EVENT_CLASS(wq_event,
+	TP_PROTO(struct evl_wait_queue *wq),
+	TP_ARGS(wq),
 
 	TP_STRUCT__entry(
-		__field(struct evl_syn *, synch)
+		__field(struct evl_wait_queue *, wq)
 	),
 
 	TP_fast_assign(
-		__entry->synch = synch;
+		__entry->wq = wq;
 	),
 
-	TP_printk("synch=%p", __entry->synch)
+	TP_printk("wq=%p", __entry->wq)
 );
 
-DECLARE_EVENT_CLASS(synch_post_event,
-	TP_PROTO(struct evl_syn *synch),
-	TP_ARGS(synch),
+DECLARE_EVENT_CLASS(mutex_event,
+	TP_PROTO(struct evl_mutex *mutex),
+	TP_ARGS(mutex),
 
 	TP_STRUCT__entry(
-		__field(struct evl_syn *, synch)
+		__field(struct evl_mutex *, mutex)
 	),
 
 	TP_fast_assign(
-		__entry->synch = synch;
+		__entry->mutex = mutex;
 	),
 
-	TP_printk("synch=%p", __entry->synch)
+	TP_printk("mutex=%p", __entry->mutex)
 );
 
 DECLARE_EVENT_CLASS(timer_event,
@@ -308,7 +310,7 @@ TRACE_EVENT(evl_init_thread,
 TRACE_EVENT(evl_block_thread,
 	TP_PROTO(struct evl_thread *thread, unsigned long mask, ktime_t timeout,
 		 enum evl_tmode timeout_mode, struct evl_clock *clock,
-		 struct evl_syn *wchan),
+		 struct evl_wait_channel *wchan),
 	TP_ARGS(thread, mask, timeout, timeout_mode, clock, wchan),
 
 	TP_STRUCT__entry(
@@ -316,7 +318,7 @@ TRACE_EVENT(evl_block_thread,
 		__field(unsigned long, mask)
 		__field(ktime_t, timeout)
 		__field(enum evl_tmode, timeout_mode)
-		__field(struct evl_syn *, wchan)
+		__field(struct evl_wait_channel *, wchan)
 		__string(clock_name, clock ? clock->name : "none")
 	),
 
@@ -648,44 +650,44 @@ TRACE_EVENT(evl_timer_shot,
 		  __entry->nsecs / 1000, div_s64(__entry->delta, 1000))
 );
 
-DEFINE_EVENT(synch_wait_event, evl_synch_sleepon,
-	TP_PROTO(struct evl_syn *synch),
-	TP_ARGS(synch)
+DEFINE_EVENT(wq_event, evl_wait,
+	TP_PROTO(struct evl_wait_queue *wq),
+	TP_ARGS(wq)
 );
 
-DEFINE_EVENT(synch_wait_event, evl_synch_try_acquire,
-	TP_PROTO(struct evl_syn *synch),
-	TP_ARGS(synch)
+DEFINE_EVENT(wq_event, evl_wait_wakeup,
+	TP_PROTO(struct evl_wait_queue *wq),
+	TP_ARGS(wq)
 );
 
-DEFINE_EVENT(synch_wait_event, evl_synch_acquire,
-	TP_PROTO(struct evl_syn *synch),
-	TP_ARGS(synch)
+DEFINE_EVENT(wq_event, evl_wait_flush,
+	TP_PROTO(struct evl_wait_queue *wq),
+	TP_ARGS(wq)
 );
 
-DEFINE_EVENT(synch_post_event, evl_synch_release,
-	TP_PROTO(struct evl_syn *synch),
-	TP_ARGS(synch)
+DEFINE_EVENT(mutex_event, evl_mutex_trylock,
+	TP_PROTO(struct evl_mutex *mutex),
+	TP_ARGS(mutex)
 );
 
-DEFINE_EVENT(synch_post_event, evl_synch_wakeup,
-	TP_PROTO(struct evl_syn *synch),
-	TP_ARGS(synch)
+DEFINE_EVENT(mutex_event, evl_mutex_lock,
+	TP_PROTO(struct evl_mutex *mutex),
+	TP_ARGS(mutex)
 );
 
-DEFINE_EVENT(synch_post_event, evl_synch_wakeup_many,
-	TP_PROTO(struct evl_syn *synch),
-	TP_ARGS(synch)
+DEFINE_EVENT(mutex_event, evl_mutex_unlock,
+	TP_PROTO(struct evl_mutex *mutex),
+	TP_ARGS(mutex)
 );
 
-DEFINE_EVENT(synch_post_event, evl_synch_flush,
-	TP_PROTO(struct evl_syn *synch),
-	TP_ARGS(synch)
+DEFINE_EVENT(mutex_event, evl_mutex_destroy,
+	TP_PROTO(struct evl_mutex *mutex),
+	TP_ARGS(mutex)
 );
 
-DEFINE_EVENT(synch_post_event, evl_synch_forget,
-	TP_PROTO(struct evl_syn *synch),
-	TP_ARGS(synch)
+DEFINE_EVENT(mutex_event, evl_mutex_flush,
+	TP_PROTO(struct evl_mutex *mutex),
+	TP_ARGS(mutex)
 );
 
 #define __timespec_fields(__name)				\
diff --git a/include/uapi/evenless/mutex.h b/include/uapi/evenless/mutex.h
new file mode 100644
index 00000000000..0944647f406
--- /dev/null
+++ b/include/uapi/evenless/mutex.h
@@ -0,0 +1,43 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Based on the Xenomai Cobalt core:
+ * Copyright (C) 2001, 2013, 2018 Philippe Gerum <rpm@xenomai.org>
+ * Copyright (C) 2008, 2009 Jan Kiszka <jan.kiszka@siemens.com>.
+ */
+
+#ifndef _EVENLESS_UAPI_MUTEX_H
+#define _EVENLESS_UAPI_MUTEX_H
+
+#include <uapi/evenless/types.h>
+
+static inline int
+evl_is_mutex_owner(atomic_t *fastlock, fundle_t ownerh)
+{
+	return evl_get_index(atomic_read(fastlock)) == ownerh;
+}
+
+static inline
+int evl_fast_lock_mutex(atomic_t *fastlock, fundle_t new_ownerh)
+{
+	fundle_t h;
+
+	h = atomic_cmpxchg(fastlock, EVL_NO_HANDLE, new_ownerh);
+	if (h != EVL_NO_HANDLE) {
+		if (evl_get_index(h) == new_ownerh)
+			return -EBUSY;
+
+		return -EAGAIN;
+	}
+
+	return 0;
+}
+
+static inline
+int evl_fast_unlock_mutex(atomic_t *fastlock, fundle_t cur_ownerh)
+{
+	return atomic_cmpxchg(fastlock, cur_ownerh, EVL_NO_HANDLE)
+		== cur_ownerh;
+}
+
+#endif /* !_EVENLESS_UAPI_MUTEX_H */
diff --git a/include/uapi/evenless/sem.h b/include/uapi/evenless/sem.h
index a934db71078..bcfda6bd2bc 100644
--- a/include/uapi/evenless/sem.h
+++ b/include/uapi/evenless/sem.h
@@ -7,7 +7,6 @@
 #ifndef _EVENLESS_UAPI_SEM_H
 #define _EVENLESS_UAPI_SEM_H
 
-#include <uapi/evenless/synch.h>
 #include <uapi/evenless/factory.h>
 
 struct timespec;
diff --git a/include/uapi/evenless/signal.h b/include/uapi/evenless/signal.h
index c7aa01c1265..8d66f64c98c 100644
--- a/include/uapi/evenless/signal.h
+++ b/include/uapi/evenless/signal.h
@@ -31,7 +31,7 @@
 #define SIGDEBUG_MIGRATE_FAULT		3
 #define SIGDEBUG_MIGRATE_PRIOINV	4
 #define SIGDEBUG_WATCHDOG		5
-#define SIGDEBUG_RESCNT_IMBALANCE	6
-#define SIGDEBUG_MONITOR_SLEEP		7
+#define SIGDEBUG_MUTEX_IMBALANCE	6
+#define SIGDEBUG_MUTEX_SLEEP		7
 
 #endif /* !_EVENLESS_UAPI_SIGNAL_H */
diff --git a/include/uapi/evenless/synch.h b/include/uapi/evenless/synch.h
deleted file mode 100644
index bebc076bf31..00000000000
--- a/include/uapi/evenless/synch.h
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
- *
- * Based on the Xenomai Cobalt core:
- * Copyright (C) 2001, 2013, 2018 Philippe Gerum <rpm@xenomai.org>
- * Copyright (C) 2008, 2009 Jan Kiszka <jan.kiszka@siemens.com>.
- */
-
-#ifndef _EVENLESS_UAPI_SYNCH_H
-#define _EVENLESS_UAPI_SYNCH_H
-
-#include <uapi/evenless/types.h>
-
-/* Type flags */
-#define EVL_SYN_FIFO    0x0
-#define EVL_SYN_PRIO    0x1
-#define EVL_SYN_PI      0x2
-#define EVL_SYN_OWNER   0x4
-#define EVL_SYN_PP      0x8
-
-static inline int evl_fast_syn_is_claimed(fundle_t handle)
-{
-	return (handle & EVL_SYN_FLCLAIM) != 0;
-}
-
-static inline fundle_t evl_syn_fast_claim(fundle_t handle)
-{
-	return handle | EVL_SYN_FLCLAIM;
-}
-
-static inline fundle_t evl_syn_fast_ceil(fundle_t handle)
-{
-	return handle | EVL_SYN_FLCEIL;
-}
-
-static inline int
-evl_is_syn_owner(atomic_t *fastlock, fundle_t ownerh)
-{
-	return evl_get_index(atomic_read(fastlock)) == ownerh;
-}
-
-static inline
-int evl_fast_acquire_syn(atomic_t *fastlock, fundle_t new_ownerh)
-{
-	fundle_t h;
-
-	h = atomic_cmpxchg(fastlock, EVL_NO_HANDLE, new_ownerh);
-	if (h != EVL_NO_HANDLE) {
-		if (evl_get_index(h) == new_ownerh)
-			return -EBUSY;
-
-		return -EAGAIN;
-	}
-
-	return 0;
-}
-
-static inline
-int evl_fast_release_syn(atomic_t *fastlock, fundle_t cur_ownerh)
-{
-	return atomic_cmpxchg(fastlock, cur_ownerh, EVL_NO_HANDLE)
-		== cur_ownerh;
-}
-
-#endif /* !_EVENLESS_UAPI_SYNCH_H */
diff --git a/include/uapi/evenless/types.h b/include/uapi/evenless/types.h
index 9e8b9538905..26a02ff9dff 100644
--- a/include/uapi/evenless/types.h
+++ b/include/uapi/evenless/types.h
@@ -13,9 +13,9 @@ typedef __u32 fundle_t;
 #define EVL_NO_HANDLE		((fundle_t)0x00000000)
 
 /* Reserved status bits */
-#define EVL_SYN_FLCLAIM		((fundle_t)0x80000000) /* Contended. */
-#define EVL_SYN_FLCEIL		((fundle_t)0x40000000) /* Ceiling active. */
-#define EVL_HANDLE_INDEX_MASK	(EVL_SYN_FLCLAIM|EVL_SYN_FLCEIL)
+#define EVL_MUTEX_FLCLAIM	((fundle_t)0x80000000) /* Contended. */
+#define EVL_MUTEX_FLCEIL	((fundle_t)0x40000000) /* Ceiling active. */
+#define EVL_HANDLE_INDEX_MASK	(EVL_MUTEX_FLCLAIM|EVL_MUTEX_FLCEIL)
 
 /*
  * Strip all reserved bits from the handle, only retaining the fast
diff --git a/kernel/evenless/Kconfig b/kernel/evenless/Kconfig
index c950d6945d3..149e0e485b3 100644
--- a/kernel/evenless/Kconfig
+++ b/kernel/evenless/Kconfig
@@ -254,24 +254,24 @@ config EVENLESS_DEBUG_USER
 
 if EVENLESS_DEBUG_USER
 
-config EVENLESS_DEBUG_MONITOR_INBAND
-       bool "Detect in-band monitor owner"
+config EVENLESS_DEBUG_MUTEX_INBAND
+       bool "Detect in-band mutex owner"
        default y
        help
-         A thread which attempts to enter a monitor currently owned by
-         another thread running in-band will suffer unwanted latencies,
-         due to a priority inversion.  This switch enables debug
-         notifications sending a SIGDEBUG signal to the monitor owner.
+         A thread which attempts to acquire a mutex currently locked by
+         another thread running in-band may experience unwanted latency
+         due to priority inversion.  This switch enables debug
+         notifications sending a SIGDEBUG signal to the lock owner.
 
 	 This option may add overhead to out-of-band execution over
-	 contented monitors.
+	 contented locks.
 
-config EVENLESS_DEBUG_MONITOR_SLEEP
-       bool "Detect sleeping while holding a monitor"
+config EVENLESS_DEBUG_MUTEX_SLEEP
+       bool "Detect sleeping while holding a mutex"
        default y
        help
-         A thread which goes sleeping while holding a monitor is prone
-         to cause unwanted latencies to other threads serialized by
+         A thread which goes sleeping while holding a mutex is prone
+         to cause unwanted latency to other threads serialized by
          the same lock. If debug notifications are enabled for such
          thread, it receives a SIGDEBUG signal right before entering
 	 sleep.
diff --git a/kernel/evenless/Makefile b/kernel/evenless/Makefile
index b98e338fb63..6175d2a744e 100644
--- a/kernel/evenless/Makefile
+++ b/kernel/evenless/Makefile
@@ -18,12 +18,12 @@ evenless-y :=		\
 	mutex.o		\
 	poller.o	\
 	sem.o		\
-	synch.o		\
 	syscall.o	\
 	thread.o	\
 	tick.o		\
 	timer.o		\
 	timerfd.o	\
+	wait.o		\
 	xbuf.o
 
 evenless-$(CONFIG_FTRACE) +=	trace.o
diff --git a/kernel/evenless/clock.c b/kernel/evenless/clock.c
index 3bc1dc2d032..cd3e6156995 100644
--- a/kernel/evenless/clock.c
+++ b/kernel/evenless/clock.c
@@ -38,8 +38,8 @@ static DEFINE_MUTEX(clocklist_lock);
 
 /* timer base locked */
 static void adjust_timer(struct evl_clock *clock,
-			 struct evl_timer *timer, struct evl_tqueue *q,
-			 ktime_t delta)
+			struct evl_timer *timer, struct evl_tqueue *q,
+			ktime_t delta)
 {
 	ktime_t period, diff;
 	s64 div;
@@ -65,8 +65,8 @@ static void adjust_timer(struct evl_clock *clock,
 		timer->periodic_ticks += div;
 		evl_update_timer_date(timer);
 	} else if (ktime_to_ns(delta) < 0
-		   && (timer->status & EVL_TIMER_FIRED)
-		   && ktime_to_ns(ktime_add(diff, period)) <= 0) {
+		&& (timer->status & EVL_TIMER_FIRED)
+		&& ktime_to_ns(ktime_add(diff, period)) <= 0) {
 		/*
 		 * Timer is periodic and NOT waiting for its first
 		 * shot, so we make it tick sooner than its original
@@ -148,7 +148,7 @@ void inband_clock_was_set(void)
 }
 
 static int init_clock(struct evl_clock *clock,
-		      struct evl_clock *master)
+		struct evl_clock *master)
 {
 	int ret;
 
@@ -179,7 +179,7 @@ static int init_clock(struct evl_clock *clock,
 }
 
 int evl_init_clock(struct evl_clock *clock,
-		   const struct cpumask *affinity)
+		const struct cpumask *affinity)
 {
 	struct evl_timerbase *tmb;
 	int cpu, ret;
@@ -240,7 +240,7 @@ int evl_init_clock(struct evl_clock *clock,
 EXPORT_SYMBOL_GPL(evl_init_clock);
 
 int evl_init_slave_clock(struct evl_clock *clock,
-			 struct evl_clock *master)
+			struct evl_clock *master)
 {
 	inband_context_only();
 
@@ -265,9 +265,9 @@ static inline bool timer_needs_enqueuing(struct evl_timer *timer)
 	 */
 	return (timer->status &
 		(EVL_TIMER_PERIODIC|EVL_TIMER_DEQUEUED|
-		 EVL_TIMER_RUNNING|EVL_TIMER_KILLED))
+			EVL_TIMER_RUNNING|EVL_TIMER_KILLED))
 		== (EVL_TIMER_PERIODIC|EVL_TIMER_DEQUEUED|
-		    EVL_TIMER_RUNNING);
+			EVL_TIMER_RUNNING);
 }
 
 /* Announce a tick from a master clock. */
@@ -288,7 +288,7 @@ void evl_announce_tick(struct evl_clock *clock)
 	 * timers will be queued to CPU0.
 	 */
 	if (clock != &evl_mono_clock &&
-	    !cpumask_test_cpu(evl_rq_cpu(rq), &clock->affinity))
+		!cpumask_test_cpu(evl_rq_cpu(rq), &clock->affinity))
 		tmb = evl_percpu_timers(clock, 0);
 	else
 #endif
@@ -328,7 +328,7 @@ void evl_announce_tick(struct evl_clock *clock)
 		raw_spin_lock_irqsave(&tmb->lock, flags);
 
 		if (timer_needs_enqueuing(timer) &&
-		    evl_timer_on_rq(timer, rq))
+			evl_timer_on_rq(timer, rq))
 			evl_enqueue_timer(timer, tq);
 	}
 
@@ -367,7 +367,7 @@ void evl_stop_timers(struct evl_clock *clock)
 }
 
 int evl_register_clock(struct evl_clock *clock,
-		       const struct cpumask *affinity)
+		const struct cpumask *affinity)
 {
 	int ret;
 
@@ -413,7 +413,7 @@ static long restart_clock_delay(struct restart_block *param)
 }
 
 static int clock_delay(struct evl_clock *clock,
-		       struct evl_clock_delayreq __user *u_req)
+		struct evl_clock_delayreq __user *u_req)
 {
 	struct evl_thread *curr = evl_current_thread();
 	struct evl_clock_delayreq req;
@@ -440,7 +440,7 @@ static int clock_delay(struct evl_clock *clock,
 				rem = evl_get_stopped_timer_delta(&curr->rtimer);
 				remain = ktime_to_timespec(rem);
 				ret = raw_copy_to_user(req.remain, &remain,
-						       sizeof(remain));
+						sizeof(remain));
 				if (ret)
 					return -EFAULT;
 			}
@@ -478,7 +478,7 @@ static int get_clock_resolution(struct evl_clock *clock,
 }
 
 static int get_clock_time(struct evl_clock *clock,
-			  struct timespec __user *u_ts)
+			struct timespec __user *u_ts)
 {
 	struct timespec ts;
 
@@ -490,7 +490,7 @@ static int get_clock_time(struct evl_clock *clock,
 }
 
 static int set_clock_time(struct evl_clock *clock,
-			  struct timespec __user *u_ts)
+			struct timespec __user *u_ts)
 {
 	struct timespec ts;
 	int ret;
@@ -508,7 +508,7 @@ static int set_clock_time(struct evl_clock *clock,
 }
 
 static int adjust_clock_time(struct evl_clock *clock,
-			     struct timex __user *u_tx)
+			struct timex __user *u_tx)
 {
 	struct timex tx;
 	int ret;
@@ -521,22 +521,22 @@ static int adjust_clock_time(struct evl_clock *clock,
 }
 
 static long clock_common_ioctl(struct evl_clock *clock,
-			       unsigned int cmd, unsigned long arg)
+			unsigned int cmd, unsigned long arg)
 {
 	int ret;
 
 	switch (cmd) {
 	case EVL_CLKIOC_GET_RES:
 		ret = get_clock_resolution(clock,
-					   (struct timespec __user *)arg);
+					(struct timespec __user *)arg);
 		break;
 	case EVL_CLKIOC_GET_TIME:
 		ret = get_clock_time(clock,
-				     (struct timespec __user *)arg);
+				(struct timespec __user *)arg);
 		break;
 	case EVL_CLKIOC_SET_TIME:
 		ret = set_clock_time(clock,
-				     (struct timespec __user *)arg);
+				(struct timespec __user *)arg);
 		break;
 	case EVL_CLKIOC_ADJ_TIME:
 		ret = adjust_clock_time(clock,
@@ -550,7 +550,7 @@ static long clock_common_ioctl(struct evl_clock *clock,
 }
 
 static long clock_oob_ioctl(struct file *filp, unsigned int cmd,
-			    unsigned long arg)
+			unsigned long arg)
 {
 	struct evl_clock *clock = element_of(filp, struct evl_clock);
 	int ret;
@@ -558,7 +558,7 @@ static long clock_oob_ioctl(struct file *filp, unsigned int cmd,
 	switch (cmd) {
 	case EVL_CLKIOC_DELAY:
 		ret = clock_delay(clock,
-				  (struct evl_clock_delayreq __user *)arg);
+				(struct evl_clock_delayreq __user *)arg);
 		break;
 	default:
 		ret = clock_common_ioctl(clock, cmd, arg);
@@ -626,25 +626,25 @@ static void clock_factory_dispose(struct evl_element *e)
 }
 
 static ssize_t gravity_show(struct device *dev,
-			    struct device_attribute *attr,
-			    char *buf)
+			struct device_attribute *attr,
+			char *buf)
 {
 	struct evl_clock *clock;
 	ssize_t ret;
 
 	clock = evl_get_element_by_dev(dev, struct evl_clock);
 	ret = snprintf(buf, PAGE_SIZE, "%Ldi %Ldk %Ldu\n",
-		       ktime_to_ns(evl_get_clock_gravity(clock, irq)),
-		       ktime_to_ns(evl_get_clock_gravity(clock, kernel)),
-		       ktime_to_ns(evl_get_clock_gravity(clock, user)));
+		ktime_to_ns(evl_get_clock_gravity(clock, irq)),
+		ktime_to_ns(evl_get_clock_gravity(clock, kernel)),
+		ktime_to_ns(evl_get_clock_gravity(clock, user)));
 	evl_put_element(&clock->element);
 
 	return ret;
 }
 
 static ssize_t gravity_store(struct device *dev,
-			     struct device_attribute *attr,
-			     const char *buf, size_t count)
+			struct device_attribute *attr,
+			const char *buf, size_t count)
 {
 	struct evl_clock_gravity gravity;
 	struct evl_clock *clock;
@@ -708,7 +708,7 @@ struct evl_factory evl_clock_factory = {
 };
 
 static int set_coreclk_gravity(struct evl_clock *clock,
-			       const struct evl_clock_gravity *p)
+			const struct evl_clock_gravity *p)
 {
 	clock->gravity = *p;
 
@@ -806,12 +806,12 @@ int __init evl_clock_init(void)
 	evl_reset_clock_gravity(&evl_realtime_clock);
 
 	ret = evl_init_clock(&evl_mono_clock,
-			     &evl_oob_cpus);
+			&evl_oob_cpus);
 	if (ret)
 		return ret;
 
 	ret = evl_init_slave_clock(&evl_realtime_clock,
-				   &evl_mono_clock);
+				&evl_mono_clock);
 	if (ret)
 		evl_put_element(&evl_mono_clock.element);
 
diff --git a/kernel/evenless/control.c b/kernel/evenless/control.c
index 3e2620e4773..8bf430b8e7f 100644
--- a/kernel/evenless/control.c
+++ b/kernel/evenless/control.c
@@ -43,8 +43,8 @@ static int start_services(void)
 	int ret = 0;
 
 	state = atomic_cmpxchg(&evl_runstate,
-			       EVL_STATE_STOPPED,
-			       EVL_STATE_WARMUP);
+			EVL_STATE_STOPPED,
+			EVL_STATE_WARMUP);
 	switch (state) {
 	case EVL_STATE_RUNNING:
 		break;
@@ -71,8 +71,8 @@ static int stop_services(void)
 	int ret = 0;
 
 	state = atomic_cmpxchg(&evl_runstate,
-			       EVL_STATE_RUNNING,
-			       EVL_STATE_TEARDOWN);
+			EVL_STATE_RUNNING,
+			EVL_STATE_TEARDOWN);
 	switch (state) {
 	case EVL_STATE_STOPPED:
 		break;
@@ -96,7 +96,7 @@ static int stop_services(void)
 }
 
 static long control_ioctl(struct file *filp, unsigned int cmd,
-			  unsigned long arg)
+			unsigned long arg)
 {
 	struct evl_core_info info;
 	long ret;
@@ -114,7 +114,7 @@ static long control_ioctl(struct file *filp, unsigned int cmd,
 		info.fpu_features = evl_detect_fpu();
 		info.shm_size = evl_shm_size;
 		ret = raw_copy_to_user((struct evl_core_info __user *)arg,
-				       &info, sizeof(info)) ? -EFAULT : 0;
+				&info, sizeof(info)) ? -EFAULT : 0;
 		break;
 	case EVL_CTLIOC_SWITCH_OOB:
 		ret = evl_switch_oob();
@@ -162,8 +162,8 @@ static const char *state_labels[] = {
 };
 
 static ssize_t state_show(struct device *dev,
-			  struct device_attribute *attr,
-			  char *buf)
+			struct device_attribute *attr,
+			char *buf)
 {
 	int st = atomic_read(&evl_runstate);
 
@@ -171,8 +171,8 @@ static ssize_t state_show(struct device *dev,
 }
 
 static ssize_t state_store(struct device *dev,
-			   struct device_attribute *attr,
-			   const char *buf, size_t count)
+			struct device_attribute *attr,
+			const char *buf, size_t count)
 {
 	size_t len = count;
 
diff --git a/kernel/evenless/factory.c b/kernel/evenless/factory.c
index 90b869114e2..d7822d39f38 100644
--- a/kernel/evenless/factory.c
+++ b/kernel/evenless/factory.c
@@ -44,7 +44,7 @@ static struct evl_factory *factories[] = {
 #endif
 };
 
-#define NR_FACTORIES	\
+#define NR_FACTORIES						\
 	(ARRAY_SIZE(early_factories) + ARRAY_SIZE(factories))
 
 static dev_t factory_rdev;
@@ -57,7 +57,7 @@ int evl_init_element(struct evl_element *e, struct evl_factory *fac)
 		minor = find_first_zero_bit(fac->minor_map, fac->nrdev);
 		if (minor >= fac->nrdev) {
 			printk_ratelimited(EVL_WARNING "out of %ss",
-					   fac->name);
+					fac->name);
 			return -EAGAIN;
 		}
 	} while (test_and_set_bit(minor, fac->minor_map));
@@ -240,8 +240,8 @@ int evl_close_element(struct inode *inode, struct file *filp)
 }
 
 int evl_create_element_device(struct evl_element *e,
-			      struct evl_factory *fac,
-			      const char *devname)
+			struct evl_factory *fac,
+			const char *devname)
 {
 	struct device *dev;
 	dev_t rdev;
@@ -286,7 +286,7 @@ void evl_remove_element_device(struct evl_element *e)
 }
 
 static long ioctl_clone_device(struct file *filp, unsigned int cmd,
-			       unsigned long arg)
+			unsigned long arg)
 {
 	struct evl_element *e = filp->private_data;
 	struct evl_clone_req req, __user *u_req;
@@ -318,7 +318,7 @@ static long ioctl_clone_device(struct file *filp, unsigned int cmd,
 
 	fac = container_of(filp->f_inode->i_cdev, struct evl_factory, cdev);
 	e = fac->build(fac, devname ? devname->name : NULL,
-		       req.attrs, &state_offset);
+		req.attrs, &state_offset);
 	if (IS_ERR(e)) {
 		if (devname)
 			putname(devname);
@@ -486,7 +486,7 @@ __evl_get_element_by_fundle(struct evl_factory *fac, fundle_t fundle)
 static char *factory_devnode(struct device *dev, umode_t *mode)
 {
 	return kasprintf(GFP_KERNEL, "evenless/%s/%s",
-			 dev->class->name, dev_name(dev));
+			dev->class->name, dev_name(dev));
 }
 
 static int create_element_class(struct evl_factory *fac)
@@ -606,7 +606,7 @@ create_factories(struct evl_factory **factories, int nr)
 
 	for (n = 0; n < nr; n++) {
 		ret = create_factory(factories[n],
-				     MKDEV(MAJOR(factory_rdev), n));
+				MKDEV(MAJOR(factory_rdev), n));
 		if (ret)
 			goto fail;
 	}
@@ -639,14 +639,14 @@ int __init evl_early_init_factories(void)
 	evl_class->devnode = evl_devnode;
 
 	ret = alloc_chrdev_region(&factory_rdev, 0, NR_FACTORIES,
-				  "evenless_factory");
+				"evenless_factory");
 	if (ret) {
 		class_destroy(evl_class);
 		return ret;
 	}
 
 	ret = create_factories(early_factories,
-			       ARRAY_SIZE(early_factories));
+			ARRAY_SIZE(early_factories));
 	if (ret) {
 		unregister_chrdev_region(factory_rdev, NR_FACTORIES);
 		class_destroy(evl_class);
diff --git a/kernel/evenless/file.c b/kernel/evenless/file.c
index dff5dad18f6..9627287f2a2 100644
--- a/kernel/evenless/file.c
+++ b/kernel/evenless/file.c
@@ -70,7 +70,7 @@ static inline int index_sfd(struct evl_fd *sfd, struct file *filp)
 
 static inline
 struct evl_fd *lookup_sfd(unsigned int fd,
-			  struct files_struct *files)
+			struct files_struct *files)
 {
 	struct evl_fd *sfd, tmp;
 	struct rb_node *rb;
@@ -93,7 +93,7 @@ struct evl_fd *lookup_sfd(unsigned int fd,
 
 static inline
 struct evl_fd *unindex_sfd(unsigned int fd,
-			   struct files_struct *files)
+			struct files_struct *files)
 {
 	struct evl_fd *sfd = lookup_sfd(fd, files);
 
@@ -104,7 +104,7 @@ struct evl_fd *unindex_sfd(unsigned int fd,
 }
 
 void install_inband_fd(unsigned int fd, struct file *filp,
-		       struct files_struct *files) /* in-band */
+		struct files_struct *files) /* in-band */
 {
 	struct evl_fd *sfd;
 	unsigned long flags;
@@ -127,7 +127,7 @@ void install_inband_fd(unsigned int fd, struct file *filp,
 }
 
 void uninstall_inband_fd(unsigned int fd, struct file *filp,
-			 struct files_struct *files) /* in-band */
+			struct files_struct *files) /* in-band */
 {
 	struct evl_fd *sfd;
 	unsigned long flags;
@@ -144,7 +144,7 @@ void uninstall_inband_fd(unsigned int fd, struct file *filp,
 }
 
 void replace_inband_fd(unsigned int fd, struct file *filp,
-		       struct files_struct *files) /* in-band */
+		struct files_struct *files) /* in-band */
 {
 	struct evl_fd *sfd;
 	unsigned long flags;
@@ -246,8 +246,8 @@ void evl_cleanup_files(void)
 int __init evl_init_files(void)
 {
 	fd_cache = kmem_cache_create("evl_fdcache",
-				     sizeof(struct evl_fd),
-				     0, 0, NULL);
+				sizeof(struct evl_fd),
+				0, 0, NULL);
 	if (fd_cache == NULL)
 		return -ENOMEM;
 
diff --git a/kernel/evenless/init.c b/kernel/evenless/init.c
index acf7533cfac..0ebb751764c 100644
--- a/kernel/evenless/init.c
+++ b/kernel/evenless/init.c
@@ -185,9 +185,9 @@ static int __init evl_init(void)
 		goto fail;
 
 	printk(EVL_INFO "core started %s%s%s\n",
-	       boot_debug_notice,
-	       boot_trace_notice,
-	       boot_state_notice);
+		boot_debug_notice,
+		boot_trace_notice,
+		boot_state_notice);
 
 	return 0;
 fail:
diff --git a/kernel/evenless/ksem.c b/kernel/evenless/ksem.c
index d48fbf42ea8..07960c3f89f 100644
--- a/kernel/evenless/ksem.c
+++ b/kernel/evenless/ksem.c
@@ -5,7 +5,7 @@
  */
 
 #include <evenless/timer.h>
-#include <evenless/synch.h>
+#include <evenless/wait.h>
 #include <evenless/clock.h>
 #include <evenless/sched.h>
 #include <evenless/ksem.h>
@@ -13,19 +13,18 @@
 void evl_init_sem(struct evl_ksem *sem, unsigned int value)
 {
 	sem->value = value;
-	evl_init_syn(&sem->wait_queue, EVL_SYN_PRIO,
-		     &evl_mono_clock, NULL);
+	evl_init_wait(&sem->wait_queue, &evl_mono_clock, EVL_WAIT_PRIO);
 }
 EXPORT_SYMBOL_GPL(evl_init_sem);
 
 void evl_destroy_sem(struct evl_ksem *sem)
 {
-	evl_destroy_syn(&sem->wait_queue);
+	evl_destroy_wait(&sem->wait_queue);
 }
 EXPORT_SYMBOL_GPL(evl_destroy_sem);
 
 static int down_sem(struct evl_ksem *sem,
-		    ktime_t timeout, enum evl_tmode tmode)
+		ktime_t timeout, enum evl_tmode tmode)
 {
 	unsigned long flags;
 	int ret = 0;
@@ -37,7 +36,7 @@ static int down_sem(struct evl_ksem *sem,
 	else if (timeout_nonblock(timeout))
 		ret = -EWOULDBLOCK;
 	else {
-		ret = evl_sleep_on_syn(&sem->wait_queue, timeout, tmode);
+		ret = evl_wait_timeout(&sem->wait_queue, timeout, tmode);
 		if (ret & T_TIMEO)
 			ret = -ETIMEDOUT;
 		else if (ret & T_RMID)
@@ -67,7 +66,7 @@ void evl_up(struct evl_ksem *sem)
 {
 	unsigned long flags;
 
-	if (evl_wake_up_syn(&sem->wait_queue))
+	if (evl_wake_up_head(&sem->wait_queue))
 		evl_schedule();
 	else {
 		xnlock_get_irqsave(&nklock, flags);
diff --git a/kernel/evenless/lock.c b/kernel/evenless/lock.c
index 14e81c7438b..56e9dd0fa16 100644
--- a/kernel/evenless/lock.c
+++ b/kernel/evenless/lock.c
@@ -44,7 +44,7 @@ void xnlock_dbg_prepare_acquire(ktime_t *start)
 EXPORT_SYMBOL_GPL(xnlock_dbg_prepare_acquire);
 
 void xnlock_dbg_acquired(struct xnlock *lock, int cpu, ktime_t *start,
-			 const char *file, int line, const char *function)
+			const char *file, int line, const char *function)
 {
 	lock->lock_date = *start;
 	lock->spin_time = ktime_sub(evl_read_clock(&evl_mono_clock), *start);
@@ -56,7 +56,7 @@ void xnlock_dbg_acquired(struct xnlock *lock, int cpu, ktime_t *start,
 EXPORT_SYMBOL_GPL(xnlock_dbg_acquired);
 
 int xnlock_dbg_release(struct xnlock *lock,
-		       const char *file, int line, const char *function)
+		const char *file, int line, const char *function)
 {
 	struct xnlockinfo *stats;
 	ktime_t lock_time;
@@ -75,8 +75,8 @@ int xnlock_dbg_release(struct xnlock *lock,
 	if (unlikely(lock->owner != cpu)) {
 		printk(EVL_ERR "lock %p already unlocked on CPU #%d\n"
 			"          last owner = %s:%u (%s(), CPU #%d)\n",
-		       lock, cpu, lock->file, lock->line, lock->function,
-		       lock->cpu);
+			lock, cpu, lock->file, lock->line, lock->function,
+			lock->cpu);
 		show_stack(NULL,NULL);
 		return 1;
 	}
diff --git a/kernel/evenless/logger.c b/kernel/evenless/logger.c
index db40f3ba303..0a91331c82c 100644
--- a/kernel/evenless/logger.c
+++ b/kernel/evenless/logger.c
@@ -60,7 +60,7 @@ static void relay_output(struct work_struct *work)
 			filp->f_pos = pos;
 
 		smp_store_release(&circ->tail,
-				  (tail + len) & (logger->logsz - 1));
+				(tail + len) & (logger->logsz - 1));
 	}
 
 	mutex_unlock(&filp->f_pos_lock);
@@ -119,7 +119,7 @@ static ssize_t logger_oob_write(struct file *filp,
 		}
 
 		smp_store_release(&circ->head,
-				  (head + len) & (logger->logsz - 1));
+				(head + len) & (logger->logsz - 1));
 		u_ptr += len;
 		rem -= len;
 		written += len;
@@ -136,7 +136,7 @@ static ssize_t logger_oob_write(struct file *filp,
 }
 
 static ssize_t logger_write(struct file *filp, const char __user *u_buf,
-			    size_t count, loff_t *ppos)
+			size_t count, loff_t *ppos)
 {
 	return logger_oob_write(filp, u_buf, count);
 }
@@ -150,7 +150,7 @@ static const struct file_operations logger_fops = {
 
 static struct evl_element *
 logger_factory_build(struct evl_factory *fac, const char *name,
-		     void __user *u_attrs, u32 *state_offp)
+		void __user *u_attrs, u32 *state_offp)
 {
 	struct evl_logger_attrs attrs;
 	struct evl_logger *logger;
diff --git a/kernel/evenless/mapper.c b/kernel/evenless/mapper.c
index 0a3b91c98b1..1f036872646 100644
--- a/kernel/evenless/mapper.c
+++ b/kernel/evenless/mapper.c
@@ -46,7 +46,7 @@ static const struct file_operations mapper_fops = {
 
 static struct evl_element *
 mapper_factory_build(struct evl_factory *fac, const char *name,
-		     void __user *u_attrs, u32 *state_offp)
+		void __user *u_attrs, u32 *state_offp)
 {
 	struct evl_mapper_attrs attrs;
 	struct evl_mapper *mapper;
diff --git a/kernel/evenless/memory.c b/kernel/evenless/memory.c
index 35073bd8fee..7419e7324b8 100644
--- a/kernel/evenless/memory.c
+++ b/kernel/evenless/memory.c
@@ -81,8 +81,8 @@ page_is_valid(struct evl_heap *heap, int pg)
 }
 
 static void mark_pages(struct evl_heap *heap,
-		       int pg, int nrpages,
-		       enum evl_heap_pgtype type)
+		int pg, int nrpages,
+		enum evl_heap_pgtype type)
 {
 	while (nrpages-- > 0)
 		heap->pagemap[pg].type = type;
@@ -97,8 +97,8 @@ page_is_valid(struct evl_heap *heap, int pg)
 }
 
 static void mark_pages(struct evl_heap *heap,
-		       int pg, int nrpages,
-		       enum evl_heap_pgtype type)
+		int pg, int nrpages,
+		enum evl_heap_pgtype type)
 { }
 
 #endif
@@ -245,7 +245,7 @@ static int reserve_page_range(struct evl_heap *heap, size_t size)
 }
 
 static void release_page_range(struct evl_heap *heap,
-			       void *page, size_t size)
+			void *page, size_t size)
 {
 	struct evl_heap_range *freed = page, *left, *right;
 	bool addr_linked = false;
@@ -274,11 +274,11 @@ static void release_page_range(struct evl_heap *heap,
 
 	insert_range_bysize(heap, freed);
 	mark_pages(heap, addr_to_pagenr(heap, page),
-		   size >> EVL_HEAP_PAGE_SHIFT, page_free);
+		size >> EVL_HEAP_PAGE_SHIFT, page_free);
 }
 
 static void add_page_front(struct evl_heap *heap,
-			   int pg, int log2size)
+			int pg, int log2size)
 {
 	struct evl_heap_pgentry *new, *head, *next;
 	int ilog;
@@ -323,7 +323,7 @@ static void remove_page(struct evl_heap *heap,
 }
 
 static void move_page_front(struct evl_heap *heap,
-			    int pg, int log2size)
+			int pg, int log2size)
 {
 	int ilog = log2size - EVL_HEAP_MIN_LOG2;
 
@@ -337,7 +337,7 @@ static void move_page_front(struct evl_heap *heap,
 }
 
 static void move_page_back(struct evl_heap *heap,
-			   int pg, int log2size)
+			int pg, int log2size)
 {
 	struct evl_heap_pgentry *old, *last, *head, *next;
 	int ilog;
@@ -361,7 +361,7 @@ static void move_page_back(struct evl_heap *heap,
 }
 
 static void *add_free_range(struct evl_heap *heap,
-			    size_t bsize, int log2size)
+			size_t bsize, int log2size)
 {
 	int pg;
 
@@ -397,7 +397,7 @@ static void *add_free_range(struct evl_heap *heap,
 		heap->pagemap[pg].type = page_list;
 		heap->pagemap[pg].bsize = (u32)bsize;
 		mark_pages(heap, pg + 1,
-			   (bsize >> EVL_HEAP_PAGE_SHIFT) - 1, page_cont);
+			(bsize >> EVL_HEAP_PAGE_SHIFT) - 1, page_cont);
 	}
 
 	heap->used_size += bsize;
@@ -574,7 +574,7 @@ void evl_free_chunk(struct evl_heap *heap, void *block)
 		if (heap->pagemap[pg].map == ~gen_block_mask(log2size)) {
 			remove_page(heap, pg, log2size);
 			release_page_range(heap, pagenr_to_addr(heap, pg),
-					   EVL_HEAP_PAGE_SIZE);
+					EVL_HEAP_PAGE_SIZE);
 		} else if (oldmap == -1U)
 			move_page_front(heap, pg, log2size);
 	}
@@ -588,8 +588,8 @@ void evl_free_chunk(struct evl_heap *heap, void *block)
 	raw_spin_unlock_irqrestore(&heap->lock, flags);
 
 	EVL_WARN(MEMORY, 1, "invalid block %p in heap %s",
-		 block, heap == &evl_shared_heap ?
-		 "shared" : "system");
+		block, heap == &evl_shared_heap ?
+		"shared" : "system");
 }
 EXPORT_SYMBOL_GPL(evl_free_chunk);
 
diff --git a/kernel/evenless/monitor.c b/kernel/evenless/monitor.c
index cf7295618f3..40e2d49fbfc 100644
--- a/kernel/evenless/monitor.c
+++ b/kernel/evenless/monitor.c
@@ -8,7 +8,8 @@
 #include <linux/list.h>
 #include <linux/slab.h>
 #include <evenless/thread.h>
-#include <evenless/synch.h>
+#include <evenless/wait.h>
+#include <evenless/mutex.h>
 #include <evenless/clock.h>
 #include <evenless/monitor.h>
 #include <evenless/thread.h>
@@ -27,11 +28,11 @@ struct evl_monitor {
 	int type;
 	union {
 		struct {
-			struct evl_syn lock;
+			struct evl_mutex lock;
 			struct list_head events;
 		};
 		struct {
-			struct evl_syn wait_queue;
+			struct evl_wait_queue wait_queue;
 			struct evl_monitor *gate;
 			struct list_head next; /* in ->events */
 		};
@@ -75,7 +76,7 @@ int evl_signal_monitor_targeted(struct evl_thread *target, int monfd)
 	 * not, we might race updating the state flags, possibly
 	 * loosing events. Too bad.
 	 */
-	if (target->wchan == &event->wait_queue) {
+	if (target->wchan == &event->wait_queue.wchan) {
 		target->info |= T_SIGNAL;
 		event->state->flags |= EVL_MONITOR_TARGETED;
 		ret = 0;
@@ -88,8 +89,9 @@ int evl_signal_monitor_targeted(struct evl_thread *target, int monfd)
 	return ret;
 }
 
-void __evl_commit_monitor_ceiling(struct evl_thread *curr)  /* nklock held, irqs off, OOB */
+void __evl_commit_monitor_ceiling(void)  /* nklock held, irqs off, OOB */
 {
+	struct evl_thread *curr = evl_current_thread();
 	struct evl_monitor *gate;
 
 	/*
@@ -97,13 +99,13 @@ void __evl_commit_monitor_ceiling(struct evl_thread *curr)  /* nklock held, irqs
 	 * pp_pending is a bad handle, just skip ceiling.
 	 */
 	gate = evl_get_element_by_fundle(&evl_monitor_factory,
-					 curr->u_window->pp_pending,
-					 struct evl_monitor);
+					curr->u_window->pp_pending,
+					struct evl_monitor);
 	if (gate == NULL)
 		goto out;
 
 	if (gate->type == EVL_MONITOR_PP)
-		evl_commit_syn_ceiling(&gate->lock, curr);
+		evl_commit_mutex_ceiling(&gate->lock);
 
 	evl_put_element(&gate->element);
 out:
@@ -133,8 +135,8 @@ static void wakeup_waiters(struct evl_monitor *event)
 	 * if at some point userland sent a targeted event to a
 	 * (valid) waiter during a transaction, the kernel will only
 	 * look for targeted waiters in the wake up loop. We don't
-	 * default to calling evl_wake_up_syn() in case those
-	 * waiters have aborted wait in the meantime.
+	 * default to calling evl_wake_up() in case those waiters have
+	 * aborted wait in the meantime.
 	 *
 	 * CAUTION: we must keep the wake up ops rescheduling-free, so
 	 * that low priority threads cannot preempt us before high
@@ -143,34 +145,32 @@ static void wakeup_waiters(struct evl_monitor *event)
 	 * careful when killing the latter.
 	 */
 	if ((state->flags & EVL_MONITOR_SIGNALED) &&
-	    evl_syn_has_waiter(&event->wait_queue)) {
+		evl_wait_active(&event->wait_queue)) {
 		if (bcast)
-			evl_flush_syn(&event->wait_queue, 0);
+			evl_flush_wait(&event->wait_queue, 0);
 		else if (state->flags & EVL_MONITOR_TARGETED) {
-			evl_for_each_syn_waiter_safe(waiter, n,
-						     &event->wait_queue) {
+			evl_for_each_waiter_safe(waiter, n,
+						&event->wait_queue) {
 				if (waiter->info & T_SIGNAL)
-					evl_wake_up_targeted_syn(&event->wait_queue,
-								 waiter);
+					evl_wake_up(&event->wait_queue, waiter);
 			}
 		} else
-			evl_wake_up_syn(&event->wait_queue);
+			evl_wake_up_head(&event->wait_queue);
 	}
 
 	state->flags &= ~(EVL_MONITOR_SIGNALED|
-			  EVL_MONITOR_BROADCAST|
-			  EVL_MONITOR_TARGETED);
+			EVL_MONITOR_BROADCAST|
+			EVL_MONITOR_TARGETED);
 }
 
 /* nklock held, irqs off */
-static int __enter_monitor(struct evl_monitor *gate,
-			   struct evl_thread *curr)
+static int __enter_monitor(struct evl_monitor *gate)
 {
 	int info;
 
-	evl_commit_monitor_ceiling(curr);
+	evl_commit_monitor_ceiling();
 
-	info = evl_acquire_syn(&gate->lock, EVL_INFINITE, EVL_REL);
+	info = evl_lock_mutex(&gate->lock);
 	if (info)
 		/* Break or error, no timeout possible. */
 		return info & T_BREAK ? -EINTR : -EINVAL;
@@ -186,15 +186,15 @@ static int enter_monitor(struct evl_monitor *gate)
 	if (gate->type == EVL_MONITOR_EV)
 		return -EINVAL;
 
-	if (evl_is_syn_owner(gate->lock.fastlock, fundle_of(curr)))
+	if (evl_is_mutex_owner(gate->lock.fastlock, fundle_of(curr)))
 		return -EDEADLK;	/* Deny recursive locking. */
 
-	return __enter_monitor(gate, curr);
+	return __enter_monitor(gate);
 }
 
 /* nklock held, irqs off */
 static void __exit_monitor(struct evl_monitor *gate,
-			   struct evl_thread *curr)
+			struct evl_thread *curr)
 {
 	/*
 	 * If we are about to release the lock which is still pending
@@ -204,7 +204,7 @@ static void __exit_monitor(struct evl_monitor *gate,
 	if (fundle_of(gate) == curr->u_window->pp_pending)
 		curr->u_window->pp_pending = EVL_NO_HANDLE;
 
-	evl_release_syn(&gate->lock);
+	__evl_unlock_mutex(&gate->lock);
 }
 
 /* nklock held, irqs off */
@@ -217,7 +217,7 @@ static int exit_monitor(struct evl_monitor *gate)
 	if (gate->type == EVL_MONITOR_EV)
 		return -EINVAL;
 
-	if (!evl_is_syn_owner(gate->lock.fastlock, fundle_of(curr)))
+	if (!evl_is_mutex_owner(gate->lock.fastlock, fundle_of(curr)))
 		return -EPERM;
 
 	if (state->flags & EVL_MONITOR_SIGNALED) {
@@ -240,10 +240,10 @@ static int exit_monitor(struct evl_monitor *gate)
 
 /* nklock held, irqs off */
 static void untrack_event(struct evl_monitor *event,
-			  struct evl_monitor *gate)
+			struct evl_monitor *gate)
 {
 	if (event->gate == gate &&
-	    !evl_syn_has_waiter(&event->wait_queue)) {
+		!evl_wait_active(&event->wait_queue)) {
 		event->state->u.gate_offset = EVL_MONITOR_NOGATE;
 		list_del(&event->next);
 		event->gate = NULL;
@@ -280,7 +280,7 @@ static int wait_monitor(struct evl_monitor *event,
 	}
 
 	/* Make sure we actually passed the gate. */
-	if (!evl_is_syn_owner(gate->lock.fastlock, fundle_of(curr))) {
+	if (!evl_is_mutex_owner(gate->lock.fastlock, fundle_of(curr))) {
 		op_ret = -EPERM;
 		goto put;
 	}
@@ -313,7 +313,7 @@ static int wait_monitor(struct evl_monitor *event,
 	 */
 	timeout = timespec_to_ktime(req->timeout);
 	tmode = timeout ? EVL_ABS : EVL_REL;
-	info = evl_sleep_on_syn(&event->wait_queue, timeout, tmode);
+	info = evl_wait_timeout(&event->wait_queue, timeout, tmode);
 	if (info) {
 		if (info & T_BREAK) {
 			ret = -EINTR;
@@ -323,7 +323,7 @@ static int wait_monitor(struct evl_monitor *event,
 			op_ret = -ETIMEDOUT;
 	}
 
-	ret = __enter_monitor(gate, curr);
+	ret = __enter_monitor(gate);
 
 	untrack_event(event, gate);
 put:
@@ -336,7 +336,7 @@ static int wait_monitor(struct evl_monitor *event,
 
 /* nklock held, irqs off */
 static int unwait_monitor(struct evl_monitor *event,
-			  struct evl_monitor_unwaitreq *req)
+			struct evl_monitor_unwaitreq *req)
 {
 	struct evl_monitor *gate;
 	struct evl_file *sfilp;
@@ -360,7 +360,7 @@ static int unwait_monitor(struct evl_monitor *event,
 }
 
 static long monitor_ioctl(struct file *filp, unsigned int cmd,
-			  unsigned long arg)
+			unsigned long arg)
 {
 	struct evl_monitor *mon = element_of(filp, struct evl_monitor);
 	struct evl_monitor_binding bind, __user *u_bind;
@@ -378,7 +378,7 @@ static long monitor_ioctl(struct file *filp, unsigned int cmd,
 }
 
 static long monitor_oob_ioctl(struct file *filp, unsigned int cmd,
-			      unsigned long arg)
+			unsigned long arg)
 {
 	struct evl_monitor *mon = element_of(filp, struct evl_monitor);
 	struct evl_monitor_unwaitreq uwreq, __user *u_uwreq;
@@ -435,7 +435,7 @@ static const struct file_operations monitor_fops = {
 
 static struct evl_element *
 monitor_factory_build(struct evl_factory *fac, const char *name,
-		      void __user *u_attrs, u32 *state_offp)
+		void __user *u_attrs, u32 *state_offp)
 {
 	struct evl_monitor_state *state;
 	struct evl_monitor_attrs attrs;
@@ -450,7 +450,7 @@ monitor_factory_build(struct evl_factory *fac, const char *name,
 	switch (attrs.type) {
 	case EVL_MONITOR_PP:
 		if (attrs.ceiling == 0 ||
-		    attrs.ceiling > EVL_CORE_MAX_PRIO)
+			attrs.ceiling > EVL_CORE_MAX_PRIO)
 			return ERR_PTR(-EINVAL);
 		break;
 	case EVL_MONITOR_PI:
@@ -486,20 +486,19 @@ monitor_factory_build(struct evl_factory *fac, const char *name,
 	switch (attrs.type) {
 	case EVL_MONITOR_PP:
 		state->u.gate.ceiling = attrs.ceiling;
-		evl_init_syn_protect(&mon->lock, clock,
-				     &state->u.gate.owner,
-				     &state->u.gate.ceiling);
+		evl_init_mutex_pp(&mon->lock, clock,
+				&state->u.gate.owner,
+				&state->u.gate.ceiling);
 		INIT_LIST_HEAD(&mon->events);
 		break;
 	case EVL_MONITOR_PI:
-		evl_init_syn(&mon->lock, EVL_SYN_PI, clock,
-			     &state->u.gate.owner);
+		evl_init_mutex_pi(&mon->lock, clock,
+				&state->u.gate.owner);
 		INIT_LIST_HEAD(&mon->events);
 		break;
 	case EVL_MONITOR_EV:
 	default:
-		evl_init_syn(&mon->wait_queue, EVL_SYN_PRIO,
-			     clock, NULL);
+		evl_init_wait(&mon->wait_queue, clock, EVL_WAIT_PRIO);
 		state->u.gate_offset = EVL_MONITOR_NOGATE;
 	}
 
@@ -536,7 +535,7 @@ static void monitor_factory_dispose(struct evl_element *e)
 
 	if (mon->type == EVL_MONITOR_EV) {
 		evl_put_clock(mon->wait_queue.clock);
-		evl_destroy_syn(&mon->wait_queue);
+		evl_destroy_wait(&mon->wait_queue);
 		if (mon->gate) {
 			xnlock_get_irqsave(&nklock, flags);
 			list_del(&mon->next);
@@ -544,7 +543,7 @@ static void monitor_factory_dispose(struct evl_element *e)
 		}
 	} else {
 		evl_put_clock(mon->lock.clock);
-		evl_destroy_syn(&mon->lock);
+		evl_destroy_mutex(&mon->lock);
 	}
 
 	evl_free_chunk(&evl_shared_heap, mon->state);
@@ -553,8 +552,8 @@ static void monitor_factory_dispose(struct evl_element *e)
 }
 
 static ssize_t state_show(struct device *dev,
-			  struct device_attribute *attr,
-			  char *buf)
+			struct device_attribute *attr,
+			char *buf)
 {
 	struct evl_thread *owner = NULL;
 	struct evl_monitor *mon;
@@ -565,15 +564,15 @@ static ssize_t state_show(struct device *dev,
 
 	if (mon->type == EVL_MONITOR_EV)
 		ret = snprintf(buf, PAGE_SIZE, "%#x\n",
-			       mon->state->flags);
+			mon->state->flags);
 	else {
 		fun = atomic_read(&mon->state->u.gate.owner);
 		if (fun != EVL_NO_HANDLE)
 			owner = evl_get_element_by_fundle(&evl_thread_factory,
-						  fun, struct evl_thread);
+							fun, struct evl_thread);
 		ret = snprintf(buf, PAGE_SIZE, "%d %u\n",
-			       owner ? evl_get_inband_pid(owner) : -1,
-			       mon->state->u.gate.ceiling);
+			owner ? evl_get_inband_pid(owner) : -1,
+			mon->state->u.gate.ceiling);
 		if (owner)
 			evl_put_element(&owner->element);
 	}
diff --git a/kernel/evenless/mutex.c b/kernel/evenless/mutex.c
index 78f64d6c210..2661165688a 100644
--- a/kernel/evenless/mutex.c
+++ b/kernel/evenless/mutex.c
@@ -1,69 +1,768 @@
 /*
  * SPDX-License-Identifier: GPL-2.0
  *
- * Copyright (C) 2019 Philippe Gerum  <rpm@xenomai.org>
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2001, 2019 Philippe Gerum  <rpm@xenomai.org>
  */
 
+#include <linux/kernel.h>
 #include <evenless/timer.h>
-#include <evenless/synch.h>
 #include <evenless/clock.h>
 #include <evenless/sched.h>
 #include <evenless/thread.h>
 #include <evenless/mutex.h>
+#include <evenless/monitor.h>
+#include <uapi/evenless/signal.h>
+#include <trace/events/evenless.h>
 
-void evl_init_mutex(struct evl_mutex *mutex)
+static inline int get_ceiling_value(struct evl_mutex *mutex)
 {
-	evl_init_syn(&mutex->wait_queue, EVL_SYN_PI,
-		     &evl_mono_clock, &mutex->fastlock);
+	/*
+	 * The ceiling priority value is stored in user-writable
+	 * memory, make sure to constrain it within valid bounds for
+	 * evl_sched_rt before using it.
+	 */
+	return clamp(*mutex->ceiling_ref, 1U, (u32)EVL_CORE_MAX_PRIO);
 }
-EXPORT_SYMBOL_GPL(evl_init_mutex);
 
-void evl_destroy_mutex(struct evl_mutex *mutex)
+static inline void disable_inband_switch(struct evl_thread *curr)
 {
-	evl_destroy_syn(&mutex->wait_queue);
+	/*
+	 * Track mutex locking depth, to prevent weak threads from
+	 * being switched back to in-band context on return from OOB
+	 * syscalls.
+	 */
+	if (curr->state & (T_WEAK|T_DEBUG))
+		atomic_inc(&curr->inband_disable_count);
+}
+
+static inline bool enable_inband_switch(struct evl_thread *curr)
+{
+	if ((curr->state & T_WEAK) ||
+		IS_ENABLED(CONFIG_EVENLESS_DEBUG_MUTEX_SLEEP)) {
+		if (unlikely(atomic_dec_return(&curr->inband_disable_count) < 0)) {
+			atomic_set(&curr->inband_disable_count, 0);
+			if (curr->state & T_WARN)
+				evl_signal_thread(curr, SIGDEBUG,
+						SIGDEBUG_MUTEX_IMBALANCE);
+			return false;
+		}
+	}
+
+	return true;
+}
+
+static inline void raise_boost_flag(struct evl_thread *owner)
+{
+	/* Backup the base priority at first boost only. */
+	if (!(owner->state & T_BOOST)) {
+		owner->bprio = owner->cprio;
+		owner->state |= T_BOOST;
+	}
+}
+
+static void inherit_thread_priority(struct evl_thread *owner,
+				struct evl_thread *target)
+{
+	struct evl_wait_channel *wchan;
+
+	if (owner->state & T_ZOMBIE)
+		return;
+
+	/* Apply the scheduling policy of "target" to "owner" */
+	evl_track_thread_policy(owner, target);
+
+	/*
+	 * Owner may be sleeping, propagate priority update through
+	 * the PI chain if needed.
+	 */
+	wchan = owner->wchan;
+	if (wchan)
+		wchan->reorder_wait(owner);
+}
+
+static void __ceil_owner_priority(struct evl_thread *owner, int prio)
+{
+	struct evl_wait_channel *wchan;
+
+	if (owner->state & T_ZOMBIE)
+		return;
+	/*
+	 * Raise owner priority to the ceiling value, this implicitly
+	 * selects SCHED_FIFO for the owner.
+	 */
+	evl_protect_thread_priority(owner, prio);
+
+	wchan = owner->wchan;
+	if (wchan)
+		wchan->reorder_wait(owner);
+}
+
+static void adjust_boost(struct evl_thread *owner, struct evl_thread *target)
+{
+	struct evl_mutex *mutex;
+
+	/*
+	 * CAUTION: we may have PI and PP-enabled mutexes among the
+	 * boosters, considering the leader of mutex->wait_list is
+	 * therefore NOT enough for determining the next boost
+	 * priority, since PP is tracked lazily on acquisition, not
+	 * immediately when a contention is detected. Check the head
+	 * of the booster list instead.
+	 */
+	mutex = list_first_entry(&owner->boosters, struct evl_mutex, next);
+	if (mutex->wprio == owner->wprio)
+		return;
+
+	if (mutex->flags & EVL_MUTEX_PP)
+		__ceil_owner_priority(owner, get_ceiling_value(mutex));
+	else {
+		if (EVL_WARN_ON(CORE, list_empty(&mutex->wait_list)))
+			return;
+		if (target == NULL)
+			target = list_first_entry(&mutex->wait_list,
+						struct evl_thread, wait_next);
+		inherit_thread_priority(owner, target);
+	}
+}
+
+static void ceil_owner_priority(struct evl_mutex *mutex)
+{
+	struct evl_thread *owner = mutex->owner;
+	int wprio;
+
+	/* PP ceiling values are implicitly based on the RT class. */
+	wprio = evl_calc_weighted_prio(&evl_sched_rt,
+				get_ceiling_value(mutex));
+	mutex->wprio = wprio;
+	list_add_priff(mutex, &owner->boosters, wprio, next);
+	raise_boost_flag(owner);
+	mutex->flags |= EVL_MUTEX_CEILING;
+
+	/*
+	 * If the ceiling value is lower than the current effective
+	 * priority, we must not adjust the latter.  BEWARE: not only
+	 * this restriction is required to keep the PP logic right,
+	 * but this is also a basic assumption made by all
+	 * evl_commit_monitor_ceiling() callers which won't check for any
+	 * rescheduling opportunity upon return.
+	 *
+	 * However we do want the mutex to be linked to the booster
+	 * list, and T_BOOST must appear in the current thread status.
+	 *
+	 * This way, setparam() won't be allowed to decrease the
+	 * current weighted priority below the ceiling value, until we
+	 * eventually release this mutex.
+	 */
+	if (wprio > owner->wprio)
+		adjust_boost(owner, NULL);
+}
+
+static inline
+void track_owner(struct evl_mutex *mutex,
+		struct evl_thread *owner)
+{
+	mutex->owner = owner;
+}
+
+static inline int fast_mutex_is_claimed(fundle_t handle)
+{
+	return (handle & EVL_MUTEX_FLCLAIM) != 0;
+}
+
+static inline fundle_t mutex_fast_claim(fundle_t handle)
+{
+	return handle | EVL_MUTEX_FLCLAIM;
+}
+
+static inline fundle_t mutex_fast_ceil(fundle_t handle)
+{
+	return handle | EVL_MUTEX_FLCEIL;
+}
+
+static inline  /* nklock held, irqs off */
+void set_current_owner_locked(struct evl_mutex *mutex,
+			struct evl_thread *owner)
+{
+	/*
+	 * Update the owner information, and apply priority protection
+	 * for PP mutexes. We may only get there if owner is current,
+	 * or blocked.
+	 */
+	track_owner(mutex, owner);
+	if (mutex->flags & EVL_MUTEX_PP)
+		ceil_owner_priority(mutex);
+}
+
+static inline
+void set_current_owner(struct evl_mutex *mutex,
+		struct evl_thread *owner)
+{
+	unsigned long flags;
+
+	track_owner(mutex, owner);
+	if (mutex->flags & EVL_MUTEX_PP) {
+		xnlock_get_irqsave(&nklock, flags);
+		ceil_owner_priority(mutex);
+		xnlock_put_irqrestore(&nklock, flags);
+	}
+}
+
+static inline
+fundle_t get_owner_handle(fundle_t ownerh, struct evl_mutex *mutex)
+{
+	/*
+	 * On acquisition from kernel space, the fast lock handle
+	 * should bear the FLCEIL bit for PP mutexes, so that userland
+	 * takes the slow path on release, jumping to the kernel for
+	 * dropping the ceiling priority boost.
+	 */
+	if (mutex->flags & EVL_MUTEX_PP)
+		ownerh = mutex_fast_ceil(ownerh);
+
+	return ownerh;
+}
+
+static void drop_booster(struct evl_mutex *mutex, struct evl_thread *owner)
+{
+	list_del(&mutex->next);	/* owner->boosters */
+
+	if (list_empty(&owner->boosters)) {
+		owner->state &= ~T_BOOST;
+		inherit_thread_priority(owner, owner);
+	} else
+		adjust_boost(owner, NULL);
+}
+
+static inline void clear_pi_boost(struct evl_mutex *mutex,
+				struct evl_thread *owner)
+{	/* nklock held, irqs off */
+	mutex->flags &= ~EVL_MUTEX_CLAIMED;
+	drop_booster(mutex, owner);
+}
+
+static inline void clear_pp_boost(struct evl_mutex *mutex,
+				struct evl_thread *owner)
+{	/* nklock held, irqs off */
+	mutex->flags &= ~EVL_MUTEX_CEILING;
+	drop_booster(mutex, owner);
+}
+
+#ifdef CONFIG_EVENLESS_DEBUG_MUTEX_INBAND
+
+/*
+ * Detect when a thread is about to wait on a mutex currently owned by
+ * someone running in-band.
+ */
+static void detect_inband_owner(struct evl_mutex *mutex,
+				struct evl_thread *waiter)
+{
+	if ((waiter->state & T_WARN) &&
+		!(waiter->info & T_PIALERT) &&
+		(mutex->owner->state & T_INBAND)) {
+		waiter->info |= T_PIALERT;
+		evl_signal_thread(waiter, SIGDEBUG,
+				SIGDEBUG_MIGRATE_PRIOINV);
+	} else
+		waiter->info &= ~T_PIALERT;
+}
+
+/*
+ * Detect when a thread is about to switch to in-band context while
+ * holding booster(s) (claimed PI or active PP mutex), which denotes a
+ * potential priority inversion. In such an event, any waiter bearing
+ * the T_WARN bit will receive a SIGDEBUG notification.
+ */
+void evl_detect_boost_drop(struct evl_thread *owner)
+{
+	struct evl_thread *waiter;
+	struct evl_mutex *mutex;
+	unsigned long flags;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	for_each_evl_booster(mutex, owner) {
+		evl_for_each_mutex_waiter(waiter, mutex) {
+			if (waiter->state & T_WARN) {
+				waiter->info |= T_PIALERT;
+				evl_signal_thread(waiter, SIGDEBUG,
+						SIGDEBUG_MIGRATE_PRIOINV);
+			}
+		}
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+}
+
+#else
+
+static inline
+void detect_inband_owner(struct evl_mutex *mutex,
+			struct evl_thread *waiter) { }
+
+#endif
+
+static void init_mutex(struct evl_mutex *mutex,
+		struct evl_clock *clock, int flags,
+		atomic_t *fastlock, u32 *ceiling_ref)
+{
+	mutex->fastlock = fastlock;
+	atomic_set(fastlock, EVL_NO_HANDLE);
+	mutex->flags = flags & ~EVL_MUTEX_CLAIMED;
+	mutex->owner = NULL;
+	mutex->wprio = -1;
+	mutex->ceiling_ref = NULL;
+	mutex->clock = clock;
+	INIT_LIST_HEAD(&mutex->wait_list);
+	mutex->wchan.abort_wait = evl_abort_mutex_wait;
+	mutex->wchan.reorder_wait = evl_reorder_mutex_wait;
+	raw_spin_lock_init(&mutex->wchan.lock);
+}
+
+void evl_init_mutex_pi(struct evl_mutex *mutex,
+		struct evl_clock *clock, atomic_t *fastlock)
+{
+	init_mutex(mutex, clock, EVL_MUTEX_PI, fastlock, NULL);
+}
+EXPORT_SYMBOL_GPL(evl_init_mutex_pi);
+
+void evl_init_mutex_pp(struct evl_mutex *mutex,
+		struct evl_clock *clock,
+		atomic_t *fastlock, u32 *ceiling_ref)
+{
+	init_mutex(mutex, clock, EVL_MUTEX_PP, fastlock, ceiling_ref);
+}
+EXPORT_SYMBOL_GPL(evl_init_mutex_pp);
+
+bool evl_destroy_mutex(struct evl_mutex *mutex)
+{
+	struct evl_thread *waiter, *tmp;
+	unsigned long flags;
+	bool ret;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	trace_evl_mutex_destroy(mutex);
+
+	if (list_empty(&mutex->wait_list)) {
+		EVL_WARN_ON(CORE, mutex->flags & EVL_MUTEX_CLAIMED);
+		ret = false;
+	} else {
+		ret = true;
+		list_for_each_entry_safe(waiter, tmp, &mutex->wait_list, wait_next) {
+			list_del(&waiter->wait_next);
+			waiter->info |= T_RMID;
+			waiter->wchan = NULL;
+			evl_resume_thread(waiter, T_PEND);
+		}
+		if (mutex->flags & EVL_MUTEX_CLAIMED)
+			clear_pi_boost(mutex, mutex->owner);
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
 }
 EXPORT_SYMBOL_GPL(evl_destroy_mutex);
 
-static int lock_timeout(struct evl_mutex *mutex, ktime_t timeout,
+int evl_trylock_mutex(struct evl_mutex *mutex)
+{
+	struct evl_thread *curr = evl_current_thread();
+	atomic_t *lockp = mutex->fastlock;
+	fundle_t h;
+
+	oob_context_only();
+
+	trace_evl_mutex_trylock(mutex);
+
+	h = atomic_cmpxchg(lockp, EVL_NO_HANDLE,
+			get_owner_handle(fundle_of(curr), mutex));
+	if (h != EVL_NO_HANDLE)
+		return evl_get_index(h) == fundle_of(curr) ?
+			-EDEADLK : -EBUSY;
+
+	set_current_owner(mutex, curr);
+	disable_inband_switch(curr);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(evl_trylock_mutex);
+
+int evl_lock_mutex_timeout(struct evl_mutex *mutex, ktime_t timeout,
 			enum evl_tmode timeout_mode)
 {
+	struct evl_thread *curr = evl_current_thread(), *owner;
+	atomic_t *lockp = mutex->fastlock;
+	fundle_t currh, h, oldh;
+	unsigned long flags;
 	int ret;
 
-	ret = evl_try_acquire_syn(&mutex->wait_queue);
-	if (ret != -EBUSY)
-		return ret;
+	oob_context_only();
+
+	currh = fundle_of(curr);
+	trace_evl_mutex_lock(mutex);
+redo:
+	/* Basic form of evl_trylock_mutex(). */
+	h = atomic_cmpxchg(lockp, EVL_NO_HANDLE,
+			get_owner_handle(currh, mutex));
+	if (likely(h == EVL_NO_HANDLE)) {
+		set_current_owner(mutex, curr);
+		disable_inband_switch(curr);
+		return 0;
+	}
+
+	xnlock_get_irqsave(&nklock, flags);
 
-	if (timeout_nonblock(timeout))
-		return -EWOULDBLOCK;
+	/*
+	 * Set claimed bit.  In case it appears to be set already,
+	 * re-read its state under nklock so that we don't miss any
+	 * change between the lock-less read and here. But also try to
+	 * avoid cmpxchg where possible. Only if it appears not to be
+	 * set, start with cmpxchg directly.
+	 */
+	if (fast_mutex_is_claimed(h)) {
+		oldh = atomic_read(lockp);
+		goto test_no_owner;
+	}
 
-	for (;;) {
-		ret = evl_acquire_syn(&mutex->wait_queue, timeout, timeout_mode);
-		if (ret == 0)
+	do {
+		oldh = atomic_cmpxchg(lockp, h, mutex_fast_claim(h));
+		if (likely(oldh == h))
 			break;
-		if (ret & T_BREAK)
-			continue;
-		ret = ret & T_TIMEO ? -ETIMEDOUT : -EIDRM;
-		break;
+	test_no_owner:
+		if (oldh == EVL_NO_HANDLE) {
+			/* Lock released from another cpu. */
+			xnlock_put_irqrestore(&nklock, flags);
+			goto redo;
+		}
+		h = oldh;
+	} while (!fast_mutex_is_claimed(h));
+
+	owner = evl_get_element_by_fundle(&evl_thread_factory, h,
+					struct evl_thread);
+	/*
+	 * If the handle is broken, pretend that the mutex was deleted
+	 * to signal an error.
+	 */
+	if (owner == NULL) {
+		xnlock_put_irqrestore(&nklock, flags);
+		return T_RMID;
+	}
+
+	/*
+	 * This is the contended path. We just detected an earlier
+	 * syscall-less fast locking, fix up the state information
+	 * accordingly.
+	 *
+	 * The consistency of such information is guaranteed, because
+	 * we just raised the claim bit atomically for this contended
+	 * lock, therefore userland will have to jump to the kernel
+	 * when releasing it, instead of doing a fast unlock. Since we
+	 * currently own the superlock, consistency wrt
+	 * transfer_ownership() is guaranteed through serialization.
+	 *
+	 * CAUTION: in this particular case, the only assumptions we
+	 * can safely make is that *owner is valid but not current on
+	 * this CPU.
+	 */
+	track_owner(mutex, owner);
+	detect_inband_owner(mutex, curr);
+
+	if (curr->wprio > owner->wprio) {
+		if ((owner->info & T_WAKEN) && owner->wwake == &mutex->wchan) {
+			/* Ownership is still pending, steal the resource. */
+			set_current_owner_locked(mutex, curr);
+			owner->info |= T_ROBBED;
+			ret = 0;
+			goto grab;
+		}
+
+		list_add_priff(curr, &mutex->wait_list, wprio, wait_next);
+
+		if (mutex->flags & EVL_MUTEX_PI) {
+			raise_boost_flag(owner);
+
+			if (mutex->flags & EVL_MUTEX_CLAIMED)
+				list_del(&mutex->next); /* owner->boosters */
+			else
+				mutex->flags |= EVL_MUTEX_CLAIMED;
+
+			mutex->wprio = curr->wprio;
+			list_add_priff(mutex, &owner->boosters, wprio, next);
+			/*
+			 * curr->wprio > owner->wprio implies that
+			 * mutex must be leading the booster list
+			 * after insertion, so we may call
+			 * inherit_thread_priority() for tracking
+			 * current's priority directly without going
+			 * through adjust_boost().
+			 */
+			inherit_thread_priority(owner, curr);
+		}
+	} else
+		list_add_priff(curr, &mutex->wait_list, wprio, wait_next);
+
+	evl_block_thread_timeout(curr, T_PEND, timeout, timeout_mode,
+				mutex->clock, &mutex->wchan);
+	evl_schedule();
+	ret = curr->info & (T_RMID|T_TIMEO|T_BREAK);
+	curr->wwake = NULL;
+	curr->info &= ~T_WAKEN;
+
+	if (ret)
+		goto out;
+
+	if (curr->info & T_ROBBED) {
+		/*
+		 * Somebody stole us the ownership while we were ready
+		 * to run, waiting for the CPU: we need to wait again
+		 * for the resource.
+		 */
+		if (timeout_mode != EVL_REL || timeout_infinite(timeout)) {
+			xnlock_put_irqrestore(&nklock, flags);
+			goto redo;
+		}
+		timeout = evl_get_stopped_timer_delta(&curr->rtimer);
+		if (timeout) { /* Otherwise, it's too late. */
+			xnlock_put_irqrestore(&nklock, flags);
+			goto redo;
+		}
+		ret = T_TIMEO;
+		goto out;
 	}
+grab:
+	disable_inband_switch(curr);
+
+	if (!list_empty(&mutex->wait_list)) /* any waiters? */
+		currh = mutex_fast_claim(currh);
+
+	/* Set new ownership. */
+	atomic_set(lockp, get_owner_handle(currh, mutex));
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	evl_put_element(&owner->element);
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(evl_lock_mutex_timeout);
+
+/* nklock held, irqs off */
+static void transfer_ownership(struct evl_mutex *mutex,
+			struct evl_thread *lastowner)
+{
+	struct evl_thread *n_owner;
+	fundle_t n_ownerh;
+	atomic_t *lockp;
+
+	lockp = mutex->fastlock;
+
+	/*
+	 * Our caller checked for contention locklessly, so we do have
+	 * to check again under lock in a different way.
+	 */
+	if (list_empty(&mutex->wait_list)) {
+		mutex->owner = NULL;
+		atomic_set(lockp, EVL_NO_HANDLE);
+		return;
+	}
+
+	n_owner = list_first_entry(&mutex->wait_list, struct evl_thread, wait_next);
+	list_del(&n_owner->wait_next);
+	n_owner->wchan = NULL;
+	n_owner->wwake = &mutex->wchan;
+	set_current_owner_locked(mutex, n_owner);
+	n_owner->info |= T_WAKEN;
+	evl_resume_thread(n_owner, T_PEND);
+
+	if (mutex->flags & EVL_MUTEX_CLAIMED)
+		clear_pi_boost(mutex, lastowner);
+
+	n_ownerh = get_owner_handle(fundle_of(n_owner), mutex);
+	if (!list_empty(&mutex->wait_list)) /* any waiters? */
+		n_ownerh = mutex_fast_claim(n_ownerh);
+
+	atomic_set(lockp, n_ownerh);
+}
+
+void __evl_unlock_mutex(struct evl_mutex *mutex)
+{
+	struct evl_thread *curr = evl_current_thread();
+	unsigned long flags;
+	fundle_t currh, h;
+	atomic_t *lockp;
+
+	trace_evl_mutex_unlock(mutex);
+
+	if (!enable_inband_switch(curr))
+		return;
+
+	lockp = mutex->fastlock;
+	currh = fundle_of(curr);
+	/*
+	 * FLCEIL may only be raised by the owner, or when the owner
+	 * is blocked waiting for the mutex (ownership transfer). In
+	 * addition, only the current owner of a mutex may release it,
+	 * therefore we can't race while testing FLCEIL locklessly.
+	 * All updates to FLCLAIM are covered by the superlock.
+	 *
+	 * Therefore, clearing the fastlock racelessly in this routine
+	 * without leaking FLCEIL/FLCLAIM updates can be achieved
+	 * locklessly.
+	 */
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (mutex->flags & EVL_MUTEX_CEILING)
+		clear_pp_boost(mutex, curr);
+
+	h = atomic_cmpxchg(lockp, currh, EVL_NO_HANDLE);
+	if ((h & ~EVL_MUTEX_FLCEIL) != currh)
+		/* FLCLAIM set, mutex is contended. */
+		transfer_ownership(mutex, curr);
+	else if (h != currh)	/* FLCEIL set, FLCLAIM clear. */
+		atomic_set(lockp, EVL_NO_HANDLE);
+
+	xnlock_put_irqrestore(&nklock, flags);
+}
 
-int evl_lock_timeout(struct evl_mutex *mutex, ktime_t timeout)
+void evl_unlock_mutex(struct evl_mutex *mutex)
 {
-	return lock_timeout(mutex, timeout, EVL_ABS);
+	__evl_unlock_mutex(mutex);
+	evl_schedule();
 }
-EXPORT_SYMBOL_GPL(evl_lock_timeout);
+EXPORT_SYMBOL_GPL(evl_unlock_mutex);
 
-int evl_lock(struct evl_mutex *mutex)
+static inline struct evl_mutex *
+wchan_to_mutex(struct evl_wait_channel *wchan)
 {
-	return lock_timeout(mutex, EVL_INFINITE, EVL_REL);
+	return container_of(wchan, struct evl_mutex, wchan);
 }
-EXPORT_SYMBOL_GPL(evl_lock);
 
-void evl_unlock(struct evl_mutex *mutex)
+/* nklock held, irqs off */
+void evl_abort_mutex_wait(struct evl_thread *thread)
 {
-	if (evl_release_syn(&mutex->wait_queue))
-		evl_schedule();
+	struct evl_mutex *mutex = wchan_to_mutex(thread->wchan);
+	struct evl_thread *owner, *target;
+
+	/*
+	 * Do all the necessary housekeeping chores to stop a thread
+	 * from waiting on a mutex. Doing so may require to update a
+	 * PI chain.
+	 */
+	thread->state &= ~T_PEND;
+	thread->wchan = NULL;
+	list_del(&thread->wait_next); /* mutex->wait_list */
+
+	/*
+	 * Only a waiter leaving a PI chain triggers an update.
+	 * NOTE: PP mutexes never bear the CLAIMED bit.
+	 */
+	if (!(mutex->flags & EVL_MUTEX_CLAIMED))
+		return;
+
+	owner = mutex->owner;
+
+	if (list_empty(&mutex->wait_list)) {
+		/* No more waiters: clear the PI boost. */
+		clear_pi_boost(mutex, owner);
+		return;
+	}
+
+	/*
+	 * Reorder the booster queue of the current owner after we
+	 * left the wait list, then set its priority to the new
+	 * required minimum required to prevent priority inversion.
+	 */
+	target = list_first_entry(&mutex->wait_list,
+				struct evl_thread, wait_next);
+	mutex->wprio = target->wprio;
+	list_del(&mutex->next);	/* owner->boosters */
+	list_add_priff(mutex, &owner->boosters, wprio, next);
+	adjust_boost(owner, target);
+}
+
+/* nklock held, irqs off */
+void evl_reorder_mutex_wait(struct evl_thread *thread)
+{
+	struct evl_mutex *mutex = wchan_to_mutex(thread->wchan);
+	struct evl_thread *owner;
+
+	/*
+	 * Update the position in the wait list of a thread waiting
+	 * for a lock. This routine propagates the change throughout
+	 * the PI chain if required.
+	 */
+	list_del(&thread->wait_next);
+	list_add_priff(thread, &mutex->wait_list, wprio, wait_next);
+
+	if (!(mutex->flags & EVL_MUTEX_PI))
+		return;
+
+	/* Update the PI chain. */
+
+	owner = mutex->owner;
+	mutex->wprio = thread->wprio;
+	if (mutex->flags & EVL_MUTEX_CLAIMED)
+		list_del(&mutex->next);
+	else {
+		mutex->flags |= EVL_MUTEX_CLAIMED;
+		raise_boost_flag(owner);
+	}
+
+	list_add_priff(mutex, &owner->boosters, wprio, next);
+	adjust_boost(owner, thread);
+}
+
+void evl_commit_mutex_ceiling(struct evl_mutex *mutex)
+{
+	struct evl_thread *curr = evl_current_thread();
+	fundle_t oldh, h;
+	atomic_t *lockp;
+
+	/*
+	 * For PP locks, userland does, in that order:
+	 *
+	 * -- LOCK
+	 * 1. curr->u_window->pp_pending = fundle_of(mutex)
+	 *    barrier();
+	 * 2. atomic_cmpxchg(lockp, EVL_NO_HANDLE, fundle_of(curr));
+	 *
+	 * -- UNLOCK
+	 * 1. atomic_cmpxchg(lockp, fundle_of(curr), EVL_NO_HANDLE); [unclaimed]
+	 *    barrier();
+	 * 2. curr->u_window->pp_pending = EVL_NO_HANDLE
+	 *
+	 * Make sure we have not been caught in a rescheduling in
+	 * between those steps. If we did, then we won't be holding
+	 * the lock as we schedule away, therefore no priority update
+	 * must take place.
+	 *
+	 * Hypothetical case: we might be called multiple times for
+	 * committing a lazy ceiling for the same mutex, e.g. if
+	 * userland is preempted in the middle of a recursive locking
+	 * sequence. Since this mutex implementation does not support
+	 * recursive locking on purpose, what has just been described
+	 * cannot happen yet though.
+	 *
+	 * This would stem from the fact that userland has to update
+	 * ->pp_pending prior to trying to grab the lock atomically,
+	 * at which point it can figure out whether a recursive
+	 * locking happened. We get out of this trap by testing the
+	 * EVL_MUTEX_CEILING flag.
+	 */
+	if (!evl_is_mutex_owner(mutex->fastlock, fundle_of(curr)) ||
+		(mutex->flags & EVL_MUTEX_CEILING))
+		return;
+
+	track_owner(mutex, curr);
+	ceil_owner_priority(mutex);
+	/*
+	 * Raise FLCEIL, which indicates a kernel entry will be
+	 * required for releasing this resource.
+	 */
+	lockp = mutex->fastlock;
+	do {
+		h = atomic_read(lockp);
+		oldh = atomic_cmpxchg(lockp, h, mutex_fast_ceil(h));
+	} while (oldh != h);
 }
-EXPORT_SYMBOL_GPL(evl_unlock);
diff --git a/kernel/evenless/poller.c b/kernel/evenless/poller.c
index 543282a924a..f0820531f00 100644
--- a/kernel/evenless/poller.c
+++ b/kernel/evenless/poller.c
@@ -21,7 +21,7 @@
 struct event_poller {
 	struct rb_root node_index;  /* struct poll_node */
 	struct list_head node_list;  /* struct poll_node */
-	struct evl_syn wait_queue;
+	struct evl_wait_queue wait_queue;
 	struct evl_element element;
 	hard_spinlock_t lock;
 	int nodenr;
@@ -51,7 +51,7 @@ struct evl_poll_watchpoint {
 };
 
 void evl_poll_watch(struct evl_poll_head *head,
-		    struct oob_poll_wait *wait)
+		struct oob_poll_wait *wait)
 {
 	struct evl_poll_watchpoint *wpt;
 	unsigned long flags;
@@ -66,7 +66,7 @@ void evl_poll_watch(struct evl_poll_head *head,
 EXPORT_SYMBOL_GPL(evl_poll_watch);
 
 void __evl_signal_poll_events(struct evl_poll_head *head,
-			      int events)
+			int events)
 {
 	struct evl_poll_watchpoint *wpt, *n;
 	struct event_poller *poller;
@@ -92,7 +92,7 @@ void __evl_signal_poll_events(struct evl_poll_head *head,
 		list_for_each_entry(poller, &wakeup_list, next) {
 			xnlock_get_irqsave(&nklock, flags);
 			poller->readied = true;
-			evl_wake_up_syn(&poller->wait_queue);
+			evl_wake_up_head(&poller->wait_queue);
 			xnlock_put_irqrestore(&nklock, flags);
 		}
 	}
@@ -102,7 +102,7 @@ void __evl_signal_poll_events(struct evl_poll_head *head,
 EXPORT_SYMBOL_GPL(__evl_signal_poll_events);
 
 void __evl_clear_poll_events(struct evl_poll_head *head,
-			     int events)
+			int events)
 {
 	struct evl_poll_watchpoint *wpt;
 	unsigned long flags;
@@ -142,7 +142,7 @@ int __index_node(struct rb_root *root, struct poll_node *node)
 }
 
 static int add_node(struct event_poller *poller,
-		    struct evl_poller_ctlreq *creq)
+		struct evl_poller_ctlreq *creq)
 {
 	struct poll_node *node;
 	unsigned long flags;
@@ -212,7 +212,7 @@ static void __del_node(struct poll_node *node)
 }
 
 static int del_node(struct event_poller *poller,
-		    struct evl_poller_ctlreq *creq)
+		struct evl_poller_ctlreq *creq)
 {
 	struct poll_node *node;
 	unsigned long flags;
@@ -239,7 +239,7 @@ static int del_node(struct event_poller *poller,
 
 static inline
 int mod_node(struct event_poller *poller,
-	     struct evl_poller_ctlreq *creq)
+	struct evl_poller_ctlreq *creq)
 {
 	struct poll_node *node;
 	unsigned long flags;
@@ -262,7 +262,7 @@ int mod_node(struct event_poller *poller,
 
 static inline
 int setup_node(struct event_poller *poller,
-	       struct evl_poller_ctlreq *creq)
+	struct evl_poller_ctlreq *creq)
 {
 	int ret;
 
@@ -282,8 +282,8 @@ int setup_node(struct event_poller *poller,
 }
 
 static int collect_events(struct event_poller *poller,
-			  struct evl_poll_event __user *u_ev,
-			  int maxevents, bool do_poll)
+			struct evl_poll_event __user *u_ev,
+			int maxevents, bool do_poll)
 {
 	struct evl_thread *curr = evl_current_thread();
 	struct evl_poll_watchpoint *wpt, *table;
@@ -414,7 +414,7 @@ int wait_events(struct file *filp,
 		return 0;
 
 	count = collect_events(poller, wreq->events,
-			       wreq->nrevents, true);
+			wreq->nrevents, true);
 	if (count > 0 || count == -EFAULT)
 		goto unwait;
 	if (count < 0)
@@ -432,7 +432,7 @@ int wait_events(struct file *filp,
 
 	info = 0;
 	if (!poller->readied)
-		info = evl_sleep_on_syn(&poller->wait_queue, timeout, tmode);
+		info = evl_wait_timeout(&poller->wait_queue, timeout, tmode);
 
 	xnlock_put_irqrestore(&nklock, flags);
 	if (info)
@@ -440,7 +440,7 @@ int wait_events(struct file *filp,
 		count = info & T_BREAK ? -EINTR : -ETIMEDOUT;
 	else
 		count = collect_events(poller, wreq->events,
-				       wreq->nrevents, false);
+				wreq->nrevents, false);
 unwait:
 	clear_wait();
 
@@ -448,7 +448,7 @@ int wait_events(struct file *filp,
 }
 
 static long poller_oob_ioctl(struct file *filp, unsigned int cmd,
-			     unsigned long arg)
+			unsigned long arg)
 {
 	struct event_poller *poller = element_of(filp, struct event_poller);
 	struct evl_poller_waitreq wreq, __user *u_wreq;
@@ -488,7 +488,7 @@ static const struct file_operations poller_fops = {
 
 static struct evl_element *
 poller_factory_build(struct evl_factory *fac, const char *name,
-		     void __user *u_attrs, u32 *state_offp)
+		void __user *u_attrs, u32 *state_offp)
 {
 	struct evl_poller_attrs attrs;
 	struct event_poller *poller;
@@ -515,7 +515,7 @@ poller_factory_build(struct evl_factory *fac, const char *name,
 
 	poller->node_index = RB_ROOT;
 	INIT_LIST_HEAD(&poller->node_list);
-	evl_init_syn(&poller->wait_queue, EVL_SYN_PRIO, clock, NULL);
+	evl_init_wait(&poller->wait_queue, clock, EVL_WAIT_PRIO);
 	raw_spin_lock_init(&poller->lock);
 
 	return &poller->element;
diff --git a/kernel/evenless/sched/core.c b/kernel/evenless/sched/core.c
index f4a118e3dd9..c5a999a8a6b 100644
--- a/kernel/evenless/sched/core.c
+++ b/kernel/evenless/sched/core.c
@@ -50,7 +50,7 @@ static void register_one_class(struct evl_sched_class *sched_class)
 	 * idle first and up.
 	 */
 	EVL_WARN_ON(CORE, sched_class->next &&
-		    sched_class->next->weight > sched_class->weight);
+		sched_class->next->weight > sched_class->weight);
 }
 
 static void register_classes(void)
@@ -91,11 +91,11 @@ static void watchdog_handler(struct evl_timer *timer) /* hard irqs off */
 
 	if (curr->state & T_USER) {
 		printk(EVL_WARNING "watchdog triggered on CPU #%d -- runaway thread "
-		       "'%s' signaled\n", evl_rq_cpu(this_rq), curr->name);
+			"'%s' signaled\n", evl_rq_cpu(this_rq), curr->name);
 		evl_call_mayday(curr, SIGDEBUG_WATCHDOG);
 	} else {
 		printk(EVL_WARNING "watchdog triggered on CPU #%d -- runaway thread "
-		       "'%s' canceled\n", evl_rq_cpu(this_rq), curr->name);
+			"'%s' canceled\n", evl_rq_cpu(this_rq), curr->name);
 		/*
 		 * On behalf on an IRQ handler, evl_cancel_thread()
 		 * would go half way cancelling the preempted
@@ -168,16 +168,16 @@ static void init_rq(struct evl_rq *rq, int cpu)
 	 * specifically by the generic timer code.
 	 */
 	evl_init_timer(&rq->htimer, &evl_mono_clock, proxy_tick_handler,
-		       rq, EVL_TIMER_IGRAVITY);
+		rq, EVL_TIMER_IGRAVITY);
 	evl_set_timer_priority(&rq->htimer, EVL_TIMER_LOPRIO);
 	evl_set_timer_name(&rq->htimer, rq->proxy_timer_name);
 	evl_init_timer(&rq->rrbtimer, &evl_mono_clock, roundrobin_handler,
-		       rq, EVL_TIMER_IGRAVITY);
+		rq, EVL_TIMER_IGRAVITY);
 	evl_set_timer_name(&rq->rrbtimer, rq->rrb_timer_name);
 	evl_set_timer_priority(&rq->rrbtimer, EVL_TIMER_LOPRIO);
 #ifdef CONFIG_EVENLESS_WATCHDOG
 	evl_init_timer(&rq->wdtimer, &evl_mono_clock, watchdog_handler,
-		       rq, EVL_TIMER_IGRAVITY);
+		rq, EVL_TIMER_IGRAVITY);
 	evl_set_timer_name(&rq->wdtimer, "[watchdog]");
 	evl_set_timer_priority(&rq->wdtimer, EVL_TIMER_LOPRIO);
 #endif /* CONFIG_EVENLESS_WATCHDOG */
@@ -195,9 +195,7 @@ static void init_rq(struct evl_rq *rq, int cpu)
 	iattr.sched_class = &evl_sched_idle;
 	iattr.sched_param.idle.prio = EVL_IDLE_PRIO;
 	evl_init_thread(&rq->root_thread, &iattr, rq, name_fmt, cpu);
-
-	evl_init_syn(&rq->yield_sync, EVL_SYN_FIFO,
-		     &evl_mono_clock, NULL);
+	evl_init_wait(&rq->yield_sync, &evl_mono_clock, EVL_WAIT_FIFO);
 
 	dovetail_init_altsched(&rq->root_thread.altsched);
 
@@ -219,7 +217,7 @@ static void destroy_rq(struct evl_rq *rq) /* nklock held, irqs off */
 }
 
 static inline void set_thread_running(struct evl_rq *rq,
-				      struct evl_thread *thread)
+				struct evl_thread *thread)
 {
 	thread->state &= ~T_READY;
 	if (thread->state & T_RRB)
@@ -305,8 +303,8 @@ void evl_putback_thread(struct evl_thread *thread)
 
 /* nklock locked, interrupts off. */
 int evl_set_thread_policy(struct evl_thread *thread,
-			  struct evl_sched_class *sched_class,
-			  const union evl_sched_param *p)
+			struct evl_sched_class *sched_class,
+			const union evl_sched_param *p)
 {
 	struct evl_sched_class *orig_effective_class __maybe_unused;
 	bool effective;
@@ -385,7 +383,7 @@ int evl_set_thread_policy(struct evl_thread *thread,
 		evl_set_resched(thread->rq);
 	else
 		EVL_WARN_ON(CORE, (thread->state & T_ROOT) &&
-			    sched_class != &evl_sched_idle);
+			sched_class != &evl_sched_idle);
 	return 0;
 }
 EXPORT_SYMBOL_GPL(evl_set_thread_policy);
@@ -418,7 +416,7 @@ bool evl_set_effective_thread_priority(struct evl_thread *thread, int prio)
 
 /* nklock locked, interrupts off. */
 void evl_track_thread_policy(struct evl_thread *thread,
-			     struct evl_thread *target)
+			struct evl_thread *target)
 {
 	union evl_sched_param param;
 
@@ -562,21 +560,21 @@ static struct list_head *add_q(struct evl_multilevel_queue *q, int prio)
 }
 
 void evl_add_schedq(struct evl_multilevel_queue *q,
-		    struct evl_thread *thread)
+		struct evl_thread *thread)
 {
 	struct list_head *head = add_q(q, thread->cprio);
 	list_add(&thread->rq_next, head);
 }
 
 void evl_add_schedq_tail(struct evl_multilevel_queue *q,
-			 struct evl_thread *thread)
+			struct evl_thread *thread)
 {
 	struct list_head *head = add_q(q, thread->cprio);
 	list_add_tail(&thread->rq_next, head);
 }
 
 static void del_q(struct evl_multilevel_queue *q,
-		  struct list_head *entry, int idx)
+		struct list_head *entry, int idx)
 {
 	struct list_head *head = q->heads + idx;
 
@@ -588,7 +586,7 @@ static void del_q(struct evl_multilevel_queue *q,
 }
 
 void evl_del_schedq(struct evl_multilevel_queue *q,
-		    struct evl_thread *thread)
+		struct evl_thread *thread)
 {
 	del_q(q, &thread->rq_next, get_qindex(q, thread->cprio));
 }
@@ -720,7 +718,7 @@ bool __evl_schedule(struct evl_rq *this_rq)
 	 * underlying inband task.
 	 */
 	if (curr->state & T_USER)
-		evl_commit_monitor_ceiling(curr);
+		evl_commit_monitor_ceiling();
 
 	switched = false;
 	if (!test_resched(this_rq))
@@ -795,8 +793,8 @@ EXPORT_SYMBOL_GPL(__evl_schedule);
 
 struct evl_sched_class *
 evl_find_sched_class(union evl_sched_param *param,
-		     const struct evl_sched_attrs *attrs,
-		     ktime_t *tslice_r)
+		const struct evl_sched_attrs *attrs,
+		ktime_t *tslice_r)
 {
 	struct evl_sched_class *sched_class;
 	int prio, policy;
@@ -825,7 +823,7 @@ evl_find_sched_class(union evl_sched_param *param,
 		/* Fallback wanted */
 	case SCHED_WEAK:
 		if (prio < EVL_WEAK_MIN_PRIO ||
-		    prio > EVL_WEAK_MAX_PRIO)
+			prio > EVL_WEAK_MAX_PRIO)
 			return NULL;
 		param->weak.prio = prio;
 		sched_class = &evl_sched_weak;
@@ -838,12 +836,12 @@ evl_find_sched_class(union evl_sched_param *param,
 		/* falldown wanted */
 	case SCHED_FIFO:
 		if (prio < EVL_FIFO_MIN_PRIO ||
-		    prio > EVL_FIFO_MAX_PRIO)
+			prio > EVL_FIFO_MAX_PRIO)
 			return NULL;
 		break;
 	case SCHED_EVL:
 		if (prio < EVL_CORE_MIN_PRIO ||
-		    prio > EVL_CORE_MAX_PRIO)
+			prio > EVL_CORE_MAX_PRIO)
 			return NULL;
 		break;
 #ifdef CONFIG_EVENLESS_SCHED_QUOTA
@@ -871,8 +869,8 @@ void evl_notify_inband_yield(void) /* In-band only */
 
 	this_rq = this_evl_rq();
 
-	if (evl_syn_has_waiter(&this_rq->yield_sync)) {
-		evl_flush_syn(&this_rq->yield_sync, 0);
+	if (evl_wait_active(&this_rq->yield_sync)) {
+		evl_flush_wait(&this_rq->yield_sync, 0);
 		evl_schedule();
 	}
 
@@ -896,12 +894,12 @@ static int yield_inband(void)	/* OOB only */
 	start = evl_read_clock(&evl_mono_clock);
 
 	do {
-		ret = evl_sleep_on_syn(&this_rq->yield_sync,
-				       TICK_NSEC, EVL_ABS);
+		ret = evl_wait_timeout(&this_rq->yield_sync,
+				TICK_NSEC, EVL_ABS);
 		if (ret)
 			break;
 	} while (ktime_before(evl_read_clock(&evl_mono_clock),
-			      TICK_NSEC));
+				TICK_NSEC));
 
 	return ret & T_BREAK ? -EINTR : 0;
 }
@@ -939,15 +937,15 @@ EXPORT_SYMBOL_GPL(evl_sched_yield);
 #ifdef CONFIG_TRACING
 
 const char *evl_trace_sched_attrs(struct trace_seq *p,
-				  struct evl_sched_attrs *attrs)
+				struct evl_sched_attrs *attrs)
 {
 	const char *ret = trace_seq_buffer_ptr(p);
 
 	switch (attrs->sched_policy) {
 	case SCHED_QUOTA:
 		trace_seq_printf(p, "priority=%d, group=%d",
-				 attrs->sched_priority,
-				 attrs->sched_quota_group);
+				attrs->sched_priority,
+				attrs->sched_quota_group);
 		break;
 	case SCHED_NORMAL:
 		break;
@@ -968,7 +966,7 @@ const char *evl_trace_sched_attrs(struct trace_seq *p,
 
 /* in-band stage, hard_irqs_disabled() */
 bool irq_cpuidle_control(struct cpuidle_device *dev,
-			 struct cpuidle_state *state)
+			struct cpuidle_state *state)
 {
 	/*
 	 * Deny entering sleep state if this entails stopping the
@@ -994,10 +992,10 @@ int __init evl_init_sched(void)
 
 	if (IS_ENABLED(CONFIG_SMP)) {
 		ret = __request_percpu_irq(RESCHEDULE_OOB_IPI,
-					   reschedule_interrupt,
-					   IRQF_OOB,
-					   "Evenless reschedule",
-					   &evl_machine_cpudata);
+					reschedule_interrupt,
+					IRQF_OOB,
+					"Evenless reschedule",
+					&evl_machine_cpudata);
 		if (ret)
 			goto cleanup_rq;
 	}
diff --git a/kernel/evenless/sched/idle.c b/kernel/evenless/sched/idle.c
index 78e61db5d63..489a6528628 100644
--- a/kernel/evenless/sched/idle.c
+++ b/kernel/evenless/sched/idle.c
@@ -13,19 +13,19 @@ static struct evl_thread *evl_idle_pick(struct evl_rq *rq)
 }
 
 static bool evl_idle_setparam(struct evl_thread *thread,
-			      const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	return __evl_set_idle_schedparam(thread, p);
 }
 
 static void evl_idle_getparam(struct evl_thread *thread,
-			      union evl_sched_param *p)
+			union evl_sched_param *p)
 {
 	__evl_get_idle_schedparam(thread, p);
 }
 
 static void evl_idle_trackprio(struct evl_thread *thread,
-			       const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	__evl_track_idle_priority(thread, p);
 }
diff --git a/kernel/evenless/sched/quota.c b/kernel/evenless/sched/quota.c
index fff24cba77c..aeda2fe1666 100644
--- a/kernel/evenless/sched/quota.c
+++ b/kernel/evenless/sched/quota.c
@@ -68,14 +68,14 @@ static inline int group_is_active(struct evl_quota_group *tg)
 	 * accounts for it).
 	 */
 	if (curr->quota == tg &&
-	    (curr->state & (T_READY|EVL_THREAD_BLOCK_BITS)) == 0)
+		(curr->state & (T_READY|EVL_THREAD_BLOCK_BITS)) == 0)
 		return 1;
 
 	return 0;
 }
 
 static inline void replenish_budget(struct evl_sched_quota *qs,
-				    struct evl_quota_group *tg)
+				struct evl_quota_group *tg)
 {
 	ktime_t budget, credit;
 
@@ -118,7 +118,7 @@ static inline void replenish_budget(struct evl_sched_quota *qs,
 		/* Too much budget, spread it over intervals. */
 		tg->run_credit =
 			ktime_add(tg->run_credit,
-				  ktime_sub(budget, tg->quota_peak));
+				ktime_sub(budget, tg->quota_peak));
 		tg->run_budget = tg->quota_peak;
 	} else if (tg->run_credit) {
 		credit = ktime_sub(tg->quota_peak, budget);
@@ -213,18 +213,18 @@ static void quota_init(struct evl_rq *rq)
 	INIT_LIST_HEAD(&qs->groups);
 
 	evl_init_timer(&qs->refill_timer,
-		       &evl_mono_clock, quota_refill_handler, rq,
-		       EVL_TIMER_IGRAVITY);
+		&evl_mono_clock, quota_refill_handler, rq,
+		EVL_TIMER_IGRAVITY);
 	evl_set_timer_name(&qs->refill_timer, "[quota-refill]");
 
 	evl_init_timer(&qs->limit_timer,
-		       &evl_mono_clock, quota_limit_handler, rq,
-		       EVL_TIMER_IGRAVITY);
+		&evl_mono_clock, quota_limit_handler, rq,
+		EVL_TIMER_IGRAVITY);
 	evl_set_timer_name(&qs->limit_timer, "[quota-limit]");
 }
 
 static bool quota_setparam(struct evl_thread *thread,
-			   const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	struct evl_quota_group *tg;
 	struct evl_sched_quota *qs;
@@ -252,20 +252,20 @@ static bool quota_setparam(struct evl_thread *thread,
 }
 
 static void quota_getparam(struct evl_thread *thread,
-			   union evl_sched_param *p)
+			union evl_sched_param *p)
 {
 	p->quota.prio = thread->cprio;
 	p->quota.tgid = thread->quota->tgid;
 }
 
 static void quota_trackprio(struct evl_thread *thread,
-			    const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	if (p) {
 		/* We should not cross groups during PI boost. */
 		EVL_WARN_ON(CORE,
-			    thread->base_class == &evl_sched_quota &&
-			    thread->quota->tgid != p->quota.tgid);
+			thread->base_class == &evl_sched_quota &&
+			thread->quota->tgid != p->quota.tgid);
 		thread->cprio = p->quota.prio;
 	} else
 		thread->cprio = thread->bprio;
@@ -280,14 +280,14 @@ static void quota_ceilprio(struct evl_thread *thread, int prio)
 }
 
 static int quota_chkparam(struct evl_thread *thread,
-			  const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	struct evl_quota_group *tg;
 	struct evl_sched_quota *qs;
 	int tgid;
 
 	if (p->quota.prio < EVL_QUOTA_MIN_PRIO ||
-	    p->quota.prio > EVL_QUOTA_MAX_PRIO)
+		p->quota.prio > EVL_QUOTA_MAX_PRIO)
 		return -EINVAL;
 
 	tgid = p->quota.tgid;
@@ -462,15 +462,15 @@ static void quota_migrate(struct evl_thread *thread, struct evl_rq *rq)
 }
 
 static ssize_t quota_show(struct evl_thread *thread,
-			  char *buf, ssize_t count)
+			char *buf, ssize_t count)
 {
 	return snprintf(buf, count, "%d\n",
 			thread->quota->tgid);
 }
 
 int evl_quota_create_group(struct evl_quota_group *tg,
-			   struct evl_rq *rq,
-			   int *quota_sum_r)
+			struct evl_rq *rq,
+			int *quota_sum_r)
 {
 	int tgid, nr_groups = CONFIG_EVENLESS_SCHED_QUOTA_NR_GROUPS;
 	struct evl_sched_quota *qs = &rq->quota;
@@ -508,7 +508,7 @@ int evl_quota_create_group(struct evl_quota_group *tg,
 EXPORT_SYMBOL_GPL(evl_quota_create_group);
 
 int evl_quota_destroy_group(struct evl_quota_group *tg,
-			    int force, int *quota_sum_r)
+			int force, int *quota_sum_r)
 {
 	struct evl_sched_quota *qs = &tg->rq->quota;
 	struct evl_thread *thread, *tmp;
@@ -539,8 +539,8 @@ int evl_quota_destroy_group(struct evl_quota_group *tg,
 EXPORT_SYMBOL_GPL(evl_quota_destroy_group);
 
 void evl_quota_set_limit(struct evl_quota_group *tg,
-			 int quota_percent, int quota_peak_percent,
-			 int *quota_sum_r)
+			int quota_percent, int quota_peak_percent,
+			int *quota_sum_r)
 {
 	struct evl_sched_quota *qs = &tg->rq->quota;
 	ktime_t now, elapsed, consumed;
diff --git a/kernel/evenless/sched/rt.c b/kernel/evenless/sched/rt.c
index 922501805d3..4f727a3be9d 100644
--- a/kernel/evenless/sched/rt.c
+++ b/kernel/evenless/sched/rt.c
@@ -39,7 +39,7 @@ static void evl_rt_dequeue(struct evl_thread *thread)
 }
 
 static void evl_rt_rotate(struct evl_rq *rq,
-			  const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	struct evl_thread *thread, *curr;
 
@@ -58,8 +58,8 @@ static void evl_rt_rotate(struct evl_rq *rq,
 	 * holds the scheduler lock.
 	 */
 	if (thread != curr ||
-	    (!(curr->state & EVL_THREAD_BLOCK_BITS) &&
-	     evl_preempt_count() == 0))
+		(!(curr->state & EVL_THREAD_BLOCK_BITS) &&
+			evl_preempt_count() == 0))
 		evl_putback_thread(thread);
 }
 
@@ -76,25 +76,25 @@ static void evl_rt_tick(struct evl_rq *rq)
 }
 
 static int evl_rt_chkparam(struct evl_thread *thread,
-			   const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	return __evl_chk_rt_schedparam(thread, p);
 }
 
 static bool evl_rt_setparam(struct evl_thread *thread,
-			    const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	return __evl_set_rt_schedparam(thread, p);
 }
 
 static void evl_rt_getparam(struct evl_thread *thread,
-			    union evl_sched_param *p)
+			union evl_sched_param *p)
 {
 	__evl_get_rt_schedparam(thread, p);
 }
 
 static void evl_rt_trackprio(struct evl_thread *thread,
-			     const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	__evl_track_rt_priority(thread, p);
 }
@@ -105,7 +105,7 @@ static void evl_rt_ceilprio(struct evl_thread *thread, int prio)
 }
 
 static ssize_t evl_rt_show(struct evl_thread *thread,
-			   char *buf, ssize_t count)
+			char *buf, ssize_t count)
 {
 	if (thread->state & T_RRB)
 		return snprintf(buf, count, "%Ld\n",
diff --git a/kernel/evenless/sched/weak.c b/kernel/evenless/sched/weak.c
index a8d94daa869..83da85783ad 100644
--- a/kernel/evenless/sched/weak.c
+++ b/kernel/evenless/sched/weak.c
@@ -34,17 +34,17 @@ static struct evl_thread *weak_pick(struct evl_rq *rq)
 }
 
 static int weak_chkparam(struct evl_thread *thread,
-			 const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	if (p->weak.prio < EVL_WEAK_MIN_PRIO ||
-	    p->weak.prio > EVL_WEAK_MAX_PRIO)
+		p->weak.prio > EVL_WEAK_MAX_PRIO)
 		return -EINVAL;
 
 	return 0;
 }
 
 static bool weak_setparam(struct evl_thread *thread,
-			  const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	if (!(thread->state & T_BOOST))
 		thread->state |= T_WEAK;
@@ -53,13 +53,13 @@ static bool weak_setparam(struct evl_thread *thread,
 }
 
 static void weak_getparam(struct evl_thread *thread,
-			  union evl_sched_param *p)
+			union evl_sched_param *p)
 {
 	p->weak.prio = thread->cprio;
 }
 
 static void weak_trackprio(struct evl_thread *thread,
-			   const union evl_sched_param *p)
+			const union evl_sched_param *p)
 {
 	if (p)
 		thread->cprio = p->weak.prio;
@@ -79,7 +79,7 @@ static int weak_declare(struct evl_thread *thread,
 			const union evl_sched_param *p)
 {
 	if (p->weak.prio < EVL_WEAK_MIN_PRIO ||
-	    p->weak.prio > EVL_WEAK_MAX_PRIO)
+		p->weak.prio > EVL_WEAK_MAX_PRIO)
 		return -EINVAL;
 
 	return 0;
diff --git a/kernel/evenless/sem.c b/kernel/evenless/sem.c
index 53722c0edca..f3488d7cc88 100644
--- a/kernel/evenless/sem.c
+++ b/kernel/evenless/sem.c
@@ -8,7 +8,7 @@
 #include <linux/list.h>
 #include <linux/slab.h>
 #include <linux/kernel.h>
-#include <evenless/synch.h>
+#include <evenless/wait.h>
 #include <evenless/thread.h>
 #include <evenless/clock.h>
 #include <evenless/sem.h>
@@ -22,7 +22,8 @@
 struct evl_sem {
 	struct evl_element element;
 	struct evl_sem_state *state;
-	struct evl_syn wait_queue;
+	struct evl_wait_queue wait_queue;
+	int initval;
 };
 
 struct sem_wait_data {
@@ -30,12 +31,13 @@ struct sem_wait_data {
 };
 
 static int acquire_sem(struct evl_sem *sem,
-		       struct evl_sem_waitreq *req)
+		struct evl_sem_waitreq *req)
 {
 	struct evl_thread *curr = evl_current_thread();
 	struct evl_sem_state *state = sem->state;
 	struct sem_wait_data wda;
 	enum evl_tmode tmode;
+	unsigned long flags;
 	int info, ret = 0;
 	ktime_t timeout;
 
@@ -48,15 +50,17 @@ static int acquire_sem(struct evl_sem *sem,
 	if (state->flags & EVL_SEM_PULSE)
 		req->count = 1;
 
+	xnlock_get_irqsave(&nklock, flags);
+
 	if (atomic_sub_return(req->count, &state->value) >= 0)
-		return 0;
+		goto out;
 
 	wda.count = req->count;
 	curr->wait_data = &wda;
 
 	timeout = timespec_to_ktime(req->timeout);
 	tmode = timeout ? EVL_ABS : EVL_REL;
-	info = evl_sleep_on_syn(&sem->wait_queue, timeout, tmode);
+	info = evl_wait_timeout(&sem->wait_queue, timeout, tmode);
 	if (info & (T_BREAK|T_BCAST|T_TIMEO)) {
 		atomic_add(req->count, &state->value);
 		ret = -ETIMEDOUT;
@@ -65,6 +69,8 @@ static int acquire_sem(struct evl_sem *sem,
 		else if (info & T_BCAST)
 			ret = -EAGAIN;
 	} /* No way we could receive T_RMID */
+out:
+	xnlock_put_irqrestore(&nklock, flags);
 
 	return ret;
 }
@@ -87,16 +93,16 @@ static int release_sem(struct evl_sem *sem, int count)
 	if (state->flags & EVL_SEM_PULSE)
 		count = 1;
 
+	xnlock_get_irqsave(&nklock, flags);
+
 	if (atomic_add_return(count, &state->value) >= count) {
 		/* Old value >= 0, nobody is waiting. */
 		if (state->flags & EVL_SEM_PULSE)
 			atomic_set(&state->value, 0);
-		return 0;
+		goto out;
 	}
 
-	xnlock_get_irqsave(&nklock, flags);
-
-	if (!evl_syn_has_waiter(&sem->wait_queue))
+	if (!evl_wait_active(&sem->wait_queue))
 		goto out;
 
 	/*
@@ -104,13 +110,13 @@ static int release_sem(struct evl_sem *sem, int count)
 	 * serve other waiters down the queue until this one is
 	 * satisfied.
 	 */
-	evl_for_each_syn_waiter_safe(waiter, n, &sem->wait_queue) {
+	evl_for_each_waiter_safe(waiter, n, &sem->wait_queue) {
 		wda = waiter->wait_data;
 		if (atomic_sub_return(wda->count, &state->value) < 0) {
 			atomic_add(wda->count, &state->value); /* Nope, undo. */
 			break;
 		}
-		evl_wake_up_targeted_syn(&sem->wait_queue, waiter);
+		evl_wake_up(&sem->wait_queue, waiter);
 	}
 
 	evl_schedule();
@@ -122,14 +128,14 @@ static int release_sem(struct evl_sem *sem, int count)
 
 static int broadcast_sem(struct evl_sem *sem)
 {
-	if (evl_flush_syn(&sem->wait_queue, T_BCAST))
-		evl_schedule();
+	evl_flush_wait(&sem->wait_queue, T_BCAST);
+	evl_schedule();
 
 	return 0;
 }
 
 static long sem_common_ioctl(struct evl_sem *sem,
-			     unsigned int cmd, unsigned long arg)
+			unsigned int cmd, unsigned long arg)
 {
 	__s32 count;
 	long ret;
@@ -152,7 +158,7 @@ static long sem_common_ioctl(struct evl_sem *sem,
 }
 
 static long sem_oob_ioctl(struct file *filp, unsigned int cmd,
-			  unsigned long arg)
+			unsigned long arg)
 {
 	struct evl_sem *sem = element_of(filp, struct evl_sem);
 	struct evl_sem_waitreq wreq, __user *u_wreq;
@@ -174,7 +180,7 @@ static long sem_oob_ioctl(struct file *filp, unsigned int cmd,
 }
 
 static long sem_ioctl(struct file *filp, unsigned int cmd,
-		      unsigned long arg)
+		unsigned long arg)
 {
 	struct evl_sem *sem = element_of(filp, struct evl_sem);
 	struct evl_element_ids eids, __user *u_eids;
@@ -199,7 +205,7 @@ static const struct file_operations sem_fops = {
 
 static struct evl_element *
 sem_factory_build(struct evl_factory *fac, const char *name,
-		  void __user *u_attrs, u32 *state_offp)
+		void __user *u_attrs, u32 *state_offp)
 {
 	struct evl_sem_state *state;
 	struct evl_sem_attrs attrs;
@@ -241,12 +247,13 @@ sem_factory_build(struct evl_factory *fac, const char *name,
 	}
 
 	if (attrs.flags & EVL_SEM_PRIO)
-		synflags |= EVL_SYN_PRIO;
+		synflags |= EVL_WAIT_PRIO;
 
-	evl_init_syn(&sem->wait_queue, synflags, clock, NULL);
+	evl_init_wait(&sem->wait_queue, clock, synflags);
 	atomic_set(&state->value, attrs.initval);
 	state->flags = attrs.flags;
 	sem->state = state;
+	sem->initval = attrs.initval;
 
 	*state_offp = evl_shared_offset(state);
 	evl_index_element(&sem->element);
@@ -271,22 +278,22 @@ static void sem_factory_dispose(struct evl_element *e)
 
 	evl_unindex_element(&sem->element);
 	evl_put_clock(sem->wait_queue.clock);
-	evl_destroy_syn(&sem->wait_queue);
+	evl_destroy_wait(&sem->wait_queue);
 	evl_free_chunk(&evl_shared_heap, sem->state);
 	evl_destroy_element(&sem->element);
 	kfree_rcu(sem, element.rcu);
 }
 
 static ssize_t value_show(struct device *dev,
-			  struct device_attribute *attr,
-			  char *buf)
+			struct device_attribute *attr,
+			char *buf)
 {
 	struct evl_sem *sem;
 	ssize_t ret;
 
 	sem = evl_get_element_by_dev(dev, struct evl_sem);
 	ret = snprintf(buf, PAGE_SIZE, "%d\n",
-		       atomic_read(&sem->state->value));
+		atomic_read(&sem->state->value));
 	evl_put_element(&sem->element);
 
 	return ret;
diff --git a/kernel/evenless/synch.c b/kernel/evenless/synch.c
deleted file mode 100644
index a5895d48e76..00000000000
--- a/kernel/evenless/synch.c
+++ /dev/null
@@ -1,898 +0,0 @@
-/*
- * SPDX-License-Identifier: GPL-2.0
- *
- * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
- * Copyright (C) 2001, 2018 Philippe Gerum  <rpm@xenomai.org>
- */
-
-#include <stdarg.h>
-#include <linux/signal.h>
-#include <linux/kernel.h>
-#include <evenless/sched.h>
-#include <evenless/synch.h>
-#include <evenless/thread.h>
-#include <evenless/monitor.h>
-#include <evenless/clock.h>
-#include <uapi/evenless/signal.h>
-#include <trace/events/evenless.h>
-
-static inline int get_ceiling_value(struct evl_syn *synch)
-{
-	/*
-	 * The ceiling priority value is stored in user-writable
-	 * memory, make sure to constrain it within valid bounds for
-	 * evl_sched_rt before using it.
-	 */
-	return clamp(*synch->ceiling_ref, 1U, (u32)EVL_CORE_MAX_PRIO);
-}
-
-static inline void get_thread_resource(struct evl_thread *curr)
-{
-	/*
-	 * Track resource locking depth, to prevent weak threads from
-	 * being switched back to in-band mode on return from OOB
-	 * syscalls.
-	 */
-	if (curr->state & (T_WEAK|T_DEBUG))
-		curr->res_count++;
-}
-
-static inline bool put_thread_resource(struct evl_thread *curr)
-{
-	if ((curr->state & T_WEAK) ||
-	    IS_ENABLED(CONFIG_EVENLESS_DEBUG_MONITOR_SLEEP)) {
-		if (unlikely(curr->res_count == 0)) {
-			if (curr->state & T_WARN)
-				evl_signal_thread(curr, SIGDEBUG,
-						  SIGDEBUG_RESCNT_IMBALANCE);
-			return false;
-		}
-		curr->res_count--;
-	}
-
-	return true;
-}
-
-void evl_init_syn(struct evl_syn *synch, int flags,
-		  struct evl_clock *clock, atomic_t *fastlock)
-{
-	if (flags & (EVL_SYN_PI|EVL_SYN_PP))
-		flags |= EVL_SYN_PRIO | EVL_SYN_OWNER;
-
-	synch->status = flags & ~EVL_SYN_CLAIMED;
-	synch->owner = NULL;
-	synch->wprio = -1;
-	synch->ceiling_ref = NULL;
-	synch->clock = clock;
-	INIT_LIST_HEAD(&synch->wait_list);
-
-	if (flags & EVL_SYN_OWNER) {
-		synch->fastlock = fastlock;
-		atomic_set(fastlock, EVL_NO_HANDLE);
-	} else
-		synch->fastlock = NULL;
-}
-EXPORT_SYMBOL_GPL(evl_init_syn);
-
-void evl_init_syn_protect(struct evl_syn *synch,
-			  struct evl_clock *clock,
-			  atomic_t *fastlock, u32 *ceiling_ref)
-{
-	evl_init_syn(synch, EVL_SYN_PP, clock, fastlock);
-	synch->ceiling_ref = ceiling_ref;
-}
-
-bool evl_destroy_syn(struct evl_syn *synch)
-{
-	bool ret;
-
-	ret = evl_flush_syn(synch, T_RMID);
-	EVL_WARN_ON(CORE, synch->status & EVL_SYN_CLAIMED);
-
-	return ret;
-}
-EXPORT_SYMBOL_GPL(evl_destroy_syn);
-
-static inline
-int block_thread_timed(ktime_t timeout, enum evl_tmode timeout_mode,
-		       struct evl_clock *clock,
-		       struct evl_syn *wchan)
-{
-	struct evl_thread *curr = evl_current_thread();
-
-	evl_block_thread_timeout(curr, T_PEND, timeout, timeout_mode,
-				 clock, wchan);
-	evl_schedule();
-
-	return curr->info & (T_RMID|T_TIMEO|T_BREAK);
-}
-
-int evl_sleep_on_syn(struct evl_syn *synch, ktime_t timeout,
-		     enum evl_tmode timeout_mode)
-{
-	struct evl_thread *curr;
-	unsigned long flags;
-	int ret;
-
-	oob_context_only();
-
-	if (EVL_WARN_ON(CORE, synch->status & EVL_SYN_OWNER))
-		return T_BREAK;
-
-	curr = evl_current_thread();
-
-	if (IS_ENABLED(CONFIG_EVENLESS_DEBUG_MONITOR_SLEEP) &&
-	    curr->res_count > 0 && (curr->state & T_WARN))
-		evl_signal_thread(curr, SIGDEBUG, SIGDEBUG_MONITOR_SLEEP);
-
-	xnlock_get_irqsave(&nklock, flags);
-
-	trace_evl_synch_sleepon(synch);
-
-	if (!(synch->status & EVL_SYN_PRIO)) /* i.e. FIFO */
-		list_add_tail(&curr->syn_next, &synch->wait_list);
-	else /* i.e. priority-sorted */
-		list_add_priff(curr, &synch->wait_list, wprio, syn_next);
-
-	ret = block_thread_timed(timeout, timeout_mode, synch->clock, synch);
-
-	xnlock_put_irqrestore(&nklock, flags);
-
-	return ret;
-}
-EXPORT_SYMBOL_GPL(evl_sleep_on_syn);
-
-struct evl_thread *evl_wake_up_syn(struct evl_syn *synch)
-{
-	struct evl_thread *thread;
-	unsigned long flags;
-
-	if (EVL_WARN_ON(CORE, synch->status & EVL_SYN_OWNER))
-		return NULL;
-
-	xnlock_get_irqsave(&nklock, flags);
-
-	if (list_empty(&synch->wait_list)) {
-		thread = NULL;
-		goto out;
-	}
-
-	trace_evl_synch_wakeup(synch);
-	thread = list_first_entry(&synch->wait_list, struct evl_thread, syn_next);
-	list_del(&thread->syn_next);
-	thread->wchan = NULL;
-	evl_resume_thread(thread, T_PEND);
-out:
-	xnlock_put_irqrestore(&nklock, flags);
-
-	return thread;
-}
-EXPORT_SYMBOL_GPL(evl_wake_up_syn);
-
-int evl_wake_up_nr_syn(struct evl_syn *synch, int nr)
-{
-	struct evl_thread *thread, *tmp;
-	unsigned long flags;
-	int nwakeups = 0;
-
-	if (EVL_WARN_ON(CORE, synch->status & EVL_SYN_OWNER))
-		return 0;
-
-	xnlock_get_irqsave(&nklock, flags);
-
-	if (list_empty(&synch->wait_list))
-		goto out;
-
-	trace_evl_synch_wakeup_many(synch);
-
-	list_for_each_entry_safe(thread, tmp, &synch->wait_list, syn_next) {
-		if (nwakeups++ >= nr)
-			break;
-		list_del(&thread->syn_next);
-		thread->wchan = NULL;
-		evl_resume_thread(thread, T_PEND);
-	}
-out:
-	xnlock_put_irqrestore(&nklock, flags);
-
-	return nwakeups;
-}
-EXPORT_SYMBOL_GPL(evl_wake_up_nr_syn);
-
-void evl_wake_up_targeted_syn(struct evl_syn *synch,
-			      struct evl_thread *waiter)
-{
-	unsigned long flags;
-
-	if (EVL_WARN_ON(CORE, synch->status & EVL_SYN_OWNER))
-		return;
-
-	xnlock_get_irqsave(&nklock, flags);
-
-	trace_evl_synch_wakeup(synch);
-	list_del(&waiter->syn_next);
-	waiter->wchan = NULL;
-	evl_resume_thread(waiter, T_PEND);
-
-	xnlock_put_irqrestore(&nklock, flags);
-}
-EXPORT_SYMBOL_GPL(evl_wake_up_targeted_syn);
-
-static inline void raise_boost_flag(struct evl_thread *owner)
-{
-	/* Backup the base priority at first boost only. */
-	if (!(owner->state & T_BOOST)) {
-		owner->bprio = owner->cprio;
-		owner->state |= T_BOOST;
-	}
-}
-
-static void inherit_thread_priority(struct evl_thread *owner,
-				    struct evl_thread *target)
-{
-	if (owner->state & T_ZOMBIE)
-		return;
-
-	/* Apply the scheduling policy of "target" to "thread" */
-	evl_track_thread_policy(owner, target);
-
-	/*
-	 * Owner may be sleeping, propagate priority update through
-	 * the PI chain if needed.
-	 */
-	if (owner->wchan)
-		evl_requeue_syn_waiter(owner);
-}
-
-static void __ceil_owner_priority(struct evl_thread *owner, int prio)
-{
-	if (owner->state & T_ZOMBIE)
-		return;
-	/*
-	 * Raise owner priority to the ceiling value, this implicitly
-	 * selects SCHED_FIFO for the owner.
-	 */
-	evl_protect_thread_priority(owner, prio);
-
-	if (owner->wchan)
-		evl_requeue_syn_waiter(owner);
-}
-
-static void adjust_boost(struct evl_thread *owner, struct evl_thread *target)
-{
-	struct evl_syn *synch;
-
-	/*
-	 * CAUTION: we may have PI and PP-enabled objects among the
-	 * boosters, considering the leader of synch->wait_list is
-	 * therefore NOT enough for determining the next boost
-	 * priority, since PP is tracked on acquisition, not on
-	 * contention. Check the head of the booster list instead.
-	 */
-	synch = list_first_entry(&owner->boosters, struct evl_syn, next);
-	if (synch->wprio == owner->wprio)
-		return;
-
-	if (synch->status & EVL_SYN_PP)
-		__ceil_owner_priority(owner, get_ceiling_value(synch));
-	else {
-		if (EVL_WARN_ON(CORE, list_empty(&synch->wait_list)))
-			return;
-		if (target == NULL)
-			target = list_first_entry(&synch->wait_list,
-						  struct evl_thread, syn_next);
-		inherit_thread_priority(owner, target);
-	}
-}
-
-static void ceil_owner_priority(struct evl_syn *synch)
-{
-	struct evl_thread *owner = synch->owner;
-	int wprio;
-
-	/* PP ceiling values are implicitly based on the RT class. */
-	wprio = evl_calc_weighted_prio(&evl_sched_rt,
-				       get_ceiling_value(synch));
-	synch->wprio = wprio;
-	list_add_priff(synch, &owner->boosters, wprio, next);
-	raise_boost_flag(owner);
-	synch->status |= EVL_SYN_CEILING;
-
-	/*
-	 * If the ceiling value is lower than the current effective
-	 * priority, we must not adjust the latter.  BEWARE: not only
-	 * this restriction is required to keep the PP logic right,
-	 * but this is also a basic assumption made by all
-	 * evl_commit_monitor_ceiling() callers which won't check for any
-	 * rescheduling opportunity upon return.
-	 *
-	 * However we do want the object to be linked to the booster
-	 * list, and T_BOOST must appear in the current thread status.
-	 *
-	 * This way, setparam() won't be allowed to decrease the
-	 * current weighted priority below the ceiling value, until we
-	 * eventually release this object.
-	 */
-	if (wprio > owner->wprio)
-		adjust_boost(owner, NULL);
-}
-
-static inline
-void track_owner(struct evl_syn *synch, struct evl_thread *owner)
-{
-	synch->owner = owner;
-}
-
-static inline  /* nklock held, irqs off */
-void set_current_owner_locked(struct evl_syn *synch, struct evl_thread *owner)
-{
-	/*
-	 * Update the owner information, and apply priority protection
-	 * for PP objects. We may only get there if owner is current,
-	 * or blocked.
-	 */
-	track_owner(synch, owner);
-	if (synch->status & EVL_SYN_PP)
-		ceil_owner_priority(synch);
-}
-
-static inline
-void set_current_owner(struct evl_syn *synch, struct evl_thread *owner)
-{
-	unsigned long flags;
-
-	track_owner(synch, owner);
-	if (synch->status & EVL_SYN_PP) {
-		xnlock_get_irqsave(&nklock, flags);
-		ceil_owner_priority(synch);
-		xnlock_put_irqrestore(&nklock, flags);
-	}
-}
-
-static inline
-fundle_t get_owner_handle(fundle_t ownerh, struct evl_syn *synch)
-{
-	/*
-	 * On acquisition from kernel space, the fast lock handle
-	 * should bear the FLCEIL bit for PP objects, so that userland
-	 * takes the slow path on release, jumping to the kernel for
-	 * dropping the ceiling priority boost.
-	 */
-	if (synch->status & EVL_SYN_PP)
-		ownerh = evl_syn_fast_ceil(ownerh);
-
-	return ownerh;
-}
-
-void evl_commit_syn_ceiling(struct evl_syn *synch,
-			    struct evl_thread *curr)
-{
-	fundle_t oldh, h;
-	atomic_t *lockp;
-
-	/*
-	 * For PP locks, userland does, in that order:
-	 *
-	 * -- LOCK
-	 * 1. curr->u_window->pp_pending = fundle_of(monitor)
-	 *    barrier();
-	 * 2. atomic_cmpxchg(lockp, EVL_NO_HANDLE, fundle_of(curr));
-	 *
-	 * -- UNLOCK
-	 * 1. atomic_cmpxchg(lockp, fundle_of(curr), EVL_NO_HANDLE); [unclaimed]
-	 *    barrier();
-	 * 2. curr->u_window->pp_pending = EVL_NO_HANDLE
-	 *
-	 * Make sure we have not been caught in a rescheduling in
-	 * between those steps. If we did, then we won't be holding
-	 * the lock as we schedule away, therefore no priority update
-	 * must take place.
-	 *
-	 * Hypothetical case: we might be called multiple times for
-	 * committing a lazy ceiling for the same object, e.g. if
-	 * userland is preempted in the middle of a recursive locking
-	 * sequence. Since the only synchronization object we have is
-	 * the monitor, and that one does not support recursive
-	 * locking, what has just been described cannot happen yet
-	 * though.
-	 *
-	 * This would stem from the fact that userland has to update
-	 * ->pp_pending prior to trying to grab the lock atomically,
-	 * at which point it can figure out whether a recursive
-	 * locking happened. We get out of this trap by testing the
-	 * EVL_SYN_CEILING flag.
-	 */
-	if (!evl_is_syn_owner(synch->fastlock, fundle_of(curr)) ||
-	    (synch->status & EVL_SYN_CEILING))
-		return;
-
-	track_owner(synch, curr);
-	ceil_owner_priority(synch);
-	/*
-	 * Raise FLCEIL, which indicates a kernel entry will be
-	 * required for releasing this resource.
-	 */
-	lockp = synch->fastlock;
-	do {
-		h = atomic_read(lockp);
-		oldh = atomic_cmpxchg(lockp, h, evl_syn_fast_ceil(h));
-	} while (oldh != h);
-}
-
-int evl_try_acquire_syn(struct evl_syn *synch)
-{
-	struct evl_thread *curr;
-	atomic_t *lockp;
-	fundle_t h;
-
-	oob_context_only();
-
-	if (EVL_WARN_ON(CORE, !(synch->status & EVL_SYN_OWNER)))
-		return -EINVAL;
-
-	curr = evl_current_thread();
-	lockp = synch->fastlock;
-	trace_evl_synch_try_acquire(synch);
-
-	h = atomic_cmpxchg(lockp, EVL_NO_HANDLE,
-			   get_owner_handle(fundle_of(curr), synch));
-	if (h != EVL_NO_HANDLE)
-		return evl_get_index(h) == fundle_of(curr) ?
-			-EDEADLK : -EBUSY;
-
-	set_current_owner(synch, curr);
-	get_thread_resource(curr);
-
-	return 0;
-}
-EXPORT_SYMBOL_GPL(evl_try_acquire_syn);
-
-#ifdef CONFIG_EVENLESS_DEBUG_MONITOR_INBAND
-
-/*
- * Detect when a thread is about to wait on a synchronization
- * object currently owned by someone running in-band.
- */
-static void detect_inband_owner(struct evl_syn *synch,
-				struct evl_thread *waiter)
-{
-	if ((waiter->state & T_WARN) &&
-	    !(waiter->info & T_PIALERT) &&
-	    (synch->owner->state & T_INBAND)) {
-		waiter->info |= T_PIALERT;
-		evl_signal_thread(waiter, SIGDEBUG,
-				  SIGDEBUG_MIGRATE_PRIOINV);
-	} else
-		waiter->info &= ~T_PIALERT;
-}
-
-/*
- * Detect when a thread is about to switch to in-band context while
- * holding booster(s) (claimed PI or active PP object), which denotes
- * a potential priority inversion. In such an event, any waiter
- * bearing the T_WARN bit will receive a SIGDEBUG notification.
- */
-void evl_detect_boost_drop(struct evl_thread *owner)
-{
-	struct evl_thread *waiter;
-	struct evl_syn *synch;
-	unsigned long flags;
-
-	xnlock_get_irqsave(&nklock, flags);
-
-	for_each_evl_booster(synch, owner) {
-		evl_for_each_syn_waiter(waiter, synch) {
-			if (waiter->state & T_WARN) {
-				waiter->info |= T_PIALERT;
-				evl_signal_thread(waiter, SIGDEBUG,
-						  SIGDEBUG_MIGRATE_PRIOINV);
-			}
-		}
-	}
-
-	xnlock_put_irqrestore(&nklock, flags);
-}
-
-#else /* !CONFIG_EVENLESS_DEBUG_MONITOR_INBAND */
-
-static inline
-void detect_inband_owner(struct evl_syn *synch,
-			 struct evl_thread *waiter) { }
-
-#endif /* !CONFIG_EVENLESS_DEBUG_MONITOR_INBAND */
-
-int evl_acquire_syn(struct evl_syn *synch, ktime_t timeout,
-		    enum evl_tmode timeout_mode)
-{
-	struct evl_thread *curr, *owner;
-	fundle_t currh, h, oldh;
-	unsigned long flags;
-	atomic_t *lockp;
-	int ret;
-
-	oob_context_only();
-
-	if (EVL_WARN_ON(CORE, !(synch->status & EVL_SYN_OWNER)))
-		return T_BREAK;
-
-	curr = evl_current_thread();
-	currh = fundle_of(curr);
-	lockp = synch->fastlock;
-	trace_evl_synch_acquire(synch);
-redo:
-	/* Basic form of evl_try_acquire_syn(). */
-	h = atomic_cmpxchg(lockp, EVL_NO_HANDLE,
-			   get_owner_handle(currh, synch));
-	if (likely(h == EVL_NO_HANDLE)) {
-		set_current_owner(synch, curr);
-		get_thread_resource(curr);
-		return 0;
-	}
-
-	xnlock_get_irqsave(&nklock, flags);
-
-	/*
-	 * Set claimed bit.  In case it appears to be set already,
-	 * re-read its state under nklock so that we don't miss any
-	 * change between the lock-less read and here. But also try to
-	 * avoid cmpxchg where possible. Only if it appears not to be
-	 * set, start with cmpxchg directly.
-	 */
-	if (evl_fast_syn_is_claimed(h)) {
-		oldh = atomic_read(lockp);
-		goto test_no_owner;
-	}
-
-	do {
-		oldh = atomic_cmpxchg(lockp, h, evl_syn_fast_claim(h));
-		if (likely(oldh == h))
-			break;
-	test_no_owner:
-		if (oldh == EVL_NO_HANDLE) {
-			/* Lock released from another cpu. */
-			xnlock_put_irqrestore(&nklock, flags);
-			goto redo;
-		}
-		h = oldh;
-	} while (!evl_fast_syn_is_claimed(h));
-
-	owner = evl_get_element_by_fundle(&evl_thread_factory, h,
-					  struct evl_thread);
-	/*
-	 * If the handle is broken, pretend that the synch object was
-	 * deleted to signal an error.
-	 */
-	if (owner == NULL) {
-		xnlock_put_irqrestore(&nklock, flags);
-		return T_RMID;
-	}
-
-	/*
-	 * This is the contended path. We just detected an earlier
-	 * syscall-less fast locking from userland, fix up the
-	 * in-kernel state information accordingly.
-	 *
-	 * The consistency of the state information is guaranteed,
-	 * because we just raised the claim bit atomically for this
-	 * contended lock, therefore userland will have to jump to the
-	 * kernel when releasing it, instead of doing a fast
-	 * unlock. Since we currently own the superlock, consistency
-	 * wrt transfer_ownership() is guaranteed through
-	 * serialization.
-	 *
-	 * CAUTION: in this particular case, the only assumptions we
-	 * can safely make is that *owner is valid but not current on
-	 * this CPU.
-	 */
-	track_owner(synch, owner);
-	detect_inband_owner(synch, curr);
-
-	if (!(synch->status & EVL_SYN_PRIO)) { /* i.e. FIFO */
-		list_add_tail(&curr->syn_next, &synch->wait_list);
-		goto block;
-	}
-
-	if (curr->wprio > owner->wprio) {
-		if ((owner->info & T_WAKEN) && owner->wwake == synch) {
-			/* Ownership is still pending, steal the resource. */
-			set_current_owner_locked(synch, curr);
-			owner->info |= T_ROBBED;
-			ret = 0;
-			goto grab;
-		}
-
-		list_add_priff(curr, &synch->wait_list, wprio, syn_next);
-
-		if (synch->status & EVL_SYN_PI) {
-			raise_boost_flag(owner);
-
-			if (synch->status & EVL_SYN_CLAIMED)
-				list_del(&synch->next); /* owner->boosters */
-			else
-				synch->status |= EVL_SYN_CLAIMED;
-
-			synch->wprio = curr->wprio;
-			list_add_priff(synch, &owner->boosters, wprio, next);
-			/*
-			 * curr->wprio > owner->wprio implies that
-			 * synch must be leading the booster list
-			 * after insertion, so we may call
-			 * inherit_thread_priority() for tracking
-			 * current's priority directly without going
-			 * through adjust_boost().
-			 */
-			inherit_thread_priority(owner, curr);
-		}
-	} else
-		list_add_priff(curr, &synch->wait_list, wprio, syn_next);
-block:
-	ret = block_thread_timed(timeout, timeout_mode, synch->clock, synch);
-	curr->wwake = NULL;
-	curr->info &= ~T_WAKEN;
-
-	if (ret)
-		goto out;
-
-	if (curr->info & T_ROBBED) {
-		/*
-		 * Somebody stole us the ownership while we were ready
-		 * to run, waiting for the CPU: we need to wait again
-		 * for the resource.
-		 */
-		if (timeout_mode != EVL_REL || timeout_infinite(timeout)) {
-			xnlock_put_irqrestore(&nklock, flags);
-			goto redo;
-		}
-		timeout = evl_get_stopped_timer_delta(&curr->rtimer);
-		if (timeout) { /* Otherwise, it's too late. */
-			xnlock_put_irqrestore(&nklock, flags);
-			goto redo;
-		}
-		ret = T_TIMEO;
-		goto out;
-	}
-grab:
-	get_thread_resource(curr);
-
-	if (evl_syn_has_waiter(synch))
-		currh = evl_syn_fast_claim(currh);
-
-	/* Set new ownership for this object. */
-	atomic_set(lockp, get_owner_handle(currh, synch));
-out:
-	xnlock_put_irqrestore(&nklock, flags);
-
-	evl_put_element(&owner->element);
-
-	return ret;
-}
-EXPORT_SYMBOL_GPL(evl_acquire_syn);
-
-static void drop_booster(struct evl_syn *synch, struct evl_thread *owner)
-{
-	list_del(&synch->next);	/* owner->boosters */
-
-	if (list_empty(&owner->boosters)) {
-		owner->state &= ~T_BOOST;
-		inherit_thread_priority(owner, owner);
-	} else
-		adjust_boost(owner, NULL);
-}
-
-static inline void clear_pi_boost(struct evl_syn *synch,
-				  struct evl_thread *owner)
-{	/* nklock held, irqs off */
-	synch->status &= ~EVL_SYN_CLAIMED;
-	drop_booster(synch, owner);
-}
-
-static inline void clear_pp_boost(struct evl_syn *synch,
-				  struct evl_thread *owner)
-{	/* nklock held, irqs off */
-	synch->status &= ~EVL_SYN_CEILING;
-	drop_booster(synch, owner);
-}
-
-static bool transfer_ownership(struct evl_syn *synch,
-			       struct evl_thread *lastowner)
-{				/* nklock held, irqs off */
-	struct evl_thread *n_owner;
-	fundle_t n_ownerh;
-	atomic_t *lockp;
-
-	lockp = synch->fastlock;
-
-	/*
-	 * Our caller checked for contention locklessly, so we do have
-	 * to check again under lock in a different way.
-	 */
-	if (list_empty(&synch->wait_list)) {
-		synch->owner = NULL;
-		atomic_set(lockp, EVL_NO_HANDLE);
-		return false;
-	}
-
-	n_owner = list_first_entry(&synch->wait_list, struct evl_thread, syn_next);
-	list_del(&n_owner->syn_next);
-	n_owner->wchan = NULL;
-	n_owner->wwake = synch;
-	set_current_owner_locked(synch, n_owner);
-	n_owner->info |= T_WAKEN;
-	evl_resume_thread(n_owner, T_PEND);
-
-	if (synch->status & EVL_SYN_CLAIMED)
-		clear_pi_boost(synch, lastowner);
-
-	n_ownerh = get_owner_handle(fundle_of(n_owner), synch);
-	if (evl_syn_has_waiter(synch))
-		n_ownerh = evl_syn_fast_claim(n_ownerh);
-
-	atomic_set(lockp, n_ownerh);
-
-	return true;
-}
-
-bool evl_release_syn(struct evl_syn *synch)
-{
-	struct evl_thread *curr = evl_current_thread();
-	bool need_resched = false;
-	unsigned long flags;
-	fundle_t currh, h;
-	atomic_t *lockp;
-
-	if (EVL_WARN_ON(CORE, !(synch->status & EVL_SYN_OWNER)))
-		return false;
-
-	trace_evl_synch_release(synch);
-
-	if (!put_thread_resource(curr))
-		return false;
-
-	lockp = synch->fastlock;
-	currh = fundle_of(curr);
-	/*
-	 * FLCEIL may only be raised by the owner, or when the owner
-	 * is blocked waiting for the synch (ownership transfer). In
-	 * addition, only the current owner of a synch may release it,
-	 * therefore we can't race while testing FLCEIL locklessly.
-	 * All updates to FLCLAIM are covered by the superlock.
-	 *
-	 * Therefore, clearing the fastlock racelessly in this routine
-	 * without leaking FLCEIL/FLCLAIM updates can be achieved by
-	 * holding the superlock.
-	 */
-	xnlock_get_irqsave(&nklock, flags);
-
-	if (synch->status & EVL_SYN_CEILING) {
-		clear_pp_boost(synch, curr);
-		need_resched = true;
-	}
-
-	h = atomic_cmpxchg(lockp, currh, EVL_NO_HANDLE);
-	if ((h & ~EVL_SYN_FLCEIL) != currh)
-		/* FLCLAIM set, synch is contended. */
-		need_resched = transfer_ownership(synch, curr);
-	else if (h != currh)	/* FLCEIL set, FLCLAIM clear. */
-		atomic_set(lockp, EVL_NO_HANDLE);
-
-	xnlock_put_irqrestore(&nklock, flags);
-
-	return need_resched;
-}
-EXPORT_SYMBOL_GPL(evl_release_syn);
-
-void evl_requeue_syn_waiter(struct evl_thread *thread)
-{				/* nklock held, irqs off */
-	struct evl_syn *synch = thread->wchan;
-	struct evl_thread *owner;
-
-	if (EVL_WARN_ON(CORE, !(synch->status & EVL_SYN_PRIO)))
-		return;
-
-	/*
-	 * Update the position in the pend queue of a thread waiting
-	 * for a lock. This routine propagates the change throughout
-	 * the PI chain if required.
-	 */
-	list_del(&thread->syn_next);
-	list_add_priff(thread, &synch->wait_list, wprio, syn_next);
-	owner = synch->owner;
-
-	/* Only PI-enabled objects are of interest here. */
-	if (!(synch->status & EVL_SYN_PI))
-		return;
-
-	synch->wprio = thread->wprio;
-	if (synch->status & EVL_SYN_CLAIMED)
-		list_del(&synch->next);
-	else {
-		synch->status |= EVL_SYN_CLAIMED;
-		raise_boost_flag(owner);
-	}
-
-	list_add_priff(synch, &owner->boosters, wprio, next);
-	adjust_boost(owner, thread);
-}
-EXPORT_SYMBOL_GPL(evl_requeue_syn_waiter);
-
-struct evl_thread *evl_syn_wait_head(struct evl_syn *synch)
-{
-	return list_first_entry_or_null(&synch->wait_list,
-					struct evl_thread, syn_next);
-}
-EXPORT_SYMBOL_GPL(evl_syn_wait_head);
-
-bool evl_flush_syn(struct evl_syn *synch, int reason)
-{
-	struct evl_thread *waiter, *tmp;
-	unsigned long flags;
-	bool ret;
-
-	xnlock_get_irqsave(&nklock, flags);
-
-	trace_evl_synch_flush(synch);
-
-	if (list_empty(&synch->wait_list)) {
-		EVL_WARN_ON(CORE, synch->status & EVL_SYN_CLAIMED);
-		ret = false;
-	} else {
-		ret = true;
-		list_for_each_entry_safe(waiter, tmp, &synch->wait_list, syn_next) {
-			list_del(&waiter->syn_next);
-			waiter->info |= reason;
-			waiter->wchan = NULL;
-			evl_resume_thread(waiter, T_PEND);
-		}
-		if (synch->status & EVL_SYN_CLAIMED)
-			clear_pi_boost(synch, synch->owner);
-	}
-
-	xnlock_put_irqrestore(&nklock, flags);
-
-	return ret;
-}
-EXPORT_SYMBOL_GPL(evl_flush_syn);
-
-void evl_forget_syn_waiter(struct evl_thread *thread)
-{				/* nklock held, irqs off */
-	struct evl_syn *synch = thread->wchan;
-	struct evl_thread *owner, *target;
-
-	/*
-	 * Do all the necessary housekeeping chores to stop a thread
-	 * from waiting on a given synchronization object. Doing so
-	 * may require to update a PI chain.
-	 */
-	trace_evl_synch_forget(synch);
-
-	thread->state &= ~T_PEND;
-	thread->wchan = NULL;
-	list_del(&thread->syn_next); /* synch->wait_list */
-
-	/*
-	 * Only a waiter leaving a PI chain triggers an update.
-	 * NOTE: PP objects never bear the CLAIMED bit.
-	 */
-	if (!(synch->status & EVL_SYN_CLAIMED))
-		return;
-
-	owner = synch->owner;
-
-	if (list_empty(&synch->wait_list)) {
-		/* No more waiters: clear the PI boost. */
-		clear_pi_boost(synch, owner);
-		return;
-	}
-
-	/*
-	 * Reorder the booster queue of the current owner after we
-	 * left the wait list, then set its priority to the new
-	 * required minimum required to prevent priority inversion.
-	 */
-	target = list_first_entry(&synch->wait_list, struct evl_thread, syn_next);
-	synch->wprio = target->wprio;
-	list_del(&synch->next);	/* owner->boosters */
-	list_add_priff(synch, &owner->boosters, wprio, next);
-	adjust_boost(owner, target);
-}
-EXPORT_SYMBOL_GPL(evl_forget_syn_waiter);
diff --git a/kernel/evenless/syscall.c b/kernel/evenless/syscall.c
index 603bae6e95e..845561deb00 100644
--- a/kernel/evenless/syscall.c
+++ b/kernel/evenless/syscall.c
@@ -13,7 +13,7 @@
 #include <linux/sched.h>
 #include <linux/dovetail.h>
 #include <linux/kconfig.h>
-#include <linux/kernel.h>
+#include <linux/atomic.h>
 #include <linux/sched/task_stack.h>
 #include <linux/sched/signal.h>
 #include <evenless/control.h>
@@ -37,8 +37,8 @@
 #define SYSCALL_STOP        1
 
 typedef long (*evl_syshand)(unsigned long arg1, unsigned long arg2,
-			    unsigned long arg3, unsigned long arg4,
-			    unsigned long arg5);
+			unsigned long arg3, unsigned long arg4,
+			unsigned long arg5);
 
 static const evl_syshand evl_syscalls[__NR_EVENLESS_SYSCALLS];
 
@@ -49,17 +49,17 @@ static inline void do_oob_request(int nr, struct pt_regs *regs)
 
 	handler = evl_syscalls[nr];
 	ret = handler(oob_arg1(regs),
-		      oob_arg2(regs),
-		      oob_arg3(regs),
-		      oob_arg4(regs),
-		      oob_arg5(regs));
+		oob_arg2(regs),
+		oob_arg3(regs),
+		oob_arg4(regs),
+		oob_arg5(regs));
 
 	set_oob_retval(regs, ret);
 }
 
 static void prepare_for_signal(struct task_struct *p,
-			       struct evl_thread *curr,
-			       struct pt_regs *regs)
+			struct evl_thread *curr,
+			struct pt_regs *regs)
 {
 	int cause = SIGDEBUG_UNDEFINED;
 	unsigned long flags;
@@ -104,8 +104,8 @@ static int do_oob_syscall(struct irq_stage *stage, struct pt_regs *regs)
 	if (curr == NULL || !cap_raised(current_cap(), CAP_SYS_NICE)) {
 		if (EVL_DEBUG(CORE))
 			printk(EVL_WARNING
-			       "OOB syscall <%d> denied to %s[%d]\n",
-			       nr, current->comm, task_pid_nr(current));
+				"OOB syscall <%d> denied to %s[%d]\n",
+				nr, current->comm, task_pid_nr(current));
 		set_oob_error(regs, -EPERM);
 		return SYSCALL_STOP;
 	}
@@ -130,7 +130,8 @@ static int do_oob_syscall(struct irq_stage *stage, struct pt_regs *regs)
 		p = current;
 		if (signal_pending(p) || (curr->info & T_KICKED))
 			prepare_for_signal(p, curr, regs);
-		else if ((curr->state & T_WEAK) && curr->res_count == 0)
+		else if ((curr->state & T_WEAK) &&
+			!atomic_read(&curr->inband_disable_count))
 			evl_switch_inband(SIGDEBUG_UNDEFINED);
 	}
 
@@ -206,7 +207,8 @@ static int do_inband_syscall(struct irq_stage *stage, struct pt_regs *regs)
 		p = current;
 		if (signal_pending(p))
 			prepare_for_signal(p, curr, regs);
-		else if ((curr->state & T_WEAK) && curr->res_count == 0)
+		else if ((curr->state & T_WEAK) &&
+			!atomic_read(&curr->inband_disable_count))
 			evl_switch_inband(SIGDEBUG_UNDEFINED);
 	}
 done:
@@ -322,17 +324,17 @@ static int EvEnLeSs_ni(void)
 
 #define __syshand__(__name)	((evl_syshand)(EvEnLeSs_ ## __name))
 
-#define __EVENLESS_CALL_ENTRIES		\
-	__EVENLESS_CALL_ENTRY(read)	\
-	__EVENLESS_CALL_ENTRY(write)	\
-	__EVENLESS_CALL_ENTRY(ioctl)
+#define __EVENLESS_CALL_ENTRIES			\
+	__EVENLESS_CALL_ENTRY(read)		\
+		__EVENLESS_CALL_ENTRY(write)	\
+		__EVENLESS_CALL_ENTRY(ioctl)
 
 #define __EVENLESS_NI	__syshand__(ni)
 
-#define __EVENLESS_CALL_NI		\
+#define __EVENLESS_CALL_NI					\
 	[0 ... __NR_EVENLESS_SYSCALLS-1] = __EVENLESS_NI,
 
-#define __EVENLESS_CALL_ENTRY(__name)	\
+#define __EVENLESS_CALL_ENTRY(__name)				\
 	[sys_evenless_ ## __name] = __syshand__(__name),
 
 static const evl_syshand evl_syscalls[] = {
diff --git a/kernel/evenless/thread.c b/kernel/evenless/thread.c
index 83d8af8a6bc..1a57be96276 100644
--- a/kernel/evenless/thread.c
+++ b/kernel/evenless/thread.c
@@ -24,7 +24,7 @@
 #include <linux/math64.h>
 #include <evenless/sched.h>
 #include <evenless/timer.h>
-#include <evenless/synch.h>
+#include <evenless/wait.h>
 #include <evenless/clock.h>
 #include <evenless/stat.h>
 #include <evenless/assert.h>
@@ -35,6 +35,7 @@
 #include <evenless/file.h>
 #include <evenless/factory.h>
 #include <evenless/monitor.h>
+#include <evenless/mutex.h>
 #include <asm/evenless/syscall.h>
 #include <trace/events/evenless.h>
 
@@ -115,9 +116,9 @@ static void pin_to_initial_cpu(struct evl_thread *thread)
 }
 
 int evl_init_thread(struct evl_thread *thread,
-		    const struct evl_init_thread_attr *iattr,
-		    struct evl_rq *rq,
-		    const char *fmt, ...)
+		const struct evl_init_thread_attr *iattr,
+		struct evl_rq *rq,
+		const char *fmt, ...)
 {
 	int flags = iattr->flags & ~T_SUSP, ret, gravity;
 	struct cpumask affinity;
@@ -150,7 +151,7 @@ int evl_init_thread(struct evl_thread *thread,
 	 * state, to speed up branch taking in libevenless wherever this
 	 * needs to be tested.
 	 */
-	if (IS_ENABLED(CONFIG_EVENLESS_DEBUG_MONITOR_SLEEP))
+	if (IS_ENABLED(CONFIG_EVENLESS_DEBUG_MUTEX_SLEEP))
 		flags |= T_DEBUG;
 
 	cpumask_and(&thread->affinity, &iattr->affinity, &evl_cpu_affinity);
@@ -165,8 +166,8 @@ int evl_init_thread(struct evl_thread *thread,
 	thread->wchan = NULL;
 	thread->wwake = NULL;
 	thread->wait_data = NULL;
-	thread->res_count = 0;
 	thread->u_window = NULL;
+	atomic_set(&thread->inband_disable_count, 0);
 	memset(&thread->poll_context, 0, sizeof(thread->poll_context));
 	memset(&thread->stat, 0, sizeof(thread->stat));
 	memset(&thread->altsched, 0, sizeof(thread->altsched));
@@ -178,11 +179,11 @@ int evl_init_thread(struct evl_thread *thread,
 
 	gravity = flags & T_USER ? EVL_TIMER_UGRAVITY : EVL_TIMER_KGRAVITY;
 	evl_init_timer(&thread->rtimer, &evl_mono_clock, timeout_handler,
-		       rq, gravity);
+		rq, gravity);
 	evl_set_timer_name(&thread->rtimer, thread->name);
 	evl_set_timer_priority(&thread->rtimer, EVL_TIMER_HIPRIO);
 	evl_init_timer(&thread->ptimer, &evl_mono_clock, periodic_handler,
-		       rq, gravity);
+		rq, gravity);
 	evl_set_timer_name(&thread->ptimer, thread->name);
 	evl_set_timer_priority(&thread->ptimer, EVL_TIMER_HIPRIO);
 
@@ -192,7 +193,7 @@ int evl_init_thread(struct evl_thread *thread,
 		goto err_out;
 
 	ret = evl_set_thread_policy(thread, iattr->sched_class,
-				    &iattr->sched_param);
+				&iattr->sched_param);
 	if (ret)
 		goto err_out;
 
@@ -225,9 +226,17 @@ static void uninit_thread(struct evl_thread *thread)
 	kfree(thread->name);
 }
 
+static inline void abort_wait(struct evl_thread *thread)
+{
+	struct evl_wait_channel *wchan = thread->wchan;
+
+	if (wchan)
+		wchan->abort_wait(thread);
+}
+
 static void do_cleanup_current(struct evl_thread *curr)
 {
-	struct evl_syn *synch, *tmp;
+	struct evl_mutex *mutex, *tmp;
 	unsigned long flags;
 
 	evl_unindex_element(&curr->element);
@@ -250,9 +259,6 @@ static void do_cleanup_current(struct evl_thread *curr)
 		curr->state &= ~T_READY;
 	}
 
-	if (curr->state & T_PEND)
-		evl_forget_syn_waiter(curr);
-
 	/*
 	 * NOTE: we must be running over the root thread, or @curr
 	 * is dormant, which means that we don't risk sched->curr to
@@ -261,9 +267,9 @@ static void do_cleanup_current(struct evl_thread *curr)
 	 */
 	curr->state |= T_ZOMBIE;
 
-	/* Release all resources owned by current. */
-	for_each_evl_booster_safe(synch, tmp, curr)
-		evl_release_syn(synch);
+	/* Release all contended mutexes owned by current. */
+	for_each_evl_booster_safe(mutex, tmp, curr)
+		__evl_unlock_mutex(mutex);
 
 	xnlock_put_irqrestore(&nklock, flags);
 
@@ -455,40 +461,39 @@ void evl_start_thread(struct evl_thread *thread)
 EXPORT_SYMBOL_GPL(evl_start_thread);
 
 void evl_block_thread_timeout(struct evl_thread *thread, int mask,
-			      ktime_t timeout, enum evl_tmode timeout_mode,
-			      struct evl_clock *clock,
-			      struct evl_syn *wchan)
+			ktime_t timeout, enum evl_tmode timeout_mode,
+			struct evl_clock *clock,
+			struct evl_wait_channel *wchan)
 {
-	unsigned long oldstate;
-	unsigned long flags;
+	unsigned long oldstate, flags;
 	struct evl_rq *rq;
 
+	oob_context_only();
+
 	/*
-	 * Things we can't do: apply T_INBAND using this call, suspend
-	 * the root thread, ask for a conjunctive wait on multiple
-	 * channels.
+	 * Things we can't do: apply T_INBAND using this call, or ask
+	 * for a conjunctive wait on multiple channels.
 	 */
-	if (EVL_WARN_ON(CORE, (mask & T_INBAND) || (thread->state & T_ROOT) ||
-			(wchan && thread->wchan)))
+	if (EVL_WARN_ON(CORE, (mask & T_INBAND) || (wchan && thread->wchan)))
 		return;
 
 	xnlock_get_irqsave(&nklock, flags);
 
 	trace_evl_block_thread(thread, mask, timeout,
-			       timeout_mode, clock, wchan);
+			timeout_mode, clock, wchan);
 	rq = thread->rq;
 	oldstate = thread->state;
 
 	/*
-	 * If attempting to suspend a runnable thread which is pending
-	 * a forced switch to in-band context (T_KICKED), just raise
-	 * the T_BREAK status and return immediately.
+	 * If attempting to block a runnable thread which is pending a
+	 * forced switch to in-band context (T_KICKED), just raise the
+	 * T_BREAK status and return immediately.
 	 */
 	if (likely(!(oldstate & EVL_THREAD_BLOCK_BITS))) {
 		if (thread->info & T_KICKED) {
 			if (wchan) {
 				thread->wchan = wchan;
-				evl_forget_syn_waiter(thread);
+				abort_wait(thread);
 			}
 			thread->info &= ~(T_RMID|T_TIMEO);
 			thread->info |= T_BREAK;
@@ -506,7 +511,7 @@ void evl_block_thread_timeout(struct evl_thread *thread, int mask,
 	 */
 	if (timeout_mode != EVL_REL || !timeout_infinite(timeout)) {
 		evl_prepare_timer_wait(&thread->rtimer, clock,
-				       evl_thread_rq(thread));
+				evl_thread_rq(thread));
 		if (timeout_mode == EVL_REL)
 			timeout = evl_abs_timeout(&thread->rtimer, timeout);
 		evl_start_timer(&thread->rtimer, timeout, EVL_INFINITE);
@@ -559,7 +564,7 @@ void evl_block_thread_timeout(struct evl_thread *thread, int mask,
 	if (likely(thread == rq->curr))
 		evl_set_resched(rq);
 	else if (((oldstate & (EVL_THREAD_BLOCK_BITS|T_USER)) ==
-		    (T_INBAND|T_USER)) && (mask & (T_SUSP | T_HALT)))
+			(T_INBAND|T_USER)) && (mask & (T_SUSP | T_HALT)))
 		evl_signal_thread(thread, SIGSHADOW, SIGSHADOW_ACTION_HOME);
 
 	xnlock_put_irqrestore(&nklock, flags);
@@ -618,8 +623,8 @@ void evl_switch_inband(int cause)
 	 * context.
 	 */
 	EVL_WARN(CORE, !running_inband(),
-		 "evl_switch_inband() failed for thread %s[%d]",
-		 curr->name, evl_get_inband_pid(curr));
+		"evl_switch_inband() failed for thread %s[%d]",
+		curr->name, evl_get_inband_pid(curr));
 
 	/* Account for switch to in-band context. */
 	evl_inc_counter(&curr->stat.isw);
@@ -715,8 +720,8 @@ int evl_switch_oob(void)
 	 */
 	if (signal_pending(p)) {
 		evl_switch_inband(!(curr->state & T_SSTEP) ?
-				  SIGDEBUG_MIGRATE_SIGNAL:
-				  SIGDEBUG_UNDEFINED);
+				SIGDEBUG_MIGRATE_SIGNAL:
+				SIGDEBUG_UNDEFINED);
 		return -ERESTARTSYS;
 	}
 
@@ -727,7 +732,7 @@ EXPORT_SYMBOL_GPL(evl_switch_oob);
 void evl_block_thread(struct evl_thread *thread, int mask)
 {
 	evl_block_thread_timeout(thread, mask, EVL_INFINITE,
-				 EVL_REL, NULL, NULL);
+				EVL_REL, NULL, NULL);
 	evl_schedule();
 }
 EXPORT_SYMBOL_GPL(evl_block_thread);
@@ -791,14 +796,14 @@ ktime_t evl_get_thread_period(struct evl_thread *thread)
 EXPORT_SYMBOL_GPL(evl_get_thread_period);
 
 ktime_t evl_delay_thread(ktime_t timeout, enum evl_tmode timeout_mode,
-			 struct evl_clock *clock)
+			struct evl_clock *clock)
 {
 	struct evl_thread *curr = evl_current_thread();
 	unsigned long flags;
 	ktime_t rem = 0;
 
 	evl_block_thread_timeout(curr, T_DELAY, timeout,
-				 timeout_mode, clock, NULL);
+				timeout_mode, clock, NULL);
 	evl_schedule();
 
 	if (curr->info & T_BREAK) {
@@ -814,8 +819,8 @@ EXPORT_SYMBOL_GPL(evl_delay_thread);
 void evl_resume_thread(struct evl_thread *thread, int mask)
 {
 	unsigned long oldstate;
-	struct evl_rq *rq;
 	unsigned long flags;
+	struct evl_rq *rq;
 
 	xnlock_get_irqsave(&nklock, flags);
 
@@ -848,8 +853,7 @@ void evl_resume_thread(struct evl_thread *thread, int mask)
 		mask = thread->state & T_PEND;
 		if (mask == 0)
 			goto unlock_and_exit;
-		if (thread->wchan)
-			evl_forget_syn_waiter(thread);
+		abort_wait(thread);
 		goto recheck_state;
 	}
 
@@ -866,11 +870,11 @@ void evl_resume_thread(struct evl_thread *thread, int mask)
 	}
 
 	/*
-	 * The thread is still suspended, but is no more pending on a
-	 * resource.
+	 * The thread is still blocked, but isn't pending on a wait
+	 * channel anymore.
 	 */
-	if ((mask & T_PEND) != 0 && thread->wchan)
-		evl_forget_syn_waiter(thread);
+	if (mask & T_PEND)
+		abort_wait(thread);
 
 	goto unlock_and_exit;
 
@@ -879,15 +883,8 @@ void evl_resume_thread(struct evl_thread *thread, int mask)
 		goto unlock_and_exit;
 
 clear_wchan:
-	/*
-	 * If the thread was actually suspended, clear the wait
-	 * channel. This allows requests like
-	 * evl_block_thread_timeout(thread, T_DELAY,...) to skip the
-	 * following code when the suspended thread is woken up while
-	 * undergoing a simple delay.
-	 */
-	if ((mask & ~T_DELAY) != 0 && thread->wchan != NULL)
-		evl_forget_syn_waiter(thread);
+	if (mask & ~T_DELAY)
+		abort_wait(thread);
 
 	if (unlikely((oldstate & mask) & T_HALT)) {
 		evl_requeue_thread(thread);
@@ -970,7 +967,7 @@ int evl_sleep(ktime_t delay)
 EXPORT_SYMBOL_GPL(evl_sleep);
 
 int evl_set_thread_period(struct evl_clock *clock,
-			  ktime_t idate, ktime_t period)
+			ktime_t idate, ktime_t period)
 {
 	struct evl_thread *curr = evl_current_thread();
 	unsigned long flags;
@@ -997,7 +994,7 @@ int evl_set_thread_period(struct evl_clock *clock,
 	}
 
 	evl_prepare_timer_wait(&curr->ptimer, clock,
-			       evl_thread_rq(curr));
+			evl_thread_rq(curr));
 
 	if (timeout_infinite(idate))
 		idate = evl_abs_timeout(&curr->ptimer, period);
@@ -1298,8 +1295,8 @@ void evl_migrate_thread(struct evl_thread *thread, struct evl_rq *rq)
 #endif	/* CONFIG_SMP */
 
 int evl_set_thread_schedparam(struct evl_thread *thread,
-			      struct evl_sched_class *sched_class,
-			      const union evl_sched_param *sched_param)
+			struct evl_sched_class *sched_class,
+			const union evl_sched_param *sched_param)
 {
 	unsigned long flags;
 	int ret;
@@ -1327,20 +1324,19 @@ int __evl_set_thread_schedparam(struct evl_thread *thread,
 	new_wprio = thread->wprio;
 
 	/*
-	 * If the thread is waiting on a synchronization object,
-	 * update its position in the corresponding wait queue, unless
-	 * the (weighted) priority has not changed (to prevent
-	 * spurious round-robin effects).
+	 * If the thread is sleeping on a wait channel, update its
+	 * position in the corresponding wait list, unless the
+	 * (weighted) priority has not changed (to prevent spurious
+	 * round-robin effects).
 	 */
-	if (old_wprio != new_wprio && thread->wchan &&
-	    (thread->wchan->status & EVL_SYN_PRIO))
-		evl_requeue_syn_waiter(thread);
+	if (old_wprio != new_wprio && (thread->state & T_PEND))
+		thread->wchan->reorder_wait(thread);
 
 	thread->info |= T_SCHEDP;
 	/* Ask the target thread to call back if in-band. */
 	if (thread->state & T_INBAND)
 		evl_signal_thread(thread, SIGSHADOW,
-				  SIGSHADOW_ACTION_HOME);
+				SIGSHADOW_ACTION_HOME);
 
 	return ret;
 }
@@ -1424,7 +1420,7 @@ static int force_wakeup(struct evl_thread *thread) /* nklock locked, irqs off */
 	 * Rationale: callers of evl_block_thread_timeout() may assume
 	 * that receiving T_BREAK means that the process that
 	 * motivated the blocking did not go to completion. E.g. the
-	 * wait context was NOT updated before evl_sleep_on_syn()
+	 * wait context was NOT updated before evl_wait_timeout()
 	 * returned, leaving no useful data there.  Therefore, in case
 	 * only T_SUSP remains set for the thread on entry to
 	 * force_wakeup(), after T_PEND was lifted earlier when the
@@ -1499,7 +1495,7 @@ void __evl_kick_thread(struct evl_thread *thread) /* nklock locked, irqs off */
 	 * mayday trap asap.
 	 */
 	if (thread != this_evl_rq_thread() &&
-	    (thread->state & T_USER))
+		(thread->state & T_USER))
 		dovetail_send_mayday(p);
 }
 
@@ -1562,7 +1558,7 @@ static void inband_task_signal(struct irq_work *work)
 
 	req = container_of(work, struct inband_signal, work);
 	thread = evl_get_element_by_fundle(&evl_thread_factory,
-			   req->fundle, struct evl_thread);
+					req->fundle, struct evl_thread);
 	if (thread == NULL)
 		goto done;
 
@@ -1662,7 +1658,7 @@ int evl_killall(int mask)
 
 		if (EVL_DEBUG(CORE))
 			printk(EVL_INFO "terminating %s[%d]\n",
-			       t->name, evl_get_inband_pid(t));
+				t->name, evl_get_inband_pid(t));
 		nrkilled++;
 		evl_cancel_thread(t);
 	}
@@ -1672,17 +1668,17 @@ int evl_killall(int mask)
 	count = nrthreads - nrkilled;
 	if (EVL_DEBUG(CORE))
 		printk(EVL_INFO "waiting for %d threads to exit\n",
-		       nrkilled);
+			nrkilled);
 
 	ret = wait_event_interruptible(join_all,
-				       evl_nrthreads == count);
+				evl_nrthreads == count);
 
 	/* Wait for a full RCU grace period to expire. */
 	wait_for_rcu_grace_period(NULL);
 
 	if (EVL_DEBUG(CORE))
 		printk(EVL_INFO "joined %d threads\n",
-		       count + nrkilled - evl_nrthreads);
+			count + nrkilled - evl_nrthreads);
 
 	return ret < 0 ? -EINTR : 0;
 }
@@ -1718,12 +1714,12 @@ void handle_oob_trap(unsigned int trapnr, struct pt_regs *regs)
 #if defined(CONFIG_EVENLESS_DEBUG_CORE) || defined(CONFIG_EVENLESS_DEBUG_USER)
 	if (xnarch_fault_notify(trapnr))
 		printk(EVL_WARNING
-		       "%s switching in-band [pid=%d, excpt=%#x, %spc=%#lx]\n",
-		       curr->name,
-		       evl_get_inband_pid(curr),
-		       trapnr,
-		       user_mode(regs) ? "" : "kernel_",
-		       instruction_pointer(regs));
+			"%s switching in-band [pid=%d, excpt=%#x, %spc=%#lx]\n",
+			curr->name,
+			evl_get_inband_pid(curr),
+			trapnr,
+			user_mode(regs) ? "" : "kernel_",
+			instruction_pointer(regs));
 #endif
 	if (xnarch_fault_pf_p(trapnr))
 		/*
@@ -1738,8 +1734,8 @@ void handle_oob_trap(unsigned int trapnr, struct pt_regs *regs)
 	 * before handling the exception.
 	 */
 	evl_switch_inband(xnarch_fault_notify(trapnr) ?
-			  SIGDEBUG_MIGRATE_FAULT :
-			  SIGDEBUG_UNDEFINED);
+			SIGDEBUG_MIGRATE_FAULT :
+			SIGDEBUG_UNDEFINED);
 }
 
 void handle_oob_mayday(struct pt_regs *regs)
@@ -1787,7 +1783,7 @@ static void handle_migration_event(struct dovetail_migration_data *d)
 	 */
 	if (thread->state & (EVL_THREAD_BLOCK_BITS & ~T_INBAND))
 		evl_signal_thread(thread, SIGSHADOW,
-				  SIGSHADOW_ACTION_HOME);
+				SIGSHADOW_ACTION_HOME);
 }
 
 static inline bool affinity_ok(struct task_struct *p) /* nklocked, IRQs off */
@@ -1816,7 +1812,7 @@ static inline bool affinity_ok(struct task_struct *p) /* nklocked, IRQs off */
 	 */
 	if (!is_threading_cpu(cpu)) {
 		printk(EVL_WARNING "thread %s[%d] switched to non-rt CPU%d, aborted.\n",
-		       thread->name, evl_get_inband_pid(thread), cpu);
+			thread->name, evl_get_inband_pid(thread), cpu);
 		/*
 		 * Can't call evl_cancel_thread() from a CPU migration
 		 * point, that would break. Since we are on the wakeup
@@ -1905,10 +1901,10 @@ static void handle_schedule_event(struct task_struct *next_task)
 			 * situations on SMP.
 			 */
 			sigorsets(&pending,
-				  &next_task->pending.signal,
-				  &next_task->signal->shared_pending.signal);
+				&next_task->pending.signal,
+				&next_task->signal->shared_pending.signal);
 			if (sigismember(&pending, SIGSTOP) ||
-			    sigismember(&pending, SIGINT))
+				sigismember(&pending, SIGINT))
 				goto check;
 		}
 		xnlock_get_irqsave(&nklock, flags);
@@ -1924,22 +1920,22 @@ static void handle_schedule_event(struct task_struct *next_task)
 	 * properly recover from a stopped state.
 	 */
 	if (!EVL_WARN(CORE, !(next->state & T_INBAND),
-		      "Ouch: out-of-band thread %s[%d] running on the in-band stage"
-		      "(status=0x%x, sig=%d, prev=%s[%d])",
-		      next->name, task_pid_nr(next_task),
-		      next->state,
-		      signal_pending(next_task),
-		      prev_task->comm, task_pid_nr(prev_task)))
+			"Ouch: out-of-band thread %s[%d] running on the in-band stage"
+			"(status=0x%x, sig=%d, prev=%s[%d])",
+			next->name, task_pid_nr(next_task),
+			next->state,
+			signal_pending(next_task),
+			prev_task->comm, task_pid_nr(prev_task)))
 		EVL_WARN(CORE,
-			 !(next_task->ptrace & PT_PTRACED) &&
-			 !(next->state & T_DORMANT)
-			 && (next->state & T_PEND),
-			 "Ouch: blocked EVL thread %s[%d] rescheduled in-band"
-			 "(status=0x%x, sig=%d, prev=%s[%d])",
-			 next->name, task_pid_nr(next_task),
-			 next->state,
-			 signal_pending(next_task), prev_task->comm,
-			 task_pid_nr(prev_task));
+			!(next_task->ptrace & PT_PTRACED) &&
+			!(next->state & T_DORMANT)
+			&& (next->state & T_PEND),
+			"Ouch: blocked EVL thread %s[%d] rescheduled in-band"
+			"(status=0x%x, sig=%d, prev=%s[%d])",
+			next->name, task_pid_nr(next_task),
+			next->state,
+			signal_pending(next_task), prev_task->comm,
+			task_pid_nr(prev_task));
 }
 
 static void handle_sigwake_event(struct task_struct *p)
@@ -1962,12 +1958,12 @@ static void handle_sigwake_event(struct task_struct *p)
 	if ((p->ptrace & PT_PTRACED) && !(thread->state & T_SSTEP)) {
 		/* We already own the siglock. */
 		sigorsets(&pending,
-			  &p->pending.signal,
-			  &p->signal->shared_pending.signal);
+			&p->pending.signal,
+			&p->signal->shared_pending.signal);
 
 		if (sigismember(&pending, SIGTRAP) ||
-		    sigismember(&pending, SIGSTOP)
-		    || sigismember(&pending, SIGINT))
+			sigismember(&pending, SIGSTOP)
+			|| sigismember(&pending, SIGINT))
 			thread->state &= ~T_SSTEP;
 	}
 
@@ -2060,7 +2056,7 @@ static int set_time_slice(struct evl_thread *thread, ktime_t quantum) /* nklock
 }
 
 static int set_sched_attrs(struct evl_thread *thread,
-			   const struct evl_sched_attrs *attrs)
+			const struct evl_sched_attrs *attrs)
 {
 	struct evl_sched_class *sched_class;
 	union evl_sched_param param;
@@ -2091,7 +2087,7 @@ static int set_sched_attrs(struct evl_thread *thread,
 }
 
 static int get_sched_attrs(struct evl_thread *thread,
-			   struct evl_sched_attrs *attrs)
+			struct evl_sched_attrs *attrs)
 {
 	struct evl_sched_class *base_class;
 	unsigned long flags;
@@ -2135,7 +2131,7 @@ static int get_sched_attrs(struct evl_thread *thread,
 }
 
 static int update_state_bits(struct evl_thread *thread,
-			     __u32 mask, bool set)
+			__u32 mask, bool set)
 {
 	struct evl_thread *curr = evl_current_thread();
 	unsigned long flags;
@@ -2169,7 +2165,7 @@ static long thread_common_ioctl(struct evl_thread *thread,
 	switch (cmd) {
 	case EVL_THRIOC_SET_SCHEDPARAM:
 		ret = raw_copy_from_user(&attrs,
-					 (struct evl_sched_attrs *)arg, sizeof(attrs));
+					(struct evl_sched_attrs *)arg, sizeof(attrs));
 		if (ret)
 			return -EFAULT;
 		ret = set_sched_attrs(thread, &attrs);
@@ -2179,7 +2175,7 @@ static long thread_common_ioctl(struct evl_thread *thread,
 		if (ret)
 			return ret;
 		ret = raw_copy_to_user((struct evl_sched_attrs *)arg,
-				       &attrs, sizeof(attrs));
+				&attrs, sizeof(attrs));
 		if (ret)
 			return -EFAULT;
 		break;
@@ -2191,7 +2187,7 @@ static long thread_common_ioctl(struct evl_thread *thread,
 }
 
 static long thread_oob_ioctl(struct file *filp, unsigned int cmd,
-			     unsigned long arg)
+			unsigned long arg)
 {
 	struct evl_thread *thread = element_of(filp, struct evl_thread);
 	__u32 monfd, mask;
@@ -2223,7 +2219,7 @@ static long thread_oob_ioctl(struct file *filp, unsigned int cmd,
 }
 
 static long thread_ioctl(struct file *filp, unsigned int cmd,
-			 unsigned long arg)
+			unsigned long arg)
 {
 	struct evl_thread *thread = element_of(filp, struct evl_thread);
 	long ret;
@@ -2309,7 +2305,7 @@ static void discard_unmapped_uthread(struct evl_thread *thread)
 
 static struct evl_element *
 thread_factory_build(struct evl_factory *fac, const char *name,
-		     void __user *u_attrs, u32 *state_offp)
+		void __user *u_attrs, u32 *state_offp)
 {
 	struct task_struct *tsk = current;
 	struct evl_init_thread_attr iattr;
@@ -2392,8 +2388,8 @@ static void thread_factory_dispose(struct evl_element *e)
 }
 
 static ssize_t state_show(struct device *dev,
-			  struct device_attribute *attr,
-			  char *buf)
+			struct device_attribute *attr,
+			char *buf)
 {
 	struct evl_thread *thread;
 	ssize_t ret;
@@ -2407,8 +2403,8 @@ static ssize_t state_show(struct device *dev,
 static DEVICE_ATTR_RO(state);
 
 static ssize_t sched_show(struct device *dev,
-			  struct device_attribute *attr,
-			  char *buf)
+			struct device_attribute *attr,
+			char *buf)
 {
 	struct evl_sched_class *sched_class;
 	struct evl_thread *thread;
@@ -2420,15 +2416,15 @@ static ssize_t sched_show(struct device *dev,
 	sched_class = thread->sched_class;
 
 	ret = snprintf(buf, PAGE_SIZE, "%d %d %d %s ",
-		       evl_rq_cpu(thread->rq),
-		       thread->bprio,
-		       thread->cprio,
-		       sched_class->name);
+		evl_rq_cpu(thread->rq),
+		thread->bprio,
+		thread->cprio,
+		sched_class->name);
 
 	if (sched_class->sched_show) {
 		xnlock_get_irqsave(&nklock, flags);
 		_ret = sched_class->sched_show(thread, buf + ret,
-					       PAGE_SIZE - ret);
+					PAGE_SIZE - ret);
 		xnlock_put_irqrestore(&nklock, flags);
 		if (_ret > 0) {
 			ret += _ret;
@@ -2446,8 +2442,8 @@ static ssize_t sched_show(struct device *dev,
 static DEVICE_ATTR_RO(sched);
 
 static ssize_t stats_show(struct device *dev,
-			  struct device_attribute *attr,
-			  char *buf)
+			struct device_attribute *attr,
+			char *buf)
 {
 	ktime_t period, exectime, account;
 	struct evl_thread *thread;
@@ -2490,11 +2486,11 @@ static ssize_t stats_show(struct device *dev,
 		usage = 0;
 
 	ret = snprintf(buf, PAGE_SIZE, "%lu %lu %lu %Lu %d\n",
-		       thread->stat.isw.counter,
-		       thread->stat.csw.counter,
-		       thread->stat.sc.counter,
-		       ktime_to_ns(thread->stat.account.total),
-		       usage);
+		thread->stat.isw.counter,
+		thread->stat.csw.counter,
+		thread->stat.sc.counter,
+		ktime_to_ns(thread->stat.account.total),
+		usage);
 
 	evl_put_element(&thread->element);
 
@@ -2503,15 +2499,15 @@ static ssize_t stats_show(struct device *dev,
 static DEVICE_ATTR_RO(stats);
 
 static ssize_t timeout_show(struct device *dev,
-			    struct device_attribute *attr,
-			    char *buf)
+			struct device_attribute *attr,
+			char *buf)
 {
 	struct evl_thread *thread;
 	ssize_t ret;
 
 	thread = evl_get_element_by_dev(dev, struct evl_thread);
 	ret = snprintf(buf, PAGE_SIZE, "%Lu\n",
-		       ktime_to_ns(evl_get_thread_timeout(thread)));
+		ktime_to_ns(evl_get_thread_timeout(thread)));
 	evl_put_element(&thread->element);
 
 	return ret;
diff --git a/kernel/evenless/tick.c b/kernel/evenless/tick.c
index 09061db666f..db114f5b563 100644
--- a/kernel/evenless/tick.c
+++ b/kernel/evenless/tick.c
@@ -85,7 +85,7 @@ static int proxy_set_oneshot_stopped(struct clock_event_device *ced)
 }
 
 static void proxy_device_register(struct clock_event_device *proxy_ced,
-				  struct clock_event_device *real_ced)
+				struct clock_event_device *real_ced)
 {
 	struct core_tick_device *ctd = this_cpu_ptr(&clock_cpu_device);
 
@@ -105,7 +105,7 @@ static void proxy_device_register(struct clock_event_device *proxy_ced,
 }
 
 static void proxy_device_unregister(struct clock_event_device *proxy_ced,
-				    struct clock_event_device *real_ced)
+				struct clock_event_device *real_ced)
 {
 	struct core_tick_device *ctd = this_cpu_ptr(&clock_cpu_device);
 
@@ -182,9 +182,9 @@ int evl_enable_tick(void)
 
 #ifdef CONFIG_SMP
 	ret = __request_percpu_irq(TIMER_OOB_IPI,
-				   clock_ipi_handler,
-				   IRQF_OOB, "Evenless timer IPI",
-				   &evl_machine_cpudata);
+				clock_ipi_handler,
+				IRQF_OOB, "Evenless timer IPI",
+				&evl_machine_cpudata);
 	if (ret)
 		return ret;
 #endif
@@ -282,7 +282,7 @@ void evl_program_proxy_tick(struct evl_clock *clock)
 	timer = container_of(tn, struct evl_timer, node);
 	if (timer == &this_rq->htimer) {
 		if (evl_need_resched(this_rq) ||
-		    !(this_rq->curr->state & T_ROOT)) {
+			!(this_rq->curr->state & T_ROOT)) {
 			tn = evl_get_tqueue_next(&tmb->q, tn);
 			if (tn) {
 				this_rq->lflags |= RQ_TDEFER;
@@ -318,6 +318,6 @@ void evl_program_proxy_tick(struct evl_clock *clock)
 void evl_send_timer_ipi(struct evl_clock *clock, struct evl_rq *rq)
 {
 	irq_pipeline_send_remote(TIMER_OOB_IPI,
-				 cpumask_of(evl_rq_cpu(rq)));
+				cpumask_of(evl_rq_cpu(rq)));
 }
 #endif
diff --git a/kernel/evenless/timer.c b/kernel/evenless/timer.c
index 11727d465f2..8edfd7b8a27 100644
--- a/kernel/evenless/timer.c
+++ b/kernel/evenless/timer.c
@@ -38,7 +38,7 @@ lock_timer_base(struct evl_timer *timer, unsigned long *flags)
 }
 
 static inline void unlock_timer_base(struct evl_timerbase *base,
-				     unsigned long flags)
+				unsigned long flags)
 {
 	raw_spin_unlock_irqrestore(&base->lock, flags);
 }
@@ -66,7 +66,7 @@ static bool timer_at_front(struct evl_timer *timer)
 
 /* timer base locked. */
 static void program_timer(struct evl_timer *timer,
-			  struct evl_tqueue *tq)
+			struct evl_tqueue *tq)
 {
 	struct evl_rq *rq;
 
@@ -83,7 +83,7 @@ static void program_timer(struct evl_timer *timer,
 }
 
 void evl_start_timer(struct evl_timer *timer,
-		     ktime_t value, ktime_t interval)
+		ktime_t value, ktime_t interval)
 {
 	struct evl_timerbase *base;
 	struct evl_tqueue *tq;
@@ -270,10 +270,10 @@ EXPORT_SYMBOL_GPL(__evl_set_timer_rq);
 #endif /* CONFIG_SMP */
 
 void __evl_init_timer(struct evl_timer *timer,
-		      struct evl_clock *clock,
-		      void (*handler)(struct evl_timer *timer),
-		      struct evl_rq *rq,
-		      int opflags)
+		struct evl_clock *clock,
+		void (*handler)(struct evl_timer *timer),
+		struct evl_rq *rq,
+		int opflags)
 {
 	int cpu __maybe_unused;
 
@@ -337,7 +337,7 @@ EXPORT_SYMBOL_GPL(evl_destroy_timer);
  * @rq:         runqueue to assign the timer to
  */
 void evl_bolt_timer(struct evl_timer *timer,
-		    struct evl_clock *clock, struct evl_rq *rq)
+		struct evl_clock *clock, struct evl_rq *rq)
 {	/* nklocked, IRQs off */
 	struct evl_timerbase *old_base, *new_base;
 	struct evl_clock *master = clock->master;
@@ -411,8 +411,8 @@ unsigned long evl_get_timer_overruns(struct evl_timer *timer)
 		goto done;
 
 	EVL_WARN_ON_ONCE(CORE, (timer->status &
-				(EVL_TIMER_DEQUEUED|EVL_TIMER_PERIODIC))
-			 != EVL_TIMER_PERIODIC);
+						(EVL_TIMER_DEQUEUED|EVL_TIMER_PERIODIC))
+			!= EVL_TIMER_PERIODIC);
 	tq = &base->q;
 	evl_dequeue_timer(timer, tq);
 	while (evl_tdate(timer) < now) {
@@ -440,7 +440,7 @@ EXPORT_SYMBOL_GPL(evl_get_timer_overruns);
 
 /* same or earlier date. */
 static inline bool date_is_earlier(struct evl_tnode *left,
-				   struct evl_tnode *right)
+				struct evl_tnode *right)
 {
 	return left->date < right->date
 		|| (left->date == right->date && left->prio > right->prio);
diff --git a/kernel/evenless/timerfd.c b/kernel/evenless/timerfd.c
index 262748cdf0c..ac74a11e1b8 100644
--- a/kernel/evenless/timerfd.c
+++ b/kernel/evenless/timerfd.c
@@ -21,14 +21,14 @@
 
 struct evl_timerfd {
 	struct evl_timer timer;
-	struct evl_syn readers;
+	struct evl_wait_queue readers;
 	bool ticked;
 	struct evl_poll_head poll_head;
 	struct evl_element element;
 };
 
 static void get_timer_value(struct evl_timer *__restrict__ timer,
-			    struct itimerspec *__restrict__ value)
+			struct itimerspec *__restrict__ value)
 {
 	value->it_interval = ktime_to_timespec(timer->interval);
 
@@ -41,7 +41,7 @@ static void get_timer_value(struct evl_timer *__restrict__ timer,
 }
 
 static int set_timer_value(struct evl_timer *__restrict__ timer,
-			   const struct itimerspec *__restrict__ value)
+			const struct itimerspec *__restrict__ value)
 {
 	ktime_t start, period;
 
@@ -51,8 +51,8 @@ static int set_timer_value(struct evl_timer *__restrict__ timer,
 	}
 
 	if ((unsigned long)value->it_value.tv_nsec >= ONE_BILLION ||
-	    ((unsigned long)value->it_interval.tv_nsec >= ONE_BILLION &&
-	     (value->it_value.tv_sec != 0 || value->it_value.tv_nsec != 0)))
+		((unsigned long)value->it_interval.tv_nsec >= ONE_BILLION &&
+			(value->it_value.tv_sec != 0 || value->it_value.tv_nsec != 0)))
 		return -EINVAL;
 
 	period = timespec_to_ktime(value->it_interval);
@@ -63,7 +63,7 @@ static int set_timer_value(struct evl_timer *__restrict__ timer,
 }
 
 static int set_timerfd(struct evl_timerfd *timerfd,
-		       struct evl_timerfd_setreq *sreq)
+		struct evl_timerfd_setreq *sreq)
 {
 	unsigned long flags;
 
@@ -76,7 +76,7 @@ static int set_timerfd(struct evl_timerfd *timerfd,
 }
 
 static long timerfd_oob_ioctl(struct file *filp,
-			      unsigned int cmd, unsigned long arg)
+			unsigned int cmd, unsigned long arg)
 {
 	struct evl_timerfd *timerfd = element_of(filp, struct evl_timerfd);
 	struct evl_timerfd_setreq sreq, __user *u_sreq;
@@ -93,14 +93,14 @@ static long timerfd_oob_ioctl(struct file *filp,
 		if (ret)
 			return ret;
 		if (raw_copy_to_user(&u_sreq->ovalue, &sreq.ovalue,
-				     sizeof(sreq.ovalue)))
+					sizeof(sreq.ovalue)))
 			return -EFAULT;
 		break;
 	case EVL_TFDIOC_GET:
 		get_timer_value(&timerfd->timer, &greq.value);
 		u_greq = (typeof(u_greq))arg;
 		if (raw_copy_to_user(&u_greq->value, &greq.value,
-				     sizeof(greq.value)))
+					sizeof(greq.value)))
 			return -EFAULT;
 		break;
 	default:
@@ -118,7 +118,7 @@ static void timerfd_handler(struct evl_timer *timer) /* hard IRQs off */
 
 	timerfd->ticked = true;
 	evl_signal_poll_events(&timerfd->poll_head, POLLIN);
-	evl_flush_syn(&timerfd->readers, 0);
+	evl_flush_wait(&timerfd->readers, 0);
 }
 
 static ssize_t timerfd_oob_read(struct file *filp,
@@ -145,8 +145,8 @@ static ssize_t timerfd_oob_read(struct file *filp,
 	}
 
 	do
-		ret = evl_sleep_on_syn(&timerfd->readers,
-				       EVL_INFINITE, EVL_REL);
+		ret = evl_wait_timeout(&timerfd->readers,
+				EVL_INFINITE, EVL_REL);
 	while (ret == 0 && !timerfd->ticked);
 
 	if (ret & T_BREAK) {
@@ -176,7 +176,7 @@ static ssize_t timerfd_oob_read(struct file *filp,
 }
 
 static __poll_t timerfd_oob_poll(struct file *filp,
-				 struct oob_poll_wait *wait)
+				struct oob_poll_wait *wait)
 {
 	struct evl_timerfd *timerfd = element_of(filp, struct evl_timerfd);
 
@@ -195,7 +195,7 @@ static const struct file_operations timerfd_fops = {
 
 static struct evl_element *
 timerfd_factory_build(struct evl_factory *fac, const char *name,
-		      void __user *u_attrs, u32 *state_offp)
+		void __user *u_attrs, u32 *state_offp)
 {
 	struct evl_timerfd_attrs attrs;
 	struct evl_timerfd *timerfd;
@@ -217,13 +217,13 @@ timerfd_factory_build(struct evl_factory *fac, const char *name,
 	}
 
 	ret = evl_init_element(&timerfd->element,
-			       &evl_timerfd_factory);
+			&evl_timerfd_factory);
 	if (ret)
 		goto fail_element;
 
 	evl_init_timer(&timerfd->timer, clock, timerfd_handler,
-		       NULL, EVL_TIMER_UGRAVITY);
-	evl_init_syn(&timerfd->readers, EVL_SYN_PRIO, clock, NULL);
+		NULL, EVL_TIMER_UGRAVITY);
+	evl_init_wait(&timerfd->readers, clock, EVL_WAIT_PRIO);
 	evl_init_poll_head(&timerfd->poll_head);
 
 	return &timerfd->element;
@@ -244,7 +244,7 @@ static void timerfd_factory_dispose(struct evl_element *e)
 
 	evl_destroy_timer(&timerfd->timer);
 	evl_put_clock(timerfd->readers.clock);
-	evl_destroy_syn(&timerfd->readers);
+	evl_destroy_wait(&timerfd->readers);
 	evl_destroy_element(&timerfd->element);
 
 	kfree_rcu(timerfd, element.rcu);
diff --git a/kernel/evenless/trace.c b/kernel/evenless/trace.c
index 8d798a74d4a..c0142aee2a8 100644
--- a/kernel/evenless/trace.c
+++ b/kernel/evenless/trace.c
@@ -12,7 +12,7 @@
 #include <trace/trace.h>
 
 static long trace_common_ioctl(struct file *filp, unsigned int cmd,
-			       unsigned long arg)
+			unsigned long arg)
 {
 	long ret = 0;
 
@@ -30,7 +30,7 @@ static long trace_common_ioctl(struct file *filp, unsigned int cmd,
 }
 
 static long trace_oob_ioctl(struct file *filp, unsigned int cmd,
-			    unsigned long arg)
+			unsigned long arg)
 {
 	return trace_common_ioctl(filp, cmd, arg);
 }
@@ -73,8 +73,8 @@ static long trace_ioctl(struct file *filp, unsigned int cmd,
 
 static notrace
 ssize_t trace_write(struct file *filp,
-		    const char __user *u_buf, size_t count,
-		    loff_t *ppos)
+		const char __user *u_buf, size_t count,
+		loff_t *ppos)
 {
 	return trace_oob_write(filp, u_buf, count);
 }
diff --git a/kernel/evenless/wait.c b/kernel/evenless/wait.c
new file mode 100644
index 00000000000..09e1b8a939b
--- /dev/null
+++ b/kernel/evenless/wait.c
@@ -0,0 +1,143 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2001, 2019 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <stdarg.h>
+#include <linux/signal.h>
+#include <linux/kernel.h>
+#include <linux/atomic.h>
+#include <evenless/sched.h>
+#include <evenless/wait.h>
+#include <evenless/thread.h>
+#include <evenless/clock.h>
+#include <uapi/evenless/signal.h>
+#include <trace/events/evenless.h>
+
+void evl_init_wait(struct evl_wait_queue *wq,
+		struct evl_clock *clock, int flags)
+{
+	wq->flags = flags;
+	wq->clock = clock;
+	INIT_LIST_HEAD(&wq->wait_list);
+	wq->wchan.abort_wait = evl_abort_wait;
+	wq->wchan.reorder_wait = evl_reorder_wait;
+	raw_spin_lock_init(&wq->wchan.lock);
+}
+EXPORT_SYMBOL_GPL(evl_init_wait);
+
+void evl_destroy_wait(struct evl_wait_queue *wq)
+{
+	evl_flush_wait(wq, T_RMID);
+	evl_schedule();
+}
+EXPORT_SYMBOL_GPL(evl_destroy_wait);
+
+int evl_wait_timeout(struct evl_wait_queue *wq, ktime_t timeout,
+		enum evl_tmode timeout_mode)
+{
+	struct evl_thread *curr = evl_current_thread();
+	unsigned long flags;
+	int ret;
+
+	if (IS_ENABLED(CONFIG_EVENLESS_DEBUG_MUTEX_SLEEP) &&
+		atomic_read(&curr->inband_disable_count) &&
+		(curr->state & T_WARN))
+		evl_signal_thread(curr, SIGDEBUG, SIGDEBUG_MUTEX_SLEEP);
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	trace_evl_wait(wq);
+
+	if (!(wq->flags & EVL_WAIT_PRIO))
+		list_add_tail(&curr->wait_next, &wq->wait_list);
+	else
+		list_add_priff(curr, &wq->wait_list, wprio, wait_next);
+
+	evl_block_thread_timeout(curr, T_PEND, timeout, timeout_mode,
+				wq->clock, &wq->wchan);
+	evl_schedule();
+	ret = curr->info & (T_RMID|T_TIMEO|T_BREAK);
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_wait_timeout);
+
+struct evl_thread *evl_wake_up(struct evl_wait_queue *wq,
+			struct evl_thread *waiter)
+{
+	unsigned long flags;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	trace_evl_wait_wakeup(wq);
+
+	if (list_empty(&wq->wait_list))
+		waiter = NULL;
+	else {
+		if (waiter == NULL)
+			waiter = list_first_entry(&wq->wait_list,
+						struct evl_thread, wait_next);
+		list_del(&waiter->wait_next);
+		waiter->wchan = NULL;
+		evl_resume_thread(waiter, T_PEND);
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return waiter;
+}
+EXPORT_SYMBOL_GPL(evl_wake_up);
+
+void evl_flush_wait(struct evl_wait_queue *wq, int reason)
+{
+	struct evl_thread *waiter, *tmp;
+	unsigned long flags;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	trace_evl_wait_flush(wq);
+
+	if (!list_empty(&wq->wait_list)) {
+		list_for_each_entry_safe(waiter, tmp, &wq->wait_list, wait_next) {
+			list_del(&waiter->wait_next);
+			waiter->info |= reason;
+			waiter->wchan = NULL;
+			evl_resume_thread(waiter, T_PEND);
+		}
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+}
+EXPORT_SYMBOL_GPL(evl_flush_wait);
+
+/* nklock held, irqs off */
+void evl_abort_wait(struct evl_thread *thread)
+{
+	list_del(&thread->wait_next);
+	thread->state &= ~T_PEND;
+	thread->wchan = NULL;
+}
+EXPORT_SYMBOL_GPL(evl_abort_wait);
+
+static inline struct evl_wait_queue *
+wchan_to_wait_queue(struct evl_wait_channel *wchan)
+{
+	return container_of(wchan, struct evl_wait_queue, wchan);
+}
+
+/* nklock held, irqs off */
+void evl_reorder_wait(struct evl_thread *thread)
+{
+	struct evl_wait_queue *wq = wchan_to_wait_queue(thread->wchan);
+
+	if (wq->flags & EVL_WAIT_PRIO) {
+		list_del(&thread->wait_next);
+		list_add_priff(thread, &wq->wait_list, wprio, wait_next);
+	}
+}
+EXPORT_SYMBOL_GPL(evl_reorder_wait);
diff --git a/kernel/evenless/xbuf.c b/kernel/evenless/xbuf.c
index daf34018bbf..ea7ca547f15 100644
--- a/kernel/evenless/xbuf.c
+++ b/kernel/evenless/xbuf.c
@@ -12,7 +12,7 @@
 #include <linux/wait.h>
 #include <linux/log2.h>
 #include <linux/atomic.h>
-#include <evenless/synch.h>
+#include <evenless/wait.h>
 #include <evenless/thread.h>
 #include <evenless/clock.h>
 #include <evenless/xbuf.h>
@@ -21,7 +21,7 @@
 #include <evenless/factory.h>
 #include <evenless/sched.h>
 #include <evenless/poller.h>
-#include <evenless/wait.h>
+#include <evenless/flag.h>
 #include <uapi/evenless/xbuf.h>
 
 struct xbuf_ring {
@@ -43,13 +43,13 @@ struct xbuf_ring {
 
 struct xbuf_inbound {		/* oob_write->read */
 	struct wait_queue_head i_event;
-	struct evl_wait_flag o_event;
+	struct evl_flag o_event;
 	struct irq_work irq_work;
 	struct xbuf_ring ring;
 };
 
 struct xbuf_outbound {		/* write->oob_read */
-	struct evl_wait_flag i_event;
+	struct evl_flag i_event;
 	struct wait_queue_head o_event;
 	struct irq_work irq_work;
 	struct xbuf_ring ring;
@@ -105,7 +105,7 @@ static int read_from_kernel(char *dst, struct xbuf_wdesc *src, size_t len)
 }
 
 static ssize_t do_xbuf_read(struct xbuf_ring *ring,
-			    struct xbuf_rdesc *rd, int f_flags)
+			struct xbuf_rdesc *rd, int f_flags)
 {
 	ssize_t len, ret, rbytes, n;
 	unsigned int rdoff, avail;
@@ -222,7 +222,7 @@ static ssize_t do_xbuf_read(struct xbuf_ring *ring,
 }
 
 static ssize_t do_xbuf_write(struct xbuf_ring *ring,
-			     struct xbuf_wdesc *wd, int f_flags)
+			struct xbuf_wdesc *wd, int f_flags)
 {
 	ssize_t len, ret, wbytes, n;
 	unsigned int wroff, avail;
@@ -390,7 +390,7 @@ static void inbound_clear_pollable(struct xbuf_ring *ring, int events)
 }
 
 static ssize_t xbuf_read(struct file *filp, char __user *u_buf,
-			 size_t count, loff_t *ppos)
+			size_t count, loff_t *ppos)
 {
 	struct evl_xbuf *xbuf = element_of(filp, struct evl_xbuf);
 	struct xbuf_rdesc rd = {
@@ -403,7 +403,7 @@ static ssize_t xbuf_read(struct file *filp, char __user *u_buf,
 }
 
 static ssize_t xbuf_write(struct file *filp, const char __user *u_buf,
-			  size_t count, loff_t *ppos)
+			size_t count, loff_t *ppos)
 {
 	struct evl_xbuf *xbuf = element_of(filp, struct evl_xbuf);
 	struct xbuf_wdesc wd = {
@@ -416,7 +416,7 @@ static ssize_t xbuf_write(struct file *filp, const char __user *u_buf,
 }
 
 static long xbuf_ioctl(struct file *filp,
-		       unsigned int cmd, unsigned long arg)
+		unsigned int cmd, unsigned long arg)
 {
 	return -ENOTTY;
 }
@@ -444,7 +444,7 @@ static __poll_t xbuf_poll(struct file *filp, poll_table *wait)
 }
 
 static long xbuf_oob_ioctl(struct file *filp,
-			   unsigned int cmd, unsigned long arg)
+			unsigned int cmd, unsigned long arg)
 {
 	return -ENOTTY;
 }
@@ -481,7 +481,7 @@ static int outbound_wait_output(struct xbuf_ring *ring, size_t len)
 	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, obnd.ring);
 
 	return wait_event_interruptible(xbuf->obnd.o_event,
-				  ring->fillsz + len <= ring->bufsz);
+					ring->fillsz + len <= ring->bufsz);
 }
 
 static void resume_inband_writer(struct irq_work *work)
@@ -522,7 +522,7 @@ static void outbound_clear_pollable(struct xbuf_ring *ring, int events)
 }
 
 static ssize_t xbuf_oob_read(struct file *filp,
-			     char __user *u_buf, size_t count)
+			char __user *u_buf, size_t count)
 {
 	struct evl_xbuf *xbuf = element_of(filp, struct evl_xbuf);
 	struct xbuf_rdesc rd = {
@@ -535,7 +535,7 @@ static ssize_t xbuf_oob_read(struct file *filp,
 }
 
 static ssize_t xbuf_oob_write(struct file *filp,
-			      const char __user *u_buf, size_t count)
+			const char __user *u_buf, size_t count)
 {
 	struct evl_xbuf *xbuf = element_of(filp, struct evl_xbuf);
 	struct xbuf_wdesc wd = {
@@ -548,7 +548,7 @@ static ssize_t xbuf_oob_write(struct file *filp,
 }
 
 static __poll_t xbuf_oob_poll(struct file *filp,
-			      struct oob_poll_wait *wait)
+			struct oob_poll_wait *wait)
 {
 	struct evl_xbuf *xbuf = element_of(filp, struct evl_xbuf);
 	unsigned long flags;
@@ -602,7 +602,7 @@ void evl_put_xbuf(struct evl_file *sfilp)
 EXPORT_SYMBOL_GPL(evl_put_xbuf);
 
 ssize_t evl_read_xbuf(struct evl_xbuf *xbuf, void *buf,
-		      size_t count, int f_flags)
+		size_t count, int f_flags)
 {
 	struct xbuf_rdesc rd = {
 		.buf = buf,
@@ -618,7 +618,7 @@ ssize_t evl_read_xbuf(struct evl_xbuf *xbuf, void *buf,
 EXPORT_SYMBOL_GPL(evl_read_xbuf);
 
 ssize_t evl_write_xbuf(struct evl_xbuf *xbuf, const void *buf,
-		       size_t count, int f_flags)
+		size_t count, int f_flags)
 {
 	struct xbuf_wdesc wd = {
 		.buf = buf,
@@ -635,7 +635,7 @@ EXPORT_SYMBOL_GPL(evl_write_xbuf);
 
 static struct evl_element *
 xbuf_factory_build(struct evl_factory *fac, const char *name,
-		   void __user *u_attrs, u32 *state_offp)
+		void __user *u_attrs, u32 *state_offp)
 {
 	void *i_bufmem = NULL, *o_bufmem = NULL;
 	struct evl_xbuf_attrs attrs;
@@ -648,8 +648,8 @@ xbuf_factory_build(struct evl_factory *fac, const char *name,
 
 	/* LART */
 	if ((attrs.i_bufsz == 0 && attrs.o_bufsz == 0) ||
-	    order_base_2(attrs.i_bufsz) > 30 ||
-	    order_base_2(attrs.o_bufsz) > 30)
+		order_base_2(attrs.i_bufsz) > 30 ||
+		order_base_2(attrs.o_bufsz) > 30)
 		return ERR_PTR(-EINVAL);
 
 	xbuf = kzalloc(sizeof(*xbuf), GFP_KERNEL);
@@ -737,8 +737,8 @@ static void xbuf_factory_dispose(struct evl_element *e)
 }
 
 static ssize_t rings_show(struct device *dev,
-			  struct device_attribute *attr,
-			  char *buf)
+			struct device_attribute *attr,
+			char *buf)
 {
 	struct evl_xbuf *xbuf;
 	ssize_t ret;
@@ -746,10 +746,10 @@ static ssize_t rings_show(struct device *dev,
 	xbuf = evl_get_element_by_dev(dev, struct evl_xbuf);
 
 	ret = snprintf(buf, PAGE_SIZE, "%zu %zu %zu %zu\n",
-		       xbuf->ibnd.ring.fillsz,
-		       xbuf->ibnd.ring.bufsz,
-		       xbuf->obnd.ring.fillsz,
-		       xbuf->obnd.ring.bufsz);
+		xbuf->ibnd.ring.fillsz,
+		xbuf->ibnd.ring.bufsz,
+		xbuf->obnd.ring.fillsz,
+		xbuf->obnd.ring.bufsz);
 
 	evl_put_element(&xbuf->element);
 
-- 
2.16.4

