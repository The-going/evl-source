From 9b96230585e3363350b7ee109b265892dbb97a56 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Sun, 23 Apr 2017 11:48:58 +0200
Subject: [PATCH] evenless: nothing but yet another co-kernel

---
 arch/arm/Kconfig                               |    6 +
 arch/arm/Makefile                              |    4 +
 arch/arm/include/asm/evenless/fptest.h         |   25 +
 arch/arm/include/asm/evenless/syscall.h        |   46 +
 arch/arm/include/asm/evenless/thread.h         |   29 +
 arch/arm/include/dovetail/thread_info.h        |    7 +
 arch/arm/include/uapi/asm/evenless/fptest.h    |   46 +
 arch/arm64/Kconfig                             |    3 +
 arch/arm64/Makefile                            |    4 +
 arch/arm64/include/asm/evenless/fptest.h       |   28 +
 arch/arm64/include/asm/evenless/syscall.h      |   45 +
 arch/arm64/include/asm/evenless/thread.h       |   12 +
 arch/arm64/include/dovetail/thread_info.h      |    7 +
 arch/arm64/include/uapi/asm/evenless/fptest.h  |   91 +
 arch/arm64/include/uapi/asm/evenless/syscall.h |    7 +
 drivers/Kconfig                                |    2 +
 drivers/Makefile                               |    2 +
 drivers/evenless/Kconfig                       |   13 +
 drivers/evenless/Makefile                      |    2 +
 drivers/evenless/hectic.c                      |  691 +++++++
 drivers/evenless/latmus.c                      | 1054 ++++++++++
 include/asm-generic/evenless/thread_info.h     |   17 +
 include/evenless/assert.h                      |   43 +
 include/evenless/clock.h                       |  172 ++
 include/evenless/control.h                     |   60 +
 include/evenless/factory.h                     |  151 ++
 include/evenless/file.h                        |   65 +
 include/evenless/init.h                        |   36 +
 include/evenless/list.h                        |   46 +
 include/evenless/lock.h                        |  225 ++
 include/evenless/memory.h                      |  145 ++
 include/evenless/monitor.h                     |   24 +
 include/evenless/poller.h                      |   67 +
 include/evenless/sched.h                       |  550 +++++
 include/evenless/sched/idle.h                  |   53 +
 include/evenless/sched/param.h                 |   37 +
 include/evenless/sched/queue.h                 |   59 +
 include/evenless/sched/quota.h                 |   75 +
 include/evenless/sched/rt.h                    |  117 ++
 include/evenless/sched/weak.h                  |   35 +
 include/evenless/stat.h                        |  139 ++
 include/evenless/synch.h                       |  116 ++
 include/evenless/thread.h                      |  350 ++++
 include/evenless/tick.h                        |   48 +
 include/evenless/timer.h                       |  456 ++++
 include/evenless/wait.h                        |  221 ++
 include/evenless/xbuf.h                        |   28 +
 include/trace/events/evenless.h                |  867 ++++++++
 include/uapi/evenless/clock.h                  |   23 +
 include/uapi/evenless/control.h                |   23 +
 include/uapi/evenless/devices/hectic.h         |   39 +
 include/uapi/evenless/devices/latmus.h         |   56 +
 include/uapi/evenless/factory.h                |   26 +
 include/uapi/evenless/logger.h                 |   15 +
 include/uapi/evenless/monitor.h                |   64 +
 include/uapi/evenless/poller.h                 |   40 +
 include/uapi/evenless/sched.h                  |   36 +
 include/uapi/evenless/sem.h                    |   45 +
 include/uapi/evenless/signal.h                 |   37 +
 include/uapi/evenless/synch.h                  |   65 +
 include/uapi/evenless/syscall.h                |   16 +
 include/uapi/evenless/thread.h                 |   85 +
 include/uapi/evenless/timerfd.h                |   31 +
 include/uapi/evenless/trace.h                  |   14 +
 include/uapi/evenless/types.h                  |   29 +
 include/uapi/evenless/xbuf.h                   |   17 +
 kernel/Kconfig.evenless                        |   29 +
 kernel/Makefile                                |    1 +
 kernel/evenless/.gitignore                     |    1 +
 kernel/evenless/Kconfig                        |  300 +++
 kernel/evenless/Makefile                       |   26 +
 kernel/evenless/clock.c                        |  825 ++++++++
 kernel/evenless/control.c                      |  212 ++
 kernel/evenless/factory.c                      |  673 ++++++
 kernel/evenless/file.c                         |  255 +++
 kernel/evenless/init.c                         |  200 ++
 kernel/evenless/irq.c                          |   34 +
 kernel/evenless/lock.c                         |  102 +
 kernel/evenless/logger.c                       |  230 +++
 kernel/evenless/memory.c                       |  723 +++++++
 kernel/evenless/monitor.c                      |  601 ++++++
 kernel/evenless/poller.c                       |  559 +++++
 kernel/evenless/sched/Makefile                 |   11 +
 kernel/evenless/sched/core.c                   | 1064 ++++++++++
 kernel/evenless/sched/idle.c                   |   47 +
 kernel/evenless/sched/quota.c                  |  660 ++++++
 kernel/evenless/sched/rt.c                     |  135 ++
 kernel/evenless/sched/weak.c                   |  104 +
 kernel/evenless/sem.c                          |  310 +++
 kernel/evenless/synch.c                        |  896 ++++++++
 kernel/evenless/syscall.c                      |  341 +++
 kernel/evenless/thread.c                       | 2643 ++++++++++++++++++++++++
 kernel/evenless/tick.c                         |  323 +++
 kernel/evenless/timer.c                        |  471 +++++
 kernel/evenless/timerfd.c                      |  260 +++
 kernel/evenless/trace.c                        |   93 +
 kernel/evenless/xbuf.c                         |  774 +++++++
 97 files changed, 19890 insertions(+)
 create mode 100644 arch/arm/include/asm/evenless/fptest.h
 create mode 100644 arch/arm/include/asm/evenless/syscall.h
 create mode 100644 arch/arm/include/asm/evenless/thread.h
 create mode 100644 arch/arm/include/dovetail/thread_info.h
 create mode 100644 arch/arm/include/uapi/asm/evenless/fptest.h
 create mode 100644 arch/arm64/include/asm/evenless/fptest.h
 create mode 100644 arch/arm64/include/asm/evenless/syscall.h
 create mode 100644 arch/arm64/include/asm/evenless/thread.h
 create mode 100644 arch/arm64/include/dovetail/thread_info.h
 create mode 100644 arch/arm64/include/uapi/asm/evenless/fptest.h
 create mode 100644 arch/arm64/include/uapi/asm/evenless/syscall.h
 create mode 100644 drivers/evenless/Kconfig
 create mode 100644 drivers/evenless/Makefile
 create mode 100644 drivers/evenless/hectic.c
 create mode 100644 drivers/evenless/latmus.c
 create mode 100644 include/asm-generic/evenless/thread_info.h
 create mode 100644 include/evenless/assert.h
 create mode 100644 include/evenless/clock.h
 create mode 100644 include/evenless/control.h
 create mode 100644 include/evenless/factory.h
 create mode 100644 include/evenless/file.h
 create mode 100644 include/evenless/init.h
 create mode 100644 include/evenless/list.h
 create mode 100644 include/evenless/lock.h
 create mode 100644 include/evenless/memory.h
 create mode 100644 include/evenless/monitor.h
 create mode 100644 include/evenless/poller.h
 create mode 100644 include/evenless/sched.h
 create mode 100644 include/evenless/sched/idle.h
 create mode 100644 include/evenless/sched/param.h
 create mode 100644 include/evenless/sched/queue.h
 create mode 100644 include/evenless/sched/quota.h
 create mode 100644 include/evenless/sched/rt.h
 create mode 100644 include/evenless/sched/weak.h
 create mode 100644 include/evenless/stat.h
 create mode 100644 include/evenless/synch.h
 create mode 100644 include/evenless/thread.h
 create mode 100644 include/evenless/tick.h
 create mode 100644 include/evenless/timer.h
 create mode 100644 include/evenless/wait.h
 create mode 100644 include/evenless/xbuf.h
 create mode 100644 include/trace/events/evenless.h
 create mode 100644 include/uapi/evenless/clock.h
 create mode 100644 include/uapi/evenless/control.h
 create mode 100644 include/uapi/evenless/devices/hectic.h
 create mode 100644 include/uapi/evenless/devices/latmus.h
 create mode 100644 include/uapi/evenless/factory.h
 create mode 100644 include/uapi/evenless/logger.h
 create mode 100644 include/uapi/evenless/monitor.h
 create mode 100644 include/uapi/evenless/poller.h
 create mode 100644 include/uapi/evenless/sched.h
 create mode 100644 include/uapi/evenless/sem.h
 create mode 100644 include/uapi/evenless/signal.h
 create mode 100644 include/uapi/evenless/synch.h
 create mode 100644 include/uapi/evenless/syscall.h
 create mode 100644 include/uapi/evenless/thread.h
 create mode 100644 include/uapi/evenless/timerfd.h
 create mode 100644 include/uapi/evenless/trace.h
 create mode 100644 include/uapi/evenless/types.h
 create mode 100644 include/uapi/evenless/xbuf.h
 create mode 100644 kernel/Kconfig.evenless
 create mode 100644 kernel/evenless/.gitignore
 create mode 100644 kernel/evenless/Kconfig
 create mode 100644 kernel/evenless/Makefile
 create mode 100644 kernel/evenless/clock.c
 create mode 100644 kernel/evenless/control.c
 create mode 100644 kernel/evenless/factory.c
 create mode 100644 kernel/evenless/file.c
 create mode 100644 kernel/evenless/init.c
 create mode 100644 kernel/evenless/irq.c
 create mode 100644 kernel/evenless/lock.c
 create mode 100644 kernel/evenless/logger.c
 create mode 100644 kernel/evenless/memory.c
 create mode 100644 kernel/evenless/monitor.c
 create mode 100644 kernel/evenless/poller.c
 create mode 100644 kernel/evenless/sched/Makefile
 create mode 100644 kernel/evenless/sched/core.c
 create mode 100644 kernel/evenless/sched/idle.c
 create mode 100644 kernel/evenless/sched/quota.c
 create mode 100644 kernel/evenless/sched/rt.c
 create mode 100644 kernel/evenless/sched/weak.c
 create mode 100644 kernel/evenless/sem.c
 create mode 100644 kernel/evenless/synch.c
 create mode 100644 kernel/evenless/syscall.c
 create mode 100644 kernel/evenless/thread.c
 create mode 100644 kernel/evenless/tick.c
 create mode 100644 kernel/evenless/timer.c
 create mode 100644 kernel/evenless/timerfd.c
 create mode 100644 kernel/evenless/trace.c
 create mode 100644 kernel/evenless/xbuf.c

diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index 982d61cc4331..4d723715cad1 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -279,6 +279,8 @@ config PGTABLE_LEVELS
 	default 3 if ARM_LPAE
 	default 2
 
+source "kernel/Kconfig.evenless"
+
 menu "System Type"
 
 config MMU
@@ -672,6 +674,10 @@ config ARCH_MULTI_V6_V7
 	select MIGHT_HAVE_CACHE_L2X0
 	select HAVE_IRQ_PIPELINE
 	select HAVE_DOVETAIL
+	select HAVE_ARCH_EVENLESS
+	select WARN_CPUFREQ_GOVERNOR if CPU_FREQ && \
+	       !(CPU_FREQ_DEFAULT_GOV_PERFORMANCE || \
+	        CPU_FREQ_DEFAULT_GOV_POWERSAVE)
 
 config ARCH_MULTI_CPU_AUTO
 	def_bool !(ARCH_MULTI_V4 || ARCH_MULTI_V4T || ARCH_MULTI_V6_V7)
diff --git a/arch/arm/Makefile b/arch/arm/Makefile
index f863c6935d0e..eb0f10b85933 100644
--- a/arch/arm/Makefile
+++ b/arch/arm/Makefile
@@ -269,6 +269,10 @@ KBUILD_CPPFLAGS += $(patsubst %,-I$(srctree)/%include,$(machdirs) $(platdirs))
 endif
 endif
 
+ifeq ($(CONFIG_EVENLESS),y)
+KBUILD_CFLAGS += -Iarch/$(SRCARCH)/evenless/include -Iinclude/evenless
+endif
+
 export	TEXT_OFFSET GZFLAGS MMUEXT
 
 # Do we have FASTFPE?
diff --git a/arch/arm/include/asm/evenless/fptest.h b/arch/arm/include/asm/evenless/fptest.h
new file mode 100644
index 000000000000..b1fb66f07195
--- /dev/null
+++ b/arch/arm/include/asm/evenless/fptest.h
@@ -0,0 +1,25 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _EVENLESS_ARM_ASM_FPTEST_H
+#define _EVENLESS_ARM_ASM_FPTEST_H
+
+#include <linux/cpufeature.h>
+#include <uapi/asm/evenless/fptest.h>
+
+static inline bool evl_begin_fpu(void)
+{
+	return false;
+}
+
+static inline void evl_end_fpu(void) { }
+
+static inline u32 evl_detect_fpu(void)
+{
+	u32 features = 0;
+
+	if (elf_hwcap & HWCAP_VFP)
+		return features |= evl_arm_vfp;
+
+	return features;
+}
+
+#endif /* _EVENLESS_ARM_ASM_FPTEST_H */
diff --git a/arch/arm/include/asm/evenless/syscall.h b/arch/arm/include/asm/evenless/syscall.h
new file mode 100644
index 000000000000..62d9ae76ab2c
--- /dev/null
+++ b/arch/arm/include/asm/evenless/syscall.h
@@ -0,0 +1,46 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+#ifndef _EVENLESS_ARM_ASM_SYSCALL_H
+#define _EVENLESS_ARM_ASM_SYSCALL_H
+
+#include <linux/uaccess.h>
+#include <asm/unistd.h>
+#include <asm/ptrace.h>
+#include <asm/syscall.h>
+#include <uapi/asm/dovetail.h>
+
+#define raw_put_user(src, dst)  __put_user(src, dst)
+#define raw_get_user(dst, src)  __get_user(dst, src)
+
+#define is_oob_syscall(__regs)	((__regs)->ARM_r7 == __ARM_NR_dovetail)
+#define oob_syscall_nr(__regs)	((__regs)->ARM_ORIG_r0)
+
+#define oob_retval(__regs)	((__regs)->ARM_r0)
+#define oob_arg1(__regs)	((__regs)->ARM_r1)
+#define oob_arg2(__regs)	((__regs)->ARM_r2)
+#define oob_arg3(__regs)	((__regs)->ARM_r3)
+#define oob_arg4(__regs)	((__regs)->ARM_r4)
+#define oob_arg5(__regs)	((__regs)->ARM_r5)
+
+/*
+ * Fetch and test inband syscall number (valid only if
+ * !is_oob_syscall(__regs)).
+ */
+#define inband_syscall_nr(__regs, __nr)					\
+	({								\
+		*(__nr) = (__regs)->ARM_r7;				\
+		*(__nr) < NR_syscalls || *(__nr) >= __ARM_NR_BASE;	\
+	})
+
+static inline void
+set_oob_error(struct pt_regs *regs, int err)
+{
+	oob_retval(regs) = err;
+}
+
+static inline
+void set_oob_retval(struct pt_regs *regs, long ret)
+{
+	oob_retval(regs) = ret;
+}
+
+#endif /* !_EVENLESS_ARM_ASM_SYSCALL_H */
diff --git a/arch/arm/include/asm/evenless/thread.h b/arch/arm/include/asm/evenless/thread.h
new file mode 100644
index 000000000000..00a4649c2885
--- /dev/null
+++ b/arch/arm/include/asm/evenless/thread.h
@@ -0,0 +1,29 @@
+/*
+ * Copyright (C) 2005 Stelian Pop
+ *
+ * Xenomai is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * Xenomai is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with Xenomai; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA
+ * 02111-1307, USA.
+ */
+#ifndef _EVENLESS_ARM_ASM_THREAD_H
+#define _EVENLESS_ARM_ASM_THREAD_H
+
+#define xnarch_fault_pf_p(__trapnr)	((__trapnr) == ARM_TRAP_ACCESS)
+#define xnarch_fault_bp_p(__trapnr)	((current->ptrace & PT_PTRACED) && \
+					 (__trapnr == ARM_TRAP_BREAK || \
+					  (__trapnr) == ARM_TRAP_UNDEFINSTR))
+
+#define xnarch_fault_notify(__trapnr) (!xnarch_fault_bp_p(__trapnr))
+
+#endif /* !_EVENLESS_ARM_ASM_THREAD_H */
diff --git a/arch/arm/include/dovetail/thread_info.h b/arch/arm/include/dovetail/thread_info.h
new file mode 100644
index 000000000000..253e15ec959c
--- /dev/null
+++ b/arch/arm/include/dovetail/thread_info.h
@@ -0,0 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _EVENLESS_DOVETAIL_THREAD_INFO_H
+#define _EVENLESS_DOVETAIL_THREAD_INFO_H
+
+#include <asm-generic/evenless/thread_info.h>
+
+#endif /* !_EVENLESS_DOVETAIL_THREAD_INFO_H */
diff --git a/arch/arm/include/uapi/asm/evenless/fptest.h b/arch/arm/include/uapi/asm/evenless/fptest.h
new file mode 100644
index 000000000000..c1d7f37642fd
--- /dev/null
+++ b/arch/arm/include/uapi/asm/evenless/fptest.h
@@ -0,0 +1,46 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ * Derived from Xenomai's Cobalt core:
+ * Copyright (C) 2006 Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>.
+ */
+#ifndef _EVENLESS_ARM_ASM_UAPI_FPTEST_H
+#define _EVENLESS_ARM_ASM_UAPI_FPTEST_H
+
+#define evl_arm_vfp  0x1
+
+#define evl_set_fpregs(__features, __val)				\
+	do {								\
+		unsigned int __i;					\
+		__u64 __e[16];						\
+									\
+		if (__features & evl_arm_vfp) {				\
+			for (__i = 0; __i < 16; __i++)			\
+				__e[__i] = (__val);			\
+			/* vldm %0!, {d0-d15}, AKA fldmiax %0!, {d0-d15} */ \
+			__asm__ __volatile__("ldc p11, cr0, [%0],#32*4": \
+					     "=r"(__i):			\
+					     "0"(&__e[0]): "memory");	\
+		}							\
+	} while (0)
+
+#define evl_check_fpregs(__features, __val, __bad)			\
+	({								\
+		unsigned int __result = (__val), __i;			\
+		__u64 __e[16];						\
+									\
+		if (__features & evl_arm_vfp) {				\
+			/* vstm %0!, {d0-d15}, AKA fstmiax %0!, {d0-d15} */ \
+			__asm__ __volatile__("stc p11, cr0, [%0],#32*4": \
+					     "=r"(__i):			\
+					     "0"(&__e[0]): "memory");	\
+			for (__i = 0; __i < 16; __i++)			\
+				if (__e[__i] != __val) {		\
+					__result = __e[__i];		\
+					(__bad) = __i;			\
+					break;				\
+				}					\
+		}							\
+		__result;						\
+	})
+
+#endif /* !_EVENLESS_ARM_ASM_UAPI_FPTEST_H */
diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 2e9dbdbcca4e..eb0e98459bb9 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -128,6 +128,7 @@ config ARM64
 	select HAVE_ARCH_TRACEHOOK
 	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
 	select HAVE_ARCH_VMAP_STACK
+	select HAVE_ARCH_EVENLESS
 	select HAVE_ARM_SMCCC
 	select HAVE_EBPF_JIT
 	select HAVE_C_RECORDMCOUNT
@@ -1475,6 +1476,8 @@ config STACKPROTECTOR_PER_TASK
 
 endmenu
 
+source "kernel/Kconfig.evenless"
+
 menu "Boot options"
 
 config ARM64_ACPI_PARKING_PROTOCOL
diff --git a/arch/arm64/Makefile b/arch/arm64/Makefile
index b025304bde46..ada574d901ca 100644
--- a/arch/arm64/Makefile
+++ b/arch/arm64/Makefile
@@ -85,6 +85,10 @@ endif
 
 CHECKFLAGS	+= -D__aarch64__
 
+ifeq ($(CONFIG_EVENLESS),y)
+KBUILD_CFLAGS += -Iarch/$(SRCARCH)/evenless/include -Iinclude/evenless
+endif
+
 ifeq ($(CONFIG_ARM64_MODULE_PLTS),y)
 KBUILD_LDFLAGS_MODULE	+= -T $(srctree)/arch/arm64/kernel/module.lds
 endif
diff --git a/arch/arm64/include/asm/evenless/fptest.h b/arch/arm64/include/asm/evenless/fptest.h
new file mode 100644
index 000000000000..dc4c6501334f
--- /dev/null
+++ b/arch/arm64/include/asm/evenless/fptest.h
@@ -0,0 +1,28 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _EVENLESS_ARM64_ASM_FPTEST_H
+#define _EVENLESS_ARM64_ASM_FPTEST_H
+
+#include <linux/cpufeature.h>
+#include <uapi/asm/evenless/fptest.h>
+
+static inline bool evl_begin_fpu(void)
+{
+	return false;
+}
+
+static inline void evl_end_fpu(void) { }
+
+static inline u32 evl_detect_fpu(void)
+{
+	u32 features = 0;
+
+	if (system_supports_fpsimd())
+		return features |= evl_arm64_fpsimd;
+
+	if (system_supports_sve())
+		return features |= evl_arm64_sve;
+
+	return features;
+}
+
+#endif /* _EVENLESS_ARM64_ASM_FPTEST_H */
diff --git a/arch/arm64/include/asm/evenless/syscall.h b/arch/arm64/include/asm/evenless/syscall.h
new file mode 100644
index 000000000000..29cea6da6ea3
--- /dev/null
+++ b/arch/arm64/include/asm/evenless/syscall.h
@@ -0,0 +1,45 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _EVENLESS_ARM64_ASM_SYSCALL_H
+#define _EVENLESS_ARM64_ASM_SYSCALL_H
+
+#include <linux/uaccess.h>
+#include <asm/unistd.h>
+#include <asm/ptrace.h>
+#include <uapi/asm/evenless/syscall.h>
+
+#define raw_put_user(src, dst)  __put_user(src, dst)
+#define raw_get_user(dst, src)  __get_user(dst, src)
+
+#define is_oob_syscall(__regs)	((__regs)->syscallno & __EVENLESS_SYSCALL_BIT)
+#define oob_syscall_nr(__regs)	((__regs)->syscallno & ~__EVENLESS_SYSCALL_BIT)
+
+#define oob_retval(__regs)	((__regs)->regs[0])
+#define oob_arg1(__regs)	((__regs)->regs[0])
+#define oob_arg2(__regs)	((__regs)->regs[1])
+#define oob_arg3(__regs)	((__regs)->regs[2])
+#define oob_arg4(__regs)	((__regs)->regs[3])
+#define oob_arg5(__regs)	((__regs)->regs[4])
+
+/*
+ * Fetch and test inband syscall number (valid only if
+ * !is_oob_syscall(__regs)).
+ */
+#define inband_syscall_nr(__regs, __nr)			\
+	({						\
+		*(__nr) = oob_syscall_nr(__regs);	\
+		!is_oob_syscall(__regs);		\
+	})
+
+static inline void
+set_oob_error(struct pt_regs *regs, int err)
+{
+	oob_retval(regs) = err;
+}
+
+static inline
+void set_oob_retval(struct pt_regs *regs, long ret)
+{
+	oob_retval(regs) = ret;
+}
+
+#endif /* !_EVENLESS_ARM64_ASM_SYSCALL_H */
diff --git a/arch/arm64/include/asm/evenless/thread.h b/arch/arm64/include/asm/evenless/thread.h
new file mode 100644
index 000000000000..07157c43d586
--- /dev/null
+++ b/arch/arm64/include/asm/evenless/thread.h
@@ -0,0 +1,12 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _EVENLESS_ARM64_ASM_THREAD_H
+#define _EVENLESS_ARM64_ASM_THREAD_H
+
+#define xnarch_fault_pf_p(__trapnr)	((__trapnr) == ARM64_TRAP_ACCESS)
+#define xnarch_fault_bp_p(__trapnr)	((current->ptrace & PT_PTRACED) && \
+					 (__trapnr == ARM64_TRAP_DEBUG || \
+					  (__trapnr) == ARM64_TRAP_UNDI))
+
+#define xnarch_fault_notify(__trapnr) (!xnarch_fault_bp_p(__trapnr))
+
+#endif /* !_EVENLESS_ARM64_ASM_THREAD_H */
diff --git a/arch/arm64/include/dovetail/thread_info.h b/arch/arm64/include/dovetail/thread_info.h
new file mode 100644
index 000000000000..253e15ec959c
--- /dev/null
+++ b/arch/arm64/include/dovetail/thread_info.h
@@ -0,0 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _EVENLESS_DOVETAIL_THREAD_INFO_H
+#define _EVENLESS_DOVETAIL_THREAD_INFO_H
+
+#include <asm-generic/evenless/thread_info.h>
+
+#endif /* !_EVENLESS_DOVETAIL_THREAD_INFO_H */
diff --git a/arch/arm64/include/uapi/asm/evenless/fptest.h b/arch/arm64/include/uapi/asm/evenless/fptest.h
new file mode 100644
index 000000000000..e78f7984838a
--- /dev/null
+++ b/arch/arm64/include/uapi/asm/evenless/fptest.h
@@ -0,0 +1,91 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ * Derived from Xenomai's Cobalt core:
+ * Copyright (C) 2006 Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>.
+ */
+#ifndef _EVENLESS_ARM64_ASM_UAPI_FPTEST_H
+#define _EVENLESS_ARM64_ASM_UAPI_FPTEST_H
+
+#define evl_arm64_fpsimd  0x1
+#define evl_arm64_sve     0x2
+
+/*
+ * CAUTION: keep this code strictly inlined in macros: we don't want
+ * GCC to apply any callee-saved logic to fpsimd registers in
+ * evl_set_fpregs() before evl_check_fpregs() can verify their
+ * contents, but we still want GCC to know about the registers we have
+ * clobbered.
+ */
+
+#define evl_set_fpregs(__features, __val)				\
+	do {								\
+		unsigned int __i;					\
+		__u64 __e[32];						\
+									\
+		if (__features & evl_arm64_fpsimd) {			\
+			for (__i = 0; __i < 32; __i++)			\
+				__e[__i] = (__val);			\
+			__asm__ __volatile__("ldp  d0, d1, [%0, #8 * 0] \n"	\
+					     "ldp  d2, d3, [%0, #8 * 2] \n"	\
+					     "ldp  d4, d5, [%0, #8 * 4]\n"	\
+					     "ldp  d6, d7, [%0, #8 * 6]\n"	\
+					     "ldp  d8, d9, [%0, #8 * 8]\n"	\
+					     "ldp  d10, d11, [%0, #8 * 10]\n"	\
+					     "ldp  d12, d13, [%0, #8 * 12]\n"	\
+					     "ldp  d14, d15, [%0, #8 * 14]\n"	\
+					     "ldp  d16, d17, [%0, #8 * 16]\n"	\
+					     "ldp  d18, d19, [%0, #8 * 18]\n"	\
+					     "ldp  d20, d21, [%0, #8 * 20]\n"	\
+					     "ldp  d22, d23, [%0, #8 * 22]\n"	\
+					     "ldp  d24, d25, [%0, #8 * 24]\n"	\
+					     "ldp  d26, d27, [%0, #8 * 26]\n"	\
+					     "ldp  d28, d29, [%0, #8 * 28]\n"	\
+					     "ldp  d30, d31, [%0, #8 * 30]\n"	\
+					     : /* No outputs. */	\
+					     : "r"(&__e[0])		\
+					     : "d0", "d1", "d2", "d3", "d4", "d5", "d6",	\
+					       "d7", "d8", "d9", "d10", "d11", "d12", "d13",	\
+					       "d14", "d15", "d16", "d17", "d18", "d19",	\
+					       "d20", "d21", "d22", "d23", "d24", "d25",	\
+					       "d26", "d27", "d28", "d29", "d30", "d31",	\
+					       "memory");		\
+		}							\
+	} while (0)
+
+#define evl_check_fpregs(__features, __val, __bad)			\
+	({								\
+		unsigned int __result = (__val), __i;			\
+		__u64 __e[32];						\
+									\
+		if (__features & evl_arm64_fpsimd) {			\
+			__asm__ __volatile__("stp  d0, d1, [%0, #8 * 0] \n"	\
+					     "stp  d2, d3, [%0, #8 * 2] \n"	\
+					     "stp  d4, d5, [%0, #8 * 4]\n"	\
+					     "stp  d6, d7, [%0, #8 * 6]\n"	\
+					     "stp  d8, d9, [%0, #8 * 8]\n"	\
+					     "stp  d10, d11, [%0, #8 * 10]\n"	\
+					     "stp  d12, d13, [%0, #8 * 12]\n"	\
+					     "stp  d14, d15, [%0, #8 * 14]\n"	\
+					     "stp  d16, d17, [%0, #8 * 16]\n"	\
+					     "stp  d18, d19, [%0, #8 * 18]\n"	\
+					     "stp  d20, d21, [%0, #8 * 20]\n"	\
+					     "stp  d22, d23, [%0, #8 * 22]\n"	\
+					     "stp  d24, d25, [%0, #8 * 24]\n"	\
+					     "stp  d26, d27, [%0, #8 * 26]\n"	\
+					     "stp  d28, d29, [%0, #8 * 28]\n"	\
+					     "stp  d30, d31, [%0, #8 * 30]\n"	\
+					     :  /* No outputs. */	\
+					     : "r"(&__e[0])		\
+					     : "memory");		\
+									\
+			for (__i = 0; __i < 32; __i++)			\
+				if (__e[__i] != __val) {		\
+					__result = __e[__i];		\
+					(__bad) = __i;			\
+					break;				\
+				}					\
+		}							\
+		__result;						\
+	})
+
+#endif /* !_EVENLESS_ARM64_ASM_UAPI_FPTEST_H */
diff --git a/arch/arm64/include/uapi/asm/evenless/syscall.h b/arch/arm64/include/uapi/asm/evenless/syscall.h
new file mode 100644
index 000000000000..bc79392559e2
--- /dev/null
+++ b/arch/arm64/include/uapi/asm/evenless/syscall.h
@@ -0,0 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+#ifndef _EVENLESS_ARM64_ASM_UAPI_SYSCALL_H
+#define _EVENLESS_ARM64_ASM_UAPI_SYSCALL_H
+
+#define __EVENLESS_SYSCALL_BIT	0x10000000
+
+#endif /* !_EVENLESS_ARM64_ASM_UAPI_SYSCALL_H */
diff --git a/drivers/Kconfig b/drivers/Kconfig
index e8231663f201..35c8d596043c 100644
--- a/drivers/Kconfig
+++ b/drivers/Kconfig
@@ -30,6 +30,8 @@ source "drivers/block/Kconfig"
 
 source "drivers/nvme/Kconfig"
 
+source "drivers/evenless/Kconfig"
+
 # misc before ide - BLK_DEV_SGIIOC4 depends on SGI_IOC4
 
 source "drivers/misc/Kconfig"
diff --git a/drivers/Makefile b/drivers/Makefile
index 28b030d7988d..6922d10219da 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -157,6 +157,8 @@ obj-$(CONFIG_REMOTEPROC)	+= remoteproc/
 obj-$(CONFIG_RPMSG)		+= rpmsg/
 obj-$(CONFIG_SOUNDWIRE)		+= soundwire/
 
+obj-$(CONFIG_EVENLESS)		+= evenless/
+
 # Virtualization drivers
 obj-$(CONFIG_VIRT_DRIVERS)	+= virt/
 obj-$(CONFIG_HYPERV)		+= hv/
diff --git a/drivers/evenless/Kconfig b/drivers/evenless/Kconfig
new file mode 100644
index 000000000000..a9c4248d8689
--- /dev/null
+++ b/drivers/evenless/Kconfig
@@ -0,0 +1,13 @@
+menu "Evenless OOB devices"
+
+config EVENLESS_LATMUS
+	bool "Latency calibration and measurement"
+	depends on EVENLESS
+	default y
+
+config EVENLESS_HECTIC
+	bool "OOB context switching validator"
+	depends on EVENLESS
+	default y
+
+endmenu
diff --git a/drivers/evenless/Makefile b/drivers/evenless/Makefile
new file mode 100644
index 000000000000..05cf4f8100a3
--- /dev/null
+++ b/drivers/evenless/Makefile
@@ -0,0 +1,2 @@
+obj-$(CONFIG_EVENLESS_LATMUS)	+= latmus.o
+obj-$(CONFIG_EVENLESS_HECTIC)	+= hectic.o
diff --git a/drivers/evenless/hectic.c b/drivers/evenless/hectic.c
new file mode 100644
index 000000000000..8d20d98cff83
--- /dev/null
+++ b/drivers/evenless/hectic.c
@@ -0,0 +1,691 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt's switchtest driver
+ * (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2010 Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>.
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>
+#include <linux/semaphore.h>
+#include <linux/irq_work.h>
+#include <evenless/synch.h>
+#include <evenless/thread.h>
+#include <evenless/wait.h>
+#include <evenless/file.h>
+#include <asm/evenless/fptest.h>
+#include <uapi/evenless/devices/hectic.h>
+
+#define RTSWITCH_RT      0x10000
+#define RTSWITCH_NRT     0
+#define RTSWITCH_KERNEL  0x20000
+
+struct rtswitch_context;
+
+struct rtswitch_task {
+	struct hectic_task_index base;
+	struct evl_wait_flag rt_synch;
+	struct semaphore nrt_synch;
+	struct evl_kthread kthread; /* For kernel-space real-time tasks. */
+	unsigned int last_switch;
+	struct rtswitch_context *ctx;
+};
+
+struct rtswitch_context {
+	struct rtswitch_task *tasks;
+	unsigned int tasks_count;
+	unsigned int next_index;
+	struct semaphore lock;
+	unsigned int cpu;
+	unsigned int switches_count;
+
+	unsigned long pause_us;
+	unsigned int next_task;
+	struct evl_timer wake_up_delay;
+
+	bool failed;
+	struct hectic_error error;
+
+	struct rtswitch_task *utask;
+	struct irq_work wake_utask;
+	struct evl_file efile;
+};
+
+static u32 fp_features;
+
+static void handle_fpu_error(struct rtswitch_context *ctx,
+			     unsigned int fp_in, unsigned int fp_out,
+			     int bad_reg)
+{
+	struct rtswitch_task *cur = &ctx->tasks[ctx->error.last_switch.to];
+	unsigned int i;
+
+	printk(EVL_ERR "fpreg%d trashed: in=%u, out=%u\n",
+	       bad_reg, fp_in, fp_out);
+
+	ctx->failed = true;
+	ctx->error.fp_val = fp_out;
+
+	if ((cur->base.flags & RTSWITCH_RT) == RTSWITCH_RT)
+		for (i = 0; i < ctx->tasks_count; i++) {
+			struct rtswitch_task *task = &ctx->tasks[i];
+
+			/* Find the first non kernel-space task. */
+			if ((task->base.flags & RTSWITCH_KERNEL))
+				continue;
+
+			/* Unblock it. */
+			switch(task->base.flags & RTSWITCH_RT) {
+			case RTSWITCH_NRT:
+				ctx->utask = task;
+				irq_work_queue(&ctx->wake_utask);
+				break;
+
+			case RTSWITCH_RT:
+				evl_raise_flag(&task->rt_synch);
+				break;
+			}
+
+			evl_stop_thread(&cur->kthread.thread, T_SUSP);
+		}
+}
+
+static int rtswitch_pend_rt(struct rtswitch_context *ctx,
+			    unsigned int idx)
+{
+	struct rtswitch_task *task;
+	int rc;
+
+	if (idx > ctx->tasks_count)
+		return -EINVAL;
+
+	task = &ctx->tasks[idx];
+	task->base.flags |= RTSWITCH_RT;
+
+	rc = evl_wait_flag(&task->rt_synch);
+	if (rc < 0)
+		return rc;
+
+	if (ctx->failed)
+		return 1;
+
+	return 0;
+}
+
+static void timed_wake_up(struct evl_timer *timer) /* hard irqs off */
+{
+	struct rtswitch_context *ctx;
+	struct rtswitch_task *task;
+
+	ctx = container_of(timer, struct rtswitch_context, wake_up_delay);
+	task = &ctx->tasks[ctx->next_task];
+
+	switch (task->base.flags & RTSWITCH_RT) {
+	case RTSWITCH_NRT:
+		ctx->utask = task;
+		irq_work_queue(&ctx->wake_utask);
+		break;
+
+	case RTSWITCH_RT:
+		evl_raise_flag(&task->rt_synch);
+	}
+}
+
+static int rtswitch_to_rt(struct rtswitch_context *ctx,
+			  unsigned int from_idx,
+			  unsigned int to_idx)
+{
+	struct rtswitch_task *from, *to;
+	int rc;
+
+	if (from_idx > ctx->tasks_count || to_idx > ctx->tasks_count)
+		return -EINVAL;
+
+	/* to == from is a special case which means
+	   "return to the previous task". */
+	if (to_idx == from_idx)
+		to_idx = ctx->error.last_switch.from;
+
+	from = &ctx->tasks[from_idx];
+	to = &ctx->tasks[to_idx];
+
+	from->base.flags |= RTSWITCH_RT;
+	from->last_switch = ++ctx->switches_count;
+	ctx->error.last_switch.from = from_idx;
+	ctx->error.last_switch.to = to_idx;
+	barrier();
+
+	if (ctx->pause_us) {
+		ctx->next_task = to_idx;
+		barrier();
+		evl_start_timer(&ctx->wake_up_delay,
+				evl_abs_timeout(&ctx->wake_up_delay,
+						ctx->pause_us * 1000),
+				EVL_INFINITE);
+		evl_disable_preempt();
+	} else
+		switch (to->base.flags & RTSWITCH_RT) {
+		case RTSWITCH_NRT:
+			ctx->utask = to;
+			barrier();
+			irq_work_queue(&ctx->wake_utask);
+			evl_disable_preempt();
+			break;
+
+		case RTSWITCH_RT:
+			evl_disable_preempt();
+			evl_raise_flag(&to->rt_synch);
+			break;
+
+		default:
+			return -EINVAL;
+		}
+
+	rc = evl_wait_flag(&from->rt_synch);
+	evl_enable_preempt();
+
+	if (rc < 0)
+		return rc;
+
+	if (ctx->failed)
+		return 1;
+
+	return 0;
+}
+
+static int rtswitch_pend_nrt(struct rtswitch_context *ctx,
+			     unsigned int idx)
+{
+	struct rtswitch_task *task;
+
+	if (idx > ctx->tasks_count)
+		return -EINVAL;
+
+	task = &ctx->tasks[idx];
+
+	task->base.flags &= ~RTSWITCH_RT;
+
+	if (down_interruptible(&task->nrt_synch))
+		return -EINTR;
+
+	if (ctx->failed)
+		return 1;
+
+	return 0;
+}
+
+static int rtswitch_to_nrt(struct rtswitch_context *ctx,
+			   unsigned int from_idx,
+			   unsigned int to_idx)
+{
+	struct rtswitch_task *from, *to;
+	unsigned int expected, fp_val;
+	bool fp_check;
+	int bad_reg;
+
+	if (from_idx > ctx->tasks_count || to_idx > ctx->tasks_count)
+		return -EINVAL;
+
+	/* to == from is a special case which means
+	   "return to the previous task". */
+	if (to_idx == from_idx)
+		to_idx = ctx->error.last_switch.from;
+
+	from = &ctx->tasks[from_idx];
+	to = &ctx->tasks[to_idx];
+
+	fp_check = ctx->switches_count == from->last_switch + 1
+		&& ctx->error.last_switch.from == to_idx
+		&& ctx->error.last_switch.to == from_idx;
+
+	from->base.flags &= ~RTSWITCH_RT;
+	from->last_switch = ++ctx->switches_count;
+	ctx->error.last_switch.from = from_idx;
+	ctx->error.last_switch.to = to_idx;
+	barrier();
+
+	if (ctx->pause_us) {
+		ctx->next_task = to_idx;
+		barrier();
+		evl_start_timer(&ctx->wake_up_delay,
+				evl_abs_timeout(&ctx->wake_up_delay,
+						ctx->pause_us * 1000),
+				EVL_INFINITE);
+	} else
+		switch (to->base.flags & RTSWITCH_RT) {
+		case RTSWITCH_NRT:
+		switch_to_nrt:
+			up(&to->nrt_synch);
+			break;
+
+		case RTSWITCH_RT:
+
+			if (!fp_check || !evl_begin_fpu())
+				goto signal_nofp;
+
+			expected = from_idx + 500 +
+				(ctx->switches_count % 4000000) * 1000;
+
+			evl_set_fpregs(fp_features, expected);
+			evl_raise_flag(&to->rt_synch);
+			fp_val = evl_check_fpregs(fp_features, expected, bad_reg);
+			evl_end_fpu();
+
+			if (down_interruptible(&from->nrt_synch))
+				return -EINTR;
+			if (ctx->failed)
+				return 1;
+			if (fp_val != expected) {
+				handle_fpu_error(ctx, expected, fp_val, bad_reg);
+				return 1;
+			}
+
+			from->base.flags &= ~RTSWITCH_RT;
+			from->last_switch = ++ctx->switches_count;
+			ctx->error.last_switch.from = from_idx;
+			ctx->error.last_switch.to = to_idx;
+			if ((to->base.flags & RTSWITCH_RT) == RTSWITCH_NRT)
+				goto switch_to_nrt;
+			expected = from_idx + 500 +
+				(ctx->switches_count % 4000000) * 1000;
+			barrier();
+
+			evl_begin_fpu();
+			evl_set_fpregs(fp_features, expected);
+			evl_raise_flag(&to->rt_synch);
+			fp_val = evl_check_fpregs(fp_features, expected, bad_reg);
+			evl_end_fpu();
+
+			if (down_interruptible(&from->nrt_synch))
+				return -EINTR;
+			if (ctx->failed)
+				return 1;
+			if (fp_val != expected) {
+				handle_fpu_error(ctx, expected, fp_val, bad_reg);
+				return 1;
+			}
+
+			from->base.flags &= ~RTSWITCH_RT;
+			from->last_switch = ++ctx->switches_count;
+			ctx->error.last_switch.from = from_idx;
+			ctx->error.last_switch.to = to_idx;
+			barrier();
+			if ((to->base.flags & RTSWITCH_RT) == RTSWITCH_NRT)
+				goto switch_to_nrt;
+
+		signal_nofp:
+			evl_raise_flag(&to->rt_synch);
+			break;
+
+		default:
+			return -EINVAL;
+		}
+
+	if (down_interruptible(&from->nrt_synch))
+		return -EINTR;
+
+	if (ctx->failed)
+		return 1;
+
+	return 0;
+}
+
+static int rtswitch_set_tasks_count(struct rtswitch_context *ctx, unsigned int count)
+{
+	struct rtswitch_task *tasks;
+
+	if (ctx->tasks_count == count)
+		return 0;
+
+	tasks = vmalloc(count * sizeof(*tasks));
+
+	if (!tasks)
+		return -ENOMEM;
+
+	down(&ctx->lock);
+
+	if (ctx->tasks)
+		vfree(ctx->tasks);
+
+	ctx->tasks = tasks;
+	ctx->tasks_count = count;
+	ctx->next_index = 0;
+
+	up(&ctx->lock);
+
+	return 0;
+}
+
+static int rtswitch_register_task(struct rtswitch_context *ctx,
+				  struct hectic_task_index *arg)
+{
+	struct rtswitch_task *t;
+
+	down(&ctx->lock);
+
+	if (ctx->next_index == ctx->tasks_count) {
+		up(&ctx->lock);
+		return -EBUSY;
+	}
+
+	arg->index = ctx->next_index;
+	t = &ctx->tasks[arg->index];
+	ctx->next_index++;
+	t->base = *arg;
+	t->last_switch = 0;
+	sema_init(&t->nrt_synch, 0);
+	evl_init_flag(&t->rt_synch);
+
+	up(&ctx->lock);
+
+	return 0;
+}
+
+static void rtswitch_kthread(struct evl_kthread *kthread)
+{
+	struct rtswitch_context *ctx;
+	struct rtswitch_task *task;
+	unsigned int to, i = 0;
+
+	task = container_of(kthread, struct rtswitch_task, kthread);
+	ctx = task->ctx;
+
+	to = task->base.index;
+
+	rtswitch_pend_rt(ctx, task->base.index);
+
+	while (!evl_kthread_should_stop()) {
+		switch(i % 3) {
+		case 0:
+			/* to == from means "return to last task" */
+			rtswitch_to_rt(ctx, task->base.index, task->base.index);
+			break;
+		case 1:
+			if (++to == task->base.index)
+				++to;
+			if (to > ctx->tasks_count - 1)
+				to = 0;
+			if (to == task->base.index)
+				++to;
+
+			/* Fall through. */
+		case 2:
+			rtswitch_to_rt(ctx, task->base.index, to);
+		}
+		if (++i == 4000000)
+			i = 0;
+	}
+}
+
+static int rtswitch_create_kthread(struct rtswitch_context *ctx,
+				   struct hectic_task_index *ptask)
+{
+	struct rtswitch_task *task;
+	int err;
+
+	ptask->flags |= RTSWITCH_KERNEL;
+	err = rtswitch_register_task(ctx, ptask);
+	if (err)
+		return err;
+
+	task = &ctx->tasks[ptask->index];
+	task->ctx = ctx;
+	err = evl_run_kthread_on_cpu(&task->kthread, ctx->cpu,
+				     rtswitch_kthread, 1,
+				     "rtk%d@%u:%d",
+				     ptask->index, ctx->cpu,
+				     task_pid_nr(current));
+	/*
+	 * On error, clear the flag bits in order to avoid calling
+	 * evl_cancel_kthread() for an invalid thread in
+	 * hectic_release().
+	 */
+	if (err)
+		task->base.flags = 0;
+
+	return err;
+}
+
+static void rtswitch_utask_waker(struct irq_work *work)
+{
+	struct rtswitch_context *ctx;
+	ctx = container_of(work, struct rtswitch_context, wake_utask);
+	up(&ctx->utask->nrt_synch);
+}
+
+static long hectic_ioctl(struct file *filp, unsigned int cmd,
+			 unsigned long arg)
+{
+	struct rtswitch_context *ctx = filp->private_data;
+	struct hectic_switch_req fromto, __user *u_fromto;
+	struct hectic_task_index task, __user *u_task;
+	struct hectic_error __user *u_lerr;
+	__u32 count;
+	int err;
+
+	switch (cmd) {
+	case EVL_HECIOC_SET_TASKS_COUNT:
+		return rtswitch_set_tasks_count(ctx, arg);
+
+	case EVL_HECIOC_SET_CPU:
+		if (arg > num_online_cpus() - 1)
+			return -EINVAL;
+
+		ctx->cpu = arg;
+		return 0;
+
+	case EVL_HECIOC_SET_PAUSE:
+		ctx->pause_us = arg;
+		return 0;
+
+	case EVL_HECIOC_REGISTER_UTASK:
+		u_task = (typeof(u_task))arg;
+		err = copy_from_user(&task, u_task, sizeof(task));
+		if (err)
+			return -EFAULT;
+
+		err = rtswitch_register_task(ctx, &task);
+		if (!err && copy_to_user(u_task, &task, sizeof(task)))
+			err = -EFAULT;
+
+		return err;
+
+	case EVL_HECIOC_CREATE_KTASK:
+		u_task = (typeof(u_task))arg;
+		err = copy_from_user(&task, u_task, sizeof(task));
+		if (err)
+			return -EFAULT;
+
+		err = rtswitch_create_kthread(ctx, &task);
+		if (!err && copy_to_user(u_task, &task, sizeof(task)))
+			err = -EFAULT;
+
+		return err;
+
+	case EVL_HECIOC_PEND:
+		u_task = (typeof(u_task))arg;
+		err = copy_from_user(&task, u_task, sizeof(task));
+		if (err)
+			return -EFAULT;
+
+		return rtswitch_pend_nrt(ctx, task.index);
+
+	case EVL_HECIOC_SWITCH_TO:
+		u_fromto = (typeof(u_fromto))arg;
+		err = copy_from_user(&fromto, u_fromto, sizeof(fromto));
+		if (err)
+			return -EFAULT;
+
+		return rtswitch_to_nrt(ctx, fromto.from, fromto.to);
+
+	case EVL_HECIOC_GET_SWITCHES_COUNT:
+		count = ctx->switches_count;
+		return copy_to_user((__u32 *)arg, &count, sizeof(count)) ?
+			-EFAULT : 0;
+
+	case EVL_HECIOC_GET_LAST_ERROR:
+		u_lerr = (typeof(u_lerr))arg;
+		return copy_to_user(u_lerr, &ctx->error, sizeof(ctx->error)) ?
+			-EFAULT : 0;
+	default:
+		return -ENOTTY;
+	}
+}
+
+static long hectic_oob_ioctl(struct file *filp, unsigned int cmd,
+			     unsigned long arg)
+{
+	struct rtswitch_context *ctx = filp->private_data;
+	struct hectic_switch_req fromto, __user *u_fromto;
+	struct hectic_task_index task, __user *u_task;
+	struct hectic_error __user *u_lerr;
+	int err;
+
+	switch (cmd) {
+	case EVL_HECIOC_PEND:
+		u_task = (typeof(u_task))arg;
+		err = raw_copy_from_user(&task, u_task, sizeof(task));
+		return err ? -EFAULT :
+			rtswitch_pend_rt(ctx, task.index);
+
+	case EVL_HECIOC_SWITCH_TO:
+		u_fromto = (typeof(u_fromto))arg;
+		err = raw_copy_from_user(&fromto, u_fromto, sizeof(fromto));
+		return err ? -EFAULT :
+			rtswitch_to_rt(ctx, fromto.from, fromto.to);
+
+	case EVL_HECIOC_GET_LAST_ERROR:
+		u_lerr = (typeof(u_lerr))arg;
+		return raw_copy_to_user(u_lerr, &ctx->error, sizeof(ctx->error)) ?
+			-EFAULT : 0;
+
+	default:
+		return -ENOTTY;
+	}
+}
+
+static int hectic_open(struct inode *inode, struct file *filp)
+{
+	struct rtswitch_context *ctx;
+	int ret;
+
+	ctx = kmalloc(sizeof(*ctx), GFP_KERNEL);
+	if (ctx == NULL)
+		return -ENOMEM;
+
+	ret = evl_open_file(&ctx->efile, filp);
+	if (ret) {
+		kfree(ctx);
+		return ret;
+	}
+
+	ctx->tasks = NULL;
+	ctx->tasks_count = ctx->next_index = ctx->cpu = ctx->switches_count = 0;
+	sema_init(&ctx->lock, 1);
+	ctx->failed = false;
+	ctx->error.last_switch.from = ctx->error.last_switch.to = -1;
+	ctx->pause_us = 0;
+
+	init_irq_work(&ctx->wake_utask, rtswitch_utask_waker);
+	evl_init_core_timer(&ctx->wake_up_delay, timed_wake_up);
+
+	filp->private_data = ctx;
+
+	return 0;
+}
+
+static int hectic_release(struct inode *inode, struct file *filp)
+{
+	struct rtswitch_context *ctx = filp->private_data;
+	unsigned int i;
+
+	evl_destroy_timer(&ctx->wake_up_delay);
+
+	if (ctx->tasks) {
+		set_cpus_allowed_ptr(current, cpumask_of(ctx->cpu));
+
+		for (i = 0; i < ctx->next_index; i++) {
+			struct rtswitch_task *task = &ctx->tasks[i];
+
+			if (task->base.flags & RTSWITCH_KERNEL)
+				evl_cancel_kthread(&task->kthread);
+
+			evl_destroy_flag(&task->rt_synch);
+		}
+		vfree(ctx->tasks);
+	}
+
+	evl_release_file(&ctx->efile);
+	kfree(ctx);
+
+	return 0;
+}
+
+static struct class hectic_class = {
+	.name = "hectic",
+	.owner = THIS_MODULE,
+};
+
+static const struct file_operations hectic_fops = {
+	.open		= hectic_open,
+	.release	= hectic_release,
+	.unlocked_ioctl	= hectic_ioctl,
+	.oob_ioctl	= hectic_oob_ioctl,
+};
+
+static dev_t hectic_devt;
+
+static struct cdev hectic_cdev;
+
+static int __init hectic_init(void)
+{
+	struct device *dev;
+	int ret;
+
+	fp_features = evl_detect_fpu();
+
+	ret = class_register(&hectic_class);
+	if (ret)
+		return ret;
+
+	ret = alloc_chrdev_region(&hectic_devt, 0, 1, "hectic");
+	if (ret)
+		goto fail_region;
+
+	cdev_init(&hectic_cdev, &hectic_fops);
+	ret = cdev_add(&hectic_cdev, hectic_devt, 1);
+	if (ret)
+		goto fail_add;
+
+	dev = device_create(&hectic_class, NULL, hectic_devt, NULL, "hectic");
+	if (IS_ERR(dev)) {
+		ret = PTR_ERR(dev);
+		goto fail_dev;
+	}
+
+	return 0;
+
+fail_dev:
+	cdev_del(&hectic_cdev);
+fail_add:
+	unregister_chrdev_region(hectic_devt, 1);
+fail_region:
+	class_unregister(&hectic_class);
+
+	return ret;
+}
+module_init(hectic_init);
+
+static void __exit hectic_exit(void)
+{
+	device_destroy(&hectic_class, MKDEV(MAJOR(hectic_devt), 0));
+	cdev_del(&hectic_cdev);
+	class_unregister(&hectic_class);
+}
+module_exit(hectic_exit);
+
+MODULE_LICENSE("GPL");
diff --git a/drivers/evenless/latmus.c b/drivers/evenless/latmus.c
new file mode 100644
index 000000000000..de6e1dea8b4b
--- /dev/null
+++ b/drivers/evenless/latmus.c
@@ -0,0 +1,1054 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt's autotune driver
+ * (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2014, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <linux/sort.h>
+#include <linux/cdev.h>
+#include <linux/fs.h>
+#include <linux/fcntl.h>
+#include <evenless/file.h>
+#include <evenless/wait.h>
+#include <evenless/clock.h>
+#include <evenless/thread.h>
+#include <evenless/xbuf.h>
+#include <uapi/evenless/devices/latmus.h>
+#include <trace/events/evenless.h>
+
+#define TUNER_SAMPLING_TIME	500000000UL
+#define TUNER_WARMUP_STEPS	10
+#define TUNER_RESULT_STEPS	40
+
+#define progress(__runner, __fmt, __args...)				\
+	do {								\
+		if ((__runner)->verbosity > 1)				\
+			printk(EVL_INFO "latmus(%s) " __fmt "\n",	\
+			       (__runner)->name, ##__args);		\
+	} while (0)
+
+struct tuning_score {
+	int pmean;
+	int stddev;
+	int minlat;
+	unsigned int step;
+	unsigned int gravity;
+};
+
+struct runner_state {
+	ktime_t ideal;
+	int min_lat;
+	int max_lat;
+	int allmax_lat;
+	int prev_mean;
+	int prev_sqs;
+	int cur_sqs;
+	unsigned int sum;
+	unsigned int overruns;
+	unsigned int cur_samples;
+	unsigned int max_samples;
+};
+
+struct latmus_runner {
+	const char *name;
+	unsigned int (*get_gravity)(struct latmus_runner *runner);
+	void (*set_gravity)(struct latmus_runner *runner, unsigned int gravity);
+	unsigned int (*adjust_gravity)(struct latmus_runner *runner, int adjust);
+	int (*start)(struct latmus_runner *runner, ktime_t start_time);
+	void (*destroy)(struct latmus_runner *runner);
+	int (*add_sample)(struct latmus_runner *runner, ktime_t timestamp);
+	int (*run)(struct latmus_runner *runner, struct latmus_result *result);
+	void (*cleanup)(struct latmus_runner *runner);
+	struct runner_state state;
+	struct evl_wait_flag done;
+	int status;
+	int verbosity;
+	ktime_t period;
+	union {
+		struct {
+			struct tuning_score scores[TUNER_RESULT_STEPS];
+			int nscores;
+		};
+		struct {
+			unsigned int warmup_samples;
+			unsigned int warmup_limit;
+			int xfd;
+			struct evl_xbuf *xbuf;
+			u32 hcells;
+			s32 *histogram;
+		};
+	};
+};
+
+struct irq_runner {
+	struct evl_timer timer;
+	struct latmus_runner runner;
+};
+
+struct kthread_runner {
+	struct evl_kthread kthread;
+	struct evl_wait_flag barrier;
+	ktime_t start_time;
+	struct latmus_runner runner;
+};
+
+struct uthread_runner {
+	struct evl_timer timer;
+	struct evl_wait_flag pulse;
+	struct latmus_runner runner;
+};
+
+struct latmus_state {
+	struct evl_file efile;
+	struct latmus_runner *runner;
+};
+
+static inline void init_runner_base(struct latmus_runner *runner)
+{
+	evl_init_flag(&runner->done);
+	runner->status = 0;
+}
+
+static inline void destroy_runner_base(struct latmus_runner *runner)
+{
+	evl_destroy_flag(&runner->done);
+	if (runner->cleanup)
+		runner->cleanup(runner);
+}
+
+static inline void done_sampling(struct latmus_runner *runner,
+				 int status)
+{
+	runner->status = status;
+	evl_raise_flag(&runner->done);
+}
+
+static void send_measurement(struct latmus_runner *runner)
+{
+	struct runner_state *state = &runner->state;
+	struct latmus_measurement meas;
+
+	meas.min_lat = state->min_lat;
+	meas.max_lat = state->max_lat;
+	meas.sum_lat = state->sum;
+	meas.overruns = state->overruns;
+	meas.samples = state->cur_samples;
+	evl_write_xbuf(runner->xbuf, &meas, sizeof(meas), O_NONBLOCK);
+
+	/* Reset counters for next round. */
+	state->min_lat = ktime_to_ns(runner->period);
+	state->max_lat = 0;
+	state->sum = 0;
+	state->overruns = 0;
+	state->cur_samples = 0;
+}
+
+static int add_measurement_sample(struct latmus_runner *runner,
+				  ktime_t timestamp)
+{
+	struct runner_state *state = &runner->state;
+	ktime_t period = runner->period;
+	int delta, cell;
+
+	/* Skip samples in warmup time. */
+	if (runner->warmup_samples < runner->warmup_limit) {
+		runner->warmup_samples++;
+		state->ideal = ktime_add(state->ideal, period);
+		return 0;
+	}
+
+	delta = (int)ktime_to_ns(ktime_sub(timestamp, state->ideal));
+	if (delta < state->min_lat)
+		state->min_lat = delta;
+	if (delta > state->max_lat) {
+		state->max_lat = delta;
+		if (delta > state->allmax_lat) {
+			state->allmax_lat = delta;
+			trace_evl_latspot(delta);
+			trace_evl_trigger("latmus");
+		}
+	}
+
+	if (runner->histogram) {
+		cell = (delta < 0 ? -delta : delta) / 1000; /* us */
+		if (cell >= runner->hcells)
+			cell = runner->hcells - 1;
+		runner->histogram[cell]++;
+	}
+
+	state->sum += delta;
+	state->ideal = ktime_add(state->ideal, period);
+
+	while (delta > ktime_to_ns(period)) { /* period > 0 */
+		state->overruns++;
+		state->ideal = ktime_add(state->ideal, period);
+		delta -= ktime_to_ns(period);
+	}
+
+	if (++state->cur_samples >= state->max_samples)
+		send_measurement(runner);
+
+	return 0;	/* Always keep going. */
+}
+
+static int add_tuning_sample(struct latmus_runner *runner,
+			     ktime_t timestamp)
+{
+	struct runner_state *state = &runner->state;
+	int n, delta, cur_mean;
+
+	delta = (int)ktime_to_ns(ktime_sub(timestamp, state->ideal));
+	if (delta < state->min_lat)
+		state->min_lat = delta;
+	if (delta > state->max_lat)
+		state->max_lat = delta;
+	if (delta < 0)
+		delta = 0;
+
+	state->sum += delta;
+	state->ideal = ktime_add(state->ideal, runner->period);
+	n = ++state->cur_samples;
+
+	/* TAOCP (Vol 2), single-pass computation of variance. */
+	if (n == 1)
+		state->prev_mean = delta;
+	else {
+		cur_mean = state->prev_mean + (delta - state->prev_mean) / n;
+                state->cur_sqs = state->prev_sqs + (delta - state->prev_mean)
+			* (delta - cur_mean);
+                state->prev_mean = cur_mean;
+                state->prev_sqs = state->cur_sqs;
+	}
+
+	if (n >= state->max_samples) {
+		done_sampling(runner, 0);
+		return 1;	/* Finished. */
+	}
+
+	return 0;	/* Keep going. */
+}
+
+static void latmus_irq_handler(struct evl_timer *timer) /* hard irqs off */
+{
+	struct irq_runner *irq_runner;
+	ktime_t now;
+
+	irq_runner = container_of(timer, struct irq_runner, timer);
+	now = evl_read_clock(&evl_mono_clock);
+
+	xnlock_get(&nklock);
+
+	if (irq_runner->runner.add_sample(&irq_runner->runner, now))
+		evl_stop_timer(timer);
+
+	xnlock_put(&nklock);
+}
+
+static void destroy_irq_runner(struct latmus_runner *runner)
+{
+	struct irq_runner *irq_runner;
+
+	irq_runner = container_of(runner, struct irq_runner, runner);
+	evl_destroy_timer(&irq_runner->timer);
+	destroy_runner_base(runner);
+	kfree(irq_runner);
+}
+
+static unsigned int get_irq_gravity(struct latmus_runner *runner)
+{
+	return evl_mono_clock.gravity.irq;
+}
+
+static void set_irq_gravity(struct latmus_runner *runner, unsigned int gravity)
+{
+	evl_mono_clock.gravity.irq = gravity;
+}
+
+static unsigned int adjust_irq_gravity(struct latmus_runner *runner, int adjust)
+{
+	return evl_mono_clock.gravity.irq += adjust;
+}
+
+static int start_irq_runner(struct latmus_runner *runner,
+			    ktime_t start_time)
+{
+	struct irq_runner *irq_runner;
+
+	irq_runner = container_of(runner, struct irq_runner, runner);
+
+	evl_start_timer(&irq_runner->timer, start_time, runner->period);
+
+	return 0;
+}
+
+static struct latmus_runner *create_irq_runner(int cpu)
+{
+	struct irq_runner *irq_runner;
+
+	irq_runner = kzalloc(sizeof(*irq_runner), GFP_KERNEL);
+	if (irq_runner == NULL)
+		return NULL;
+
+	irq_runner->runner = (struct latmus_runner){
+		.name = "irqhand",
+		.destroy = destroy_irq_runner,
+		.get_gravity = get_irq_gravity,
+		.set_gravity = set_irq_gravity,
+		.adjust_gravity = adjust_irq_gravity,
+		.start = start_irq_runner,
+	};
+
+	init_runner_base(&irq_runner->runner);
+	evl_init_timer_on_cpu(&irq_runner->timer, cpu, latmus_irq_handler);
+
+	return &irq_runner->runner;
+}
+
+void kthread_handler(struct evl_kthread *kthread)
+{
+	struct kthread_runner *k_runner;
+	ktime_t now;
+	int ret = 0;
+
+	k_runner = container_of(kthread, struct kthread_runner, kthread);
+
+	for (;;) {
+		if (evl_kthread_should_stop())
+			break;
+
+		ret = evl_wait_flag(&k_runner->barrier);
+		if (ret)
+			break;
+
+		ret = evl_set_thread_period(&evl_mono_clock,
+					    k_runner->start_time,
+					    k_runner->runner.period);
+		if (ret)
+			break;
+
+		for (;;) {
+			ret = evl_wait_thread_period(NULL);
+			if (ret && ret != -ETIMEDOUT)
+				goto out;
+
+			now = evl_read_clock(&evl_mono_clock);
+			if (k_runner->runner.add_sample(&k_runner->runner, now)) {
+				evl_set_thread_period(NULL, 0, 0);
+				break;
+			}
+		}
+	}
+out:
+	done_sampling(&k_runner->runner, ret);
+	evl_cancel_kthread(&k_runner->kthread);
+}
+
+static void destroy_kthread_runner(struct latmus_runner *runner)
+{
+	struct kthread_runner *k_runner;
+
+	k_runner = container_of(runner, struct kthread_runner, runner);
+	evl_cancel_kthread(&k_runner->kthread);
+	evl_destroy_flag(&k_runner->barrier);
+	destroy_runner_base(runner);
+	kfree(k_runner);
+}
+
+static unsigned int get_kthread_gravity(struct latmus_runner *runner)
+{
+	return evl_mono_clock.gravity.kernel;
+}
+
+static void
+set_kthread_gravity(struct latmus_runner *runner, unsigned int gravity)
+{
+	evl_mono_clock.gravity.kernel = gravity;
+}
+
+static unsigned int
+adjust_kthread_gravity(struct latmus_runner *runner, int adjust)
+{
+	return evl_mono_clock.gravity.kernel += adjust;
+}
+
+static int start_kthread_runner(struct latmus_runner *runner,
+				ktime_t start_time)
+{
+	struct kthread_runner *k_runner;
+
+	k_runner = container_of(runner, struct kthread_runner, runner);
+
+	k_runner->start_time = start_time;
+	evl_raise_flag(&k_runner->barrier);
+
+	return 0;
+}
+
+static struct latmus_runner *
+create_kthread_runner(int priority, int cpu)
+{
+	struct kthread_runner *k_runner;
+	int ret;
+
+	k_runner = kzalloc(sizeof(*k_runner), GFP_KERNEL);
+	if (k_runner == NULL)
+		return NULL;
+
+	k_runner->runner = (struct latmus_runner){
+		.name = "kthread",
+		.destroy = destroy_kthread_runner,
+		.get_gravity = get_kthread_gravity,
+		.set_gravity = set_kthread_gravity,
+		.adjust_gravity = adjust_kthread_gravity,
+		.start = start_kthread_runner,
+	};
+
+	init_runner_base(&k_runner->runner);
+	evl_init_flag(&k_runner->barrier);
+
+	ret = evl_run_kthread_on_cpu(&k_runner->kthread, cpu,
+				     kthread_handler,
+				     priority, "latmus-klat:%d",
+				     task_pid_nr(current));
+	if (ret) {
+		kfree(k_runner);
+		return ERR_PTR(ret);
+	}
+
+	return &k_runner->runner;
+}
+
+static void latmus_pulse_handler(struct evl_timer *timer)
+{
+	struct uthread_runner *u_runner;
+
+	u_runner = container_of(timer, struct uthread_runner, timer);
+	evl_raise_flag(&u_runner->pulse);
+}
+
+static void destroy_uthread_runner(struct latmus_runner *runner)
+{
+	struct uthread_runner *u_runner;
+
+	u_runner = container_of(runner, struct uthread_runner, runner);
+	evl_destroy_timer(&u_runner->timer);
+	evl_destroy_flag(&u_runner->pulse);
+	destroy_runner_base(runner);
+	kfree(u_runner);
+}
+
+static unsigned int get_uthread_gravity(struct latmus_runner *runner)
+{
+	return evl_mono_clock.gravity.user;
+}
+
+static void set_uthread_gravity(struct latmus_runner *runner,
+				unsigned int gravity)
+{
+	evl_mono_clock.gravity.user = gravity;
+}
+
+static unsigned int
+adjust_uthread_gravity(struct latmus_runner *runner, int adjust)
+{
+	return evl_mono_clock.gravity.user += adjust;
+}
+
+static int start_uthread_runner(struct latmus_runner *runner,
+				ktime_t start_time)
+{
+	struct uthread_runner *u_runner;
+
+	u_runner = container_of(runner, struct uthread_runner, runner);
+
+	evl_start_timer(&u_runner->timer, start_time, runner->period);
+
+	return 0;
+}
+
+static int add_uthread_sample(struct latmus_runner *runner,
+			      ktime_t user_timestamp)
+{
+	struct uthread_runner *u_runner;
+	int ret;
+
+	u_runner = container_of(runner, struct uthread_runner, runner);
+
+	if (user_timestamp &&
+	    u_runner->runner.add_sample(runner, user_timestamp)) {
+		evl_stop_timer(&u_runner->timer);
+		/* Tell the caller to park until next round. */
+		ret = -EPIPE;
+	} else
+		ret = evl_wait_flag(&u_runner->pulse);
+
+	return ret;
+}
+
+static struct latmus_runner *create_uthread_runner(int cpu)
+{
+	struct uthread_runner *u_runner;
+
+	u_runner = kzalloc(sizeof(*u_runner), GFP_KERNEL);
+	if (u_runner == NULL)
+		return NULL;
+
+	u_runner->runner = (struct latmus_runner){
+		.name = "uthread",
+		.destroy = destroy_uthread_runner,
+		.get_gravity = get_uthread_gravity,
+		.set_gravity = set_uthread_gravity,
+		.adjust_gravity = adjust_uthread_gravity,
+		.start = start_uthread_runner,
+	};
+
+	init_runner_base(&u_runner->runner);
+	evl_init_timer_on_cpu(&u_runner->timer, cpu, latmus_pulse_handler);
+	evl_set_timer_gravity(&u_runner->timer, EVL_TIMER_UGRAVITY);
+	evl_init_flag(&u_runner->pulse);
+
+	return &u_runner->runner;
+}
+
+static inline void build_score(struct latmus_runner *runner, int step)
+{
+	struct runner_state *state = &runner->state;
+	unsigned int variance, n;
+
+	n = state->cur_samples;
+	runner->scores[step].pmean = state->sum / n;
+	variance = n > 1 ? state->cur_sqs / (n - 1) : 0;
+	runner->scores[step].stddev = int_sqrt(variance);
+	runner->scores[step].minlat = state->min_lat;
+	runner->scores[step].gravity = runner->get_gravity(runner);
+	runner->scores[step].step = step;
+	runner->nscores++;
+}
+
+static int cmp_score_mean(const void *c, const void *r)
+{
+	const struct tuning_score *sc = c, *sr = r;
+	return sc->pmean - sr->pmean;
+}
+
+static int cmp_score_stddev(const void *c, const void *r)
+{
+	const struct tuning_score *sc = c, *sr = r;
+	return sc->stddev - sr->stddev;
+}
+
+static int cmp_score_minlat(const void *c, const void *r)
+{
+	const struct tuning_score *sc = c, *sr = r;
+	return sc->minlat - sr->minlat;
+}
+
+static int cmp_score_gravity(const void *c, const void *r)
+{
+	const struct tuning_score *sc = c, *sr = r;
+	return sc->gravity - sr->gravity;
+}
+
+static int filter_mean(struct latmus_runner *runner)
+{
+	sort(runner->scores, runner->nscores,
+	     sizeof(struct tuning_score),
+	     cmp_score_mean, NULL);
+
+	/* Top half of the best pondered means. */
+
+	return (runner->nscores + 1) / 2;
+}
+
+static int filter_stddev(struct latmus_runner *runner)
+{
+	sort(runner->scores, runner->nscores,
+	     sizeof(struct tuning_score),
+	     cmp_score_stddev, NULL);
+
+	/* Top half of the best standard deviations. */
+
+	return (runner->nscores + 1) / 2;
+}
+
+static int filter_minlat(struct latmus_runner *runner)
+{
+	sort(runner->scores, runner->nscores,
+	     sizeof(struct tuning_score),
+	     cmp_score_minlat, NULL);
+
+	/* Top half of the minimum latencies. */
+
+	return (runner->nscores + 1) / 2;
+}
+
+static int filter_gravity(struct latmus_runner *runner)
+{
+	sort(runner->scores, runner->nscores,
+	     sizeof(struct tuning_score),
+	     cmp_score_gravity, NULL);
+
+	/* Smallest gravity required among the shortest latencies. */
+
+	return runner->nscores;
+}
+
+static void dump_scores(struct latmus_runner *runner)
+{
+	int n;
+
+	if (runner->verbosity < 2)
+		return;
+
+	for (n = 0; n < runner->nscores; n++)
+		printk(EVL_INFO
+		       ".. S%.2d pmean=%d stddev=%u minlat=%u gravity=%u\n",
+		       runner->scores[n].step,
+		       runner->scores[n].pmean,
+		       runner->scores[n].stddev,
+		       runner->scores[n].minlat,
+		       runner->scores[n].gravity);
+}
+
+static inline void filter_score(struct latmus_runner *runner,
+				int (*filter)(struct latmus_runner *runner))
+{
+	runner->nscores = filter(runner);
+	dump_scores(runner);
+}
+
+static int measure_continously(struct latmus_runner *runner)
+{
+	struct runner_state *state = &runner->state;
+	ktime_t period = runner->period;
+	struct evl_file *sfilp;
+	struct evl_xbuf *xbuf;
+	int ret;
+
+	/*
+	 * Get a reference on the cross-buffer we should use to send
+	 * interval results to userland. This may delay the disposal
+	 * of such element when the last in-band file reference is
+	 * dropped until we are done with OOB operations
+	 * (evl_put_xbuf).
+	 */
+	xbuf = evl_get_xbuf(runner->xfd, &sfilp);
+	if (xbuf == NULL)
+		return -EBADF;	/* muhh? */
+
+	state->max_samples = ONE_BILLION / (int)ktime_to_ns(period);
+	runner->add_sample = add_measurement_sample;
+	runner->xbuf = xbuf;
+	state->min_lat = ktime_to_ns(period);
+	state->max_lat = 0;
+	state->allmax_lat = 0;
+	state->sum = 0;
+	state->overruns = 0;
+	state->cur_samples = 0;
+	state->ideal = ktime_add(evl_read_clock(&evl_mono_clock), period);
+
+	ret = runner->start(runner, state->ideal);
+	if (ret)
+		goto out;
+
+	ret = evl_wait_flag(&runner->done) ?: runner->status;
+out:
+	evl_put_xbuf(sfilp);
+
+	return ret;
+}
+
+static int tune_gravity(struct latmus_runner *runner)
+{
+	struct runner_state *state = &runner->state;
+	int ret, step, gravity_limit, adjust;
+	ktime_t period = runner->period;
+	unsigned int orig_gravity;
+
+	state->max_samples = TUNER_SAMPLING_TIME / (int)ktime_to_ns(period);
+	orig_gravity = runner->get_gravity(runner);
+	runner->add_sample = add_tuning_sample;
+	runner->set_gravity(runner, 0);
+	runner->nscores = 0;
+	adjust = 500; /* Gravity adjustment step */
+	gravity_limit = 0;
+	progress(runner, "warming up...");
+
+	for (step = 0; step < TUNER_WARMUP_STEPS + TUNER_RESULT_STEPS; step++) {
+		state->ideal = ktime_add_ns(evl_read_clock(&evl_mono_clock),
+			    ktime_to_ns(period) * TUNER_WARMUP_STEPS);
+		state->min_lat = ktime_to_ns(period);
+		state->max_lat = 0;
+		state->prev_mean = 0;
+		state->prev_sqs = 0;
+		state->cur_sqs = 0;
+		state->sum = 0;
+		state->cur_samples = 0;
+
+		ret = runner->start(runner, state->ideal);
+		if (ret)
+			goto fail;
+
+		/* Runner stops when posting. */
+		ret = evl_wait_flag(&runner->done);
+		if (ret)
+			goto fail;
+
+		ret = runner->status;
+		if (ret)
+			goto fail;
+
+		if (step < TUNER_WARMUP_STEPS) {
+			if (state->min_lat > gravity_limit) {
+				gravity_limit = state->min_lat;
+				progress(runner, "gravity limit set to %u ns (%d)",
+					 gravity_limit, state->min_lat);
+			}
+			continue;
+		}
+
+		if (state->min_lat < 0) {
+			if (runner->get_gravity(runner) < -state->min_lat) {
+				printk(EVL_WARNING
+				       "latmus(%s) failed with early shot (%d ns)\n",
+				       runner->name,
+				       -(runner->get_gravity(runner) + state->min_lat));
+				ret = -EAGAIN;
+				goto fail;
+			}
+			break;
+		}
+
+		if (((step - TUNER_WARMUP_STEPS) % 5) == 0)
+			progress(runner, "calibrating... (slice %d)",
+				 (step - TUNER_WARMUP_STEPS) / 5 + 1);
+
+		build_score(runner, step - TUNER_WARMUP_STEPS);
+
+		/*
+		 * Anticipating by more than the minimum latency
+		 * detected at warmup would make no sense: cap the
+		 * gravity we may try.
+		 */
+		if (runner->adjust_gravity(runner, adjust) > gravity_limit) {
+			progress(runner, "beyond gravity limit at %u ns",
+				 runner->get_gravity(runner));
+			break;
+		}
+	}
+
+	progress(runner, "calibration scores");
+	dump_scores(runner);
+	progress(runner, "pondered mean filter");
+	filter_score(runner, filter_mean);
+	progress(runner, "standard deviation filter");
+	filter_score(runner, filter_stddev);
+	progress(runner, "minimum latency filter");
+	filter_score(runner, filter_minlat);
+	progress(runner, "gravity filter");
+	filter_score(runner, filter_gravity);
+	runner->set_gravity(runner, runner->scores[0].gravity);
+
+	return 0;
+fail:
+	runner->set_gravity(runner, orig_gravity);
+
+	return ret;
+}
+
+static int setup_tuning(struct latmus_runner *runner,
+			struct latmus_setup *setup)
+{
+	runner->verbosity = setup->u.tune.verbosity;
+	runner->period = setup->period;
+
+	return 0;
+}
+
+static int run_tuning(struct latmus_runner *runner,
+		      struct latmus_result *result)
+{
+	__u32 gravity;
+	int ret;
+
+	ret = tune_gravity(runner);
+	if (ret)
+		return ret;
+
+	gravity = runner->get_gravity(runner);
+
+	if (raw_copy_to_user(result->data, &gravity, sizeof(gravity)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int setup_measurement(struct latmus_runner *runner,
+			     struct latmus_setup *setup)
+{
+	runner->period = setup->period;
+	runner->warmup_limit = ONE_BILLION / (int)ktime_to_ns(setup->period); /* 1s warmup */
+	runner->xfd = setup->u.measure.xfd;
+	runner->histogram = NULL;
+	runner->hcells = setup->u.measure.hcells;
+	if (runner->hcells == 0)
+		return 0;
+
+	if (runner->hcells > 1000) /* LART */
+		return -EINVAL;
+
+	runner->histogram = kzalloc(runner->hcells * sizeof(s32),
+				    GFP_KERNEL);
+
+	return runner->histogram ? 0 : -ENOMEM;
+}
+
+static int run_measurement(struct latmus_runner *runner,
+			   struct latmus_result *result)
+{
+	size_t len;
+	int ret;
+
+	ret = measure_continously(runner);
+	if (ret != -EINTR)
+		return ret;
+
+	/* Copy distribution data back to userland. */
+	if (runner->histogram) {
+		len = runner->hcells * sizeof(s32);
+		if (len > result->len)
+			len = result->len;
+		if (len > 0 &&
+		    raw_copy_to_user(result->data, runner->histogram, len))
+			return -EFAULT;
+	}
+
+	return 0;
+}
+
+static void cleanup_measurement(struct latmus_runner *runner)
+{
+	if (runner->histogram)
+		kfree(runner->histogram);
+}
+
+static long latmus_ioctl(struct file *filp, unsigned int cmd,
+			 unsigned long arg)
+{
+	struct latmus_state *ls = filp->private_data;
+	int (*setup)(struct latmus_runner *runner,
+		     struct latmus_setup *setup_data);
+	int (*run)(struct latmus_runner *runner,
+		   struct latmus_result *result);
+	void (*cleanup)(struct latmus_runner *runner);
+	struct latmus_setup setup_data;
+	struct latmus_runner *runner;
+	int ret;
+
+	switch (cmd) {
+	case EVL_LATIOC_RESET:
+		evl_reset_clock_gravity(&evl_mono_clock);
+		return 0;
+	case EVL_LATIOC_TUNE:
+		setup = setup_tuning;
+		run = run_tuning;
+		cleanup = NULL;
+		break;
+	case EVL_LATIOC_MEASURE:
+		setup = setup_measurement;
+		run = run_measurement;
+		cleanup = cleanup_measurement;
+		break;
+	default:
+		return -ENOTTY;
+	}
+
+	if (copy_from_user(&setup_data, (struct latmus_setup __user *)arg,
+			   sizeof(setup_data)))
+		return -EFAULT;
+
+	if (setup_data.period <= 0 ||
+	    setup_data.period > ONE_BILLION)
+		return -EINVAL;
+
+	if (setup_data.priority < 1 ||
+	    setup_data.priority > EVL_FIFO_MAX_PRIO)
+		return -EINVAL;
+
+	/* Clear previous runner. */
+	runner = ls->runner;
+	if (runner) {
+		runner->destroy(runner);
+		ls->runner = NULL;
+	}
+
+	switch (setup_data.type) {
+	case EVL_LAT_IRQ:
+		runner = create_irq_runner(setup_data.cpu);
+		break;
+	case EVL_LAT_KERN:
+		runner = create_kthread_runner(setup_data.priority,
+					       setup_data.cpu);
+		break;
+	case EVL_LAT_USER:
+		runner = create_uthread_runner(setup_data.cpu);
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	if (IS_ERR(runner))
+		return PTR_ERR(runner);
+
+	ret = setup(runner, &setup_data);
+	if (ret) {
+		runner->destroy(runner);
+		return ret;
+	}
+
+	runner->run = run;
+	runner->cleanup = cleanup;
+	ls->runner = runner;
+
+	return 0;
+}
+
+static long latmus_oob_ioctl(struct file *filp, unsigned int cmd,
+			     unsigned long arg)
+{
+	struct latmus_state *ls = filp->private_data;
+	struct latmus_runner *runner;
+	struct latmus_result result;
+	__u64 timestamp;
+	int ret;
+
+	runner = ls->runner;
+	if (runner == NULL)
+		return -EAGAIN;
+
+	switch (cmd) {
+	case EVL_LATIOC_RUN:
+		ret = raw_copy_from_user(&result,
+				 (struct latmus_result __user *)arg,
+				 sizeof(result));
+		if (ret)
+			return -EFAULT;
+		ret = runner->run(runner, &result);
+		break;
+	case EVL_LATIOC_PULSE:
+		if (runner->start != start_uthread_runner)
+			return -EINVAL;
+		ret = raw_copy_from_user(&timestamp, (__u64 __user *)arg,
+					 sizeof(timestamp));
+		if (ret)
+			return -EFAULT;
+		ret = add_uthread_sample(runner, ns_to_ktime(timestamp));
+		break;
+	default:
+		ret = -ENOTTY;
+	}
+
+	return ret;
+}
+
+static int latmus_open(struct inode *inode, struct file *filp)
+{
+	struct latmus_state *ls;
+	int ret;
+
+	ls = kzalloc(sizeof(*ls), GFP_KERNEL);
+	if (ls == NULL)
+		return -ENOMEM;
+
+	ret = evl_open_file(&ls->efile, filp);
+	if (ret)
+		kfree(ls);
+
+	filp->private_data = ls;
+
+	return ret;
+}
+
+static int latmus_release(struct inode *inode, struct file *filp)
+{
+	struct latmus_state *ls = filp->private_data;
+	struct latmus_runner *runner;
+
+	runner = ls->runner;
+	if (runner)
+		runner->destroy(runner);
+
+	evl_release_file(&ls->efile);
+	kfree(ls);
+
+	return 0;
+}
+
+static struct class latmus_class = {
+	.name = "latmus",
+	.owner = THIS_MODULE,
+};
+
+static const struct file_operations latmus_fops = {
+	.open		= latmus_open,
+	.release	= latmus_release,
+	.unlocked_ioctl	= latmus_ioctl,
+	.oob_ioctl	= latmus_oob_ioctl,
+};
+
+static dev_t latmus_devt;
+
+static struct cdev latmus_cdev;
+
+static int __init latmus_init(void)
+{
+	struct device *dev;
+	int ret;
+
+	ret = class_register(&latmus_class);
+	if (ret)
+		return ret;
+
+	ret = alloc_chrdev_region(&latmus_devt, 0, 1, "latmus");
+	if (ret)
+		goto fail_region;
+
+	cdev_init(&latmus_cdev, &latmus_fops);
+	ret = cdev_add(&latmus_cdev, latmus_devt, 1);
+	if (ret)
+		goto fail_add;
+
+	dev = device_create(&latmus_class, NULL, latmus_devt, NULL, "latmus");
+	if (IS_ERR(dev)) {
+		ret = PTR_ERR(dev);
+		goto fail_dev;
+	}
+
+	return 0;
+
+fail_dev:
+	cdev_del(&latmus_cdev);
+fail_add:
+	unregister_chrdev_region(latmus_devt, 1);
+fail_region:
+	class_unregister(&latmus_class);
+
+	return ret;
+}
+module_init(latmus_init);
+
+static void __exit latmus_exit(void)
+{
+	device_destroy(&latmus_class, MKDEV(MAJOR(latmus_devt), 0));
+	cdev_del(&latmus_cdev);
+	class_unregister(&latmus_class);
+}
+module_exit(latmus_exit);
+
+MODULE_LICENSE("GPL");
diff --git a/include/asm-generic/evenless/thread_info.h b/include/asm-generic/evenless/thread_info.h
new file mode 100644
index 000000000000..233d737a3001
--- /dev/null
+++ b/include/asm-generic/evenless/thread_info.h
@@ -0,0 +1,17 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_GENERIC_EVENLESS_THREAD_INFO_H
+#define _ASM_GENERIC_EVENLESS_THREAD_INFO_H
+
+struct evl_thread;
+
+struct oob_thread_state {
+	struct evl_thread *thread;
+};
+
+static inline
+void evl_init_thread_state(struct oob_thread_state *p)
+{
+	p->thread = NULL;
+}
+
+#endif /* !_ASM_GENERIC_EVENLESS_THREAD_INFO_H */
diff --git a/include/evenless/assert.h b/include/evenless/assert.h
new file mode 100644
index 000000000000..af06494b8987
--- /dev/null
+++ b/include/evenless/assert.h
@@ -0,0 +1,43 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2006, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_ASSERT_H
+#define _EVENLESS_ASSERT_H
+
+#include <linux/kconfig.h>
+
+#define EVL_INFO	KERN_INFO    "EVL: "
+#define EVL_WARNING	KERN_WARNING "EVL: "
+#define EVL_ERR		KERN_ERR     "EVL: "
+
+#define EVL_DEBUG(__subsys)				\
+	IS_ENABLED(CONFIG_EVENLESS_DEBUG_##__subsys)
+#define EVL_ASSERT(__subsys, __cond)			\
+	(!WARN_ON(EVL_DEBUG(__subsys) && !(__cond)))
+#define EVL_WARN(__subsys, __cond, __fmt...)		\
+	WARN(EVL_DEBUG(__subsys) && (__cond), __fmt)
+#define EVL_WARN_ON(__subsys, __cond)			\
+	WARN_ON(EVL_DEBUG(__subsys) && (__cond))
+#define EVL_WARN_ON_ONCE(__subsys, __cond)		\
+	WARN_ON_ONCE(EVL_DEBUG(__subsys) && (__cond))
+#ifdef CONFIG_SMP
+#define EVL_WARN_ON_SMP(__subsys, __cond)		\
+	EVL_WARN_ON(__subsys, __cond)
+#else
+#define EVL_WARN_ON_SMP(__subsys, __cond)		\
+	do { } while (0)
+#endif
+
+#define oob_context_only()	EVL_WARN_ON_ONCE(CONTEXT, running_inband())
+#define inband_context_only()	EVL_WARN_ON_ONCE(CONTEXT, !running_inband())
+#ifdef CONFIG_EVENLESS_DEBUG_LOCKING
+#define atomic_only()		EVL_WARN_ON_ONCE(CONTEXT, !(xnlock_is_owner(&nklock) && hard_irqs_disabled()))
+#else
+#define atomic_only()		EVL_WARN_ON_ONCE(CONTEXT, !hard_irqs_disabled())
+#endif
+
+#endif /* !_EVENLESS_ASSERT_H */
diff --git a/include/evenless/clock.h b/include/evenless/clock.h
new file mode 100644
index 000000000000..346c4d17423f
--- /dev/null
+++ b/include/evenless/clock.h
@@ -0,0 +1,172 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2006, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_CLOCK_H
+#define _EVENLESS_CLOCK_H
+
+#include <linux/types.h>
+#include <linux/time.h>
+#include <linux/ktime.h>
+#include <linux/cpumask.h>
+#include <evenless/list.h>
+#include <evenless/factory.h>
+#include <uapi/evenless/clock.h>
+
+#define ONE_BILLION  1000000000
+
+struct evl_rq;
+struct evl_timerbase;
+
+struct evl_clock_gravity {
+	ktime_t irq;
+	ktime_t kernel;
+	ktime_t user;
+};
+
+struct evl_clock {
+	/* (ns) */
+	ktime_t resolution;
+	/* Anticipation values for timer shots. */
+	struct evl_clock_gravity gravity;
+	/* Clock name. */
+	const char *name;
+	struct {
+		ktime_t (*read)(struct evl_clock *clock);
+		u64 (*read_cycles)(struct evl_clock *clock);
+		int (*set_time)(struct evl_clock *clock,
+				const struct timespec *ts);
+		void (*program_local_shot)(struct evl_clock *clock);
+		void (*program_remote_shot)(struct evl_clock *clock,
+					    struct evl_rq *rq);
+		int (*set_gravity)(struct evl_clock *clock,
+				   const struct evl_clock_gravity *p);
+		void (*reset_gravity)(struct evl_clock *clock);
+		void (*adjust)(struct evl_clock *clock);
+		int (*adjust_time)(struct evl_clock *clock,
+				   struct timex *tx);
+	} ops;
+	struct evl_timerbase *timerdata;
+	struct evl_clock *master;
+	/* Offset from master clock. */
+	ktime_t offset;
+#ifdef CONFIG_SMP
+	/* CPU affinity of clock beat. */
+	struct cpumask affinity;
+#endif
+	struct list_head next;
+	struct evl_element element;
+	void (*dispose)(struct evl_clock *clock);
+} ____cacheline_aligned;
+
+extern struct evl_clock evl_mono_clock;
+
+extern struct evl_clock evl_realtime_clock;
+
+int evl_init_clock(struct evl_clock *clock,
+		   const struct cpumask *affinity);
+
+int evl_init_slave_clock(struct evl_clock *clock,
+			 struct evl_clock *master);
+
+void evl_announce_tick(struct evl_clock *clock);
+
+void evl_adjust_timers(struct evl_clock *clock,
+		       ktime_t delta);
+
+void evl_stop_timers(struct evl_clock *clock);
+
+static inline u64 evl_read_clock_cycles(struct evl_clock *clock)
+{
+	return clock->ops.read_cycles(clock);
+}
+
+static ktime_t evl_ktime_monotonic(void)
+{
+	return ktime_get_mono_fast_ns();
+}
+
+static inline ktime_t evl_read_clock(struct evl_clock *clock)
+{
+	/*
+	 * In many occasions on the fast path, evl_read_clock() is
+	 * explicitly called with &evl_mono_clock which resolves as
+	 * a constant. Skip the clock trampoline handler, branching
+	 * immediately to the final code for such clock.
+	 */
+	if (clock == &evl_mono_clock)
+		return evl_ktime_monotonic();
+
+	return clock->ops.read(clock);
+}
+
+static inline int
+evl_set_clock_time(struct evl_clock *clock,
+		   const struct timespec *ts)
+{
+	if (clock->ops.set_time)
+		return clock->ops.set_time(clock, ts);
+
+	return -EOPNOTSUPP;
+}
+
+static inline
+ktime_t evl_get_clock_resolution(struct evl_clock *clock)
+{
+	return clock->resolution;
+}
+
+static inline
+void evl_set_clock_resolution(struct evl_clock *clock,
+			      ktime_t resolution)
+{
+	clock->resolution = resolution;
+}
+
+static inline
+int evl_set_clock_gravity(struct evl_clock *clock,
+			  const struct evl_clock_gravity *gravity)
+{
+	if (clock->ops.set_gravity)
+		return clock->ops.set_gravity(clock, gravity);
+
+	return -EOPNOTSUPP;
+}
+
+static inline void evl_reset_clock_gravity(struct evl_clock *clock)
+{
+	if (clock->ops.reset_gravity)
+		clock->ops.reset_gravity(clock);
+}
+
+#define evl_get_clock_gravity(__clock, __type)  ((__clock)->gravity.__type)
+
+static inline
+int evl_clock_adjust_time(struct evl_clock *clock, struct timex *tx)
+{
+	if (clock->ops.adjust_time)
+		return clock->ops.adjust_time(clock, tx);
+
+	return -EOPNOTSUPP;
+}
+
+int evl_clock_init(void);
+
+void evl_clock_cleanup(void);
+
+int evl_register_clock(struct evl_clock *clock,
+		       const struct cpumask *affinity);
+
+void evl_unregister_clock(struct evl_clock *clock);
+
+struct evl_clock *evl_get_clock_by_fd(int efd);
+
+static inline void evl_put_clock(struct evl_clock *clock)
+{
+	evl_put_element(&clock->element);
+}
+
+#endif /* !_EVENLESS_CLOCK_H */
diff --git a/include/evenless/control.h b/include/evenless/control.h
new file mode 100644
index 000000000000..83cbdc0a4e74
--- /dev/null
+++ b/include/evenless/control.h
@@ -0,0 +1,60 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_CONTROL_H
+#define _EVENLESS_CONTROL_H
+
+#include <linux/atomic.h>
+#include <linux/notifier.h>
+#include <evenless/factory.h>
+
+#define EVL_ABI_LEVEL  0
+
+enum evl_run_states {
+	EVL_STATE_DISABLED,
+	EVL_STATE_RUNNING,
+	EVL_STATE_STOPPED,
+	EVL_STATE_TEARDOWN,
+	EVL_STATE_WARMUP,
+};
+
+extern atomic_t evl_runstate;
+
+static inline enum evl_run_states get_evl_state(void)
+{
+	return atomic_read(&evl_runstate);
+}
+
+static inline int evl_is_warming(void)
+{
+	return get_evl_state() == EVL_STATE_WARMUP;
+}
+
+static inline int evl_is_running(void)
+{
+	return get_evl_state() == EVL_STATE_RUNNING;
+}
+
+static inline int evl_is_enabled(void)
+{
+	return get_evl_state() != EVL_STATE_DISABLED;
+}
+
+static inline int evl_is_stopped(void)
+{
+	return get_evl_state() == EVL_STATE_STOPPED;
+}
+
+static inline void set_evl_state(enum evl_run_states state)
+{
+	atomic_set(&evl_runstate, state);
+}
+
+void evl_add_state_chain(struct notifier_block *nb);
+
+void evl_remove_state_chain(struct notifier_block *nb);
+
+#endif /* !_EVENLESS_CONTROL_H */
diff --git a/include/evenless/factory.h b/include/evenless/factory.h
new file mode 100644
index 000000000000..3a68a6c2e076
--- /dev/null
+++ b/include/evenless/factory.h
@@ -0,0 +1,151 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_FACTORY_H
+#define _EVENLESS_FACTORY_H
+
+#include <linux/device.h>
+#include <linux/cdev.h>
+#include <linux/fs.h>
+#include <linux/bits.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/rbtree.h>
+#include <linux/rcupdate.h>
+#include <linux/workqueue.h>
+#include <linux/irq_work.h>
+#include <evenless/file.h>
+#include <uapi/evenless/types.h>
+
+#define element_of(__filp, __type)					\
+	({								\
+		struct evl_file_binding *__fbind = (__filp)->private_data; \
+		container_of(__fbind->element, __type, element);	\
+	})								\
+
+#define fundle_of(__obj)	((__obj)->element.fundle)
+
+struct evl_element;
+
+#define EVL_FACTORY_CLONE	BIT(0)
+#define EVL_FACTORY_SINGLE	BIT(1)
+
+struct evl_factory {
+	const char *name;
+	const struct file_operations *fops;
+	unsigned int nrdev;
+	struct evl_element *(*build)(struct evl_factory *fac,
+				     const char *name,
+				     void __user *u_attrs,
+				     u32 *state_offp);
+	void (*dispose)(struct evl_element *e);
+	const struct attribute_group **attrs;
+	int flags;
+	struct {
+		struct class *class;
+		struct cdev cdev;
+		dev_t rdev;
+		dev_t sub_rdev;
+		unsigned long *minor_map;
+		struct evl_index {
+			struct rb_root root;
+			hard_spinlock_t lock;
+			fundle_t generator;
+		} index;
+	}; /* Internal. */
+};
+
+struct evl_element {
+	struct evl_factory *factory;
+	struct cdev cdev;
+	struct filename *devname;
+	unsigned int minor;
+	int refs;
+	bool zombie;
+	hard_spinlock_t ref_lock;
+	fundle_t fundle;
+	struct rb_node index_node;
+	struct irq_work irq_work;
+	struct work_struct work;
+	struct rcu_head rcu;
+};
+
+static inline const char *
+evl_element_name(struct evl_element *e)
+{
+	return e->devname->name;
+}
+
+int evl_init_element(struct evl_element *e,
+		     struct evl_factory *fac);
+
+void evl_destroy_element(struct evl_element *e);
+
+void evl_get_element(struct evl_element *e);
+
+struct evl_element *
+__evl_get_element_by_fundle(struct evl_factory *fac,
+			    fundle_t fundle);
+
+#define evl_get_element_by_fundle(__fac, __fundle, __type)		\
+	({								\
+		struct evl_element *__e;				\
+		__e = __evl_get_element_by_fundle(__fac, __fundle);	\
+		__e ? container_of(__e, __type, element) : NULL;	\
+	})
+
+/*
+ * An element can be disposed of only after the device backing it is
+ * removed. If @dev is valid, so is @e at the time of the call.
+ */
+#define evl_get_element_by_dev(__dev, __type)				\
+	({								\
+		struct evl_element *__e = dev_get_drvdata(__dev);	\
+		evl_get_element(__e);					\
+		container_of(__e, __type, element);			\
+	})
+
+void evl_put_element(struct evl_element *e);
+
+int evl_open_element(struct inode *inode,
+		     struct file *filp);
+
+int evl_close_element(struct inode *inode,
+		      struct file *filp);
+
+int evl_create_element_device(struct evl_element *e,
+			      struct evl_factory *fac,
+			      const char *devname);
+
+void evl_remove_element_device(struct evl_element *e);
+
+void evl_index_element(struct evl_element *e);
+
+int evl_index_element_at(struct evl_element *e,
+			 fundle_t fundle);
+
+void evl_unindex_element(struct evl_element *e);
+
+int evl_early_init_factories(void);
+
+void evl_early_cleanup_factories(void);
+
+int evl_late_init_factories(void);
+
+void evl_cleanup_factories(void);
+
+extern struct evl_factory evl_clock_factory;
+extern struct evl_factory evl_control_factory;
+extern struct evl_factory evl_logger_factory;
+extern struct evl_factory evl_monitor_factory;
+extern struct evl_factory evl_poller_factory;
+extern struct evl_factory evl_sem_factory;
+extern struct evl_factory evl_thread_factory;
+extern struct evl_factory evl_timerfd_factory;
+extern struct evl_factory evl_trace_factory;
+extern struct evl_factory evl_xbuf_factory;
+
+#endif /* !_EVENLESS_FACTORY_H */
diff --git a/include/evenless/file.h b/include/evenless/file.h
new file mode 100644
index 000000000000..74755b13505f
--- /dev/null
+++ b/include/evenless/file.h
@@ -0,0 +1,65 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_FILE_H
+#define _EVENLESS_FILE_H
+
+#include <linux/spinlock.h>
+#include <linux/atomic.h>
+#include <linux/rbtree.h>
+#include <linux/completion.h>
+#include <linux/irq_work.h>
+
+struct file;
+struct files_struct;
+struct evl_element;
+
+struct evl_file {
+	struct file *filp;
+	atomic_t oob_refs;
+	struct completion oob_done;
+	struct irq_work oob_work;
+};
+
+struct evl_fd {
+	unsigned int fd;
+	struct evl_file *sfilp;
+	struct files_struct *files;
+	struct rb_node rb;
+};
+
+struct evl_file_binding {
+	struct evl_file efile;
+	struct evl_element *element;
+};
+
+int evl_open_file(struct evl_file *sfilp,
+		  struct file *filp);
+
+void evl_release_file(struct evl_file *sfilp);
+
+static inline
+void evl_get_fileref(struct evl_file *sfilp)
+{
+	atomic_inc(&sfilp->oob_refs);
+}
+
+struct evl_file *evl_get_file(unsigned int fd);
+
+void __evl_put_file(struct evl_file *sfilp);
+
+static inline
+void evl_put_file(struct evl_file *sfilp) /* OOB */
+{
+	if (atomic_dec_return(&sfilp->oob_refs) == 0)
+		__evl_put_file(sfilp);
+}
+
+int evl_init_files(void);
+
+void evl_cleanup_files(void);
+
+#endif /* !_EVENLESS_FILE_H */
diff --git a/include/evenless/init.h b/include/evenless/init.h
new file mode 100644
index 000000000000..ff319a340069
--- /dev/null
+++ b/include/evenless/init.h
@@ -0,0 +1,36 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_INIT_H
+#define _EVENLESS_INIT_H
+
+#include <linux/dovetail.h>
+
+struct evl_machine_cpudata {
+};
+
+DECLARE_PER_CPU(struct evl_machine_cpudata, evl_machine_cpudata);
+
+#ifdef CONFIG_SMP
+extern struct cpumask evl_oob_cpus;
+#endif
+
+#ifdef CONFIG_EVENLESS_DEBUG
+void evl_warn_init(const char *fn, int level, int status);
+#else
+static inline void evl_warn_init(const char *fn, int level, int status)
+{ }
+#endif
+
+#define EVL_INIT_CALL(__level, __call)				\
+	({							\
+		int __ret = __call;				\
+		if (__ret)					\
+			evl_warn_init(#__call, __level, __ret);	\
+		__ret;						\
+	})
+
+#endif /* !_EVENLESS_INIT_H_ */
diff --git a/include/evenless/list.h b/include/evenless/list.h
new file mode 100644
index 000000000000..234272530dd2
--- /dev/null
+++ b/include/evenless/list.h
@@ -0,0 +1,46 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2013, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_LIST_H
+#define _EVENLESS_LIST_H
+
+#include <linux/list.h>
+
+#define __list_add_pri(__new, __head, __member_pri, __member_next, __relop)	\
+do {										\
+	typeof(*__new) *__pos;							\
+	if (list_empty(__head))							\
+		list_add(&(__new)->__member_next, __head);		 	\
+	else {									\
+		list_for_each_entry_reverse(__pos, __head, __member_next) {	\
+			if ((__new)->__member_pri __relop __pos->__member_pri)	\
+				break;						\
+		}								\
+		list_add(&(__new)->__member_next, &__pos->__member_next); 	\
+	}									\
+} while (0)
+
+#define list_add_priff(__new, __head, __member_pri, __member_next)		\
+	__list_add_pri(__new, __head, __member_pri, __member_next, <=)
+
+#define list_add_prilf(__new, __head, __member_pri, __member_next)		\
+	__list_add_pri(__new, __head, __member_pri, __member_next, <)
+
+#define list_get_entry(__head, __type, __member)		\
+  ({								\
+	  __type *__item;					\
+	  __item = list_first_entry(__head, __type, __member);	\
+	  list_del(&__item->__member);				\
+	  __item;						\
+  })
+
+#ifndef list_next_entry
+#define list_next_entry(__item, __member)			\
+	list_entry((__item)->__member.next, typeof(*(__item)), __member)
+#endif
+
+#endif /* !_EVENLESS_LIST_H_ */
diff --git a/include/evenless/lock.h b/include/evenless/lock.h
new file mode 100644
index 000000000000..28825c887e24
--- /dev/null
+++ b/include/evenless/lock.h
@@ -0,0 +1,225 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Inherited from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2001-2008, 2018 Philippe Gerum <rpm@xenomai.org>
+ * Copyright (C) 2004,2005 Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>
+ *
+ * NOTE: this code is on its way out, the ugly superlock is the only
+ * remaining lock of this type. Moving away from the inefficient
+ * single-lock model is planned.
+ */
+#ifndef _EVENLESS_LOCK_H
+#define _EVENLESS_LOCK_H
+
+#include <linux/irq_pipeline.h>
+#include <linux/percpu.h>
+#include <linux/ktime.h>
+#include <evenless/assert.h>
+
+#define splhigh(x)  ((x) = oob_irq_save() & 1)
+#ifdef CONFIG_SMP
+#define splexit(x)  oob_irq_restore(x & 1)
+#else /* !CONFIG_SMP */
+#define splexit(x)  oob_irq_restore(x)
+#endif /* !CONFIG_SMP */
+
+#ifdef CONFIG_EVENLESS_DEBUG_LOCKING
+
+struct xnlock {
+	unsigned owner;
+	arch_spinlock_t alock;
+	const char *file;
+	const char *function;
+	unsigned int line;
+	int cpu;
+	ktime_t spin_time;
+	ktime_t lock_date;
+};
+
+struct xnlockinfo {
+	ktime_t spin_time;
+	ktime_t lock_time;
+	const char *file;
+	const char *function;
+	unsigned int line;
+};
+
+#define XNARCH_LOCK_UNLOCKED (struct xnlock) {	\
+	~0,					\
+	__ARCH_SPIN_LOCK_UNLOCKED,		\
+	NULL,					\
+	NULL,					\
+	0,					\
+	-1,					\
+	0LL,					\
+	0LL,					\
+}
+
+#define T_LOCK_DBG_CONTEXT		, __FILE__, __LINE__, __FUNCTION__
+#define T_LOCK_DBG_CONTEXT_ARGS					\
+	, const char *file, int line, const char *function
+#define T_LOCK_DBG_PASS_CONTEXT		, file, line, function
+
+void xnlock_dbg_prepare_acquire(ktime_t *start);
+void xnlock_dbg_prepare_spin(unsigned int *spin_limit);
+void xnlock_dbg_acquired(struct xnlock *lock, int cpu,
+			 ktime_t *start,
+			 const char *file, int line,
+			 const char *function);
+int xnlock_dbg_release(struct xnlock *lock,
+			 const char *file, int line,
+			 const char *function);
+
+DECLARE_PER_CPU(struct xnlockinfo, xnlock_stats);
+
+#else /* !CONFIG_EVENLESS_DEBUG_LOCKING */
+
+struct xnlock {
+	unsigned owner;
+	arch_spinlock_t alock;
+};
+
+#define XNARCH_LOCK_UNLOCKED			\
+	(struct xnlock) {			\
+		~0,				\
+		__ARCH_SPIN_LOCK_UNLOCKED,	\
+	}
+
+#define T_LOCK_DBG_CONTEXT
+#define T_LOCK_DBG_CONTEXT_ARGS
+#define T_LOCK_DBG_PASS_CONTEXT
+
+static inline
+void xnlock_dbg_prepare_acquire(ktime_t *start)
+{
+}
+
+static inline
+void xnlock_dbg_prepare_spin(unsigned int *spin_limit)
+{
+}
+
+static inline void
+xnlock_dbg_acquired(struct xnlock *lock, int cpu,
+		    ktime_t *start)
+{
+}
+
+static inline int xnlock_dbg_release(struct xnlock *lock)
+{
+	return 0;
+}
+
+#endif /* !CONFIG_EVENLESS_DEBUG_LOCKING */
+
+#if defined(CONFIG_SMP) || defined(CONFIG_EVENLESS_DEBUG_LOCKING)
+
+#define xnlock_get(lock)		__xnlock_get(lock  T_LOCK_DBG_CONTEXT)
+#define xnlock_put(lock)		__xnlock_put(lock  T_LOCK_DBG_CONTEXT)
+#define xnlock_get_irqsave(lock,x) \
+	((x) = __xnlock_get_irqsave(lock  T_LOCK_DBG_CONTEXT))
+#define xnlock_put_irqrestore(lock,x) \
+	__xnlock_put_irqrestore(lock,x  T_LOCK_DBG_CONTEXT)
+#define xnlock_clear_irqoff(lock)	xnlock_put_irqrestore(lock, 1)
+#define xnlock_clear_irqon(lock)	xnlock_put_irqrestore(lock, 0)
+
+static inline void xnlock_init (struct xnlock *lock)
+{
+	*lock = XNARCH_LOCK_UNLOCKED;
+}
+
+#define DECLARE_XNLOCK(lock)		struct xnlock lock
+#define DECLARE_EXTERN_XNLOCK(lock)	extern struct xnlock lock
+#define DEFINE_XNLOCK(lock)		struct xnlock lock = XNARCH_LOCK_UNLOCKED
+#define DEFINE_PRIVATE_XNLOCK(lock)	static DEFINE_XNLOCK(lock)
+
+static inline int ____xnlock_get(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+{
+	int cpu = raw_smp_processor_id();
+	ktime_t start;
+
+	if (lock->owner == cpu)
+		return 2;
+
+	xnlock_dbg_prepare_acquire(&start);
+
+	arch_spin_lock(&lock->alock);
+	lock->owner = cpu;
+
+	xnlock_dbg_acquired(lock, cpu, &start /*, */ T_LOCK_DBG_PASS_CONTEXT);
+
+	return 0;
+}
+
+static inline void ____xnlock_put(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+{
+	if (xnlock_dbg_release(lock /*, */ T_LOCK_DBG_PASS_CONTEXT))
+		return;
+
+	lock->owner = ~0U;
+	arch_spin_unlock(&lock->alock);
+}
+
+int ___xnlock_get(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS);
+
+void ___xnlock_put(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS);
+
+static inline unsigned long
+__xnlock_get_irqsave(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+{
+	unsigned long flags;
+
+	splhigh(flags);
+
+	flags |= ___xnlock_get(lock /*, */ T_LOCK_DBG_PASS_CONTEXT);
+
+	return flags;
+}
+
+static inline void __xnlock_put_irqrestore(struct xnlock *lock, unsigned long flags
+					   /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+{
+	/* Only release the lock if we didn't take it recursively. */
+	if (!(flags & 2))
+		___xnlock_put(lock /*, */ T_LOCK_DBG_PASS_CONTEXT);
+
+	splexit(flags & 1);
+}
+
+static inline int xnlock_is_owner(struct xnlock *lock)
+{
+	return lock->owner == raw_smp_processor_id();
+}
+
+static inline int __xnlock_get(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+{
+	return ___xnlock_get(lock /* , */ T_LOCK_DBG_PASS_CONTEXT);
+}
+
+static inline void __xnlock_put(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+{
+	___xnlock_put(lock /*, */ T_LOCK_DBG_PASS_CONTEXT);
+}
+
+#else /* !(CONFIG_SMP || CONFIG_EVENLESS_DEBUG_LOCKING) */
+
+#define xnlock_init(lock)		do { } while(0)
+#define xnlock_get(lock)		do { } while(0)
+#define xnlock_put(lock)		do { } while(0)
+#define xnlock_get_irqsave(lock,x)	splhigh(x)
+#define xnlock_put_irqrestore(lock,x)	splexit(x)
+#define xnlock_clear_irqoff(lock)	oob_irq_disable()
+#define xnlock_clear_irqon(lock)	oob_irq_enable()
+#define xnlock_is_owner(lock)		1
+
+#define DECLARE_XNLOCK(lock)
+#define DECLARE_EXTERN_XNLOCK(lock)
+#define DEFINE_XNLOCK(lock)
+#define DEFINE_PRIVATE_XNLOCK(lock)
+
+#endif /* !(CONFIG_SMP || CONFIG_EVENLESS_DEBUG_LOCKING) */
+
+DECLARE_EXTERN_XNLOCK(nklock);
+
+#endif /* !_EVENLESS_LOCK_H */
diff --git a/include/evenless/memory.h b/include/evenless/memory.h
new file mode 100644
index 000000000000..558904f7251c
--- /dev/null
+++ b/include/evenless/memory.h
@@ -0,0 +1,145 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_MEMORY_H
+#define _EVENLESS_MEMORY_H
+
+#include <linux/types.h>
+#include <linux/atomic.h>
+#include <linux/spinlock.h>
+#include <linux/rbtree.h>
+#include <evenless/list.h>
+#include <evenless/factory.h>
+#include <uapi/evenless/types.h>
+
+#define EVL_HEAP_PAGE_SHIFT	9 /* 2^9 => 512 bytes */
+#define EVL_HEAP_PAGE_SIZE	(1UL << EVL_HEAP_PAGE_SHIFT)
+#define EVL_HEAP_PAGE_MASK	(~(EVL_HEAP_PAGE_SIZE - 1))
+#define EVL_HEAP_MIN_LOG2	4 /* 16 bytes */
+/*
+ * Use bucketed memory for sizes between 2^EVL_HEAP_MIN_LOG2 and
+ * 2^(EVL_HEAP_PAGE_SHIFT-1).
+ */
+#define EVL_HEAP_MAX_BUCKETS	(EVL_HEAP_PAGE_SHIFT - EVL_HEAP_MIN_LOG2)
+#define EVL_HEAP_MIN_ALIGN	(1U << EVL_HEAP_MIN_LOG2)
+/* Maximum size of a heap (4Gb - PAGE_SIZE). */
+#define EVL_HEAP_MAX_HEAPSZ	(4294967295U - PAGE_SIZE + 1)
+/* Bits we need for encoding a page # */
+#define EVL_HEAP_PGENT_BITS      (32 - EVL_HEAP_PAGE_SHIFT)
+/* Each page is represented by a page map entry. */
+#define EVL_HEAP_PGMAP_BYTES	sizeof(struct evl_heap_pgentry)
+
+struct evl_heap_pgentry {
+	/* Linkage in bucket list. */
+	unsigned int prev : EVL_HEAP_PGENT_BITS;
+	unsigned int next : EVL_HEAP_PGENT_BITS;
+	/*  page_list or log2. */
+	unsigned int type : 6;
+	/*
+	 * We hold either a spatial map of busy blocks within the page
+	 * for bucketed memory (up to 32 blocks per page), or the
+	 * overall size of the multi-page block if entry.type ==
+	 * page_list.
+	 */
+	union {
+		u32 map;
+		u32 bsize;
+	};
+};
+
+/*
+ * A range descriptor is stored at the beginning of the first page of
+ * a range of free pages. evl_heap_range.size is nrpages *
+ * EVL_HEAP_PAGE_SIZE. Ranges are indexed by address and size in
+ * rbtrees.
+ */
+struct evl_heap_range {
+	struct rb_node addr_node;
+	struct rb_node size_node;
+	size_t size;
+};
+
+struct evl_heap {
+	void *membase;
+	struct rb_root addr_tree;
+	struct rb_root size_tree;
+	struct evl_heap_pgentry *pagemap;
+	size_t usable_size;
+	size_t used_size;
+	u32 buckets[EVL_HEAP_MAX_BUCKETS];
+	hard_spinlock_t lock;
+	struct list_head next;
+};
+
+extern struct evl_heap evl_system_heap;
+
+extern struct evl_heap evl_shared_heap;
+
+static inline size_t evl_get_heap_size(const struct evl_heap *heap)
+{
+	return heap->usable_size;
+}
+
+static inline size_t evl_get_heap_free(const struct evl_heap *heap)
+{
+	return heap->usable_size - heap->used_size;
+}
+
+static inline void *evl_get_heap_base(const struct evl_heap *heap)
+{
+	return heap->membase;
+}
+
+int evl_init_heap(struct evl_heap *heap, void *membase,
+		  size_t size);
+
+void evl_destroy_heap(struct evl_heap *heap);
+
+void *evl_alloc_chunk(struct evl_heap *heap, size_t size);
+
+void evl_free_chunk(struct evl_heap *heap, void *block);
+
+ssize_t evl_check_chunk(struct evl_heap *heap, void *block);
+
+static inline void *evl_zalloc_chunk(struct evl_heap *heap, u32 size)
+{
+	void *p;
+
+	p = evl_alloc_chunk(heap, size);
+	if (p)
+		memset(p, 0, size);
+
+	return p;
+}
+
+void *evl_alloc_irq_work(size_t size);
+
+void evl_free_irq_work(void *p);
+
+static inline
+int evl_shared_offset(void *p)
+{
+	return p - evl_get_heap_base(&evl_shared_heap);
+}
+
+static inline void *evl_alloc(size_t size)
+{
+	return evl_alloc_chunk(&evl_system_heap, size);
+}
+
+static inline void evl_free(void *ptr)
+{
+	evl_free_chunk(&evl_system_heap, ptr);
+}
+
+int evl_init_memory(void);
+
+void evl_cleanup_memory(void);
+
+extern size_t evl_shm_size;
+
+#endif /* !_EVENLESS_MEMORY_H */
diff --git a/include/evenless/monitor.h b/include/evenless/monitor.h
new file mode 100644
index 000000000000..c442df28666c
--- /dev/null
+++ b/include/evenless/monitor.h
@@ -0,0 +1,24 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_MONITOR_H
+#define _EVENLESS_MONITOR_H
+
+#include <evenless/factory.h>
+#include <evenless/thread.h>
+
+int evl_signal_monitor_targeted(struct evl_thread *target,
+				int monfd);
+
+void __evl_commit_monitor_ceiling(struct evl_thread *curr);
+
+static inline void evl_commit_monitor_ceiling(struct evl_thread *curr)
+{
+	if (curr->u_window->pp_pending != EVL_NO_HANDLE)
+		__evl_commit_monitor_ceiling(curr);
+}
+
+#endif /* !_EVENLESS_MONITOR_H */
diff --git a/include/evenless/poller.h b/include/evenless/poller.h
new file mode 100644
index 000000000000..5ca68e9200c3
--- /dev/null
+++ b/include/evenless/poller.h
@@ -0,0 +1,67 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_POLLER_H
+#define _EVENLESS_POLLER_H
+
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/rbtree.h>
+#include <linux/spinlock.h>
+#include <linux/poll.h>
+#include <evenless/synch.h>
+#include <evenless/factory.h>
+#include <uapi/evenless/poller.h>
+
+struct oob_poll_wait;
+
+#define EVL_POLLHEAD_INITIALIZER(__name) {				\
+		.watchpoints = LIST_HEAD_INIT((__name).watchpoints),	\
+		__HARD_SPIN_LOCK_INIT((__name).__lock),			\
+	}
+
+struct evl_poll_head {
+	struct list_head watchpoints; /* struct poll_watchpoint */
+	hard_spinlock_t lock;
+};
+
+static inline
+void evl_init_poll_head(struct evl_poll_head *head)
+{
+	INIT_LIST_HEAD(&head->watchpoints);
+	raw_spin_lock_init(&head->lock);
+}
+
+void evl_poll_watch(struct evl_poll_head *head,
+		    struct oob_poll_wait *wait);
+
+void __evl_signal_poll_events(struct evl_poll_head *head,
+			      int events);
+
+void __evl_clear_poll_events(struct evl_poll_head *head,
+			     int events);
+
+static inline void
+evl_signal_poll_events(struct evl_poll_head *head,
+		       int events)
+{
+	/* Quick check. We'll redo under lock */
+	if (!list_empty(&head->watchpoints))
+		__evl_signal_poll_events(head, events);
+
+}
+
+static inline void
+evl_clear_poll_events(struct evl_poll_head *head,
+		      int events)
+{
+	/* Quick check. We'll redo under lock */
+	if (!list_empty(&head->watchpoints))
+		__evl_clear_poll_events(head, events);
+
+}
+
+#endif /* !_EVENLESS_POLLER_H */
diff --git a/include/evenless/sched.h b/include/evenless/sched.h
new file mode 100644
index 000000000000..5c2e19b5f31b
--- /dev/null
+++ b/include/evenless/sched.h
@@ -0,0 +1,550 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2008, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_SCHED_H
+#define _EVENLESS_SCHED_H
+
+#include <linux/percpu.h>
+#include <linux/list.h>
+#include <evenless/lock.h>
+#include <evenless/thread.h>
+#include <evenless/sched/queue.h>
+#include <evenless/sched/weak.h>
+#include <evenless/sched/quota.h>
+#include <evenless/assert.h>
+#include <evenless/init.h>
+
+/** Shared scheduler status bits **/
+
+/*
+ * A rescheduling call is pending.
+ */
+#define RQ_SCHED	0x10000000
+
+/**
+ * Private scheduler flags (combined in test operations with shared
+ * bits, must not conflict with them).
+ */
+
+/*
+ * Currently running in tick handler context.
+ */
+#define RQ_TIMER	0x00010000
+/*
+ * A proxy tick is being processed, i.e. matching an earlier timing
+ * request from the regular kernel.
+ */
+#define RQ_TPROXY	0x00008000
+/*
+ * Currently running in IRQ handling context.
+ */
+#define RQ_IRQ		0x00004000
+/*
+ * Proxy tick is deferred, because we have more urgent real-time
+ * duties to carry out first.
+ */
+#define RQ_TDEFER	0x00002000
+/*
+ * Idle state: there is no outstanding timer. We check this flag to
+ * know whether we may allow the regular kernel to enter the CPU idle
+ * state.
+ */
+#define RQ_IDLE		0x00001000
+/*
+ * Hardware timer is stopped.
+ */
+#define RQ_TSTOPPED	0x00000800
+
+struct evl_sched_rt {
+	evl_schedqueue_t runnable;	/* Runnable thread queue. */
+};
+
+struct evl_rq {
+	/* Shared status bitmask. */
+	unsigned long status;
+	/* Private status bitmask. */
+	unsigned long lflags;
+	/* Current thread. */
+	struct evl_thread *curr;
+#ifdef CONFIG_SMP
+	/* Owner CPU id. */
+	int cpu;
+	/* Mask of CPUs needing rescheduling. */
+	struct cpumask resched;
+#endif
+	/* Context of built-in real-time class. */
+	struct evl_sched_rt rt;
+	/* Context of weak scheduling class. */
+	struct evl_sched_weak weak;
+#ifdef CONFIG_EVENLESS_SCHED_QUOTA
+	/* Context of runtime quota scheduling. */
+	struct evl_sched_quota quota;
+#endif
+	/* Host timer. */
+	struct evl_timer htimer;
+	/* Round-robin timer. */
+	struct evl_timer rrbtimer;
+	/* In-band kernel placeholder. */
+	struct evl_thread root_thread;
+	char *proxy_timer_name;
+	char *rrb_timer_name;
+#ifdef CONFIG_EVENLESS_WATCHDOG
+	/* Watchdog timer object. */
+	struct evl_timer wdtimer;
+#endif
+#ifdef CONFIG_EVENLESS_STATS
+	/* Last account switch date (ticks). */
+	ktime_t last_account_switch;
+	/* Currently active account */
+	struct evl_account *current_account;
+#endif
+	struct evl_syn yield_sync;
+};
+
+DECLARE_PER_CPU(struct evl_rq, evl_runqueues);
+
+extern struct cpumask evl_cpu_affinity;
+
+extern struct list_head evl_thread_list;
+
+extern int evl_nrthreads;
+
+union evl_sched_param;
+
+struct evl_sched_class {
+	void (*sched_init)(struct evl_rq *rq);
+	void (*sched_enqueue)(struct evl_thread *thread);
+	void (*sched_dequeue)(struct evl_thread *thread);
+	void (*sched_requeue)(struct evl_thread *thread);
+	struct evl_thread *(*sched_pick)(struct evl_rq *rq);
+	void (*sched_tick)(struct evl_rq *rq);
+	void (*sched_rotate)(struct evl_rq *rq,
+			     const union evl_sched_param *p);
+	void (*sched_migrate)(struct evl_thread *thread,
+			      struct evl_rq *rq);
+	/*
+	 * Set base scheduling parameters. This routine is indirectly
+	 * called upon a change of base scheduling settings through
+	 * __evl_set_thread_schedparam() -> evl_set_thread_policy(),
+	 * exclusively.
+	 *
+	 * The scheduling class implementation should do the necessary
+	 * housekeeping to comply with the new settings.
+	 * thread->base_class is up to date before the call is made,
+	 * and should be considered for the new weighted priority
+	 * calculation. On the contrary, thread->sched_class should
+	 * NOT be referred to by this handler.
+	 *
+	 * sched_setparam() is NEVER involved in PI or PP
+	 * management. However it must deny a priority update if it
+	 * contradicts an ongoing boost for @a thread. This is
+	 * typically what the evl_set_effective_thread_priority() helper
+	 * does for such handler.
+	 *
+	 * Returns true if the effective priority was updated
+	 * (thread->cprio).
+	 */
+	bool (*sched_setparam)(struct evl_thread *thread,
+			       const union evl_sched_param *p);
+	void (*sched_getparam)(struct evl_thread *thread,
+			       union evl_sched_param *p);
+	int (*sched_chkparam)(struct evl_thread *thread,
+			      const union evl_sched_param *p);
+	void (*sched_trackprio)(struct evl_thread *thread,
+				const union evl_sched_param *p);
+	void (*sched_ceilprio)(struct evl_thread *thread, int prio);
+	/* Prep work for assigning a policy to a thread. */
+	int (*sched_declare)(struct evl_thread *thread,
+			     const union evl_sched_param *p);
+	void (*sched_forget)(struct evl_thread *thread);
+	void (*sched_kick)(struct evl_thread *thread);
+	ssize_t (*sched_show)(struct evl_thread *thread,
+			      char *buf, ssize_t count);
+	int nthreads;
+	struct evl_sched_class *next;
+	int weight;
+	int policy;
+	const char *name;
+};
+
+#define EVL_CLASS_WEIGHT(n)	(n * EVL_CLASS_WEIGHT_FACTOR)
+
+#define for_each_evl_thread(__thread)				\
+	list_for_each_entry(__thread, &evl_thread_list, next)
+
+#ifdef CONFIG_SMP
+static inline int evl_rq_cpu(struct evl_rq *rq)
+{
+	return rq->cpu;
+}
+#else /* !CONFIG_SMP */
+static inline int evl_rq_cpu(struct evl_rq *rq)
+{
+	return 0;
+}
+#endif /* CONFIG_SMP */
+
+static inline struct evl_rq *evl_cpu_rq(int cpu)
+{
+	return &per_cpu(evl_runqueues, cpu);
+}
+
+static inline struct evl_rq *this_evl_rq(void)
+{
+	/* IRQs off */
+	return raw_cpu_ptr(&evl_runqueues);
+}
+
+static inline struct evl_thread *this_evl_rq_thread(void)
+{
+	return this_evl_rq()->curr;
+}
+
+/* Test resched flag of given rq. */
+static inline int evl_need_resched(struct evl_rq *rq)
+{
+	return rq->status & RQ_SCHED;
+}
+
+/* Set self resched flag for the current scheduler. */
+static inline void evl_set_self_resched(struct evl_rq *rq)
+{
+	atomic_only();
+	rq->status |= RQ_SCHED;
+}
+
+static inline bool is_evl_cpu(int cpu)
+{
+	return !!cpumask_test_cpu(cpu, &evl_oob_cpus);
+}
+
+/* Set resched flag for the given scheduler. */
+#ifdef CONFIG_SMP
+
+static inline void evl_set_resched(struct evl_rq *rq)
+{
+	struct evl_rq *this_rq = this_evl_rq();
+
+	if (this_rq == rq)
+		this_rq->status |= RQ_SCHED;
+	else if (!evl_need_resched(rq)) {
+		cpumask_set_cpu(evl_rq_cpu(rq), &this_rq->resched);
+		rq->status |= RQ_SCHED;
+		this_rq->status |= RQ_SCHED;
+	}
+}
+
+static inline bool is_threading_cpu(int cpu)
+{
+	return !!cpumask_test_cpu(cpu, &evl_cpu_affinity);
+}
+
+#else /* !CONFIG_SMP */
+
+static inline void evl_set_resched(struct evl_rq *rq)
+{
+	evl_set_self_resched(rq);
+}
+
+static inline bool is_evl_cpu(int cpu)
+{
+	return true;
+}
+
+static inline bool is_threading_cpu(int cpu)
+{
+	return true;
+}
+
+#endif /* !CONFIG_SMP */
+
+#define for_each_evl_cpu(cpu)		\
+	for_each_online_cpu(cpu)	\
+		if (is_evl_cpu(cpu))
+
+bool ___evl_schedule(struct evl_rq *this_rq);
+
+irqreturn_t __evl_schedule_handler(int irq, void *dev_id);
+
+static inline bool __evl_schedule(struct evl_rq *this_rq)
+{
+	/*
+	 * If we race here reading the scheduler state locklessly
+	 * because of a CPU migration, we must be running over the
+	 * in-band stage, in which case the call to ___evl_schedule()
+	 * will be escalated to the oob stage where migration cannot
+	 * happen, ensuring safe access to the runqueue state.
+	 *
+	 * Remote RQ_SCHED requests are paired with out-of-band IPIs
+	 * running on the oob stage by definition, so we can't miss
+	 * them here.
+	 *
+	 * Finally, RQ_IRQ is always tested from the CPU which handled
+	 * an out-of-band interrupt, there is no coherence issue.
+	 */
+	if (((this_rq->status|this_rq->lflags) & (RQ_IRQ|RQ_SCHED)) != RQ_SCHED)
+		return false;
+
+	return (bool)run_oob_call((int (*)(void *))___evl_schedule, this_rq);
+}
+
+static inline bool evl_schedule(void)
+{
+	struct evl_rq *this_rq = this_evl_rq();
+
+	/*
+	 * Block rescheduling if either the current thread holds the
+	 * scheduler lock, or an interrupt context is active.
+	 */
+	smp_rmb();
+	if (unlikely(this_rq->curr->lock_count > 0))
+		return false;
+
+	return __evl_schedule(this_rq);
+}
+
+#ifdef CONFIG_EVENLESS_DEBUG_LOCKING
+
+void evl_disable_preempt(void);
+void evl_enable_preempt(void);
+
+#else
+
+static inline void evl_disable_preempt(void)
+{
+	struct evl_rq *rq = this_evl_rq();
+	struct evl_thread *curr = rq->curr;
+
+	if (!(rq->lflags & RQ_IRQ))
+		curr->lock_count++;
+}
+
+static inline void evl_enable_preempt(void)
+{
+	struct evl_rq *rq = this_evl_rq();
+	struct evl_thread *curr = rq->curr;
+
+	if (!(rq->lflags & RQ_IRQ) && --curr->lock_count == 0)
+		evl_schedule();
+}
+
+#endif /* !CONFIG_EVENLESS_DEBUG_LOCKING */
+
+static inline bool evl_in_irq(void)
+{
+	return !!(this_evl_rq()->lflags & RQ_IRQ);
+}
+
+static inline bool evl_is_inband(void)
+{
+	return !!(this_evl_rq_thread()->state & T_ROOT);
+}
+
+static inline bool evl_cannot_block(void)
+{
+	return evl_in_irq() || evl_is_inband();
+}
+
+static inline int evl_may_block(void)
+{
+	return !evl_cannot_block();
+}
+
+bool evl_set_effective_thread_priority(struct evl_thread *thread,
+				       int prio);
+
+#include <evenless/sched/idle.h>
+#include <evenless/sched/rt.h>
+
+struct evl_thread *evl_pick_thread(struct evl_rq *rq);
+
+void evl_putback_thread(struct evl_thread *thread);
+
+int evl_set_thread_policy(struct evl_thread *thread,
+			  struct evl_sched_class *sched_class,
+			  const union evl_sched_param *p);
+
+void evl_track_thread_policy(struct evl_thread *thread,
+			     struct evl_thread *target);
+
+void evl_protect_thread_priority(struct evl_thread *thread,
+				 int prio);
+
+void evl_migrate_rq(struct evl_thread *thread,
+		    struct evl_rq *rq);
+
+static inline
+void evl_rotate_rq(struct evl_rq *rq,
+		   struct evl_sched_class *sched_class,
+		   const union evl_sched_param *sched_param)
+{
+	sched_class->sched_rotate(rq, sched_param);
+}
+
+static inline int evl_init_rq_thread(struct evl_thread *thread)
+{
+	int ret = 0;
+
+	evl_init_idle_thread(thread);
+	evl_init_rt_thread(thread);
+#ifdef CONFIG_EVENLESS_SCHED_QUOTA
+	ret = evl_quota_init_thread(thread);
+	if (ret)
+		return ret;
+#endif /* CONFIG_EVENLESS_SCHED_QUOTA */
+
+	return ret;
+}
+
+/* nklock held, irqs off */
+static inline void evl_sched_tick(struct evl_rq *rq)
+{
+	struct evl_thread *curr = rq->curr;
+	struct evl_sched_class *sched_class = curr->sched_class;
+	/*
+	 * A thread that undergoes round-robin scheduling only
+	 * consumes its time slice when it runs within its own
+	 * scheduling class, which excludes temporary PI boosts, and
+	 * does not hold the scheduler lock.
+	 */
+	if (sched_class == curr->base_class &&
+	    sched_class->sched_tick &&
+	    (curr->state & (EVL_THREAD_BLOCK_BITS|T_RRB)) == T_RRB &&
+	    curr->lock_count == 0)
+		sched_class->sched_tick(rq);
+}
+
+static inline
+int evl_check_schedparams(struct evl_sched_class *sched_class,
+			  struct evl_thread *thread,
+			  const union evl_sched_param *p)
+{
+	int ret = 0;
+
+	if (sched_class->sched_chkparam)
+		ret = sched_class->sched_chkparam(thread, p);
+
+	return ret;
+}
+
+static inline
+int evl_declare_thread(struct evl_sched_class *sched_class,
+		       struct evl_thread *thread,
+		       const union evl_sched_param *p)
+{
+	int ret;
+
+	if (sched_class->sched_declare) {
+		ret = sched_class->sched_declare(thread, p);
+		if (ret)
+			return ret;
+	}
+	if (sched_class != thread->base_class)
+		sched_class->nthreads++;
+
+	return 0;
+}
+
+static inline int evl_calc_weighted_prio(struct evl_sched_class *sched_class,
+					 int prio)
+{
+	return prio + sched_class->weight;
+}
+
+static inline void evl_enqueue_thread(struct evl_thread *thread)
+{
+	struct evl_sched_class *sched_class = thread->sched_class;
+
+	if (sched_class != &evl_sched_idle)
+		sched_class->sched_enqueue(thread);
+}
+
+static inline void evl_dequeue_thread(struct evl_thread *thread)
+{
+	struct evl_sched_class *sched_class = thread->sched_class;
+
+	if (sched_class != &evl_sched_idle)
+		sched_class->sched_dequeue(thread);
+}
+
+static inline void evl_requeue_thread(struct evl_thread *thread)
+{
+	struct evl_sched_class *sched_class = thread->sched_class;
+
+	if (sched_class != &evl_sched_idle)
+		sched_class->sched_requeue(thread);
+}
+
+static inline
+bool evl_set_schedparam(struct evl_thread *thread,
+			const union evl_sched_param *p)
+{
+	return thread->base_class->sched_setparam(thread, p);
+}
+
+static inline void evl_get_schedparam(struct evl_thread *thread,
+				      union evl_sched_param *p)
+{
+	thread->sched_class->sched_getparam(thread, p);
+}
+
+static inline void evl_track_priority(struct evl_thread *thread,
+				      const union evl_sched_param *p)
+{
+	thread->sched_class->sched_trackprio(thread, p);
+	thread->wprio = evl_calc_weighted_prio(thread->sched_class, thread->cprio);
+}
+
+static inline void evl_ceil_priority(struct evl_thread *thread, int prio)
+{
+	thread->sched_class->sched_ceilprio(thread, prio);
+	thread->wprio = evl_calc_weighted_prio(thread->sched_class, thread->cprio);
+}
+
+static inline void evl_forget_thread(struct evl_thread *thread)
+{
+	struct evl_sched_class *sched_class = thread->base_class;
+
+	--sched_class->nthreads;
+
+	if (sched_class->sched_forget)
+		sched_class->sched_forget(thread);
+}
+
+static inline void evl_force_thread(struct evl_thread *thread)
+{
+	struct evl_sched_class *sched_class = thread->base_class;
+
+	thread->info |= T_KICKED;
+
+	if (sched_class->sched_kick)
+		sched_class->sched_kick(thread);
+
+	evl_set_resched(thread->rq);
+}
+
+struct evl_sched_group {
+#ifdef CONFIG_EVENLESS_SCHED_QUOTA
+	struct evl_quota_group quota;
+#endif
+	struct list_head next;
+};
+
+struct evl_sched_class *
+evl_find_sched_class(union evl_sched_param *param,
+		     const struct evl_sched_attrs *attrs,
+		     ktime_t *tslice_r);
+
+int evl_sched_yield(void);
+
+void evl_notify_inband_yield(void);
+
+int __init evl_init_sched(void);
+
+void __init evl_cleanup_sched(void);
+
+#endif /* !_EVENLESS_SCHED_H */
diff --git a/include/evenless/sched/idle.h b/include/evenless/sched/idle.h
new file mode 100644
index 000000000000..a7e6f006b1cd
--- /dev/null
+++ b/include/evenless/sched/idle.h
@@ -0,0 +1,53 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2008, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_SCHED_IDLE_H
+#define _EVENLESS_SCHED_IDLE_H
+
+#ifndef _EVENLESS_SCHED_H
+#error "please don't include evenless/sched/idle.h directly"
+#endif
+
+/* Idle priority level - actually never used for indexing. */
+#define EVL_IDLE_PRIO  -1
+
+extern struct evl_sched_class evl_sched_idle;
+
+static inline bool __evl_set_idle_schedparam(struct evl_thread *thread,
+					     const union evl_sched_param *p)
+{
+	thread->state &= ~T_WEAK;
+	return evl_set_effective_thread_priority(thread, p->idle.prio);
+}
+
+static inline void __evl_get_idle_schedparam(struct evl_thread *thread,
+					     union evl_sched_param *p)
+{
+	p->idle.prio = thread->cprio;
+}
+
+static inline void __evl_track_idle_priority(struct evl_thread *thread,
+					     const union evl_sched_param *p)
+{
+	if (p)
+		/* Inheriting a priority-less class makes no sense. */
+		EVL_WARN_ON_ONCE(CORE, 1);
+	else
+		thread->cprio = EVL_IDLE_PRIO;
+}
+
+static inline void __evl_ceil_idle_priority(struct evl_thread *thread, int prio)
+{
+	EVL_WARN_ON_ONCE(CORE, 1);
+}
+
+static inline int evl_init_idle_thread(struct evl_thread *thread)
+{
+	return 0;
+}
+
+#endif /* !_EVENLESS_SCHED_IDLE_H */
diff --git a/include/evenless/sched/param.h b/include/evenless/sched/param.h
new file mode 100644
index 000000000000..d11f874f25ff
--- /dev/null
+++ b/include/evenless/sched/param.h
@@ -0,0 +1,37 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Based on the Xenomai Cobalt core:
+ * Copyright (C) 2008, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_SCHED_PARAM_H
+#define _EVENLESS_SCHED_PARAM_H
+
+struct evl_idle_param {
+	int prio;
+};
+
+struct evl_weak_param {
+	int prio;
+};
+
+struct evl_rt_param {
+	int prio;
+};
+
+struct evl_quota_param {
+	int prio;
+	int tgid;	/* thread group id. */
+};
+
+union evl_sched_param {
+	struct evl_idle_param idle;
+	struct evl_rt_param rt;
+	struct evl_weak_param weak;
+#ifdef CONFIG_EVENLESS_SCHED_QUOTA
+	struct evl_quota_param quota;
+#endif
+};
+
+#endif /* !_EVENLESS_SCHED_PARAM_H */
diff --git a/include/evenless/sched/queue.h b/include/evenless/sched/queue.h
new file mode 100644
index 000000000000..d3763cbe37cf
--- /dev/null
+++ b/include/evenless/sched/queue.h
@@ -0,0 +1,59 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2008, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_SCHED_QUEUE_H
+#define _EVENLESS_SCHED_QUEUE_H
+
+#include <linux/bitmap.h>
+#include <evenless/list.h>
+
+#define EVL_CLASS_WEIGHT_FACTOR	1024
+
+/*
+ * Multi-level priority queue, suitable for handling the runnable
+ * thread queue of the core scheduling class with O(1) property. We
+ * only manage a descending queuing order, i.e. highest numbered
+ * priorities come first.
+ */
+#define EVL_MLQ_LEVELS  (MAX_RT_PRIO + 1) /* i.e. EVL_CORE_NR_PRIO */
+
+struct evl_multilevel_queue {
+	int elems;
+	DECLARE_BITMAP(prio_map, EVL_MLQ_LEVELS);
+	struct list_head heads[EVL_MLQ_LEVELS];
+};
+
+struct evl_thread;
+
+void evl_init_schedq(struct evl_multilevel_queue *q);
+
+void evl_add_schedq(struct evl_multilevel_queue *q,
+		    struct evl_thread *thread);
+
+void evl_add_schedq_tail(struct evl_multilevel_queue *q,
+			 struct evl_thread *thread);
+
+void evl_del_schedq(struct evl_multilevel_queue *q,
+		    struct evl_thread *thread);
+
+struct evl_thread *evl_get_schedq(struct evl_multilevel_queue *q);
+
+static inline int evl_schedq_is_empty(struct evl_multilevel_queue *q)
+{
+	return q->elems == 0;
+}
+
+static inline int evl_get_schedq_weight(struct evl_multilevel_queue *q)
+{
+	return find_first_bit(q->prio_map, EVL_MLQ_LEVELS);
+}
+
+typedef struct evl_multilevel_queue evl_schedqueue_t;
+
+struct evl_thread *evl_lookup_schedq(evl_schedqueue_t *q, int prio);
+
+#endif /* !_EVENLESS_SCHED_QUEUE_H */
diff --git a/include/evenless/sched/quota.h b/include/evenless/sched/quota.h
new file mode 100644
index 000000000000..601a6c9ccc67
--- /dev/null
+++ b/include/evenless/sched/quota.h
@@ -0,0 +1,75 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2013, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_SCHED_QUOTA_H
+#define _EVENLESS_SCHED_QUOTA_H
+
+#ifndef _EVENLESS_SCHED_H
+#error "please don't include evenless/sched/quota.h directly"
+#endif
+
+#ifdef CONFIG_EVENLESS_SCHED_QUOTA
+
+#define EVL_QUOTA_MIN_PRIO	1
+#define EVL_QUOTA_MAX_PRIO	255
+#define EVL_QUOTA_NR_PRIO					\
+	(EVL_QUOTA_MAX_PRIO - EVL_QUOTA_MIN_PRIO + 1)
+
+extern struct evl_sched_class evl_sched_quota;
+
+struct evl_quota_group {
+	struct evl_rq *rq;
+	ktime_t quota;
+	ktime_t quota_peak;
+	ktime_t run_start;
+	ktime_t run_budget;
+	ktime_t run_credit;
+	struct list_head members;
+	struct list_head expired;
+	struct list_head next;
+	int nr_active;
+	int nr_threads;
+	int tgid;
+	int quota_percent;
+	int quota_peak_percent;
+};
+
+struct evl_sched_quota {
+	ktime_t period;
+	struct evl_timer refill_timer;
+	struct evl_timer limit_timer;
+	struct list_head groups;
+};
+
+static inline int evl_quota_init_thread(struct evl_thread *thread)
+{
+	thread->quota = NULL;
+	INIT_LIST_HEAD(&thread->quota_expired);
+
+	return 0;
+}
+
+int evl_quota_create_group(struct evl_quota_group *tg,
+			   struct evl_rq *rq,
+			   int *quota_sum_r);
+
+int evl_quota_destroy_group(struct evl_quota_group *tg,
+			    int force,
+			    int *quota_sum_r);
+
+void evl_quota_set_limit(struct evl_quota_group *tg,
+			 int quota_percent, int quota_peak_percent,
+			 int *quota_sum_r);
+
+struct evl_quota_group *
+evl_quota_find_group(struct evl_rq *rq, int tgid);
+
+int evl_quota_sum_all(struct evl_rq *rq);
+
+#endif /* !CONFIG_EVENLESS_SCHED_QUOTA */
+
+#endif /* !_EVENLESS_SCHED_QUOTA_H */
diff --git a/include/evenless/sched/rt.h b/include/evenless/sched/rt.h
new file mode 100644
index 000000000000..da5ed06fa256
--- /dev/null
+++ b/include/evenless/sched/rt.h
@@ -0,0 +1,117 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2008, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_SCHED_RT_H
+#define _EVENLESS_SCHED_RT_H
+
+#ifndef _EVENLESS_SCHED_H
+#error "please don't include evenless/sched/rt.h directly"
+#endif
+
+/*
+ * Global priority scale for the core scheduling class, available to
+ * SCHED_EVL members.
+ */
+#define EVL_CORE_MIN_PRIO  0
+#define EVL_CORE_MAX_PRIO  MAX_RT_PRIO
+#define EVL_CORE_NR_PRIO   (EVL_CORE_MAX_PRIO - EVL_CORE_MIN_PRIO + 1)
+
+/* Priority range for SCHED_FIFO. */
+#define EVL_FIFO_MIN_PRIO  1
+#define EVL_FIFO_MAX_PRIO  (MAX_USER_RT_PRIO - 1)
+
+#if EVL_CORE_NR_PRIO > EVL_CLASS_WEIGHT_FACTOR ||	\
+	EVL_CORE_NR_PRIO > EVL_MLQ_LEVELS
+#error "EVL_MLQ_LEVELS is too low"
+#endif
+
+extern struct evl_sched_class evl_sched_rt;
+
+static inline void __evl_requeue_rt_thread(struct evl_thread *thread)
+{
+	evl_add_schedq(&thread->rq->rt.runnable, thread);
+}
+
+static inline void __evl_enqueue_rt_thread(struct evl_thread *thread)
+{
+	evl_add_schedq_tail(&thread->rq->rt.runnable, thread);
+}
+
+static inline void __evl_dequeue_rt_thread(struct evl_thread *thread)
+{
+	evl_del_schedq(&thread->rq->rt.runnable, thread);
+}
+
+static inline
+int __evl_chk_rt_schedparam(struct evl_thread *thread,
+			    const union evl_sched_param *p)
+{
+	if (p->rt.prio < EVL_CORE_MIN_PRIO ||
+	    p->rt.prio > EVL_CORE_MAX_PRIO)
+		return -EINVAL;
+
+	return 0;
+}
+
+static inline
+bool __evl_set_rt_schedparam(struct evl_thread *thread,
+			     const union evl_sched_param *p)
+{
+	bool ret = evl_set_effective_thread_priority(thread, p->rt.prio);
+
+	if (!(thread->state & T_BOOST))
+		thread->state &= ~T_WEAK;
+
+	return ret;
+}
+
+static inline
+void __evl_get_rt_schedparam(struct evl_thread *thread,
+			     union evl_sched_param *p)
+{
+	p->rt.prio = thread->cprio;
+}
+
+static inline
+void __evl_track_rt_priority(struct evl_thread *thread,
+			     const union evl_sched_param *p)
+{
+	if (p)
+		thread->cprio = p->rt.prio; /* Force update. */
+	else {
+		thread->cprio = thread->bprio;
+		/* Leaving PI/PP, so neither boosted nor weak. */
+		thread->state &= ~T_WEAK;
+	}
+}
+
+static inline
+void __evl_ceil_rt_priority(struct evl_thread *thread, int prio)
+{
+	/*
+	 * The RT class supports the widest priority range from
+	 * EVL_CORE_MIN_PRIO to EVL_CORE_MAX_PRIO inclusive,
+	 * no need to cap the input value which is guaranteed to be in
+	 * the range [1..EVL_CORE_MAX_PRIO].
+	 */
+	thread->cprio = prio;
+}
+
+static inline
+void __evl_forget_rt_thread(struct evl_thread *thread)
+{
+}
+
+static inline
+int evl_init_rt_thread(struct evl_thread *thread)
+{
+	return 0;
+}
+
+struct evl_thread *evl_rt_pick(struct evl_rq *rq);
+
+#endif /* !_EVENLESS_SCHED_RT_H */
diff --git a/include/evenless/sched/weak.h b/include/evenless/sched/weak.h
new file mode 100644
index 000000000000..2a38e33927e4
--- /dev/null
+++ b/include/evenless/sched/weak.h
@@ -0,0 +1,35 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2013, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_SCHED_WEAK_H
+#define _EVENLESS_SCHED_WEAK_H
+
+#ifndef _EVENLESS_SCHED_H
+#error "please don't include evenless/sched/weak.h directly"
+#endif
+
+#define EVL_WEAK_MIN_PRIO  0
+#define EVL_WEAK_MAX_PRIO  99
+#define EVL_WEAK_NR_PRIO   (EVL_WEAK_MAX_PRIO - EVL_WEAK_MIN_PRIO + 1)
+
+#if EVL_WEAK_NR_PRIO > EVL_CLASS_WEIGHT_FACTOR ||	\
+	 EVL_WEAK_NR_PRIO > EVL_MLQ_LEVELS
+#error "WEAK class has too many priority levels"
+#endif
+
+extern struct evl_sched_class evl_sched_weak;
+
+struct evl_sched_weak {
+	evl_schedqueue_t runnable;	/*!< Runnable thread queue. */
+};
+
+static inline int evl_weak_init_thread(struct evl_thread *thread)
+{
+	return 0;
+}
+
+#endif /* !_EVENLESS_SCHED_WEAK_H */
diff --git a/include/evenless/stat.h b/include/evenless/stat.h
new file mode 100644
index 000000000000..6016242a6b43
--- /dev/null
+++ b/include/evenless/stat.h
@@ -0,0 +1,139 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2006 Jan Kiszka <jan.kiszka@web.de>.
+ * Copyright (C) 2006 Dmitry Adamushko <dmitry.adamushko@gmail.com>
+ * Copyright (C) 2008, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_STAT_H
+#define _EVENLESS_STAT_H
+
+#include <evenless/clock.h>
+
+struct evl_rq;
+
+#ifdef CONFIG_EVENLESS_STATS
+
+struct evl_account {
+	ktime_t start;   /* Start of execution time accumulation */
+	ktime_t total; /* Accumulated execution time */
+};
+
+/*
+ * Return current date which can be passed to other accounting
+ * services for immediate accounting.
+ */
+static inline ktime_t evl_get_timestamp(void)
+{
+	return evl_read_clock(&evl_mono_clock);
+}
+
+static inline ktime_t evl_get_account_total(struct evl_account *account)
+{
+	return account->total;
+}
+
+/*
+ * Reset statistics from inside the accounted entity (e.g. after CPU
+ * migration).
+ */
+static inline void evl_reset_account(struct evl_account *account)
+{
+	account->total = 0;
+	account->start = evl_get_timestamp();
+}
+
+/*
+ * Accumulate the time spent for the current account until now.
+ * CAUTION: all changes must be committed before changing the
+ * current_account reference in rq.
+ */
+#define evl_update_account(__rq)				\
+	do {							\
+		ktime_t __now = evl_get_timestamp();		\
+		(__rq)->current_account->total +=		\
+			__now - (__rq)->last_account_switch;	\
+		(__rq)->last_account_switch = __now;		\
+		smp_wmb();					\
+	} while (0)
+
+/* Obtain last account switch date of considered runqueue */
+#define evl_get_last_account_switch(__rq)	((__rq)->last_account_switch)
+
+/*
+ * Update the current account reference, returning the previous one.
+ */
+#define evl_set_current_account(__rq, __new_account)			\
+	({								\
+		struct evl_account *__prev;				\
+		__prev = (struct evl_account *)				\
+			atomic_long_xchg(&(__rq)->current_account,	\
+					 (long)(__new_account));	\
+		__prev;							\
+	})
+
+/*
+ * Finalize an account (no need to accumulate the exectime, just mark
+ * the switch date and set the new account).
+ */
+#define evl_close_account(__rq, __new_account)			\
+	do {							\
+		(__rq)->last_account_switch =			\
+			evl_read_coreclk_monotonic();		\
+		(__rq)->current_account = (__new_account);	\
+	} while (0)
+
+	struct evl_counter {
+		unsigned long counter;
+	};
+
+static inline unsigned long evl_inc_counter(struct evl_counter *c)
+{
+	return c->counter++;
+}
+
+static inline unsigned long evl_get_counter(struct evl_counter *c)
+{
+	return c->counter;
+}
+
+static inline void evl_set_counter(struct evl_counter *c, unsigned long value)
+{
+	c->counter = value;
+}
+
+#else /* !CONFIG_EVENLESS_STATS */
+
+struct evl_account {
+};
+
+#define evl_get_timestamp()				({ 0; })
+#define evl_get_account_total(__account)		({ 0; })
+#define evl_reset_account(__account)			do { } while (0)
+#define evl_update_account(__rq)			do { } while (0)
+#define evl_set_current_account(__rq, __new_account)	({ (void)__rq; NULL; })
+#define evl_close_account(__rq, __new_account)	do { } while (0)
+#define evl_get_last_account_switch(__rq)		({ 0; })
+
+struct evl_counter {
+};
+
+#define evl_inc_counter(__c) 	({ do { } while(0); 0; })
+#define evl_get_counter(__c) 	({ 0; })
+#define evl_set_counter(_c, __value)	do { } while (0)
+
+#endif /* CONFIG_EVENLESS_STATS */
+
+/*
+ * Account the exectime of the current account until now, switch to
+ * new_account, return the previous one.
+ */
+#define evl_switch_account(__rq, __new_account)			\
+	({							\
+		evl_update_account(__rq);			\
+		evl_set_current_account(__rq, __new_account);	\
+	})
+
+#endif /* !_EVENLESS_STAT_H */
diff --git a/include/evenless/synch.h b/include/evenless/synch.h
new file mode 100644
index 000000000000..45255018615f
--- /dev/null
+++ b/include/evenless/synch.h
@@ -0,0 +1,116 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2001, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_SYN_H
+#define _EVENLESS_SYN_H
+
+#include <linux/types.h>
+#include <evenless/list.h>
+#include <evenless/assert.h>
+#include <evenless/timer.h>
+#include <uapi/evenless/synch.h>
+#include <uapi/evenless/thread.h>
+
+#define EVL_SYN_CLAIMED  0x100	/* Claimed by other thread(s) (PI) */
+#define EVL_SYN_CEILING  0x200	/* Actively boosting (PP) */
+
+struct evl_thread;
+struct evl_syn;
+struct evl_clock;
+
+struct evl_syn {
+	/* wait (weighted) prio in thread->boosters */
+	int wprio;
+	/* thread->boosters */
+	struct list_head next;
+	/*
+	 *  &variable holding the current priority ceiling value
+	 *  (evl_sched_rt-based, [1..255], EVL_SYN_PP).
+	 */
+	u32 *ceiling_ref;
+	/* Status word */
+	unsigned long status;
+	/* Waiting threads */
+	struct list_head wait_list;
+	/* Thread which owns the resource */
+	struct evl_thread *owner;
+	/* Pointer to fast lock word */
+	atomic_t *fastlock;
+	/* Reference clock. */
+	struct evl_clock *clock;
+};
+
+#define EVL_SYN_INITIALIZER(__name, __type)				\
+	{								\
+		.status = __type,					\
+			.wprio = -1,					\
+			.owner = NULL,					\
+			.fastlock = NULL,				\
+			.ceiling_ref = NULL,				\
+			.wait_list = LIST_HEAD_INIT((__name).wait_list), \
+			.next = LIST_HEAD_INIT((__name).next),		\
+			}
+
+#define evl_for_each_syn_waiter(__pos, __synch)				\
+	list_for_each_entry(__pos, &(__synch)->wait_list, syn_next)
+
+#define evl_for_each_syn_waiter_safe(__pos, __tmp, __synch)		\
+	list_for_each_entry_safe(__pos, __tmp, &(__synch)->wait_list, syn_next)
+
+static inline int evl_syn_has_waiter(struct evl_syn *synch)
+{
+	return !list_empty(&synch->wait_list);
+}
+
+struct evl_thread *evl_syn_wait_head(struct evl_syn *synch);
+
+#ifdef CONFIG_EVENLESS_DEBUG_MONITOR_INBAND
+void evl_detect_boost_drop(struct evl_thread *owner);
+#else
+static inline
+void evl_detect_boost_drop(struct evl_thread *owner) { }
+#endif
+
+void evl_init_syn(struct evl_syn *synch, int flags,
+		  struct evl_clock *clock,
+		  atomic_t *fastlock);
+
+void evl_init_syn_protect(struct evl_syn *synch,
+			  struct evl_clock *clock,
+			  atomic_t *fastlock, u32 *ceiling_ref);
+
+bool evl_destroy_syn(struct evl_syn *synch);
+
+int __must_check evl_sleep_on_syn(struct evl_syn *synch,
+				  ktime_t timeout,
+				  enum evl_tmode timeout_mode);
+
+struct evl_thread *evl_wake_up_syn(struct evl_syn *synch);
+
+int evl_wake_up_nr_syn(struct evl_syn *synch, int nr);
+
+void evl_wake_up_targeted_syn(struct evl_syn *synch,
+			      struct evl_thread *waiter);
+
+int __must_check evl_acquire_syn(struct evl_syn *synch,
+				 ktime_t timeout,
+				 enum evl_tmode timeout_mode);
+
+int __must_check evl_try_acquire_syn(struct evl_syn *synch);
+
+bool evl_release_syn(struct evl_syn *synch, struct evl_thread *thread);
+
+bool evl_flush_syn(struct evl_syn *synch, int reason);
+
+void evl_requeue_syn_waiter(struct evl_thread *thread);
+
+void evl_forget_syn_waiter(struct evl_thread *thread);
+
+void evl_commit_syn_ceiling(struct evl_syn *synch,
+			    struct evl_thread *curr);
+
+#endif /* !_EVENLESS_SYN_H_ */
diff --git a/include/evenless/thread.h b/include/evenless/thread.h
new file mode 100644
index 000000000000..34a32fc629e0
--- /dev/null
+++ b/include/evenless/thread.h
@@ -0,0 +1,350 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2001, 2018 Philippe Gerum  <rpm@xenomai.org>
+ * Copyright Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>.
+ */
+
+#ifndef _EVENLESS_THREAD_H
+#define _EVENLESS_THREAD_H
+
+#include <linux/types.h>
+#include <linux/wait.h>
+#include <linux/mutex.h>
+#include <linux/sched.h>
+#include <linux/dovetail.h>
+#include <linux/sched/rt.h>
+#include <linux/completion.h>
+#include <linux/irq_work.h>
+#include <evenless/list.h>
+#include <evenless/stat.h>
+#include <evenless/timer.h>
+#include <evenless/sched/param.h>
+#include <evenless/synch.h>
+#include <evenless/factory.h>
+#include <uapi/evenless/thread.h>
+#include <uapi/evenless/signal.h>
+#include <uapi/evenless/sched.h>
+#include <asm/evenless/thread.h>
+
+#define EVL_THREAD_BLOCK_BITS   (T_SUSP|T_PEND|T_DELAY|T_DORMANT|T_INBAND|T_HALT)
+#define EVL_THREAD_MODE_BITS    (T_RRB|T_WARN)
+
+struct evl_thread;
+struct evl_rq;
+struct evl_sched_class;
+struct evl_poll_watchpoint;
+
+struct evl_init_thread_attr {
+	struct cpumask affinity;
+	int flags;
+	struct evl_sched_class *sched_class;
+	union evl_sched_param sched_param;
+};
+
+struct evl_thread {
+	struct evl_element element;
+
+	__u32 state;		/* Thread state flags */
+	__u32 info;		/* Thread information flags */
+	__u32 local_info;	/* Local thread information flags */
+	struct dovetail_altsched_context altsched;
+
+	struct evl_rq *rq;		/* Run queue */
+	struct evl_sched_class *sched_class; /* Current scheduling class */
+	struct evl_sched_class *base_class; /* Base scheduling class */
+
+#ifdef CONFIG_EVENLESS_SCHED_QUOTA
+	struct evl_quota_group *quota; /* Quota scheduling group. */
+	struct list_head quota_expired;
+	struct list_head quota_next;
+#endif
+	struct cpumask affinity;	/* Processor affinity. */
+
+	/* Base priority (before PI/PP boost) */
+	int bprio;
+
+	/* Current (effective) priority */
+	int cprio;
+
+	/*
+	 * Weighted priority (cprio + scheduling class weight).
+	 */
+	int wprio;
+
+	int lock_count;	/* Scheduler lock count. */
+
+	struct list_head rq_next;	/* evl_rq->policy.runqueue */
+
+	struct list_head syn_next;	/* evl_syn->wait_list */
+
+	struct list_head next;	/* evl_thread_list */
+
+	/*
+	 * List of evl_syn owned by this thread causing a priority
+	 * boost due to one of the following reasons:
+	 *
+	 * - they are currently claimed by other thread(s) when
+	 * enforcing the priority inheritance protocol (EVL_SYN_PI).
+	 *
+	 * - they require immediate priority ceiling (EVL_SYN_PP).
+	 *
+	 * This list is ordered by decreasing (weighted) thread
+	 * priorities.
+	 */
+	struct list_head boosters;
+
+	struct evl_syn *wchan;		/* Resource the thread pends on */
+
+	struct evl_syn *wwake;		/* Wait channel the thread was resumed from */
+
+	int res_count;			/* Held resources count */
+
+	struct evl_timer rtimer;		/* Resource timer */
+
+	struct evl_timer ptimer;		/* Periodic timer */
+
+	ktime_t rrperiod;		/* Allotted round-robin period (ns) */
+
+	void *wait_data;		/* Active wait data. */
+
+	struct {
+		struct evl_poll_watchpoint *table;
+		unsigned int generation;
+		int nr;
+	} poll_context;
+
+	struct {
+		struct evl_counter isw;	/* in-band switches */
+		struct evl_counter csw;	/* context switches */
+		struct evl_counter sc;	/* OOB syscalls */
+		struct evl_counter pf;	/* Number of page faults */
+		struct evl_account account; /* Execution time accounting entity */
+		struct evl_account lastperiod; /* Interval marker for execution time reports */
+	} stat;
+
+	char *name;
+
+	/*
+	 * Thread data visible from userland through a window on the
+	 * global heap.
+	 */
+	struct evl_user_window *u_window;
+
+	struct completion exited;
+};
+
+struct evl_kthread {
+	struct evl_thread thread;
+	struct completion done;
+	void (*threadfn)(struct evl_kthread *kthread);
+	int status;
+	struct irq_work irq_work;
+};
+
+#define for_each_evl_booster(__pos, __thread)			\
+	list_for_each_entry(__pos, &(__thread)->boosters, next)
+
+#define for_each_evl_booster_safe(__pos, __tmp, __thread)	\
+	list_for_each_entry_safe(__pos, __tmp, &(__thread)->boosters, next)
+
+static inline void evl_sync_uwindow(struct evl_thread *curr)
+{
+	if (curr->u_window) {
+		curr->u_window->state = curr->state;
+		curr->u_window->info = curr->info;
+	}
+}
+
+static inline
+void evl_clear_sync_uwindow(struct evl_thread *curr, int state_bits)
+{
+	if (curr->u_window) {
+		curr->u_window->state = curr->state & ~state_bits;
+		curr->u_window->info = curr->info;
+	}
+}
+
+static inline
+void evl_set_sync_uwindow(struct evl_thread *curr, int state_bits)
+{
+	if (curr->u_window) {
+		curr->u_window->state = curr->state | state_bits;
+		curr->u_window->info = curr->info;
+	}
+}
+
+void __evl_test_cancel(struct evl_thread *curr);
+
+void evl_discard_thread(struct evl_thread *thread);
+
+static inline struct evl_thread *evl_current_thread(void)
+{
+	return dovetail_current_state()->thread;
+}
+
+static inline
+struct evl_rq *evl_thread_rq(struct evl_thread *thread)
+{
+	return thread->rq;
+}
+
+static inline struct evl_rq *evl_current_thread_rq(void)
+{
+	return evl_thread_rq(evl_current_thread());
+}
+
+static inline
+struct evl_thread *evl_thread_from_task(struct task_struct *p)
+{
+	return dovetail_task_state(p)->thread;
+}
+
+static inline void evl_test_cancel(void)
+{
+	struct evl_thread *curr = evl_current_thread();
+
+	if (curr && (curr->info & T_CANCELD))
+		__evl_test_cancel(curr);
+}
+
+ktime_t evl_get_thread_timeout(struct evl_thread *thread);
+
+ktime_t evl_get_thread_period(struct evl_thread *thread);
+
+int evl_init_thread(struct evl_thread *thread,
+		    const struct evl_init_thread_attr *attr,
+		    struct evl_rq *rq,
+		    const char *fmt, ...);
+
+void evl_start_thread(struct evl_thread *thread);
+
+void evl_suspend_thread(struct evl_thread *thread, int mask,
+			ktime_t timeout, enum evl_tmode timeout_mode,
+			struct evl_clock *clock,
+			struct evl_syn *wchan);
+
+void evl_resume_thread(struct evl_thread *thread,
+		       int mask);
+
+int evl_unblock_thread(struct evl_thread *thread);
+
+void evl_stop_thread(struct evl_thread *thread,
+		     int mask);
+
+ktime_t evl_delay_thread(ktime_t timeout,
+			 enum evl_tmode timeout_mode,
+			 struct evl_clock *clock);
+
+int evl_sleep_until(ktime_t timeout);
+
+int evl_sleep(ktime_t delay);
+
+int evl_set_thread_period(struct evl_clock *clock,
+			  ktime_t idate, /* abs */
+			  ktime_t period);
+
+int evl_wait_thread_period(unsigned long *overruns_r);
+
+void evl_cancel_thread(struct evl_thread *thread);
+
+int evl_join_thread(struct evl_thread *thread, bool uninterruptible);
+
+int evl_switch_oob(void);
+
+void evl_switch_inband(int cause);
+
+int evl_detach_self(void);
+
+void __evl_kick_thread(struct evl_thread *thread);
+
+void evl_kick_thread(struct evl_thread *thread);
+
+void __evl_demote_thread(struct evl_thread *thread);
+
+void evl_demote_thread(struct evl_thread *thread);
+
+void evl_signal_thread(struct evl_thread *thread,
+		       int sig, int arg);
+
+void evl_call_mayday(struct evl_thread *thread, int reason);
+
+#ifdef CONFIG_SMP
+void evl_migrate_thread(struct evl_thread *thread,
+			struct evl_rq *rq);
+#else
+static inline void evl_migrate_thread(struct evl_thread *thread,
+				      struct evl_rq *rq)
+{ }
+#endif
+
+int __evl_set_thread_schedparam(struct evl_thread *thread,
+				struct evl_sched_class *sched_class,
+				const union evl_sched_param *sched_param);
+
+int evl_set_thread_schedparam(struct evl_thread *thread,
+			      struct evl_sched_class *sched_class,
+			      const union evl_sched_param *sched_param);
+
+int evl_killall(int mask);
+
+void __evl_propagate_schedparam_change(struct evl_thread *curr);
+
+static inline void evl_propagate_schedparam_change(struct evl_thread *curr)
+{
+	if (curr->info & T_SCHEDP)
+		__evl_propagate_schedparam_change(curr);
+}
+
+int __evl_run_kthread(struct evl_kthread *kthread);
+
+#define _evl_run_kthread(__kthread, __affinity, __fn, __priority,	\
+			 __fmt, __args...)				\
+	({								\
+		int __ret;						\
+		struct evl_init_thread_attr __iattr = {			\
+			.flags = 0,					\
+			.affinity = __affinity,				\
+			.sched_class = &evl_sched_rt,			\
+			.sched_param.rt.prio = __priority,		\
+		};							\
+		(__kthread)->threadfn = __fn;				\
+		(__kthread)->status = 0;				\
+		init_completion(&(__kthread)->done);			\
+		__ret = evl_init_thread(&(__kthread)->thread, &__iattr,	\
+					NULL, __fmt, ##__args);		\
+		if (!__ret)						\
+			__ret = __evl_run_kthread(__kthread);		\
+		__ret;							\
+	})
+
+#define evl_run_kthread(__kthread, __fn, __priority,			\
+			__fmt, __args...)				\
+	_evl_run_kthread(__kthread, CPU_MASK_ALL, __fn, __priority,	\
+			 __fmt, ##__args)
+
+#define evl_run_kthread_on_cpu(__kthread, __cpu, __fn, __priority,	\
+			       __fmt, __args...)			\
+	_evl_run_kthread(__kthread, *cpumask_of(__cpu), __fn, __priority, \
+			 __fmt, ##__args)
+
+static inline void evl_cancel_kthread(struct evl_kthread *kthread)
+{
+	evl_cancel_thread(&kthread->thread);
+	evl_join_thread(&kthread->thread, true);
+}
+
+static inline int evl_kthread_should_stop(void)
+{
+	return evl_current_thread()->info & T_CANCELD;
+}
+
+void evl_set_kthread_priority(struct evl_kthread *thread,
+			      int priority);
+
+int evl_unblock_kthread(struct evl_kthread *thread);
+
+pid_t evl_get_inband_pid(struct evl_thread *thread);
+
+#endif /* !_EVENLESS_THREAD_H */
diff --git a/include/evenless/tick.h b/include/evenless/tick.h
new file mode 100644
index 000000000000..c485ca2da9e6
--- /dev/null
+++ b/include/evenless/tick.h
@@ -0,0 +1,48 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2016 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_TICK_H
+#define _EVENLESS_TICK_H
+
+#include <linux/types.h>
+#include <linux/ktime.h>
+#include <evenless/clock.h>
+#include <uapi/evenless/types.h>
+
+struct evl_rq;
+
+static inline void evl_program_local_tick(struct evl_clock *clock)
+{
+	struct evl_clock *master = clock->master;
+
+	if (master->ops.program_local_shot)
+		master->ops.program_local_shot(master);
+}
+
+static inline void evl_program_remote_tick(struct evl_clock *clock,
+					   struct evl_rq *rq)
+{
+#ifdef CONFIG_SMP
+	struct evl_clock *master = clock->master;
+
+	if (master->ops.program_remote_shot)
+		master->ops.program_remote_shot(master, rq);
+#endif
+}
+
+int evl_enable_tick(void);
+
+void evl_disable_tick(void);
+
+void evl_notify_proxy_tick(struct evl_rq *this_rq);
+
+void evl_program_proxy_tick(struct evl_clock *clock);
+
+void evl_send_timer_ipi(struct evl_clock *clock,
+			struct evl_rq *rq);
+
+#endif /* !_EVENLESS_TICK_H */
diff --git a/include/evenless/timer.h b/include/evenless/timer.h
new file mode 100644
index 000000000000..87a3fd7167ad
--- /dev/null
+++ b/include/evenless/timer.h
@@ -0,0 +1,456 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2001, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_TIMER_H
+#define _EVENLESS_TIMER_H
+
+#include <linux/types.h>
+#include <linux/time.h>
+#include <linux/list.h>
+#include <linux/rbtree.h>
+#include <evenless/clock.h>
+#include <evenless/stat.h>
+#include <evenless/list.h>
+#include <evenless/assert.h>
+
+/*
+ * Basic assumption throughout the code: ktime_t is a 64bit signed
+ * scalar type holding an internal time unit, which means that:
+ *
+ * - we may compare two ktime_t values using basic relational operators
+ * - we may check for nullness by comparing to 0 directly
+ * - we must use ktime_to_ns()/ns_to_ktime() helpers for converting
+ *   to/from nanoseconds.
+ */
+#define EVL_INFINITE   0
+#define EVL_NONBLOCK   ((s64)((u64)1 << 63))
+
+static inline bool timeout_infinite(ktime_t kt)
+{
+	return kt == 0;
+}
+
+static inline bool timeout_nonblock(ktime_t kt)
+{
+	return kt < 0;
+}
+
+static inline bool timeout_valid(ktime_t kt)
+{
+	return kt > 0;
+}
+
+/* Timer modes */
+enum evl_tmode {
+	EVL_REL,
+	EVL_ABS,
+};
+
+/* Timer status */
+#define EVL_TIMER_DEQUEUED  0x00000001
+#define EVL_TIMER_KILLED    0x00000002
+#define EVL_TIMER_PERIODIC  0x00000004
+#define EVL_TIMER_FIRED     0x00000010
+#define EVL_TIMER_RUNNING   0x00000020
+#define EVL_TIMER_KGRAVITY  0x00000040
+#define EVL_TIMER_UGRAVITY  0x00000080
+#define EVL_TIMER_IGRAVITY  0	     /* most conservative */
+
+#define EVL_TIMER_GRAVITY_MASK	(EVL_TIMER_KGRAVITY|EVL_TIMER_UGRAVITY)
+#define EVL_TIMER_INIT_MASK	EVL_TIMER_GRAVITY_MASK
+
+/* Timer priorities */
+#define EVL_TIMER_LOPRIO  (-999999999)
+#define EVL_TIMER_STDPRIO 0
+#define EVL_TIMER_HIPRIO  999999999
+
+struct evl_tnode {
+	struct rb_node rb;
+	ktime_t date;
+	int prio;
+};
+
+struct evl_tqueue {
+	struct rb_root root;
+	struct evl_tnode *head;
+};
+
+static inline void evl_init_tqueue(struct evl_tqueue *tq)
+{
+	tq->root = RB_ROOT;
+	tq->head = NULL;
+}
+
+#define evl_destroy_tqueue(__tq)	do { } while (0)
+
+static inline bool evl_tqueue_is_empty(struct evl_tqueue *tq)
+{
+	return tq->head == NULL;
+}
+
+static inline
+struct evl_tnode *evl_get_tqueue_head(struct evl_tqueue *tq)
+{
+	return tq->head;
+}
+
+static inline
+struct evl_tnode *evl_get_tqueue_next(struct evl_tqueue *tq,
+				      struct evl_tnode *node)
+{
+	struct rb_node *_node = rb_next(&node->rb);
+	return _node ? container_of(_node, struct evl_tnode, rb) : NULL;
+}
+
+static inline
+void evl_remove_tnode(struct evl_tqueue *tq, struct evl_tnode *node)
+{
+	if (node == tq->head)
+		tq->head = evl_get_tqueue_next(tq, node);
+
+	rb_erase(&node->rb, &tq->root);
+}
+
+#define for_each_evl_tnode(__node, __tq)			\
+	for ((__node) = evl_get_tqueue_head(__tq); (__node);	\
+	     (__node) = evl_get_tqueue_next(__tq, __node))
+
+void evl_insert_tnode(struct evl_tqueue *tq,
+		      struct evl_tnode *node);
+
+struct evl_rq;
+
+struct evl_timerbase {
+	hard_spinlock_t lock;
+	struct evl_tqueue q;
+};
+
+static inline struct evl_timerbase *
+evl_percpu_timers(struct evl_clock *clock, int cpu)
+{
+	return per_cpu_ptr(clock->timerdata, cpu);
+}
+
+static inline struct evl_timerbase *
+evl_this_cpu_timers(struct evl_clock *clock)
+{
+	return raw_cpu_ptr(clock->timerdata);
+}
+
+struct evl_timer {
+	struct evl_clock *clock;
+	/* Link in timers list. */
+	struct evl_tnode node;
+	struct list_head adjlink;
+	/* Timer status. */
+	int status;
+	/* Periodic interval (clock ticks, 0 == one shot). */
+	ktime_t interval;
+	/* First tick date in periodic mode. */
+	ktime_t start_date;
+	/* Position of next periodic release point. */
+	u64 pexpect_ticks;
+	/* Count of timer ticks in periodic mode. */
+	u64 periodic_ticks;
+#ifdef CONFIG_SMP
+	/* Runqueue the timer is affine to. */
+	struct evl_rq *rq;
+#endif
+	/* Per-cpu base. */
+	struct evl_timerbase *base;
+	/* Timeout handler. */
+	void (*handler)(struct evl_timer *timer);
+#ifdef CONFIG_EVENLESS_STATS
+	/* Timer name to be displayed. */
+	const char *name;
+	/* Timer holder in timebase. */
+	struct list_head next_stat;
+	/* Number of timer schedules. */
+	struct evl_counter scheduled;
+	/* Number of timer events. */
+	struct evl_counter fired;
+#endif /* CONFIG_EVENLESS_STATS */
+};
+
+#define evl_tdate(__timer)	((__timer)->node.date)
+
+void evl_start_timer(struct evl_timer *timer,
+		     ktime_t value,
+		     ktime_t interval);
+
+void __evl_stop_timer(struct evl_timer *timer);
+
+static inline int evl_timer_is_running(struct evl_timer *timer)
+{
+	return (timer->status & EVL_TIMER_RUNNING) != 0;
+}
+
+static inline int evl_timer_is_periodic(struct evl_timer *timer)
+{
+	return (timer->status & EVL_TIMER_PERIODIC) != 0;
+}
+
+static inline void evl_stop_timer(struct evl_timer *timer)
+{
+	if (evl_timer_is_running(timer))
+		__evl_stop_timer(timer);
+}
+
+void evl_destroy_timer(struct evl_timer *timer);
+
+static inline ktime_t evl_abs_timeout(struct evl_timer *timer,
+				      ktime_t delta)
+{
+	return ktime_add(evl_read_clock(timer->clock), delta);
+}
+
+#ifdef CONFIG_SMP
+static inline struct evl_rq *evl_get_timer_rq(struct evl_timer *timer)
+{
+	return timer->rq;
+}
+#else /* !CONFIG_SMP */
+#define evl_get_timer_rq(t)	this_evl_rq()
+#endif /* !CONFIG_SMP */
+
+/*
+ * timer base locked so that ->clock does not change under our
+ * feet.
+ */
+static inline unsigned long evl_get_timer_gravity(struct evl_timer *timer)
+{
+	struct evl_clock *clock = timer->clock;
+
+	if (timer->status & EVL_TIMER_KGRAVITY)
+		return clock->gravity.kernel;
+
+	if (timer->status & EVL_TIMER_UGRAVITY)
+		return clock->gravity.user;
+
+	return clock->gravity.irq;
+}
+
+/* timer base locked. */
+static inline void evl_update_timer_date(struct evl_timer *timer)
+{
+	evl_tdate(timer) = ktime_add_ns(timer->start_date,
+					(timer->periodic_ticks * ktime_to_ns(timer->interval))
+					- evl_get_timer_gravity(timer));
+}
+
+static inline
+ktime_t evl_get_timer_next_date(struct evl_timer *timer)
+{
+	return ktime_add_ns(timer->start_date,
+			    timer->pexpect_ticks * ktime_to_ns(timer->interval));
+}
+
+static inline
+void evl_set_timer_priority(struct evl_timer *timer, int prio)
+{
+	timer->node.prio = prio;
+}
+
+void __evl_init_timer(struct evl_timer *timer,
+		      struct evl_clock *clock,
+		      void (*handler)(struct evl_timer *timer),
+		      struct evl_rq *rq,
+		      int flags);
+
+void evl_set_timer_gravity(struct evl_timer *timer,
+			   int gravity);
+
+#ifdef CONFIG_EVENLESS_STATS
+
+#define evl_init_timer(__timer, __clock, __handler, __rq, __flags)	\
+	do {								\
+		__evl_init_timer(__timer, __clock, __handler, __rq, __flags); \
+		evl_set_timer_name(__timer, #__handler);		\
+	} while (0)
+
+static inline
+void evl_reset_timer_stats(struct evl_timer *timer)
+{
+	evl_set_counter(&timer->scheduled, 0);
+	evl_set_counter(&timer->fired, 0);
+}
+
+static inline
+void evl_account_timer_scheduled(struct evl_timer *timer)
+{
+	evl_inc_counter(&timer->scheduled);
+}
+
+static inline
+void evl_account_timer_fired(struct evl_timer *timer)
+{
+	evl_inc_counter(&timer->fired);
+}
+
+static inline
+void evl_set_timer_name(struct evl_timer *timer, const char *name)
+{
+	timer->name = name;
+}
+
+#else /* !CONFIG_EVENLESS_STATS */
+
+#define evl_init_timer	__evl_init_timer
+
+static inline
+void evl_reset_timer_stats(struct evl_timer *timer) { }
+
+static inline
+void evl_account_timer_scheduled(struct evl_timer *timer) { }
+
+static inline
+void evl_account_timer_fired(struct evl_timer *timer) { }
+
+static inline
+void evl_set_timer_name(struct evl_timer *timer, const char *name) { }
+
+#endif /* !CONFIG_EVENLESS_STATS */
+
+#define evl_init_core_timer(__timer, __handler)				\
+	evl_init_timer(__timer, &evl_mono_clock, __handler, NULL,	\
+		       EVL_TIMER_IGRAVITY)
+
+#define evl_init_timer_on_cpu(__timer, __cpu, __handler)		\
+	do {								\
+		struct evl_rq *__rq = evl_cpu_rq(__cpu);		\
+		evl_init_timer(__timer, &evl_mono_clock, __handler,	\
+			       __rq, EVL_TIMER_IGRAVITY);		\
+	} while (0)
+
+bool evl_timer_deactivate(struct evl_timer *timer);
+
+/* timer base locked. */
+static inline ktime_t evl_get_timer_expiry(struct evl_timer *timer)
+{
+	/* Ideal expiry date without anticipation (no gravity) */
+	return ktime_add(evl_tdate(timer),
+			 evl_get_timer_gravity(timer));
+}
+
+static inline
+void evl_move_timer_backward(struct evl_timer *timer, ktime_t delta)
+{
+	evl_tdate(timer) = ktime_sub(evl_tdate(timer), delta);
+}
+
+/* no lock required. */
+ktime_t evl_get_timer_date(struct evl_timer *timer);
+
+/* no lock required. */
+ktime_t __evl_get_timer_delta(struct evl_timer *timer);
+
+ktime_t xntimer_get_interval(struct evl_timer *timer);
+
+/* no lock required. */
+static inline ktime_t evl_get_timer_delta(struct evl_timer *timer)
+{
+	if (!evl_timer_is_running(timer))
+		return EVL_INFINITE;
+
+	return __evl_get_timer_delta(timer);
+}
+
+/* no lock required. */
+static inline
+ktime_t __evl_get_stopped_timer_delta(struct evl_timer *timer)
+{
+	return __evl_get_timer_delta(timer);
+}
+
+static inline
+ktime_t evl_get_stopped_timer_delta(struct evl_timer *timer)
+{
+	ktime_t t = __evl_get_stopped_timer_delta(timer);
+
+	if (ktime_to_ns(t) <= 1)
+		return EVL_INFINITE;
+
+	return t;
+}
+
+static inline void evl_dequeue_timer(struct evl_timer *timer,
+				     struct evl_tqueue *tq)
+{
+	evl_remove_tnode(tq, &timer->node);
+	timer->status |= EVL_TIMER_DEQUEUED;
+}
+
+/* timer base locked. */
+static inline
+void evl_enqueue_timer(struct evl_timer *timer,
+		       struct evl_tqueue *tq)
+{
+	evl_insert_tnode(tq, &timer->node);
+	timer->status &= ~EVL_TIMER_DEQUEUED;
+	evl_account_timer_scheduled(timer);
+}
+
+void evl_enqueue_timer(struct evl_timer *timer,
+		       struct evl_tqueue *tq);
+
+unsigned long evl_get_timer_overruns(struct evl_timer *timer);
+
+void evl_bolt_timer(struct evl_timer *timer,
+		    struct evl_clock *clock,
+		    struct evl_rq *rq);
+
+#ifdef CONFIG_SMP
+
+void __evl_set_timer_rq(struct evl_timer *timer,
+			struct evl_clock *clock,
+			struct evl_rq *rq);
+
+static inline void evl_set_timer_rq(struct evl_timer *timer,
+				    struct evl_rq *rq)
+{
+	if (rq != timer->rq)
+		__evl_set_timer_rq(timer, timer->clock, rq);
+}
+
+static inline void evl_prepare_timer_wait(struct evl_timer *timer,
+					  struct evl_clock *clock,
+					  struct evl_rq *rq)
+{
+	/* We may change the reference clock before waiting. */
+	if (rq != timer->rq || clock != timer->clock)
+		__evl_set_timer_rq(timer, clock, rq);
+}
+
+static inline bool evl_timer_on_rq(struct evl_timer *timer,
+				   struct evl_rq *rq)
+{
+	return timer->rq == rq;
+}
+
+#else /* ! CONFIG_SMP */
+
+static inline void evl_set_timer_rq(struct evl_timer *timer,
+				    struct evl_rq *rq)
+{ }
+
+static inline void evl_prepare_timer_wait(struct evl_timer *timer,
+					  struct evl_clock *clock,
+					  struct evl_rq *rq)
+{
+	if (clock != timer->clock)
+		evl_bolt_timer(timer, clock, rq);
+}
+
+static inline bool evl_timer_on_rq(struct evl_timer *timer,
+				   struct evl_rq *rq)
+{
+	return true;
+}
+
+#endif /* CONFIG_SMP */
+
+#endif /* !_EVENLESS_TIMER_H */
diff --git a/include/evenless/wait.h b/include/evenless/wait.h
new file mode 100644
index 000000000000..1fe8e5f8655d
--- /dev/null
+++ b/include/evenless/wait.h
@@ -0,0 +1,221 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2017 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_WAIT_H
+#define _EVENLESS_WAIT_H
+
+#include <evenless/synch.h>
+#include <evenless/sched.h>
+#include <evenless/poller.h>
+
+/*
+ * FIXME: general rework pending. Maybe merge that with synch.h. as
+ * evenless/wait.h.
+ */
+
+struct evl_wait_queue {
+	struct evl_syn wait;
+};
+
+#define EVL_WAIT_INITIALIZER(__name) {	\
+		.wait = EVL_SYN_INITIALIZER((__name).wait, EVL_SYN_PRIO), \
+	}
+
+#define DEFINE_EVL_WAIT(__name)	\
+	struct evl_wait_queue __name = EVL_WAIT_INITIALIZER(__name)
+
+#define DEFINE_EVL_WAIT_ONSTACK(__name)  DEFINE_EVL_WAIT(__name)
+
+static inline void evl_init_wait(struct evl_wait_queue *wq)
+{
+	*wq = (struct evl_wait_queue)EVL_WAIT_INITIALIZER(*wq);
+}
+
+static inline void evl_destroy_wait(struct evl_wait_queue *wq)
+{
+	evl_destroy_syn(&wq->wait);
+}
+
+static inline int evl_wait(struct evl_wait_queue *wq,
+			   ktime_t timeout, enum evl_tmode timeout_mode)
+{
+	int ret;
+
+	ret = evl_sleep_on_syn(&wq->wait, timeout, timeout_mode);
+	if (ret & T_BREAK)
+		return -EINTR;
+	if (ret & T_TIMEO)
+		return -ETIMEDOUT;
+	if (ret & T_RMID)
+		return -EIDRM;
+	return 0;
+}
+
+static inline
+int evl_wait_timeout(struct evl_wait_queue *wq,
+		     ktime_t timeout, enum evl_tmode timeout_mode)
+{
+	return evl_wait(wq, timeout, timeout_mode);
+}
+
+/*
+ * nklock held. Whine loudly if not atomic until we have eliminated
+ * the superlock.
+ */
+#define evl_wait_event(__wq, __cond)					\
+	({								\
+		int __ret = 0;						\
+		atomic_only();						\
+		while (__ret == 0 && !(__cond))				\
+			__ret = evl_wait(__wq,				\
+					 EVL_INFINITE, EVL_REL); \
+		__ret;							\
+	})
+
+/* nklock held. */
+#define evl_wait_event_timeout(__wq, __cond, __timeout, __mode)		\
+	({								\
+		int __ret = 0;						\
+		atomic_only();						\
+		while (__ret == 0 && !(__cond))				\
+			__ret = evl_wait_timeout(__wq, __timeout, __mode); \
+		__ret;							\
+	})
+
+/* nklock held. */
+static inline
+struct evl_thread *evl_wait_head(struct evl_wait_queue *wq)
+{
+	atomic_only();
+	return evl_syn_wait_head(&wq->wait);
+}
+
+static inline
+bool evl_wait_active(struct evl_wait_queue *wq)
+{
+	return evl_syn_has_waiter(&wq->wait);
+}
+
+#define evl_wake_up_nosched(__wq)				\
+	({							\
+		struct evl_thread *__waiter;			\
+		__waiter = evl_wake_up_syn(&(__wq)->wait);	\
+		__waiter != NULL;				\
+	})
+
+#define evl_wake_up(__wq)				\
+	({						\
+		bool __ret = evl_wake_up_nosched(__wq);	\
+		evl_schedule();				\
+		__ret;					\
+	})
+
+#define evl_wake_up_all_nosched(__wq)		\
+	evl_flush_syn(&(__wq)->wait, 0)
+
+#define evl_wake_up_all(__wq)					\
+	({							\
+		bool __ret = evl_wake_up_all_nosched(__wq);	\
+		evl_schedule();					\
+		__ret;						\
+	})
+
+#define evl_flush_wait_nosched(__wq)		\
+	evl_flush_syn(&(__wq)->wait, T_BREAK)
+
+#define evl_flush_wait(__wq)				\
+	({						\
+		__ret = evl_flush_wait_nosched(__wq);	\
+		evl_schedule();				\
+		__ret;					\
+	})
+
+/* Does not reschedule(), complete with evl_schedule(). */
+#define evl_wake_up_targeted(__wq, __waiter)			\
+	evl_wake_up_targeted_syn(&(__wq)->wait, __waiter)
+
+/* nklock held */
+#define evl_for_each_waiter(__pos, __wq)		\
+	evl_for_each_syn_waiter(__pos, &(__wq)->wait)
+
+/* nklock held */
+#define evl_for_each_waiter_safe(__pos, __tmp, __wq)			\
+	evl_for_each_syn_waiter_safe(__pos, __tmp, &(__wq)->wait)
+
+	struct evl_wait_flag {
+		struct evl_wait_queue wq;
+		struct evl_poll_head poll_head;
+		bool signaled;
+	};
+
+#define DEFINE_EVL_WAIT_FLAG(__name)				\
+	struct evl_wait_flag __name = {					\
+		.wq = EVL_WAIT_INITIALIZER((__name).wq),		\
+		.poll_head = EVL_POLLHEAD_INITIALIZER((__name).poll_head), \
+		.signaled = false,					\
+	}
+
+static inline void evl_init_flag(struct evl_wait_flag *wf)
+{
+	evl_init_wait(&wf->wq);
+	evl_init_poll_head(&wf->poll_head);
+	wf->signaled = false;
+}
+
+static inline void evl_destroy_flag(struct evl_wait_flag *wf)
+{
+	evl_destroy_wait(&wf->wq);
+}
+
+static inline int evl_wait_flag_timeout(struct evl_wait_flag *wf,
+					ktime_t timeout, enum evl_tmode timeout_mode)
+{
+	unsigned long flags;
+	int ret = 0;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (!wf->signaled)
+		ret = evl_wait_event_timeout(&wf->wq, wf->signaled,
+					     timeout, timeout_mode);
+	if (ret == 0) {
+		wf->signaled = false;
+		evl_clear_poll_events(&wf->poll_head, POLLIN|POLLRDNORM);
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+
+static inline int evl_wait_flag(struct evl_wait_flag *wf)
+{
+	return evl_wait_flag_timeout(wf, EVL_INFINITE, EVL_REL);
+}
+
+static inline
+struct evl_thread *evl_wait_flag_head(struct evl_wait_flag *wf)
+{
+	return evl_wait_head(&wf->wq);
+}
+
+static inline bool evl_raise_flag(struct evl_wait_flag *wf)
+{
+	unsigned long flags;
+	bool ret;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	wf->signaled = true;
+	evl_signal_poll_events(&wf->poll_head, POLLIN|POLLRDNORM);
+	ret = evl_wake_up(&wf->wq);
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+
+#endif /* _EVENLESS_WAIT_H */
diff --git a/include/evenless/xbuf.h b/include/evenless/xbuf.h
new file mode 100644
index 000000000000..0489594e237c
--- /dev/null
+++ b/include/evenless/xbuf.h
@@ -0,0 +1,28 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_XBUF_H
+#define _EVENLESS_XBUF_H
+
+#include <evenless/factory.h>
+
+struct evl_file;
+struct evl_xbuf;
+
+struct evl_xbuf *evl_get_xbuf(int efd,
+			      struct evl_file **sfilpp);
+
+void evl_put_xbuf(struct evl_file *sfilp);
+
+ssize_t evl_read_xbuf(struct evl_xbuf *xbuf,
+		      void *buf, size_t count,
+		      int f_flags);
+
+ssize_t evl_write_xbuf(struct evl_xbuf *xbuf,
+		       const void *buf, size_t count,
+		       int f_flags);
+
+#endif /* !_EVENLESS_XBUF_H */
diff --git a/include/trace/events/evenless.h b/include/trace/events/evenless.h
new file mode 100644
index 000000000000..c071e5854ad2
--- /dev/null
+++ b/include/trace/events/evenless.h
@@ -0,0 +1,867 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai's Cobalt core:
+ * Copyright (C) 2014 Jan Kiszka <jan.kiszka@siemens.com>.
+ * Copyright (C) 2014, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#if !defined(_TRACE_EVENLESS_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_EVENLESS_H
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM evenless
+
+#include <linux/mman.h>
+#include <linux/sched.h>
+#include <linux/math64.h>
+#include <linux/tracepoint.h>
+#include <linux/trace_seq.h>
+#include <evenless/timer.h>
+
+struct evl_rq;
+struct evl_thread;
+struct evl_sched_attrs;
+struct evl_init_thread_attr;
+struct evl_syn;
+struct evl_clock;
+
+DECLARE_EVENT_CLASS(thread_event,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread),
+
+	TP_STRUCT__entry(
+		__field(pid_t, pid)
+		__field(u32, state)
+		__field(u32, info)
+	),
+
+	TP_fast_assign(
+		__entry->state = thread->state;
+		__entry->info = thread->info;
+		__entry->pid = evl_get_inband_pid(thread);
+	),
+
+	TP_printk("pid=%d state=%#x info=%#x",
+		  __entry->pid, __entry->state, __entry->info)
+);
+
+DECLARE_EVENT_CLASS(curr_thread_event,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread),
+
+	TP_STRUCT__entry(
+		__field(struct evl_thread *, thread)
+		__field(u32, state)
+		__field(u32, info)
+	),
+
+	TP_fast_assign(
+		__entry->state = thread->state;
+		__entry->info = thread->info;
+	),
+
+	TP_printk("state=%#x info=%#x",
+		  __entry->state, __entry->info)
+);
+
+DECLARE_EVENT_CLASS(synch_wait_event,
+	TP_PROTO(struct evl_syn *synch),
+	TP_ARGS(synch),
+
+	TP_STRUCT__entry(
+		__field(struct evl_syn *, synch)
+	),
+
+	TP_fast_assign(
+		__entry->synch = synch;
+	),
+
+	TP_printk("synch=%p", __entry->synch)
+);
+
+DECLARE_EVENT_CLASS(synch_post_event,
+	TP_PROTO(struct evl_syn *synch),
+	TP_ARGS(synch),
+
+	TP_STRUCT__entry(
+		__field(struct evl_syn *, synch)
+	),
+
+	TP_fast_assign(
+		__entry->synch = synch;
+	),
+
+	TP_printk("synch=%p", __entry->synch)
+);
+
+DECLARE_EVENT_CLASS(timer_event,
+	TP_PROTO(struct evl_timer *timer),
+	TP_ARGS(timer),
+
+	TP_STRUCT__entry(
+		__field(struct evl_timer *, timer)
+	),
+
+	TP_fast_assign(
+		__entry->timer = timer;
+	),
+
+	TP_printk("timer=%p", __entry->timer)
+);
+
+#define evl_print_syscall(__nr)			\
+	__print_symbolic(__nr,			\
+			 { 0, "oob_read"  },	\
+			 { 1, "oob_write" },	\
+			 { 2, "oob_ioctl" })
+
+DECLARE_EVENT_CLASS(syscall_entry,
+	TP_PROTO(unsigned int nr),
+	TP_ARGS(nr),
+
+	TP_STRUCT__entry(
+		__field(unsigned int, nr)
+	),
+
+	TP_fast_assign(
+		__entry->nr = nr;
+	),
+
+	TP_printk("syscall=%s", evl_print_syscall(__entry->nr))
+);
+
+DECLARE_EVENT_CLASS(syscall_exit,
+	TP_PROTO(long result),
+	TP_ARGS(result),
+
+	TP_STRUCT__entry(
+		__field(long, result)
+	),
+
+	TP_fast_assign(
+		__entry->result = result;
+	),
+
+	TP_printk("result=%ld", __entry->result)
+);
+
+#define evl_print_sched_policy(__policy)			\
+	__print_symbolic(__policy,				\
+			 {SCHED_NORMAL, "normal"},		\
+			 {SCHED_FIFO, "fifo"},			\
+			 {SCHED_RR, "rr"},			\
+			 {SCHED_QUOTA, "quota"},		\
+			 {SCHED_EVL, "evenless"},		\
+			 {SCHED_WEAK, "weak"})
+
+const char *evl_trace_sched_attrs(struct trace_seq *seq,
+				  struct evl_sched_attrs *attrs);
+
+DECLARE_EVENT_CLASS(evl_sched_attrs,
+	TP_PROTO(struct evl_thread *thread,
+		 const struct evl_sched_attrs *attrs),
+	TP_ARGS(thread, attrs),
+
+	TP_STRUCT__entry(
+		__field(struct evl_thread *, thread)
+		__field(int, policy)
+		__dynamic_array(char, attrs, sizeof(struct evl_sched_attrs))
+	),
+
+	TP_fast_assign(
+		__entry->thread = thread;
+		__entry->policy = attrs->sched_policy;
+		memcpy(__get_dynamic_array(attrs), attrs, sizeof(*attrs));
+	),
+
+	TP_printk("thread=%s policy=%s param={ %s }",
+		  evl_element_name(&__entry->thread->element),
+		  evl_print_sched_policy(__entry->policy),
+		  evl_trace_sched_attrs(p,
+					(struct evl_sched_attrs *)
+					__get_dynamic_array(attrs))
+	)
+);
+
+DECLARE_EVENT_CLASS(evl_clock_timespec,
+	TP_PROTO(struct evl_clock *clock, const struct timespec *val),
+	TP_ARGS(clock, val),
+
+	TP_STRUCT__entry(
+		__field(struct evl_clock *, clock)
+		__timespec_fields(val)
+	),
+
+	TP_fast_assign(
+		__entry->clock = clock;
+		__assign_timespec(val, val);
+	),
+
+	TP_printk("clock=%s timeval=(%ld.%09ld)",
+		  __entry->clock->name,
+		  __timespec_args(val)
+	)
+);
+
+DECLARE_EVENT_CLASS(evl_clock_ident,
+	TP_PROTO(const char *name),
+	TP_ARGS(name),
+	TP_STRUCT__entry(
+		__string(name, name)
+	),
+	TP_fast_assign(
+		__assign_str(name, name);
+	),
+	TP_printk("name=%s", __get_str(name))
+);
+
+TRACE_EVENT(evl_schedule,
+	TP_PROTO(struct evl_rq *rq),
+	TP_ARGS(rq),
+
+	TP_STRUCT__entry(
+		__field(unsigned long, status)
+	),
+
+	TP_fast_assign(
+		__entry->status = rq->status;
+	),
+
+	TP_printk("status=%#lx", __entry->status)
+);
+
+TRACE_EVENT(evl_schedule_remote,
+	TP_PROTO(struct evl_rq *rq),
+	TP_ARGS(rq),
+
+	TP_STRUCT__entry(
+		__field(unsigned long, status)
+	),
+
+	TP_fast_assign(
+		__entry->status = rq->status;
+	),
+
+	TP_printk("status=%#lx", __entry->status)
+);
+
+TRACE_EVENT(evl_switch_context,
+	TP_PROTO(struct evl_thread *prev, struct evl_thread *next),
+	TP_ARGS(prev, next),
+
+	TP_STRUCT__entry(
+		__string(prev_name, prev->name)
+		__string(next_name, next->name)
+		__field(pid_t, prev_pid)
+		__field(int, prev_prio)
+		__field(u32, prev_state)
+		__field(pid_t, next_pid)
+		__field(int, next_prio)
+	),
+
+	TP_fast_assign(
+		__entry->prev_pid = evl_get_inband_pid(prev);
+		__entry->prev_prio = prev->cprio;
+		__entry->prev_state = prev->state;
+		__entry->next_pid = evl_get_inband_pid(next);
+		__entry->next_prio = next->cprio;
+		__assign_str(prev_name, prev->name);
+		__assign_str(next_name, next->name);
+	),
+
+	TP_printk("{ %s[%d] prio=%d, state=%#x } => { %s[%d] prio=%d }",
+		  __get_str(prev_name), __entry->prev_pid,
+		  __entry->prev_prio, __entry->prev_state,
+		  __get_str(next_name), __entry->next_pid, __entry->next_prio)
+);
+
+TRACE_EVENT(evl_init_thread,
+	TP_PROTO(struct evl_thread *thread,
+		 const struct evl_init_thread_attr *iattr,
+		 int status),
+	TP_ARGS(thread, iattr, status),
+
+	TP_STRUCT__entry(
+		__field(struct evl_thread *, thread)
+		__string(thread_name, thread->name)
+		__string(class_name, iattr->sched_class->name)
+		__field(unsigned long, flags)
+		__field(int, cprio)
+		__field(int, status)
+	),
+
+	TP_fast_assign(
+		__entry->thread = thread;
+		__assign_str(thread_name, thread->name);
+		__entry->flags = iattr->flags;
+		__assign_str(class_name, iattr->sched_class->name);
+		__entry->cprio = thread->cprio;
+		__entry->status = status;
+	),
+
+	TP_printk("thread=%p name=%s flags=%#lx class=%s prio=%d status=%#x",
+		   __entry->thread, __get_str(thread_name), __entry->flags,
+		  __get_str(class_name), __entry->cprio, __entry->status)
+);
+
+TRACE_EVENT(evl_suspend_thread,
+	TP_PROTO(struct evl_thread *thread, unsigned long mask, ktime_t timeout,
+		 enum evl_tmode timeout_mode, struct evl_clock *clock,
+		 struct evl_syn *wchan),
+	TP_ARGS(thread, mask, timeout, timeout_mode, clock, wchan),
+
+	TP_STRUCT__entry(
+		__field(pid_t, pid)
+		__field(unsigned long, mask)
+		__field(ktime_t, timeout)
+		__field(enum evl_tmode, timeout_mode)
+		__field(struct evl_syn *, wchan)
+		__string(clock_name, clock ? clock->name : "none")
+	),
+
+	TP_fast_assign(
+		__entry->pid = evl_get_inband_pid(thread);
+		__entry->mask = mask;
+		__entry->timeout = timeout;
+		__entry->timeout_mode = timeout_mode;
+		__entry->wchan = wchan;
+		__assign_str(clock_name, clock ? clock->name : "none");
+	),
+
+	TP_printk("pid=%d mask=%#lx timeout=%Lu timeout_mode=%d clock=%s wchan=%p",
+		  __entry->pid, __entry->mask,
+		  ktime_to_ns(__entry->timeout), __entry->timeout_mode,
+		  __get_str(clock_name),
+		  __entry->wchan)
+);
+
+TRACE_EVENT(evl_resume_thread,
+	TP_PROTO(struct evl_thread *thread, unsigned long mask),
+	TP_ARGS(thread, mask),
+
+	TP_STRUCT__entry(
+		__string(name, thread->name)
+		__field(pid_t, pid)
+		__field(unsigned long, mask)
+	),
+
+	TP_fast_assign(
+		__assign_str(name, thread->name);
+		__entry->pid = evl_get_inband_pid(thread);
+		__entry->mask = mask;
+	),
+
+	TP_printk("name=%s pid=%d mask=%#lx",
+		  __get_str(name), __entry->pid, __entry->mask)
+);
+
+TRACE_EVENT(evl_thread_fault,
+	TP_PROTO(int trapnr, struct pt_regs *regs),
+	TP_ARGS(trapnr, regs),
+
+	TP_STRUCT__entry(
+		__field(long,	ip)
+		__field(unsigned int, trapnr)
+	),
+
+	TP_fast_assign(
+		__entry->ip = instruction_pointer(regs);
+		__entry->trapnr = trapnr;
+	),
+
+	TP_printk("ip=%#lx trapnr=%#x",
+		  __entry->ip, __entry->trapnr)
+);
+
+TRACE_EVENT(evl_thread_set_current_prio,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread),
+
+	TP_STRUCT__entry(
+		__field(struct evl_thread *, thread)
+		__field(pid_t, pid)
+		__field(int, cprio)
+	),
+
+	TP_fast_assign(
+		__entry->thread = thread;
+		__entry->pid = evl_get_inband_pid(thread);
+		__entry->cprio = thread->cprio;
+	),
+
+	TP_printk("thread=%p pid=%d prio=%d",
+		  __entry->thread, __entry->pid, __entry->cprio)
+);
+
+DEFINE_EVENT(curr_thread_event, evl_thread_start,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread)
+);
+
+DEFINE_EVENT(thread_event, evl_thread_cancel,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread)
+);
+
+DEFINE_EVENT(thread_event, evl_thread_join,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread)
+);
+
+DEFINE_EVENT(thread_event, evl_unblock_thread,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread)
+);
+
+DEFINE_EVENT(curr_thread_event, evl_thread_wait_period,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread)
+);
+
+DEFINE_EVENT(curr_thread_event, evl_thread_missed_period,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread)
+);
+
+DEFINE_EVENT(curr_thread_event, evl_thread_set_mode,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread)
+);
+
+TRACE_EVENT(evl_thread_migrate,
+	TP_PROTO(struct evl_thread *thread, unsigned int cpu),
+	TP_ARGS(thread, cpu),
+
+	TP_STRUCT__entry(
+		__field(struct evl_thread *, thread)
+		__field(pid_t, pid)
+		__field(unsigned int, cpu)
+	),
+
+	TP_fast_assign(
+		__entry->thread = thread;
+		__entry->pid = evl_get_inband_pid(thread);
+		__entry->cpu = cpu;
+	),
+
+	TP_printk("thread=%p pid=%d cpu=%u",
+		  __entry->thread, __entry->pid, __entry->cpu)
+);
+
+DEFINE_EVENT(curr_thread_event, evl_watchdog_signal,
+	TP_PROTO(struct evl_thread *curr),
+	TP_ARGS(curr)
+);
+
+DEFINE_EVENT(curr_thread_event, evl_switching_oob,
+	TP_PROTO(struct evl_thread *curr),
+	TP_ARGS(curr)
+);
+
+DEFINE_EVENT(curr_thread_event, evl_switched_oob,
+	TP_PROTO(struct evl_thread *curr),
+	TP_ARGS(curr)
+);
+
+#define evl_print_switch_cause(cause)				\
+	__print_symbolic(cause,						\
+			 { SIGDEBUG_UNDEFINED,		"undefined" },	\
+			 { SIGDEBUG_MIGRATE_SIGNAL,	"signal" },	\
+			 { SIGDEBUG_MIGRATE_SYSCALL,	"syscall" },	\
+			 { SIGDEBUG_MIGRATE_FAULT,	"fault" })
+
+TRACE_EVENT(evl_switching_inband,
+	TP_PROTO(int cause),
+	TP_ARGS(cause),
+
+	TP_STRUCT__entry(
+		__field(int, cause)
+	),
+
+	TP_fast_assign(
+		__entry->cause = cause;
+	),
+
+	TP_printk("cause=%s", evl_print_switch_cause(__entry->cause))
+);
+
+DEFINE_EVENT(curr_thread_event, evl_switched_inband,
+	TP_PROTO(struct evl_thread *curr),
+	TP_ARGS(curr)
+);
+
+DEFINE_EVENT(curr_thread_event, evl_kthread_entry,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread)
+);
+
+TRACE_EVENT(evl_thread_map,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread),
+
+	TP_STRUCT__entry(
+		__field(struct evl_thread *, thread)
+		__field(pid_t, pid)
+		__field(int, prio)
+	),
+
+	TP_fast_assign(
+		__entry->thread = thread;
+		__entry->pid = evl_get_inband_pid(thread);
+		__entry->prio = thread->bprio;
+	),
+
+	TP_printk("thread=%p pid=%d prio=%d",
+		  __entry->thread, __entry->pid, __entry->prio)
+);
+
+DEFINE_EVENT(curr_thread_event, evl_thread_unmap,
+	TP_PROTO(struct evl_thread *thread),
+	TP_ARGS(thread)
+);
+
+TRACE_EVENT(evl_inband_request,
+        TP_PROTO(const char *type, struct task_struct *task),
+	TP_ARGS(type, task),
+
+	TP_STRUCT__entry(
+		__field(pid_t, pid)
+		__array(char, comm, TASK_COMM_LEN)
+		__field(const char *, type)
+	),
+
+	TP_fast_assign(
+		__entry->type = type;
+		__entry->pid = task_pid_nr(task);
+		memcpy(__entry->comm, task->comm, TASK_COMM_LEN);
+	),
+
+	TP_printk("request=%s pid=%d comm=%s",
+		  __entry->type, __entry->pid, __entry->comm)
+);
+
+TRACE_EVENT(evl_inband_wakeup,
+	TP_PROTO(struct task_struct *task),
+	TP_ARGS(task),
+
+	TP_STRUCT__entry(
+		__field(pid_t, pid)
+		__array(char, comm, TASK_COMM_LEN)
+	),
+
+	TP_fast_assign(
+		__entry->pid = task_pid_nr(task);
+		memcpy(__entry->comm, task->comm, TASK_COMM_LEN);
+	),
+
+	TP_printk("pid=%d comm=%s",
+		  __entry->pid, __entry->comm)
+);
+
+TRACE_EVENT(evl_inband_signal,
+	TP_PROTO(struct task_struct *task, int sig),
+	TP_ARGS(task, sig),
+
+	TP_STRUCT__entry(
+		__field(pid_t, pid)
+		__array(char, comm, TASK_COMM_LEN)
+		__field(int, sig)
+	),
+
+	TP_fast_assign(
+		__entry->pid = task_pid_nr(task);
+		__entry->sig = sig;
+		memcpy(__entry->comm, task->comm, TASK_COMM_LEN);
+	),
+
+	TP_printk("pid=%d comm=%s sig=%d",
+		  __entry->pid, __entry->comm, __entry->sig)
+);
+
+DEFINE_EVENT(timer_event, evl_timer_stop,
+	TP_PROTO(struct evl_timer *timer),
+	TP_ARGS(timer)
+);
+
+DEFINE_EVENT(timer_event, evl_timer_expire,
+	TP_PROTO(struct evl_timer *timer),
+	TP_ARGS(timer)
+);
+
+#define evl_print_timer_mode(mode)			\
+	__print_symbolic(mode,				\
+			 { EVENLESS_REL, "rel" },	\
+			 { EVENLESS_ABS, "abs" })
+
+TRACE_EVENT(evl_timer_start,
+	TP_PROTO(struct evl_timer *timer, ktime_t value, ktime_t interval),
+	TP_ARGS(timer, value, interval),
+
+	TP_STRUCT__entry(
+		__field(struct evl_timer *, timer)
+#ifdef CONFIG_EVENLESS_STATS
+		__string(name, timer->name)
+#endif
+		__field(ktime_t, value)
+		__field(ktime_t, interval)
+	),
+
+	TP_fast_assign(
+#ifdef CONFIG_EVENLESS_STATS
+		__assign_str(name, timer->name);
+#endif
+		__entry->value = value;
+		__entry->interval = interval;
+	),
+
+	TP_printk("timer=%s value=%Lu interval=%Lu",
+#ifdef CONFIG_EVENLESS_STATS
+		  __get_str(name),
+#else
+		  "?",
+#endif
+		  ktime_to_ns(__entry->value),
+		  ktime_to_ns(__entry->interval))
+);
+
+TRACE_EVENT(evl_timer_bolt,
+	TP_PROTO(struct evl_timer *timer,
+		 struct evl_clock *clock,
+		 unsigned int cpu),
+	    TP_ARGS(timer, clock, cpu),
+
+	TP_STRUCT__entry(
+		__field(struct evl_timer *, timer)
+		__field(struct evl_clock *, clock)
+		__field(unsigned int, cpu)
+	),
+
+	TP_fast_assign(
+		__entry->timer = timer;
+		__entry->clock = clock;
+		__entry->cpu = cpu;
+	),
+
+	TP_printk("timer=%p clock=%s, cpu=%u",
+		  __entry->timer, __entry->clock->name, __entry->cpu)
+);
+
+TRACE_EVENT(evl_timer_shot,
+	TP_PROTO(s64 delta),
+	TP_ARGS(delta),
+
+	TP_STRUCT__entry(
+		__field(u64, secs)
+		__field(u32, nsecs)
+		__field(s64, delta)
+	),
+
+	TP_fast_assign(
+		__entry->delta = delta;
+		__entry->secs = div_u64_rem(trace_clock_local() + delta,
+					    NSEC_PER_SEC, &__entry->nsecs);
+	),
+
+	TP_printk("tick at %Lu.%06u (delay: %Ld us)",
+		  (unsigned long long)__entry->secs,
+		  __entry->nsecs / 1000, div_s64(__entry->delta, 1000))
+);
+
+DEFINE_EVENT(synch_wait_event, evl_synch_sleepon,
+	TP_PROTO(struct evl_syn *synch),
+	TP_ARGS(synch)
+);
+
+DEFINE_EVENT(synch_wait_event, evl_synch_try_acquire,
+	TP_PROTO(struct evl_syn *synch),
+	TP_ARGS(synch)
+);
+
+DEFINE_EVENT(synch_wait_event, evl_synch_acquire,
+	TP_PROTO(struct evl_syn *synch),
+	TP_ARGS(synch)
+);
+
+DEFINE_EVENT(synch_post_event, evl_synch_release,
+	TP_PROTO(struct evl_syn *synch),
+	TP_ARGS(synch)
+);
+
+DEFINE_EVENT(synch_post_event, evl_synch_wakeup,
+	TP_PROTO(struct evl_syn *synch),
+	TP_ARGS(synch)
+);
+
+DEFINE_EVENT(synch_post_event, evl_synch_wakeup_many,
+	TP_PROTO(struct evl_syn *synch),
+	TP_ARGS(synch)
+);
+
+DEFINE_EVENT(synch_post_event, evl_synch_flush,
+	TP_PROTO(struct evl_syn *synch),
+	TP_ARGS(synch)
+);
+
+DEFINE_EVENT(synch_post_event, evl_synch_forget,
+	TP_PROTO(struct evl_syn *synch),
+	TP_ARGS(synch)
+);
+
+#define __timespec_fields(__name)				\
+	__field(__kernel_time_t, tv_sec_##__name)		\
+	__field(long, tv_nsec_##__name)
+
+#define __assign_timespec(__to, __from)				\
+	do {							\
+		__entry->tv_sec_##__to = (__from)->tv_sec;	\
+		__entry->tv_nsec_##__to = (__from)->tv_nsec;	\
+	} while (0)
+
+#define __timespec_args(__name)					\
+	__entry->tv_sec_##__name, __entry->tv_nsec_##__name
+
+DEFINE_EVENT(syscall_entry, evl_oob_sysentry,
+	TP_PROTO(unsigned int nr),
+	TP_ARGS(nr)
+);
+
+DEFINE_EVENT(syscall_exit, evl_oob_sysexit,
+	TP_PROTO(long result),
+	TP_ARGS(result)
+);
+
+DEFINE_EVENT(syscall_entry, evl_inband_sysentry,
+	TP_PROTO(unsigned int nr),
+	TP_ARGS(nr)
+);
+
+DEFINE_EVENT(syscall_exit, evl_inband_sysexit,
+	TP_PROTO(long result),
+	TP_ARGS(result)
+);
+
+DEFINE_EVENT(evl_sched_attrs, evl_thread_setsched,
+	TP_PROTO(struct evl_thread *thread,
+		 const struct evl_sched_attrs *attrs),
+	TP_ARGS(thread, attrs)
+);
+
+DEFINE_EVENT(evl_sched_attrs, evl_thread_getsched,
+	TP_PROTO(struct evl_thread *thread,
+		 const struct evl_sched_attrs *attrs),
+	TP_ARGS(thread, attrs)
+);
+
+#define evl_print_thread_mode(__mode)		\
+	__print_flags(__mode, "|",		\
+		      {T_WARN, "warnsw"})
+
+TRACE_EVENT(evl_thread_update_mode,
+	TP_PROTO(int mode, bool set),
+	TP_ARGS(mode, set),
+	TP_STRUCT__entry(
+		__field(int, mode)
+		__field(bool, set)
+	),
+	TP_fast_assign(
+		__entry->mode = mode;
+		__entry->set = set;
+	),
+	TP_printk("%s %#x(%s)",
+		  __entry->set ? "set" : "clear",
+		  __entry->mode, evl_print_thread_mode(__entry->mode))
+);
+
+DEFINE_EVENT(evl_clock_timespec, evl_clock_getres,
+	TP_PROTO(struct evl_clock *clock, const struct timespec *res),
+	TP_ARGS(clock, res)
+);
+
+DEFINE_EVENT(evl_clock_timespec, evl_clock_gettime,
+	TP_PROTO(struct evl_clock *clock, const struct timespec *time),
+	TP_ARGS(clock, time)
+);
+
+DEFINE_EVENT(evl_clock_timespec, evl_clock_settime,
+	TP_PROTO(struct evl_clock *clock, const struct timespec *time),
+	TP_ARGS(clock, time)
+);
+
+TRACE_EVENT(evl_clock_adjtime,
+	TP_PROTO(struct evl_clock *clock, struct timex *tx),
+	TP_ARGS(clock, tx),
+
+	TP_STRUCT__entry(
+		__field(struct evl_clock *, clock)
+		__field(struct timex *, tx)
+	),
+
+	TP_fast_assign(
+		__entry->clock = clock;
+		__entry->tx = tx;
+	),
+
+	TP_printk("clock=%s timex=%p",
+		  __entry->clock->name,
+		  __entry->tx
+	)
+);
+
+#define evl_print_timer_flags(__flags)			\
+	__print_flags(__flags, "|",			\
+		      {TIMER_ABSTIME, "TIMER_ABSTIME"})
+
+DEFINE_EVENT(evl_clock_ident, evl_register_clock,
+	TP_PROTO(const char *name),
+	TP_ARGS(name)
+);
+
+DEFINE_EVENT(evl_clock_ident, evl_unregister_clock,
+	TP_PROTO(const char *name),
+	TP_ARGS(name)
+);
+
+TRACE_EVENT(evl_trace,
+	TP_PROTO(const char *msg),
+	TP_ARGS(msg),
+	TP_STRUCT__entry(
+		__string(msg, msg)
+	),
+	TP_fast_assign(
+		__assign_str(msg, msg);
+	),
+	TP_printk("%s", __get_str(msg))
+);
+
+TRACE_EVENT(evl_latspot,
+	TP_PROTO(int latmax_ns),
+	TP_ARGS(latmax_ns),
+	TP_STRUCT__entry(
+		 __field(int, latmax_ns)
+	),
+	TP_fast_assign(
+		__entry->latmax_ns = latmax_ns;
+	),
+	TP_printk("** latency spot: %d.%.3d us **",
+		  __entry->latmax_ns / 1000,
+		  __entry->latmax_ns % 1000)
+);
+
+/* Basically evl_trace() + trigger point */
+TRACE_EVENT(evl_trigger,
+	TP_PROTO(const char *issuer),
+	TP_ARGS(issuer),
+	TP_STRUCT__entry(
+		__string(issuer, issuer)
+	),
+	TP_fast_assign(
+		__assign_str(issuer, issuer);
+	),
+	TP_printk("%s", __get_str(issuer))
+);
+
+#endif /* _TRACE_EVENLESS_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/uapi/evenless/clock.h b/include/uapi/evenless/clock.h
new file mode 100644
index 000000000000..7abf84b8ed0c
--- /dev/null
+++ b/include/uapi/evenless/clock.h
@@ -0,0 +1,23 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_CLOCK_H
+#define _EVENLESS_UAPI_CLOCK_H
+
+#define EVL_CLOCK_IOCBASE	'c'
+
+struct evl_clock_delayreq {
+	struct timespec timeout;
+	struct timespec *remain;
+};
+
+#define EVL_CLKIOC_DELAY	_IOWR(EVL_CLOCK_IOCBASE, 0, struct evl_clock_delayreq)
+#define EVL_CLKIOC_GET_RES	_IOR(EVL_CLOCK_IOCBASE, 1, struct timespec)
+#define EVL_CLKIOC_GET_TIME	_IOR(EVL_CLOCK_IOCBASE, 2, struct timespec)
+#define EVL_CLKIOC_SET_TIME	_IOR(EVL_CLOCK_IOCBASE, 3, struct timespec)
+#define EVL_CLKIOC_ADJ_TIME	_IOR(EVL_CLOCK_IOCBASE, 4, struct timex)
+
+#endif /* !_EVENLESS_UAPI_CLOCK_H */
diff --git a/include/uapi/evenless/control.h b/include/uapi/evenless/control.h
new file mode 100644
index 000000000000..9c00a4e5bcf1
--- /dev/null
+++ b/include/uapi/evenless/control.h
@@ -0,0 +1,23 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_CONTROL_H
+#define _EVENLESS_UAPI_CONTROL_H
+
+struct evl_core_info {
+	__u32 abi_level;
+	__u32 fpu_features;
+	__u64 shm_size;
+};
+
+#define EVL_CONTROL_IOCBASE	'C'
+
+#define EVL_CTLIOC_GET_COREINFO		_IOR(EVL_CONTROL_IOCBASE, 0, struct evl_core_info)
+#define EVL_CTLIOC_SWITCH_OOB		_IO(EVL_CONTROL_IOCBASE, 1)
+#define EVL_CTLIOC_SWITCH_INBAND	_IO(EVL_CONTROL_IOCBASE, 2)
+#define EVL_CTLIOC_DETACH_SELF		_IO(EVL_CONTROL_IOCBASE, 3)
+
+#endif /* !_EVENLESS_UAPI_CONTROL_H */
diff --git a/include/uapi/evenless/devices/hectic.h b/include/uapi/evenless/devices/hectic.h
new file mode 100644
index 000000000000..051029da1d6a
--- /dev/null
+++ b/include/uapi/evenless/devices/hectic.h
@@ -0,0 +1,39 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Derived from Xenomai Cobalt's switchtest driver
+ * Copyright (C) 2010 Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_DEVICES_HECTIC_H
+#define _EVENLESS_UAPI_DEVICES_HECTIC_H
+
+struct hectic_task_index {
+	unsigned int index;
+	unsigned int flags;
+};
+
+struct hectic_switch_req {
+	unsigned int from;
+	unsigned int to;
+};
+
+struct hectic_error {
+	struct hectic_switch_req last_switch;
+	unsigned int fp_val;
+};
+
+#define EVL_HECTIC_IOCBASE	'H'
+
+#define EVL_HECIOC_SET_TASKS_COUNT	_IOW(EVL_HECTIC_IOCBASE, 0, __u32)
+#define EVL_HECIOC_SET_CPU		_IOW(EVL_HECTIC_IOCBASE, 1, __u32)
+#define EVL_HECIOC_REGISTER_UTASK 	_IOW(EVL_HECTIC_IOCBASE, 2, struct hectic_task_index)
+#define EVL_HECIOC_CREATE_KTASK 	_IOWR(EVL_HECTIC_IOCBASE, 3, struct hectic_task_index)
+#define EVL_HECIOC_PEND 		_IOR(EVL_HECTIC_IOCBASE, 4, struct hectic_task_index)
+#define EVL_HECIOC_SWITCH_TO 		_IOR(EVL_HECTIC_IOCBASE, 5, struct hectic_switch_req)
+#define EVL_HECIOC_GET_SWITCHES_COUNT 	_IOR(EVL_HECTIC_IOCBASE, 6, __u32)
+#define EVL_HECIOC_GET_LAST_ERROR 	_IOR(EVL_HECTIC_IOCBASE, 7, struct hectic_error)
+#define EVL_HECIOC_SET_PAUSE 		_IOW(EVL_HECTIC_IOCBASE, 8, __u32)
+
+#endif /* !_EVENLESS_UAPI_DEVICES_HECTIC_H */
diff --git a/include/uapi/evenless/devices/latmus.h b/include/uapi/evenless/devices/latmus.h
new file mode 100644
index 000000000000..68718295b71d
--- /dev/null
+++ b/include/uapi/evenless/devices/latmus.h
@@ -0,0 +1,56 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_DEVICES_LATMUS_H
+#define _EVENLESS_UAPI_DEVICES_LATMUS_H
+
+/* Latmus context types. */
+#define EVL_LAT_IRQ   0
+#define EVL_LAT_KERN  1
+#define EVL_LAT_USER  2
+
+struct latmus_setup {
+	__u32 type;
+	__u64 period;
+	__s32 priority;
+	__u32 cpu;
+	union {
+		struct {
+			__u32 verbosity;
+		} tune;
+		struct {
+			__u32 xfd;
+			__u32 hcells;
+		} measure;
+	} u;
+};
+
+struct latmus_result {
+	__s32 *data;
+	__u32 len;
+};
+
+/*
+ * The measurement record which the driver sends to userland each
+ * second through an xbuf channel.
+ */
+struct latmus_measurement {
+	__u64 sum_lat;
+	__s32 min_lat;
+	__s32 max_lat;
+	__u32 overruns;
+	__u32 samples;
+};
+
+#define EVL_LATMUS_IOCBASE	'L'
+
+#define EVL_LATIOC_TUNE		_IOWR(EVL_LATMUS_IOCBASE, 0, struct latmus_setup)
+#define EVL_LATIOC_MEASURE	_IOWR(EVL_LATMUS_IOCBASE, 1, struct latmus_setup)
+#define EVL_LATIOC_RUN		_IOR(EVL_LATMUS_IOCBASE, 2, struct latmus_result)
+#define EVL_LATIOC_PULSE	_IOW(EVL_LATMUS_IOCBASE, 3, __u64)
+#define EVL_LATIOC_RESET	_IO(EVL_LATMUS_IOCBASE, 4)
+
+#endif /* !_EVENLESS_UAPI_DEVICES_LATMUS_H */
diff --git a/include/uapi/evenless/factory.h b/include/uapi/evenless/factory.h
new file mode 100644
index 000000000000..d7839aadf710
--- /dev/null
+++ b/include/uapi/evenless/factory.h
@@ -0,0 +1,26 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_FACTORY_H
+#define _EVENLESS_UAPI_FACTORY_H
+
+#define EVL_FACTORY_IOCBASE	'f'
+
+struct evl_element_ids {
+	__u32 minor;
+	__u32 fundle;
+	__u32 state_offset;
+};
+
+struct evl_clone_req {
+	const char *name;
+	void *attrs;
+	struct evl_element_ids eids;
+};
+
+#define EVL_IOC_CLONE	_IOWR(EVL_FACTORY_IOCBASE, 0, struct evl_clone_req)
+
+#endif /* !_EVENLESS_UAPI_FACTORY_H */
diff --git a/include/uapi/evenless/logger.h b/include/uapi/evenless/logger.h
new file mode 100644
index 000000000000..3f23cebc27bf
--- /dev/null
+++ b/include/uapi/evenless/logger.h
@@ -0,0 +1,15 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_LOGGER_H
+#define _EVENLESS_UAPI_LOGGER_H
+
+struct evl_logger_attrs {
+	__u32 fd;
+	__u32 logsz;
+};
+
+#endif /* !_EVENLESS_UAPI_LOGGER_H */
diff --git a/include/uapi/evenless/monitor.h b/include/uapi/evenless/monitor.h
new file mode 100644
index 000000000000..b4ace6058054
--- /dev/null
+++ b/include/uapi/evenless/monitor.h
@@ -0,0 +1,64 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_MONITOR_H
+#define _EVENLESS_UAPI_MONITOR_H
+
+#include <uapi/evenless/types.h>
+#include <uapi/evenless/factory.h>
+
+#define EVL_MONITOR_EV  0	/* Event monitor. */
+#define EVL_MONITOR_PI  1	/* Gate with priority inheritance. */
+#define EVL_MONITOR_PP  2	/* Gate with priority protection (ceiling). */
+
+struct evl_monitor_attrs {
+	__u32 clockfd;
+	__u32 type : 2,
+	      ceiling : 30;
+};
+
+/* State flags. */
+#define EVL_MONITOR_SIGNALED   0x1 /* Gate/Event */
+#define EVL_MONITOR_BROADCAST  0x2 /* Event */
+#define EVL_MONITOR_TARGETED   0x4 /* Event */
+
+#define EVL_MONITOR_NOGATE  -1U
+
+struct evl_monitor_state {
+	__u32 flags;
+	union {
+		struct {
+			atomic_t owner;
+			__u32 ceiling;
+		} gate;
+		__u32 gate_offset;
+	} u;
+};
+
+struct evl_monitor_waitreq {
+	struct timespec timeout;
+	__u32 gatefd;
+	__s32 status;
+};
+
+struct evl_monitor_unwaitreq {
+	__u32 gatefd;
+};
+
+struct evl_monitor_binding {
+	__u32 type;
+	struct evl_element_ids eids;
+};
+
+#define EVL_MONITOR_IOCBASE	'm'
+
+#define EVL_MONIOC_ENTER	_IO(EVL_MONITOR_IOCBASE, 0)
+#define EVL_MONIOC_EXIT		_IO(EVL_MONITOR_IOCBASE, 1)
+#define EVL_MONIOC_WAIT		_IOWR(EVL_MONITOR_IOCBASE, 2, struct evl_monitor_waitreq)
+#define EVL_MONIOC_UNWAIT	_IOWR(EVL_MONITOR_IOCBASE, 3, struct evl_monitor_unwaitreq)
+#define EVL_MONIOC_BIND		_IOR(EVL_MONITOR_IOCBASE, 4, struct evl_monitor_binding)
+
+#endif /* !_EVENLESS_UAPI_MONITOR_H */
diff --git a/include/uapi/evenless/poller.h b/include/uapi/evenless/poller.h
new file mode 100644
index 000000000000..2263861f06f3
--- /dev/null
+++ b/include/uapi/evenless/poller.h
@@ -0,0 +1,40 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_POLLER_H
+#define _EVENLESS_UAPI_POLLER_H
+
+struct evl_poller_attrs {
+	__u32 clockfd;
+};
+
+#define EVL_POLLER_IOCBASE	'p'
+
+#define EVL_POLLER_CTLADD   0
+#define EVL_POLLER_CTLDEL   1
+#define EVL_POLLER_CTLMOD   2
+
+struct evl_poller_ctlreq {
+	__u32 action;
+	__u32 fd;
+	__u32 events;
+};
+
+struct evl_poll_event {
+	__u32 fd;
+	__u32 events;
+};
+
+struct evl_poller_waitreq {
+	struct timespec timeout;
+	struct evl_poll_event *events;
+	int nrevents;
+};
+
+#define EVL_POLIOC_CTL	_IOW(EVL_POLLER_IOCBASE, 0, struct evl_poller_ctlreq)
+#define EVL_POLIOC_WAIT	_IOWR(EVL_POLLER_IOCBASE, 1, struct evl_poller_waitreq)
+
+#endif /* !_EVENLESS_UAPI_POLLER_H */
diff --git a/include/uapi/evenless/sched.h b/include/uapi/evenless/sched.h
new file mode 100644
index 000000000000..38a0e05a1bcd
--- /dev/null
+++ b/include/uapi/evenless/sched.h
@@ -0,0 +1,36 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Based on the Xenomai Cobalt core:
+ * Copyright (C) 2005, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_SCHED_H
+#define _EVENLESS_UAPI_SCHED_H
+
+#define SCHED_EVL	42
+#define SCHED_WEAK	43
+
+#define sched_rr_quantum	sched_u.rr.__sched_rr_quantum
+
+struct __sched_rr_param {
+	struct timespec __sched_rr_quantum;
+};
+
+#define SCHED_QUOTA		44
+#define sched_quota_group	sched_u.quota.__sched_group
+
+struct __sched_quota_param {
+	int __sched_group;
+};
+
+struct evl_sched_attrs {
+	int sched_policy;
+	int sched_priority;
+	union {
+		struct __sched_rr_param rr;
+		struct __sched_quota_param quota;
+	} sched_u;
+};
+
+#endif /* !_EVENLESS_UAPI_SCHED_H */
diff --git a/include/uapi/evenless/sem.h b/include/uapi/evenless/sem.h
new file mode 100644
index 000000000000..a934db71078c
--- /dev/null
+++ b/include/uapi/evenless/sem.h
@@ -0,0 +1,45 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_SEM_H
+#define _EVENLESS_UAPI_SEM_H
+
+#include <uapi/evenless/synch.h>
+#include <uapi/evenless/factory.h>
+
+struct timespec;
+
+#define EVL_SEM_VALUE_MAX	INT_MAX
+
+/* Attribute flags. */
+#define EVL_SEM_FIFO      0x0
+#define EVL_SEM_PRIO      0x1
+#define EVL_SEM_PULSE     0x4
+
+struct evl_sem_attrs {
+	__u32 clockfd;
+	__u32 flags;
+	__s32 initval;
+};
+
+struct evl_sem_state {
+	atomic_t value;
+	__u32 flags;
+};
+
+struct evl_sem_waitreq {
+	struct timespec timeout;
+	__s32 count;
+};
+
+#define EVL_SEM_IOCBASE	's'
+
+#define EVL_SEMIOC_GET		_IOW(EVL_SEM_IOCBASE, 0, __u32)
+#define EVL_SEMIOC_PUT		_IOW(EVL_SEM_IOCBASE, 1, __u32)
+#define EVL_SEMIOC_BCAST	_IO(EVL_SEM_IOCBASE, 2)
+#define EVL_SEMIOC_BIND		_IOR(EVL_SEM_IOCBASE, 3, struct evl_element_ids)
+
+#endif /* !_EVENLESS_UAPI_SEM_H */
diff --git a/include/uapi/evenless/signal.h b/include/uapi/evenless/signal.h
new file mode 100644
index 000000000000..c7aa01c12651
--- /dev/null
+++ b/include/uapi/evenless/signal.h
@@ -0,0 +1,37 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Derived from the Xenomai Cobalt core:
+ * Copyright (C) 2006 Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>
+ * Copyright (C) 2013, 2018 Philippe Gerum <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_SIGNAL_H
+#define _EVENLESS_UAPI_SIGNAL_H
+
+#define SIGSHADOW			SIGWINCH
+#define sigshadow_action(code)		((code) & 0xff)
+#define sigshadow_arg(code)		(((code) >> 8) & 0xff)
+#define sigshadow_int(action, arg)	((action) | ((arg) << 8))
+
+/* SIGSHADOW action codes. */
+#define SIGSHADOW_ACTION_HOME		1
+
+#define SIGDEBUG			SIGXCPU
+#define sigdebug_code(si)		((si)->si_value.sival_int)
+#define sigdebug_cause(si)		(sigdebug_code(si) & 0xff)
+#define sigdebug_marker			0xfccf0000
+#define sigdebug_marked(si)		\
+	((sigdebug_code(si) & 0xffff0000) == sigdebug_marker)
+
+/* Possible values of sigdebug_cause() */
+#define SIGDEBUG_UNDEFINED		0
+#define SIGDEBUG_MIGRATE_SIGNAL		1
+#define SIGDEBUG_MIGRATE_SYSCALL	2
+#define SIGDEBUG_MIGRATE_FAULT		3
+#define SIGDEBUG_MIGRATE_PRIOINV	4
+#define SIGDEBUG_WATCHDOG		5
+#define SIGDEBUG_RESCNT_IMBALANCE	6
+#define SIGDEBUG_MONITOR_SLEEP		7
+
+#endif /* !_EVENLESS_UAPI_SIGNAL_H */
diff --git a/include/uapi/evenless/synch.h b/include/uapi/evenless/synch.h
new file mode 100644
index 000000000000..bebc076bf31f
--- /dev/null
+++ b/include/uapi/evenless/synch.h
@@ -0,0 +1,65 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Based on the Xenomai Cobalt core:
+ * Copyright (C) 2001, 2013, 2018 Philippe Gerum <rpm@xenomai.org>
+ * Copyright (C) 2008, 2009 Jan Kiszka <jan.kiszka@siemens.com>.
+ */
+
+#ifndef _EVENLESS_UAPI_SYNCH_H
+#define _EVENLESS_UAPI_SYNCH_H
+
+#include <uapi/evenless/types.h>
+
+/* Type flags */
+#define EVL_SYN_FIFO    0x0
+#define EVL_SYN_PRIO    0x1
+#define EVL_SYN_PI      0x2
+#define EVL_SYN_OWNER   0x4
+#define EVL_SYN_PP      0x8
+
+static inline int evl_fast_syn_is_claimed(fundle_t handle)
+{
+	return (handle & EVL_SYN_FLCLAIM) != 0;
+}
+
+static inline fundle_t evl_syn_fast_claim(fundle_t handle)
+{
+	return handle | EVL_SYN_FLCLAIM;
+}
+
+static inline fundle_t evl_syn_fast_ceil(fundle_t handle)
+{
+	return handle | EVL_SYN_FLCEIL;
+}
+
+static inline int
+evl_is_syn_owner(atomic_t *fastlock, fundle_t ownerh)
+{
+	return evl_get_index(atomic_read(fastlock)) == ownerh;
+}
+
+static inline
+int evl_fast_acquire_syn(atomic_t *fastlock, fundle_t new_ownerh)
+{
+	fundle_t h;
+
+	h = atomic_cmpxchg(fastlock, EVL_NO_HANDLE, new_ownerh);
+	if (h != EVL_NO_HANDLE) {
+		if (evl_get_index(h) == new_ownerh)
+			return -EBUSY;
+
+		return -EAGAIN;
+	}
+
+	return 0;
+}
+
+static inline
+int evl_fast_release_syn(atomic_t *fastlock, fundle_t cur_ownerh)
+{
+	return atomic_cmpxchg(fastlock, cur_ownerh, EVL_NO_HANDLE)
+		== cur_ownerh;
+}
+
+#endif /* !_EVENLESS_UAPI_SYNCH_H */
diff --git a/include/uapi/evenless/syscall.h b/include/uapi/evenless/syscall.h
new file mode 100644
index 000000000000..5bfc4e2a3830
--- /dev/null
+++ b/include/uapi/evenless/syscall.h
@@ -0,0 +1,16 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_SYSCALL_H
+#define _EVENLESS_UAPI_SYSCALL_H
+
+#define __NR_EVENLESS_SYSCALLS	3
+
+#define sys_evenless_read	0	/* oob_read() */
+#define sys_evenless_write	1	/* oob_write() */
+#define sys_evenless_ioctl	2	/* oob_ioctl() */
+
+#endif /* !_EVENLESS_UAPI_SYSCALL_H */
diff --git a/include/uapi/evenless/thread.h b/include/uapi/evenless/thread.h
new file mode 100644
index 000000000000..51c499b024c1
--- /dev/null
+++ b/include/uapi/evenless/thread.h
@@ -0,0 +1,85 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Derived from the Xenomai/cobalt core:
+ * Copyright (C) 2005, 2018 Philippe Gerum <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_THREAD_H
+#define _EVENLESS_UAPI_THREAD_H
+
+#include <uapi/evenless/sched.h>
+
+/* State flags (shared) */
+
+#define T_SUSP    0x00000001 /*< Suspended. */
+#define T_PEND    0x00000002 /*< Sleep-wait for a resource. */
+#define T_DELAY   0x00000004 /*< Delayed */
+#define T_READY   0x00000008 /*< Linked to the ready queue. */
+#define T_DORMANT 0x00000010 /*< Not started yet */
+#define T_ZOMBIE  0x00000020 /*< Zombie thread in deletion process */
+#define T_INBAND  0x00000040 /*< Thread is running in-band */
+#define T_HALT    0x00000080 /*< Thread is halted. */
+#define T_BOOST   0x00000100 /*< PI/PP boost undergoing */
+#define T_SSTEP   0x00000200 /*< Single-stepped by debugger */
+#define T_RRB     0x00000400 /*< Undergoes a round-robin scheduling */
+#define T_WARN    0x00000800 /*< Issue SIGDEBUG on error detection */
+#define T_ROOT    0x00001000 /*< Root thread (in-band context placeholder) */
+#define T_WEAK    0x00002000 /*< Weak scheduling (non real-time) */
+#define T_USER    0x00004000 /*< Userland thread */
+#define T_DEBUG   0x00008000 /*< User-level debugging enabled */
+
+/* Information flags (shared) */
+
+#define T_TIMEO   0x00000001 /*< Woken up due to a timeout condition */
+#define T_RMID    0x00000002 /*< Pending on a removed resource */
+#define T_BREAK   0x00000004 /*< Forcibly awaken from a wait state */
+#define T_KICKED  0x00000008 /*< Forced out of OOB context */
+#define T_WAKEN   0x00000010 /*< Thread waken up upon resource availability */
+#define T_ROBBED  0x00000020 /*< Robbed from resource ownership */
+#define T_CANCELD 0x00000040 /*< Cancellation request is pending */
+#define T_PIALERT 0x00000080 /*< Priority inversion alert (SIGDEBUG sent) */
+#define T_SCHEDP  0x00000100 /*< schedparam propagation is pending */
+#define T_BCAST   0x00000200 /*< Woken up upon resource broadcast */
+#define T_SIGNAL  0x00000400 /*< Event monitor signaled */
+
+/* Local information flags (private to current thread) */
+
+#define T_MOVED   0x00000001 /*< CPU migration request issued from OOB context */
+#define T_SYSRST  0x00000002 /*< Thread awaiting syscall restart after signal */
+#define T_HICCUP  0x00000004 /*< Just left from ptracing - timings wrecked */
+
+/*
+ * Must follow strictly the declaration order of the state flags
+ * defined above. Status symbols are defined as follows:
+ *
+ * 'S' -> Forcibly suspended.
+ * 'w'/'W' -> Waiting for a resource, with or without timeout.
+ * 'D' -> Delayed (without any other wait condition).
+ * 'R' -> Runnable.
+ * 'U' -> Unstarted or dormant.
+ * 'X' -> Relaxed shadow.
+ * 'H' -> Held in emergency.
+ * 'b' -> Priority boost undergoing.
+ * 'T' -> Ptraced and stopped.
+ * 'r' -> Undergoes round-robin.
+ * 't' -> Runtime mode errors notified.
+ */
+#define EVL_THREAD_STATE_LABELS  "SWDRU.XHbTrt...."
+
+struct evl_user_window {
+	__u32 state;
+	__u32 info;
+	__u32 pp_pending;
+};
+
+#define EVL_THREAD_IOCBASE	'T'
+
+#define EVL_THRIOC_SIGNAL		_IOW(EVL_THREAD_IOCBASE, 0, __u32)
+#define EVL_THRIOC_SET_SCHEDPARAM	_IOW(EVL_THREAD_IOCBASE, 1, struct evl_sched_attrs)
+#define EVL_THRIOC_GET_SCHEDPARAM	_IOR(EVL_THREAD_IOCBASE, 2, struct evl_sched_attrs)
+#define EVL_THRIOC_JOIN			_IO(EVL_THREAD_IOCBASE, 3)
+#define EVL_THRIOC_SET_MODE		_IOW(EVL_THREAD_IOCBASE, 4, __u32)
+#define EVL_THRIOC_CLEAR_MODE		_IOW(EVL_THREAD_IOCBASE, 5, __u32)
+
+#endif /* !_EVENLESS_UAPI_THREAD_H */
diff --git a/include/uapi/evenless/timerfd.h b/include/uapi/evenless/timerfd.h
new file mode 100644
index 000000000000..2018997b93f4
--- /dev/null
+++ b/include/uapi/evenless/timerfd.h
@@ -0,0 +1,31 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_TIMERFD_H
+#define _EVENLESS_UAPI_TIMERFD_H
+
+struct evl_timerfd_attrs {
+	__u32 clockfd;
+};
+
+/* Set operation flag. */
+#define EVL_TIMERFD_ABSTIME  0x1
+
+struct evl_timerfd_setreq {
+	struct itimerspec value;
+	struct itimerspec ovalue;
+};
+
+struct evl_timerfd_getreq {
+	struct itimerspec value;
+};
+
+#define EVL_TIMERFD_IOCBASE	't'
+
+#define EVL_TFDIOC_SET	_IOWR(EVL_TIMERFD_IOCBASE, 0, struct evl_timerfd_setreq)
+#define EVL_TFDIOC_GET	_IOR(EVL_TIMERFD_IOCBASE, 1, struct evl_timerfd_getreq)
+
+#endif /* !_EVENLESS_UAPI_TIMERFD_H */
diff --git a/include/uapi/evenless/trace.h b/include/uapi/evenless/trace.h
new file mode 100644
index 000000000000..24fd0a1d3d2f
--- /dev/null
+++ b/include/uapi/evenless/trace.h
@@ -0,0 +1,14 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_TRACE_H
+#define _EVENLESS_UAPI_TRACE_H
+
+#define EVL_TRACE_IOCBASE	'O'
+
+#define EVL_TRCIOC_TRACE_SNAPSHOT	_IO(EVL_TRACE_IOCBASE, 0)
+
+#endif /* !_EVENLESS_UAPI_TRACE_H */
diff --git a/include/uapi/evenless/types.h b/include/uapi/evenless/types.h
new file mode 100644
index 000000000000..9e8b9538905f
--- /dev/null
+++ b/include/uapi/evenless/types.h
@@ -0,0 +1,29 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Based on the Xenomai Cobalt core:
+ * Copyright (C) 2013, 2018 Philippe Gerum <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_TYPES_H
+#define _EVENLESS_UAPI_TYPES_H
+
+typedef __u32 fundle_t;
+
+#define EVL_NO_HANDLE		((fundle_t)0x00000000)
+
+/* Reserved status bits */
+#define EVL_SYN_FLCLAIM		((fundle_t)0x80000000) /* Contended. */
+#define EVL_SYN_FLCEIL		((fundle_t)0x40000000) /* Ceiling active. */
+#define EVL_HANDLE_INDEX_MASK	(EVL_SYN_FLCLAIM|EVL_SYN_FLCEIL)
+
+/*
+ * Strip all reserved bits from the handle, only retaining the fast
+ * index value.
+ */
+static inline fundle_t evl_get_index(fundle_t handle)
+{
+	return handle & ~EVL_HANDLE_INDEX_MASK;
+}
+
+#endif /* !_EVENLESS_UAPI_TYPES_H */
diff --git a/include/uapi/evenless/xbuf.h b/include/uapi/evenless/xbuf.h
new file mode 100644
index 000000000000..697463e99cb3
--- /dev/null
+++ b/include/uapi/evenless/xbuf.h
@@ -0,0 +1,17 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#ifndef _EVENLESS_UAPI_XBUF_H
+#define _EVENLESS_UAPI_XBUF_H
+
+struct evl_xbuf_attrs {
+	__u32 i_bufsz;
+	__u32 o_bufsz;
+};
+
+#define EVL_XBUF_IOCBASE	'x'
+
+#endif /* !_EVENLESS_UAPI_XBUF_H */
diff --git a/kernel/Kconfig.evenless b/kernel/Kconfig.evenless
new file mode 100644
index 000000000000..968b5efccd90
--- /dev/null
+++ b/kernel/Kconfig.evenless
@@ -0,0 +1,29 @@
+
+# Evenless real-time co-kernel
+config HAVE_ARCH_EVENLESS
+	bool
+
+menuconfig EVENLESS
+	bool "Enable the Evenless core"
+	depends on HAVE_ARCH_EVENLESS
+	select DOVETAIL
+	help
+
+	  The Evenless core is a real-time component of the Linux
+	  kernel, which exhibits very short interrupt and scheduling
+	  latency, without affecting the regular kernel services.
+
+if EVENLESS
+
+source "kernel/evenless/Kconfig"
+
+if WARN_CPUFREQ_GOVERNOR
+comment "WARNING! CPU_FREQ governors other than 'performance'"
+comment "or 'powersave' may significantly increase latency"
+comment "on this platform during the frequency transitions."
+endif
+
+endif
+
+config WARN_CPUFREQ_GOVERNOR
+       def_bool n
diff --git a/kernel/Makefile b/kernel/Makefile
index d62e48828c3f..d23ebdb6656b 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -101,6 +101,7 @@ obj-$(CONFIG_IRQ_WORK) += irq_work.o
 obj-$(CONFIG_DOVETAIL) += dovetail.o
 obj-$(CONFIG_CPU_PM) += cpu_pm.o
 obj-$(CONFIG_BPF) += bpf/
+obj-$(CONFIG_EVENLESS) += evenless/
 
 obj-$(CONFIG_PERF_EVENTS) += events/
 
diff --git a/kernel/evenless/.gitignore b/kernel/evenless/.gitignore
new file mode 100644
index 000000000000..beea4d8a9c77
--- /dev/null
+++ b/kernel/evenless/.gitignore
@@ -0,0 +1 @@
+syscall_entries.h
diff --git a/kernel/evenless/Kconfig b/kernel/evenless/Kconfig
new file mode 100644
index 000000000000..b751624ce192
--- /dev/null
+++ b/kernel/evenless/Kconfig
@@ -0,0 +1,300 @@
+
+menu "Core features"
+
+config EVENLESS_SCHED_QUOTA
+	bool "Thread groups with runtime quota"
+	default n
+	help
+	This option enables the SCHED_QUOTA scheduling policy in the
+	Evenless kernel.
+
+	This policy enforces a limitation on the CPU consumption of
+	threads over a globally defined period, known as the quota
+	interval. This is done by pooling threads with common
+	requirements in groups, and giving each group a share of the
+	global period (see CONFIG_EVENLESS_SCHED_QUOTA_PERIOD).
+
+	When threads have entirely consumed the quota allotted to the
+	group they belong to, the latter is suspended as a whole,
+	until the next quota interval starts. At this point, a new
+	runtime budget is given to each group, in accordance with its
+	share.
+
+	If in doubt, say N.
+
+config EVENLESS_SCHED_QUOTA_PERIOD
+	int "Quota interval (us)"
+	default 10000
+	range 100 1000000000
+	depends on EVENLESS_SCHED_QUOTA
+	help
+	The global period thread groups can get a share of.
+
+config EVENLESS_SCHED_QUOTA_NR_GROUPS
+	int "Number of thread groups"
+	default 32
+	range 1 1024
+	depends on EVENLESS_SCHED_QUOTA
+	help
+	The overall number of thread groups which may be defined
+	across all CPUs.
+
+config EVENLESS_STATS
+	bool "Runtime statistics"
+	default y
+	help
+	This option causes the Evenless kernel to collect various
+	per-thread runtime statistics, which are accessible through
+	the /proc/evenless/sched/stat interface.
+
+endmenu
+
+menu "Sizes and static limits"
+
+config EVENLESS_SYS_HEAPSZ
+	int "Size of system heap (Kb)"
+	default 512
+	help
+	The system heap is used for various internal allocations by
+	the Evenless kernel. The size is expressed in Kilobytes.
+
+config EVENLESS_SHARED_HEAPSZ
+	int "Size of shared heap (Kb)"
+	default 64
+	help
+	The Evenless core implements fast IPC mechanisms between
+	processes which require a shared kernel memory heap to be
+	mapped in the address space of all Evenless application
+	processes. This option can be used to set the size of this
+	system-wide heap.
+
+	64k is considered a large enough size for common use cases.
+
+config EVENLESS_NR_THREADS
+	int "Maximum number of threads"
+	range 1 4096
+	default 128
+	help
+
+	The maximum number of user-space threads attached to the
+	Evenless core which can run concurrently in the system.
+
+config EVENLESS_NR_MONITORS
+	int "Maximum number of monitors"
+	range 1 16384
+	default 512
+	help
+
+	The monitor is the fundamental synchronization element
+	implemented by the Evenless core, which can underpin any other
+	synchronization mechanism. This value gives the maximum number
+	of monitors which can be alive concurrently in the system.
+
+config EVENLESS_NR_CLOCKS
+	int "Maximum number of clocks"
+	range 1 16384
+	default 8
+	help
+
+	This value gives the maximum number of semaphores which can be
+	alive concurrently in the system for user-space applications.
+
+config EVENLESS_NR_SEMAPHORES
+	int "Maximum number of semaphores"
+	range 1 16384
+	default 512
+	help
+
+	This value gives the maximum number of semaphores which can be
+	alive concurrently in the system for user-space applications.
+
+config EVENLESS_NR_TIMERFDS
+	int "Maximum number of timerfds"
+	range 1 16384
+	default 512
+	help
+
+	This value gives the maximum number of timerfds which can be
+	alive concurrently in the system for user-space applications.
+
+config EVENLESS_NR_POLLERS
+	int "Maximum number of pollers"
+	range 1 16384
+	default 64
+	help
+
+	This value gives the maximum number of pollers which can be
+	alive concurrently in the system for user-space applications.
+
+config EVENLESS_NR_XBUFS
+	int "Maximum number of x-buffers"
+	range 1 16384
+	default 16
+	help
+
+	This value gives the maximum number of x-buffers which can be
+	alive concurrently in the system for user-space applications.
+
+config EVENLESS_NR_LOGGERS
+	int "Maximum number of loggers"
+	range 1 16384
+	default 64
+	help
+
+	This value gives the maximum number of loggers which can be
+	alive concurrently in the system for user-space applications.
+
+endmenu
+
+menu "Latency settings"
+
+config EVENLESS_TIMING_SCHEDLAT
+	int "User scheduling latency (ns)"
+	default 0
+	help
+	The user scheduling latency is the time between the
+	termination of an interrupt handler and the execution of the
+	first instruction of the real-time application thread this
+	handler resumes. A default value of 0 (recommended) will cause
+	a pre-calibrated value to be used.
+
+	If the auto-tuner is enabled, this value will be used as the
+	factory default when running "autotune --reset".
+
+config EVENLESS_TIMING_KSCHEDLAT
+	int "Intra-kernel scheduling latency (ns)"
+	default 0
+	help
+	The intra-kernel scheduling latency is the time between the
+	termination of an interrupt handler and the execution of the
+	first instruction of the Evenless kernel thread this handler
+	resumes. A default value of 0 (recommended) will cause a
+	pre-calibrated value to be used.
+
+	Intra-kernel latency is usually significantly lower than user
+	scheduling latency on MMU-enabled platforms, due to CPU cache
+	latency.
+
+	If the auto-tuner is enabled, this value will be used as the
+	factory default when running "autotune --reset".
+
+config EVENLESS_TIMING_IRQLAT
+	int "Interrupt latency (ns)"
+	default 0
+	help
+	The interrupt latency is the time between the occurrence of an
+	IRQ and the first instruction of the interrupt handler which
+	will service it. A default value of 0 (recommended) will cause
+	a pre-calibrated value to be used.
+
+	If the auto-tuner is enabled, this value will be used as the
+	factory default when running "autotune --reset".
+
+endmenu
+
+menuconfig EVENLESS_DEBUG
+	bool "Debug support"
+	help
+	  When enabled, various debugging features can be switched
+	  on. They can help to find problems in applications, drivers,
+	  and the Evenless kernel. EVENLESS_DEBUG by itself does not have
+	  any impact on the generated code.
+
+if EVENLESS_DEBUG
+
+config EVENLESS_DEBUG_CORE
+	bool "Core runtime assertions"
+	help
+	  This option activates various assertions inside the Evenless
+	  core. This option has limited overhead.
+
+config EVENLESS_DEBUG_CONTEXT
+       bool "Check for calling context"
+       help
+         This option enables checks for the calling context in the
+         Evenless kernel, aimed at detecting when regular Linux routines
+         are entered from a real-time context, and conversely.
+
+config EVENLESS_DEBUG_MEMORY
+	bool "Memory checks"
+	help
+	  This option enables memory debug checks inside the Evenless
+	  kernel. This option may induce significant overhead with large
+	  heaps.
+
+config EVENLESS_DEBUG_LOCKING
+	bool "Spinlock debugging support"
+	default n
+	help
+	  This option activates runtime assertions, and measurements
+	  of spinlocks spinning time and duration in the Evenless
+	  kernel. It helps finding latency spots due to interrupt
+	  masked sections. Statistics about the longest masked section
+	  can be found in /proc/evenless/debug/lock.
+
+	  This option may induce a measurable overhead on low end
+	  machines.
+
+config EVENLESS_DEBUG_USER
+	bool "User consistency checks"
+	help
+	  This option enables a set of consistency checks for
+	  detecting wrong runtime behavior in user applications.
+	  Some of these runtime checks may induce overhead, enable
+	  them for debugging purposes only.
+
+if EVENLESS_DEBUG_USER
+
+config EVENLESS_DEBUG_MONITOR_INBAND
+       bool "Detect in-band monitor owner"
+       default y
+       help
+         A thread which attempts to enter a monitor currently owned by
+         another thread running in-band will suffer unwanted latencies,
+         due to a priority inversion.  This switch enables debug
+         notifications sending a SIGDEBUG signal to the monitor owner.
+
+	 This option may add overhead to out-of-band execution over
+	 contented monitors.
+
+config EVENLESS_DEBUG_MONITOR_SLEEP
+       bool "Detect sleeping while holding a monitor"
+       default y
+       help
+         A thread which goes sleeping while holding a monitor is prone
+         to cause unwanted latencies to other threads serialized by
+         the same lock. If debug notifications are enabled for such
+         thread, it receives a SIGDEBUG signal right before entering
+	 sleep.
+
+	 This option has noticeable overhead in real-time mode as it
+	 disables the normal fast locking operations from user-space,
+	 causing a system call for each monitor enter/exit operation.
+
+endif # EVENLESS_DEBUG_USER
+
+config EVENLESS_WATCHDOG
+	bool "Watchdog support"
+	default y
+	help
+	  This option activates a watchdog aimed at detecting runaway
+	  Evenless threads. If enabled, the watchdog triggers after a
+	  given period of uninterrupted real-time activity has elapsed
+	  without Linux interaction in the meantime.
+
+	  In such an event, the current thread is moved out the
+	  real-time domain, receiving a SIGDEBUG signal from the Linux
+	  kernel immediately after.
+
+	  The timeout value of the watchdog can be set using the
+	  EVENLESS_WATCHDOG_TIMEOUT parameter.
+
+config EVENLESS_WATCHDOG_TIMEOUT
+	depends on EVENLESS_WATCHDOG
+	int "Watchdog timeout"
+	default 4
+	range 1 60
+	help
+	  Watchdog timeout value (in seconds).
+
+endif # EVENLESS_DEBUG
diff --git a/kernel/evenless/Makefile b/kernel/evenless/Makefile
new file mode 100644
index 000000000000..aeb57b9599db
--- /dev/null
+++ b/kernel/evenless/Makefile
@@ -0,0 +1,26 @@
+obj-$(CONFIG_EVENLESS) += evenless.o sched/
+
+ccflags-y += -Ikernel
+
+evenless-y :=		\
+	clock.o		\
+	control.o	\
+	factory.o	\
+	file.o		\
+	init.o		\
+	irq.o		\
+	lock.o		\
+	logger.o	\
+	memory.o	\
+	monitor.o	\
+	poller.o	\
+	sem.o		\
+	synch.o		\
+	syscall.o	\
+	thread.o	\
+	tick.o		\
+	timer.o		\
+	timerfd.o	\
+	xbuf.o
+
+evenless-$(CONFIG_FTRACE) +=	trace.o
diff --git a/kernel/evenless/clock.c b/kernel/evenless/clock.c
new file mode 100644
index 000000000000..3bc1dc2d0327
--- /dev/null
+++ b/kernel/evenless/clock.c
@@ -0,0 +1,825 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2006, 2018 Philippe Gerum  <rpm@xenomai.org>
+ * Copyright Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>
+ */
+
+#include <linux/kernel.h>
+#include <linux/percpu.h>
+#include <linux/errno.h>
+#include <linux/clockchips.h>
+#include <linux/interrupt.h>
+#include <linux/tick.h>
+#include <linux/kconfig.h>
+#include <linux/clocksource.h>
+#include <linux/bitmap.h>
+#include <linux/sched/signal.h>
+#include <linux/mutex.h>
+#include <linux/slab.h>
+#include <evenless/sched.h>
+#include <evenless/timer.h>
+#include <evenless/clock.h>
+#include <evenless/tick.h>
+#include <evenless/thread.h>
+#include <evenless/factory.h>
+#include <evenless/control.h>
+#include <evenless/file.h>
+#include <asm/evenless/syscall.h>
+#include <uapi/evenless/clock.h>
+#include <trace/events/evenless.h>
+
+static const struct file_operations clock_fops;
+
+static LIST_HEAD(clock_list);
+
+static DEFINE_MUTEX(clocklist_lock);
+
+/* timer base locked */
+static void adjust_timer(struct evl_clock *clock,
+			 struct evl_timer *timer, struct evl_tqueue *q,
+			 ktime_t delta)
+{
+	ktime_t period, diff;
+	s64 div;
+
+	evl_move_timer_backward(timer, delta);
+
+	if (!evl_timer_is_periodic(timer))
+		goto enqueue;
+
+	timer->start_date = ktime_sub(timer->start_date, delta);
+	period = timer->interval;
+	diff = ktime_sub(evl_read_clock(clock), evl_get_timer_expiry(timer));
+
+	if (diff >= period) {
+		/*
+		 * Timer should tick several times before now, instead
+		 * of calling timer->handler several times, we change
+		 * the timer date without changing its pexpect, so
+		 * that timer will tick only once and the lost ticks
+		 * will be counted as overruns.
+		 */
+		div = ktime_divns(diff, ktime_to_ns(period));
+		timer->periodic_ticks += div;
+		evl_update_timer_date(timer);
+	} else if (ktime_to_ns(delta) < 0
+		   && (timer->status & EVL_TIMER_FIRED)
+		   && ktime_to_ns(ktime_add(diff, period)) <= 0) {
+		/*
+		 * Timer is periodic and NOT waiting for its first
+		 * shot, so we make it tick sooner than its original
+		 * date in order to avoid the case where by adjusting
+		 * time to a sooner date, real-time periodic timers do
+		 * not tick until the original date has passed.
+		 */
+		div = ktime_divns(-diff, ktime_to_ns(period));
+		timer->periodic_ticks -= div;
+		timer->pexpect_ticks -= div;
+		evl_update_timer_date(timer);
+	}
+
+enqueue:
+	evl_enqueue_timer(timer, q);
+}
+
+void evl_adjust_timers(struct evl_clock *clock, ktime_t delta)
+{
+	struct evl_timer *timer, *tmp;
+	struct evl_timerbase *tmb;
+	struct evl_tqueue *tq;
+	struct evl_tnode *tn;
+	struct list_head adjq;
+	struct evl_rq *rq;
+	unsigned long flags;
+	int cpu;
+
+	INIT_LIST_HEAD(&adjq);
+
+	for_each_online_cpu(cpu) {
+		rq = evl_cpu_rq(cpu);
+		tmb = evl_percpu_timers(clock, cpu);
+		tq = &tmb->q;
+		raw_spin_lock_irqsave(&tmb->lock, flags);
+
+		for_each_evl_tnode(tn, tq) {
+			timer = container_of(tn, struct evl_timer, node);
+			if (timer->clock == clock)
+				list_add_tail(&timer->adjlink, &adjq);
+		}
+
+		if (list_empty(&adjq))
+			goto next;
+
+		list_for_each_entry_safe(timer, tmp, &adjq, adjlink) {
+			list_del(&timer->adjlink);
+			evl_dequeue_timer(timer, tq);
+			adjust_timer(clock, timer, tq, delta);
+		}
+
+		if (rq != this_evl_rq())
+			evl_program_remote_tick(clock, rq);
+		else
+			evl_program_local_tick(clock);
+	next:
+		raw_spin_unlock_irqrestore(&tmb->lock, flags);
+	}
+}
+EXPORT_SYMBOL_GPL(evl_adjust_timers);
+
+void inband_clock_was_set(void)
+{
+	struct evl_clock *clock;
+
+	if (!evl_is_enabled())
+		return;
+
+	mutex_lock(&clocklist_lock);
+
+	if (!list_empty(&clock_list)) {
+		list_for_each_entry(clock, &clock_list, next) {
+			if (clock->ops.adjust)
+				clock->ops.adjust(clock);
+		}
+	}
+
+	mutex_unlock(&clocklist_lock);
+}
+
+static int init_clock(struct evl_clock *clock,
+		      struct evl_clock *master)
+{
+	int ret;
+
+	ret = evl_init_element(&clock->element, &evl_clock_factory);
+	if (ret)
+		return ret;
+
+	clock->master = master;
+
+	/*
+	 * Once the device appears in the filesystem, it has to be
+	 * usable. Make sure all inits have been completed before this
+	 * point.
+	 */
+	ret = evl_create_element_device(&clock->element,
+					&evl_clock_factory,
+					clock->name);
+	if (ret) {
+		evl_destroy_element(&clock->element);
+		return ret;
+	}
+
+	mutex_lock(&clocklist_lock);
+	list_add(&clock->next, &clock_list);
+	mutex_unlock(&clocklist_lock);
+
+	return 0;
+}
+
+int evl_init_clock(struct evl_clock *clock,
+		   const struct cpumask *affinity)
+{
+	struct evl_timerbase *tmb;
+	int cpu, ret;
+
+	inband_context_only();
+
+	/*
+	 * A CPU affinity set may be defined for each clock,
+	 * enumerating the CPUs which can receive ticks from the
+	 * backing clock device.  When given, this set must be a
+	 * subset of the out-of-band CPU set.
+	 */
+#ifdef CONFIG_SMP
+	if (!affinity)	/* Is this device global? */
+		cpumask_clear(&clock->affinity);
+	else {
+		cpumask_and(&clock->affinity, affinity, &evl_oob_cpus);
+		if (cpumask_empty(&clock->affinity))
+			return -EINVAL;
+	}
+#endif
+
+	clock->timerdata = alloc_percpu(struct evl_timerbase);
+	if (clock->timerdata == NULL)
+		return -ENOMEM;
+
+	/*
+	 * POLA: init all timer slots for the new clock, although some
+	 * of them might remain unused depending on the CPU affinity
+	 * of the event source(s). If the clock device is global
+	 * without any particular IRQ affinity, all timers will be
+	 * queued to CPU0.
+	 */
+	for_each_online_cpu(cpu) {
+		tmb = evl_percpu_timers(clock, cpu);
+		evl_init_tqueue(&tmb->q);
+		raw_spin_lock_init(&tmb->lock);
+	}
+
+	clock->offset = 0;
+
+	ret = init_clock(clock, clock);
+	if (ret)
+		goto fail;
+
+	return 0;
+
+fail:
+	for_each_online_cpu(cpu) {
+		tmb = evl_percpu_timers(clock, cpu);
+		evl_destroy_tqueue(&tmb->q);
+	}
+
+	free_percpu(clock->timerdata);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_init_clock);
+
+int evl_init_slave_clock(struct evl_clock *clock,
+			 struct evl_clock *master)
+{
+	inband_context_only();
+
+	/* A slave clock shares its master's device. */
+#ifdef CONFIG_SMP
+	clock->affinity = master->affinity;
+#endif
+	clock->timerdata = master->timerdata;
+	clock->offset = evl_read_clock(clock) -
+		evl_read_clock(master);
+	init_clock(clock, master);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(evl_init_slave_clock);
+
+static inline bool timer_needs_enqueuing(struct evl_timer *timer)
+{
+	/*
+	 * True for periodic timers which have not been requeued,
+	 * stopped or killed, false otherwise.
+	 */
+	return (timer->status &
+		(EVL_TIMER_PERIODIC|EVL_TIMER_DEQUEUED|
+		 EVL_TIMER_RUNNING|EVL_TIMER_KILLED))
+		== (EVL_TIMER_PERIODIC|EVL_TIMER_DEQUEUED|
+		    EVL_TIMER_RUNNING);
+}
+
+/* Announce a tick from a master clock. */
+void evl_announce_tick(struct evl_clock *clock)
+{
+	struct evl_rq *rq = this_evl_rq();
+	struct evl_timerbase *tmb;
+	struct evl_timer *timer;
+	struct evl_tqueue *tq;
+	struct evl_tnode *tn;
+	unsigned long flags;
+	ktime_t now;
+
+#ifdef CONFIG_SMP
+	/*
+	 * Some external clock devices may be global without any
+	 * particular IRQ affinity, in which case the associated
+	 * timers will be queued to CPU0.
+	 */
+	if (clock != &evl_mono_clock &&
+	    !cpumask_test_cpu(evl_rq_cpu(rq), &clock->affinity))
+		tmb = evl_percpu_timers(clock, 0);
+	else
+#endif
+		tmb = evl_this_cpu_timers(clock);
+
+	tq = &tmb->q;
+	raw_spin_lock_irqsave(&tmb->lock, flags);
+
+	/*
+	 * Optimisation: any local timer reprogramming triggered by
+	 * invoked timer handlers can wait until we leave this tick
+	 * handler. This is a hint for the program_local_shot()
+	 * handler of the ticking clock.
+	 */
+	rq->lflags |= RQ_TIMER;
+
+	while ((tn = evl_get_tqueue_head(tq)) != NULL) {
+		timer = container_of(tn, struct evl_timer, node);
+		now = evl_read_clock(clock);
+		if (now < evl_tdate(timer))
+			break;
+
+		trace_evl_timer_expire(timer);
+		evl_dequeue_timer(timer, tq);
+		evl_account_timer_fired(timer);
+		timer->status |= EVL_TIMER_FIRED;
+
+		if (timer->status & EVL_TIMER_PERIODIC) {
+			do {
+				timer->periodic_ticks++;
+				evl_update_timer_date(timer);
+			} while (evl_tdate(timer) < now);
+		}
+
+		raw_spin_unlock_irqrestore(&tmb->lock, flags);
+		timer->handler(timer);
+		raw_spin_lock_irqsave(&tmb->lock, flags);
+
+		if (timer_needs_enqueuing(timer) &&
+		    evl_timer_on_rq(timer, rq))
+			evl_enqueue_timer(timer, tq);
+	}
+
+	rq->lflags &= ~RQ_TIMER;
+
+	evl_program_local_tick(clock);
+
+	raw_spin_unlock_irqrestore(&tmb->lock, flags);
+}
+EXPORT_SYMBOL_GPL(evl_announce_tick);
+
+void evl_stop_timers(struct evl_clock *clock)
+{
+	struct evl_timerbase *tmb;
+	struct evl_timer *timer;
+	struct evl_tqueue *tq;
+	struct evl_tnode *tn;
+	unsigned long flags;
+	int cpu;
+
+	/* Deactivate all outstanding timers on the clock. */
+
+	for_each_evl_cpu(cpu) {
+		tmb = evl_percpu_timers(clock, cpu);
+		raw_spin_lock_irqsave(&tmb->lock, flags);
+		tq = &tmb->q;
+		while (!evl_tqueue_is_empty(tq)) {
+			tn = evl_get_tqueue_head(tq);
+			timer = container_of(tn, struct evl_timer, node);
+			if (EVL_WARN_ON(CORE, timer->status & EVL_TIMER_DEQUEUED))
+				continue;
+			evl_timer_deactivate(timer);
+		}
+		raw_spin_unlock_irqrestore(&tmb->lock, flags);
+	}
+}
+
+int evl_register_clock(struct evl_clock *clock,
+		       const struct cpumask *affinity)
+{
+	int ret;
+
+	inband_context_only();
+
+	ret = evl_init_clock(clock, affinity);
+	if (ret)
+		return ret;
+
+	trace_evl_register_clock(clock->name);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(evl_register_clock);
+
+void evl_unregister_clock(struct evl_clock *clock)
+{
+	inband_context_only();
+
+	trace_evl_unregister_clock(clock->name);
+	evl_put_element(&clock->element);
+}
+EXPORT_SYMBOL_GPL(evl_unregister_clock);
+
+struct evl_clock *evl_get_clock_by_fd(int efd)
+{
+	struct evl_clock *clock = NULL;
+	struct evl_file *sfilp;
+
+	sfilp = evl_get_file(efd);
+	if (sfilp && sfilp->filp->f_op == &clock_fops) {
+		clock = element_of(sfilp->filp, struct evl_clock);
+		evl_get_element(&clock->element);
+		evl_put_file(sfilp);
+	}
+
+	return clock;
+}
+
+static long restart_clock_delay(struct restart_block *param)
+{
+	return -EINVAL;
+}
+
+static int clock_delay(struct evl_clock *clock,
+		       struct evl_clock_delayreq __user *u_req)
+{
+	struct evl_thread *curr = evl_current_thread();
+	struct evl_clock_delayreq req;
+	struct restart_block *restart;
+	struct timespec remain;
+	ktime_t timeout, rem;
+	int ret;
+
+	ret = raw_copy_from_user(&req, u_req, sizeof(req));
+	if (ret)
+		return -EFAULT;
+
+	if (req.timeout.tv_sec < 0)
+		return -EINVAL;
+
+	if ((unsigned long)req.timeout.tv_nsec >= ONE_BILLION)
+		return -EINVAL;
+
+	if (curr->local_info & T_SYSRST) {
+		curr->local_info &= ~T_SYSRST;
+		restart = &current->restart_block;
+		if (restart->fn != restart_clock_delay) {
+			if (req.remain) {
+				rem = evl_get_stopped_timer_delta(&curr->rtimer);
+				remain = ktime_to_timespec(rem);
+				ret = raw_copy_to_user(req.remain, &remain,
+						       sizeof(remain));
+				if (ret)
+					return -EFAULT;
+			}
+			return -EINTR;
+		}
+		timeout = restart->nanosleep.expires;
+	} else
+		timeout = timespec_to_ktime(req.timeout);
+
+	rem = evl_delay_thread(timeout, EVL_ABS, clock);
+	if (!rem)
+		return 0;
+
+	if (signal_pending(current)) {
+		restart = &current->restart_block;
+		restart->nanosleep.expires = timeout;
+		restart->fn = restart_clock_delay;
+		curr->local_info |= T_SYSRST;
+		return -ERESTARTSYS;
+	}
+
+	return -EINTR;
+}
+
+static int get_clock_resolution(struct evl_clock *clock,
+				struct timespec __user *u_res)
+{
+	struct timespec res;
+
+	res = ktime_to_timespec(evl_get_clock_resolution(clock));
+
+	trace_evl_clock_getres(clock, &res);
+
+	return raw_copy_to_user(u_res, &res, sizeof(res)) ? -EFAULT : 0;
+}
+
+static int get_clock_time(struct evl_clock *clock,
+			  struct timespec __user *u_ts)
+{
+	struct timespec ts;
+
+	ts = ktime_to_timespec(evl_read_clock(clock));
+
+	trace_evl_clock_gettime(clock, &ts);
+
+	return raw_copy_to_user(u_ts, &ts, sizeof(ts)) ? -EFAULT : 0;
+}
+
+static int set_clock_time(struct evl_clock *clock,
+			  struct timespec __user *u_ts)
+{
+	struct timespec ts;
+	int ret;
+
+	ret = raw_copy_from_user(&ts, u_ts, sizeof(ts));
+	if (ret)
+		return -EFAULT;
+
+	if ((unsigned long)ts.tv_nsec >= ONE_BILLION)
+		return -EINVAL;
+
+	trace_evl_clock_settime(clock, &ts);
+
+	return evl_set_clock_time(clock, &ts);
+}
+
+static int adjust_clock_time(struct evl_clock *clock,
+			     struct timex __user *u_tx)
+{
+	struct timex tx;
+	int ret;
+
+	ret = raw_copy_from_user(&tx, u_tx, sizeof(tx));
+	if (ret)
+		return -EFAULT;
+
+	return evl_clock_adjust_time(clock, &tx);
+}
+
+static long clock_common_ioctl(struct evl_clock *clock,
+			       unsigned int cmd, unsigned long arg)
+{
+	int ret;
+
+	switch (cmd) {
+	case EVL_CLKIOC_GET_RES:
+		ret = get_clock_resolution(clock,
+					   (struct timespec __user *)arg);
+		break;
+	case EVL_CLKIOC_GET_TIME:
+		ret = get_clock_time(clock,
+				     (struct timespec __user *)arg);
+		break;
+	case EVL_CLKIOC_SET_TIME:
+		ret = set_clock_time(clock,
+				     (struct timespec __user *)arg);
+		break;
+	case EVL_CLKIOC_ADJ_TIME:
+		ret = adjust_clock_time(clock,
+					(struct timex __user *)arg);
+		break;
+	default:
+		ret = -ENOTTY;
+	}
+
+	return ret;
+}
+
+static long clock_oob_ioctl(struct file *filp, unsigned int cmd,
+			    unsigned long arg)
+{
+	struct evl_clock *clock = element_of(filp, struct evl_clock);
+	int ret;
+
+	switch (cmd) {
+	case EVL_CLKIOC_DELAY:
+		ret = clock_delay(clock,
+				  (struct evl_clock_delayreq __user *)arg);
+		break;
+	default:
+		ret = clock_common_ioctl(clock, cmd, arg);
+	}
+
+	return ret;
+}
+
+static long clock_ioctl(struct file *filp, unsigned int cmd,
+			unsigned long arg)
+{
+	struct evl_clock *clock = element_of(filp, struct evl_clock);
+
+	return clock_common_ioctl(clock, cmd, arg);
+}
+
+static const struct file_operations clock_fops = {
+	.unlocked_ioctl	= clock_ioctl,
+	.oob_ioctl	= clock_oob_ioctl,
+};
+
+/*
+ * Once created, a clock must be deleted by dropping the last
+ * reference to it via a call to evl_put_element(), so that we
+ * remove the factory device and properly synchronize. No direct call
+ * to destroy_clock() except from the element disposal routine,
+ * please.
+ */
+static void destroy_clock(struct evl_clock *clock)
+{
+	struct evl_timerbase *tmb;
+	int cpu;
+
+	inband_context_only();
+
+	/*
+	 * Slave clocks use the timer queues from their master.
+	 */
+	if (clock->master != clock)
+		return;
+
+	for_each_online_cpu(cpu) {
+		tmb = evl_percpu_timers(clock, cpu);
+		EVL_WARN_ON(CORE, !evl_tqueue_is_empty(&tmb->q));
+		evl_destroy_tqueue(&tmb->q);
+	}
+
+	free_percpu(clock->timerdata);
+	mutex_lock(&clocklist_lock);
+	list_del(&clock->next);
+	mutex_unlock(&clocklist_lock);
+
+	evl_destroy_element(&clock->element);
+
+	if (clock->dispose)
+		clock->dispose(clock);
+}
+
+static void clock_factory_dispose(struct evl_element *e)
+{
+	struct evl_clock *clock;
+
+	clock = container_of(e, struct evl_clock, element);
+	destroy_clock(clock);
+}
+
+static ssize_t gravity_show(struct device *dev,
+			    struct device_attribute *attr,
+			    char *buf)
+{
+	struct evl_clock *clock;
+	ssize_t ret;
+
+	clock = evl_get_element_by_dev(dev, struct evl_clock);
+	ret = snprintf(buf, PAGE_SIZE, "%Ldi %Ldk %Ldu\n",
+		       ktime_to_ns(evl_get_clock_gravity(clock, irq)),
+		       ktime_to_ns(evl_get_clock_gravity(clock, kernel)),
+		       ktime_to_ns(evl_get_clock_gravity(clock, user)));
+	evl_put_element(&clock->element);
+
+	return ret;
+}
+
+static ssize_t gravity_store(struct device *dev,
+			     struct device_attribute *attr,
+			     const char *buf, size_t count)
+{
+	struct evl_clock_gravity gravity;
+	struct evl_clock *clock;
+	char *dups, *args, *p;
+	ssize_t ret;
+	long ns;
+
+	if (!*buf)
+		return 0;
+
+	dups = args = kstrdup(buf, GFP_KERNEL);
+
+	clock = evl_get_element_by_dev(dev, struct evl_clock);
+
+	gravity = clock->gravity;
+
+	while ((p = strsep(&args, " \t:/,")) != NULL) {
+		if (*p == '\0')
+			continue;
+		ns = simple_strtol(p, &p, 10);
+		switch (*p) {
+		case 'i':
+			gravity.irq = ns;
+			break;
+		case 'k':
+			gravity.kernel = ns;
+			break;
+		case 'u':
+		case '\0':
+			gravity.user = ns;
+			break;
+		default:
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	ret = evl_set_clock_gravity(clock, &gravity) ?: count;
+out:
+	evl_put_element(&clock->element);
+
+	kfree(dups);
+
+	return ret;
+}
+
+static DEVICE_ATTR_RW(gravity);
+
+static struct attribute *clock_attrs[] = {
+	&dev_attr_gravity.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(clock);
+
+struct evl_factory evl_clock_factory = {
+	.name	=	"clock",
+	.fops	=	&clock_fops,
+	.nrdev	=	CONFIG_EVENLESS_NR_CLOCKS,
+	.attrs	=	clock_groups,
+	.dispose =	clock_factory_dispose,
+};
+
+static int set_coreclk_gravity(struct evl_clock *clock,
+			       const struct evl_clock_gravity *p)
+{
+	clock->gravity = *p;
+
+	return 0;
+}
+
+static void get_default_gravity(struct evl_clock_gravity *p)
+{
+	unsigned int ulat = 4000; /* ns */
+
+#if CONFIG_EVENLESS_TIMING_SCHEDLAT != 0
+	ulat = CONFIG_XENO_OPT_TIMING_SCHEDLAT;
+#endif
+	p->user = ulat;
+	p->kernel = CONFIG_EVENLESS_TIMING_KSCHEDLAT;
+	p->irq = CONFIG_EVENLESS_TIMING_IRQLAT;
+}
+
+static void reset_coreclk_gravity(struct evl_clock *clock)
+{
+	struct evl_clock_gravity gravity;
+
+	get_default_gravity(&gravity);
+
+	if (gravity.kernel == 0)
+		gravity.kernel = gravity.user;
+
+	set_coreclk_gravity(clock, &gravity);
+}
+
+static ktime_t read_mono_clock(struct evl_clock *clock)
+{
+	return evl_ktime_monotonic();
+}
+
+static u64 read_mono_clock_cycles(struct evl_clock *clock)
+{
+	return read_mono_clock(clock);
+}
+
+static ktime_t read_realtime_clock(struct evl_clock *clock)
+{
+	return ktime_get_real_fast();
+}
+
+static u64 read_realtime_clock_cycles(struct evl_clock *clock)
+{
+	return read_realtime_clock(clock);
+}
+
+static void adjust_realtime_clock(struct evl_clock *clock)
+{
+	ktime_t old_offset = clock->offset;
+
+	clock->offset = evl_read_clock(clock) -
+		evl_read_clock(&evl_mono_clock);
+
+	evl_adjust_timers(clock, clock->offset - old_offset);
+}
+
+struct evl_clock evl_mono_clock = {
+	.name = "monotonic",
+	.resolution = 1,	/* nanosecond. */
+	.ops = {
+		.read = read_mono_clock,
+		.read_cycles = read_mono_clock_cycles,
+		.program_local_shot = evl_program_proxy_tick,
+#ifdef CONFIG_SMP
+		.program_remote_shot = evl_send_timer_ipi,
+#endif
+		.set_gravity = set_coreclk_gravity,
+		.reset_gravity = reset_coreclk_gravity,
+	},
+};
+EXPORT_SYMBOL_GPL(evl_mono_clock);
+
+struct evl_clock evl_realtime_clock = {
+	.name = "realtime",
+	.resolution = 1,	/* nanosecond. */
+	.ops = {
+		.read = read_realtime_clock,
+		.read_cycles = read_realtime_clock_cycles,
+		.set_gravity = set_coreclk_gravity,
+		.reset_gravity = reset_coreclk_gravity,
+		.adjust = adjust_realtime_clock,
+	},
+};
+EXPORT_SYMBOL_GPL(evl_realtime_clock);
+
+int __init evl_clock_init(void)
+{
+	int ret;
+
+	evl_reset_clock_gravity(&evl_mono_clock);
+	evl_reset_clock_gravity(&evl_realtime_clock);
+
+	ret = evl_init_clock(&evl_mono_clock,
+			     &evl_oob_cpus);
+	if (ret)
+		return ret;
+
+	ret = evl_init_slave_clock(&evl_realtime_clock,
+				   &evl_mono_clock);
+	if (ret)
+		evl_put_element(&evl_mono_clock.element);
+
+	return ret;
+}
+
+void __init evl_clock_cleanup(void)
+{
+	evl_put_element(&evl_realtime_clock.element);
+	evl_put_element(&evl_mono_clock.element);
+}
diff --git a/kernel/evenless/control.c b/kernel/evenless/control.c
new file mode 100644
index 000000000000..3e2620e47737
--- /dev/null
+++ b/kernel/evenless/control.c
@@ -0,0 +1,212 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/types.h>
+#include <linux/mm.h>
+#include <evenless/memory.h>
+#include <evenless/thread.h>
+#include <evenless/factory.h>
+#include <evenless/tick.h>
+#include <evenless/control.h>
+#include <asm/evenless/syscall.h>
+#include <asm/evenless/fptest.h>
+#include <uapi/evenless/control.h>
+
+static BLOCKING_NOTIFIER_HEAD(state_notifier_list);
+
+atomic_t evl_runstate = ATOMIC_INIT(EVL_STATE_WARMUP);
+EXPORT_SYMBOL_GPL(evl_runstate);
+
+void evl_add_state_chain(struct notifier_block *nb)
+{
+	blocking_notifier_chain_register(&state_notifier_list, nb);
+}
+EXPORT_SYMBOL_GPL(evl_add_state_chain);
+
+void evl_remove_state_chain(struct notifier_block *nb)
+{
+	blocking_notifier_chain_unregister(&state_notifier_list, nb);
+}
+EXPORT_SYMBOL_GPL(evl_remove_state_chain);
+
+static inline void call_state_chain(enum evl_run_states newstate)
+{
+	blocking_notifier_call_chain(&state_notifier_list, newstate, NULL);
+}
+
+static int start_services(void)
+{
+	enum evl_run_states state;
+	int ret = 0;
+
+	state = atomic_cmpxchg(&evl_runstate,
+			       EVL_STATE_STOPPED,
+			       EVL_STATE_WARMUP);
+	switch (state) {
+	case EVL_STATE_RUNNING:
+		break;
+	case EVL_STATE_STOPPED:
+		ret = evl_enable_tick();
+		if (ret) {
+			atomic_set(&evl_runstate, EVL_STATE_STOPPED);
+			return ret;
+		}
+		call_state_chain(EVL_STATE_WARMUP);
+		set_evl_state(EVL_STATE_RUNNING);
+		printk(EVL_INFO "core started\n");
+		break;
+	default:
+		ret = -EINPROGRESS;
+	}
+
+	return ret;
+}
+
+static int stop_services(void)
+{
+	enum evl_run_states state;
+	int ret = 0;
+
+	state = atomic_cmpxchg(&evl_runstate,
+			       EVL_STATE_RUNNING,
+			       EVL_STATE_TEARDOWN);
+	switch (state) {
+	case EVL_STATE_STOPPED:
+		break;
+	case EVL_STATE_RUNNING:
+		ret = evl_killall(T_USER);
+		if (ret) {
+			set_evl_state(state);
+			return ret;
+		}
+		call_state_chain(EVL_STATE_TEARDOWN);
+		ret = evl_killall(0);
+		evl_disable_tick();
+		set_evl_state(EVL_STATE_STOPPED);
+		printk(EVL_INFO "core stopped\n");
+		break;
+	default:
+		ret = -EINPROGRESS;
+	}
+
+	return ret;
+}
+
+static long control_ioctl(struct file *filp, unsigned int cmd,
+			  unsigned long arg)
+{
+	struct evl_core_info info;
+	long ret;
+
+	/*
+	 * NOTE: OOB <-> in-band switching services can only apply to
+	 * the current thread, which should not need a file descriptor
+	 * on its own element for issuing them. The control device is
+	 * the right channel for requesting such services.
+	 */
+
+	switch (cmd) {
+	case EVL_CTLIOC_GET_COREINFO:
+		info.abi_level = EVL_ABI_LEVEL;
+		info.fpu_features = evl_detect_fpu();
+		info.shm_size = evl_shm_size;
+		ret = raw_copy_to_user((struct evl_core_info __user *)arg,
+				       &info, sizeof(info)) ? -EFAULT : 0;
+		break;
+	case EVL_CTLIOC_SWITCH_OOB:
+		ret = evl_switch_oob();
+		break;
+	case EVL_CTLIOC_SWITCH_INBAND:
+		/*
+		 * We already switched an OOB caller to inband mode as
+		 * a result of handling this ioctl() call. Yummie.
+		 */
+		ret = 0;
+		break;
+	case EVL_CTLIOC_DETACH_SELF:
+		ret = evl_detach_self();
+		break;
+	default:
+		ret = -ENOTTY;
+	}
+
+	return ret;
+}
+
+static int control_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	void *p = evl_get_heap_base(&evl_shared_heap);
+	unsigned long pfn = __pa(p) >> PAGE_SHIFT;
+	size_t len = vma->vm_end - vma->vm_start;
+
+	if (len != evl_shm_size)
+		return -EINVAL;
+
+	return remap_pfn_range(vma, vma->vm_start, pfn, len, PAGE_SHARED);
+}
+
+static const struct file_operations control_fops = {
+	.unlocked_ioctl	=	control_ioctl,
+	.mmap		=	control_mmap,
+};
+
+static const char *state_labels[] = {
+	[EVL_STATE_DISABLED] = "disabled",
+	[EVL_STATE_RUNNING] = "running",
+	[EVL_STATE_STOPPED] = "stopped",
+	[EVL_STATE_TEARDOWN] = "teardown",
+	[EVL_STATE_WARMUP] = "warmup",
+};
+
+static ssize_t state_show(struct device *dev,
+			  struct device_attribute *attr,
+			  char *buf)
+{
+	int st = atomic_read(&evl_runstate);
+
+	return snprintf(buf, PAGE_SIZE, "%s\n", state_labels[st]);
+}
+
+static ssize_t state_store(struct device *dev,
+			   struct device_attribute *attr,
+			   const char *buf, size_t count)
+{
+	size_t len = count;
+
+	if (len && buf[len - 1] == '\n')
+		len--;
+
+	if (!strncmp(buf, "start", len))
+		return start_services() ?: count;
+
+	if (!strncmp(buf, "stop", len))
+		return stop_services() ?: count;
+
+	return -EINVAL;
+}
+static DEVICE_ATTR_RW(state);
+
+static ssize_t abi_show(struct device *dev,
+			struct device_attribute *attr,
+			char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", EVL_ABI_LEVEL);
+}
+static DEVICE_ATTR_RO(abi);
+
+static struct attribute *control_attrs[] = {
+	&dev_attr_state.attr,
+	&dev_attr_abi.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(control);
+
+struct evl_factory evl_control_factory = {
+	.name	=	"control",
+	.fops	=	&control_fops,
+	.attrs	=	control_groups,
+	.flags	=	EVL_FACTORY_SINGLE,
+};
diff --git a/kernel/evenless/factory.c b/kernel/evenless/factory.c
new file mode 100644
index 000000000000..2512f2bc896d
--- /dev/null
+++ b/kernel/evenless/factory.c
@@ -0,0 +1,673 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/init.h>
+#include <linux/device.h>
+#include <linux/kernel.h>
+#include <linux/bitmap.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/module.h>
+#include <linux/rcupdate.h>
+#include <linux/irq_work.h>
+#include <linux/uaccess.h>
+#include <evenless/assert.h>
+#include <evenless/file.h>
+#include <evenless/control.h>
+#include <evenless/syscall.h>
+#include <evenless/factory.h>
+#include <uapi/evenless/factory.h>
+
+static struct class *evl_class;
+
+static struct evl_factory *early_factories[] = {
+	&evl_clock_factory,
+};
+
+static struct evl_factory *factories[] = {
+	&evl_control_factory,
+	&evl_thread_factory,
+	&evl_monitor_factory,
+	&evl_sem_factory,
+	&evl_timerfd_factory,
+	&evl_poller_factory,
+	&evl_xbuf_factory,
+	&evl_logger_factory,
+#ifdef CONFIG_FTRACE
+	&evl_trace_factory,
+#endif
+};
+
+#define NR_FACTORIES	\
+	(ARRAY_SIZE(early_factories) + ARRAY_SIZE(factories))
+
+static dev_t factory_rdev;
+
+int evl_init_element(struct evl_element *e, struct evl_factory *fac)
+{
+	int minor;
+
+	do {
+		minor = find_first_zero_bit(fac->minor_map, fac->nrdev);
+		if (minor >= fac->nrdev) {
+			printk_ratelimited(EVL_WARNING "out of %ss",
+					   fac->name);
+			return -EAGAIN;
+		}
+	} while (test_and_set_bit(minor, fac->minor_map));
+
+	e->factory = fac;
+	e->minor = minor;
+	e->refs = 1;
+	e->zombie = false;
+	e->fundle = EVL_NO_HANDLE;
+	e->devname = NULL;
+	raw_spin_lock_init(&e->ref_lock);
+
+	return 0;
+}
+
+void evl_destroy_element(struct evl_element *e)
+{
+	clear_bit(e->minor, e->factory->minor_map);
+	if (e->devname)
+		putname(e->devname);
+}
+
+void evl_get_element(struct evl_element *e)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&e->ref_lock, flags);
+	e->refs++;
+	raw_spin_unlock_irqrestore(&e->ref_lock, flags);
+}
+
+int evl_open_element(struct inode *inode, struct file *filp)
+{
+	struct evl_file_binding *fbind;
+	struct evl_element *e;
+	unsigned long flags;
+	int ret = 0;
+
+	e = container_of(inode->i_cdev, struct evl_element, cdev);
+
+	rcu_read_lock();
+
+	raw_spin_lock_irqsave(&e->ref_lock, flags);
+
+	if (e->zombie)
+		ret = -ESTALE;
+	else
+		e->refs++;
+
+	raw_spin_unlock_irqrestore(&e->ref_lock, flags);
+
+	rcu_read_unlock();
+
+	if (ret)
+		return ret;
+
+	fbind = kmalloc(sizeof(*fbind), GFP_KERNEL);
+	if (fbind == NULL)
+		return -ENOMEM;
+
+	ret = evl_open_file(&fbind->efile, filp);
+	if (ret) {
+		kfree(fbind);
+		return ret;
+	}
+
+	fbind->element = e;
+	filp->private_data = fbind;
+
+	return 0;
+}
+
+static void __put_element(struct evl_element *e)
+{
+	struct evl_factory *fac = e->factory;
+
+	/*
+	 * e->minor won't be free for use until evl_destroy_element()
+	 * is called from the disposal handler, so there is no risk of
+	 * reusing it too early.
+	 */
+	evl_remove_element_device(e);
+	/*
+	 * Serialize with evl_open_element().
+	 */
+	synchronize_rcu();
+	/*
+	 * CAUTION: the disposal handler should delay the release of
+	 * e's container at the next rcu idle period via kfree_rcu(),
+	 * because the embedded e->cdev is still needed ahead for
+	 * completing the file release process (see __fput()).
+	 */
+	fac->dispose(e);
+}
+
+static void put_element_work(struct work_struct *work)
+{
+	struct evl_element *e;
+
+	e = container_of(work, struct evl_element, work);
+	__put_element(e);
+}
+
+static void put_element_irq(struct irq_work *work)
+{
+	struct evl_element *e;
+
+	e = container_of(work, struct evl_element, irq_work);
+	INIT_WORK(&e->work, put_element_work);
+	schedule_work(&e->work);
+}
+
+static void put_element(struct evl_element *e)
+{
+	/*
+	 * These trampolines may look like a bit cheesy but we have no
+	 * choice but offloading the disposal to an in-band task
+	 * context. In (the rare) case the last ref. to an element was
+	 * dropped from OOB context, we need to go via an
+	 * irq_work->workqueue chain in order to run __put_element()
+	 * eventually.
+	 */
+	if (unlikely(running_oob())) {
+		init_irq_work(&e->irq_work, put_element_irq);
+		irq_work_queue(&e->irq_work);
+	} else
+		__put_element(e);
+}
+
+void evl_put_element(struct evl_element *e) /* in-band or OOB */
+{
+	unsigned long flags;
+
+	/*
+	 * Multiple files may reference a single element on
+	 * open(). The element release logic competes with
+	 * evl_open_element() as follows:
+	 *
+	 * a) evl_put_element() grabs the ->ref_lock first and raises
+	 * the zombie flag iff the refcount drops to zero, or
+	 * evl_open_element() gets it first.
+
+	 * b) evl_open_element() races with evl_put_element() and
+	 * detects an ongoing deletion of @ent, returning -ESTALE.
+
+	 * c) evl_open_element() is first and increments the refcount
+	 * which should lead us to skip the whole release process
+	 * in evl_put_element() when it runs next.
+	 *
+	 * In any case, evl_open_element() is protected against
+	 * referencing stale @ent memory by a read-side RCU
+	 * section. Meanwhile we wait for all read-sides to complete
+	 * after calling cdev_del().  Once cdev_del() returns, the
+	 * device cannot be opened anymore, without affecting the
+	 * files that might still be opened on this device though.
+	 *
+	 * In the c) case, the last file release will dispose of the
+	 * element eventually.
+	 */
+	raw_spin_lock_irqsave(&e->ref_lock, flags);
+
+	if (--e->refs == 0) {
+		e->zombie = true;
+		raw_spin_unlock_irqrestore(&e->ref_lock, flags);
+		put_element(e);
+	} else
+		raw_spin_unlock_irqrestore(&e->ref_lock, flags);
+}
+
+int evl_close_element(struct inode *inode, struct file *filp)
+{
+	struct evl_file_binding *fbind = filp->private_data;
+	struct evl_element *e = fbind->element;
+
+	evl_release_file(&fbind->efile);
+	kfree(fbind);
+	evl_put_element(e);
+
+	return 0;
+}
+
+int evl_create_element_device(struct evl_element *e,
+			      struct evl_factory *fac,
+			      const char *devname)
+{
+	struct device *dev;
+	dev_t rdev;
+	int ret;
+
+	rdev = MKDEV(MAJOR(fac->sub_rdev), e->minor);
+	cdev_init(&e->cdev, fac->fops);
+	ret = cdev_add(&e->cdev, rdev, 1);
+	if (ret)
+		return ret;
+
+	dev = device_create(fac->class, NULL, rdev, e, "%s", devname);
+	if (IS_ERR(dev)) {
+		ret = PTR_ERR(dev);
+		goto fail_dev;
+	}
+
+	if (fac->attrs) {
+		ret = device_add_groups(dev, fac->attrs);
+		if (ret)
+			goto fail_groups;
+	}
+
+	return 0;
+
+fail_groups:
+	device_destroy(fac->class, rdev);
+fail_dev:
+	cdev_del(&e->cdev);
+
+	return ret;
+}
+
+void evl_remove_element_device(struct evl_element *e)
+{
+	struct evl_factory *fac = e->factory;
+	dev_t rdev;
+
+	rdev = MKDEV(MAJOR(fac->sub_rdev), e->minor);
+	device_destroy(fac->class, rdev);
+	cdev_del(&e->cdev);
+}
+
+static long ioctl_clone_device(struct file *filp, unsigned int cmd,
+			       unsigned long arg)
+{
+	struct evl_element *e = filp->private_data;
+	struct evl_clone_req req, __user *u_req;
+	struct filename *devname = NULL;
+	__u32 val, state_offset = -1U;
+	struct evl_factory *fac;
+	char tmpbuf[16];
+	int ret;
+
+	if (cmd != EVL_IOC_CLONE)
+		return -ENOTTY;
+
+	if (!evl_is_running())
+		return -EAGAIN;
+
+	if (e)
+		return -EBUSY;
+
+	u_req = (typeof(u_req))arg;
+	ret = copy_from_user(&req, u_req, sizeof(req));
+	if (ret)
+		return -EFAULT;
+
+	if (req.name) {
+		devname = getname(req.name);
+		if (IS_ERR(devname))
+			return PTR_ERR(devname);
+	}
+
+	fac = container_of(filp->f_inode->i_cdev, struct evl_factory, cdev);
+	e = fac->build(fac, devname ? devname->name : NULL,
+		       req.attrs, &state_offset);
+	if (IS_ERR(e)) {
+		if (devname)
+			putname(devname);
+		return PTR_ERR(e);
+	}
+
+	if (!devname) {
+		/* If no name specified, default to device minor. */
+		snprintf(tmpbuf, sizeof(tmpbuf), "%d", e->minor);
+		devname = getname_kernel(tmpbuf);
+		if (IS_ERR(devname)) {
+			fac->dispose(e);
+			return PTR_ERR(devname);
+		}
+	}
+
+	/* The device name is valid throughout the element's lifetime. */
+	e->devname = devname;
+
+	/* This must be set before the device appears. */
+	filp->private_data = e;
+	barrier();
+
+	ret = evl_create_element_device(e, fac, devname->name);
+	if (ret) {
+		/* release_clone_device() must skip cleanup. */
+		filp->private_data = NULL;
+		fac->dispose(e);
+		return ret;
+	}
+
+	val = e->minor;
+	ret |= put_user(val, &u_req->eids.minor) ? -EFAULT : 0;
+	val = e->fundle;
+	ret |= put_user(val, &u_req->eids.fundle) ? -EFAULT : 0;
+	ret |= put_user(state_offset, &u_req->eids.state_offset) ? -EFAULT : 0;
+
+	return ret ? -EFAULT : 0;
+}
+
+static int release_clone_device(struct inode *inode, struct file *filp)
+{
+	struct evl_element *e = filp->private_data;
+
+	if (e)
+		evl_put_element(e);
+
+	return 0;
+}
+
+static const struct file_operations clone_fops = {
+	.unlocked_ioctl	= ioctl_clone_device,
+	.release	= release_clone_device,
+};
+
+static int index_element_at(struct evl_element *e, fundle_t fundle)
+{
+	struct evl_index *map = &e->factory->index;
+	struct rb_node **rbp, *parent;
+	struct evl_element *tmp;
+
+	parent = NULL;
+	rbp = &map->root.rb_node;
+	while (*rbp) {
+		tmp = rb_entry(*rbp, struct evl_element, index_node);
+		parent = *rbp;
+		if (fundle < tmp->fundle)
+			rbp = &(*rbp)->rb_left;
+		else if (fundle > tmp->fundle)
+			rbp = &(*rbp)->rb_right;
+		else
+			return -EEXIST;
+	}
+
+	e->fundle = fundle;
+	rb_link_node(&e->index_node, parent, rbp);
+	rb_insert_color(&e->index_node, &map->root);
+
+	return 0;
+}
+
+int evl_index_element_at(struct evl_element *e, fundle_t fundle)
+{
+	struct evl_index *map = &e->factory->index;
+	unsigned long flags;
+	int ret;
+
+	raw_spin_lock_irqsave(&map->lock, flags);
+	ret = index_element_at(e, fundle);
+	raw_spin_unlock_irqrestore(&map->lock, flags);
+
+	return ret;
+}
+
+void evl_index_element(struct evl_element *e)
+{
+	struct evl_index *map = &e->factory->index;
+	fundle_t fundle, guard = 0;
+	unsigned long flags;
+	int ret;
+
+	do {
+		if (evl_get_index(++guard) == 0) { /* Paranoid. */
+			e->fundle = EVL_NO_HANDLE;
+			WARN_ON_ONCE("out of fundle index space");
+			return;
+		}
+
+		raw_spin_lock_irqsave(&map->lock, flags);
+
+		fundle = evl_get_index(++map->generator);
+		if (!fundle)		/* Exclude EVL_NO_HANDLE */
+			fundle = map->generator = 1;
+
+		ret = index_element_at(e, fundle);
+
+		raw_spin_unlock_irqrestore(&map->lock, flags);
+	} while (ret);
+}
+
+void evl_unindex_element(struct evl_element *e)
+{
+	struct evl_index *map = &e->factory->index;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&map->lock, flags);
+	rb_erase(&e->index_node, &map->root);
+	raw_spin_unlock_irqrestore(&map->lock, flags);
+}
+
+struct evl_element *
+__evl_get_element_by_fundle(struct evl_factory *fac, fundle_t fundle)
+{
+	struct evl_index *map = &fac->index;
+	struct evl_element *e;
+	unsigned long flags;
+	struct rb_node *rb;
+
+	raw_spin_lock_irqsave(&map->lock, flags);
+
+	rb = map->root.rb_node;
+	while (rb) {
+		e = rb_entry(rb, struct evl_element, index_node);
+		if (fundle < e->fundle)
+			rb = rb->rb_left;
+		else if (fundle > e->fundle)
+			rb = rb->rb_right;
+		else {
+			raw_spin_lock(&e->ref_lock);
+			if (unlikely(e->zombie))
+				e = NULL;
+			else
+				e->refs++;
+			raw_spin_unlock(&e->ref_lock);
+			raw_spin_unlock_irqrestore(&map->lock, flags);
+			return e;
+		}
+	}
+
+	raw_spin_unlock_irqrestore(&map->lock, flags);
+
+	return NULL;
+}
+
+static char *factory_devnode(struct device *dev, umode_t *mode)
+{
+	return kasprintf(GFP_KERNEL, "evenless/%s/%s",
+			 dev->class->name, dev_name(dev));
+}
+
+static int create_element_class(struct evl_factory *fac)
+{
+	struct class *class;
+	int ret = -ENOMEM;
+
+	fac->minor_map = bitmap_zalloc(fac->nrdev, GFP_KERNEL);
+	if (fac->minor_map == NULL)
+		return ret;
+
+	class = class_create(THIS_MODULE, fac->name);
+	if (IS_ERR(class)) {
+		ret = PTR_ERR(class);
+		goto cleanup_minor;
+	}
+
+	class->devnode = factory_devnode;
+
+	ret = alloc_chrdev_region(&fac->sub_rdev, 0, fac->nrdev, fac->name);
+	if (ret)
+		goto cleanup_class;
+
+	fac->class = class;
+
+	return 0;
+
+cleanup_class:
+	class_destroy(class);
+cleanup_minor:
+	bitmap_free(fac->minor_map);
+
+	return ret;
+}
+
+static void delete_element_class(struct evl_factory *fac)
+{
+	unregister_chrdev_region(fac->sub_rdev, fac->nrdev);
+	class_destroy(fac->class);
+	bitmap_free(fac->minor_map);
+}
+
+static int create_factory(struct evl_factory *fac, dev_t rdev)
+{
+	const char *idevname = "clone"; /* Initial device in factory. */
+	struct device *dev;
+	int ret;
+
+	if (fac->flags & EVL_FACTORY_SINGLE) {
+		idevname = fac->name;
+		fac->class = evl_class;
+		fac->minor_map = NULL;
+		fac->sub_rdev = MKDEV(0, 0);
+		cdev_init(&fac->cdev, fac->fops);
+	} else {
+		ret = create_element_class(fac);
+		if (ret)
+			return ret;
+		if (fac->flags & EVL_FACTORY_CLONE)
+			cdev_init(&fac->cdev, &clone_fops);
+	}
+
+	if (fac->flags & (EVL_FACTORY_CLONE|EVL_FACTORY_SINGLE)) {
+		ret = cdev_add(&fac->cdev, rdev, 1);
+		if (ret)
+			goto fail_cdev;
+
+		dev = device_create(fac->class, NULL, rdev, fac, idevname);
+		if (IS_ERR(dev))
+			goto fail_dev;
+
+		if ((fac->flags & EVL_FACTORY_SINGLE) && fac->attrs) {
+			ret = device_add_groups(dev, fac->attrs);
+			if (ret)
+				goto fail_groups;
+		}
+	}
+
+	fac->rdev = rdev;
+	raw_spin_lock_init(&fac->index.lock);
+	fac->index.root = RB_ROOT;
+	fac->index.generator = EVL_NO_HANDLE;
+
+	return 0;
+
+fail_groups:
+	device_destroy(fac->class, rdev);
+fail_dev:
+	cdev_del(&fac->cdev);
+fail_cdev:
+	if (!(fac->flags & EVL_FACTORY_SINGLE))
+		delete_element_class(fac);
+
+	return ret;
+}
+
+static void delete_factory(struct evl_factory *fac)
+{
+	device_destroy(fac->class, fac->rdev);
+
+	if (fac->flags & (EVL_FACTORY_CLONE|EVL_FACTORY_SINGLE))
+		cdev_del(&fac->cdev);
+
+	if (!(fac->flags & EVL_FACTORY_SINGLE))
+		delete_element_class(fac);
+}
+
+static char *evl_devnode(struct device *dev, umode_t *mode)
+{
+	return kasprintf(GFP_KERNEL, "evenless/%s", dev_name(dev));
+}
+
+static int __init
+create_factories(struct evl_factory **factories, int nr)
+{
+	int ret, n;
+
+	for (n = 0; n < nr; n++) {
+		ret = create_factory(factories[n],
+				     MKDEV(MAJOR(factory_rdev), n));
+		if (ret)
+			goto fail;
+	}
+
+	return 0;
+fail:
+	while (n-- > 0)
+		delete_factory(factories[n]);
+
+	return ret;
+}
+
+static void __init
+remove_factories(struct evl_factory **factories, int nr)
+{
+	int n;
+
+	for (n = 0; n < nr; n++)
+		delete_factory(factories[n]);
+}
+
+int __init evl_early_init_factories(void)
+{
+	int ret;
+
+	evl_class = class_create(THIS_MODULE, "evenless");
+	if (IS_ERR(evl_class))
+		return PTR_ERR(evl_class);
+
+	evl_class->devnode = evl_devnode;
+
+	ret = alloc_chrdev_region(&factory_rdev, 0, NR_FACTORIES,
+				  "evenless_factory");
+	if (ret) {
+		class_destroy(evl_class);
+		return ret;
+	}
+
+	ret = create_factories(early_factories,
+			       ARRAY_SIZE(early_factories));
+	if (ret) {
+		unregister_chrdev_region(factory_rdev, NR_FACTORIES);
+		class_destroy(evl_class);
+	}
+
+	return ret;
+}
+
+void __init evl_early_cleanup_factories(void)
+{
+	remove_factories(early_factories, ARRAY_SIZE(early_factories));
+	unregister_chrdev_region(factory_rdev, NR_FACTORIES);
+	class_destroy(evl_class);
+}
+
+int __init evl_late_init_factories(void)
+{
+	return create_factories(factories, ARRAY_SIZE(factories));
+}
+
+void __init evl_cleanup_factories(void)
+{
+	remove_factories(factories, ARRAY_SIZE(factories));
+	evl_early_cleanup_factories();
+}
diff --git a/kernel/evenless/file.c b/kernel/evenless/file.c
new file mode 100644
index 000000000000..dff5dad18f6d
--- /dev/null
+++ b/kernel/evenless/file.c
@@ -0,0 +1,255 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <stdarg.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/completion.h>
+#include <linux/irq_work.h>
+#include <evenless/file.h>
+#include <evenless/assert.h>
+#include <evenless/sched.h>
+
+static struct kmem_cache *fd_cache;
+
+static struct rb_root fd_tree = RB_ROOT;
+
+static DEFINE_HARD_SPINLOCK(fdt_lock);
+
+/*
+ * We could have a per-files_struct table of OOB fds, but this looks
+ * overkill at the moment. So we only have a single rb-tree for now,
+ * indexing our file descriptors on a composite key which pairs the
+ * the in-band fd and the originating files struct pointer.
+ */
+
+static inline bool lean_left(struct evl_fd *lh, struct evl_fd *rh)
+{
+	if (lh->files == rh->files)
+		return lh->fd < rh->fd;
+
+	return lh->files < rh->files;
+}
+
+static inline bool lean_right(struct evl_fd *lh, struct evl_fd *rh)
+{
+	if (lh->files == rh->files)
+		return lh->fd > rh->fd;
+
+	return lh->files > rh->files;
+}
+
+static inline int index_sfd(struct evl_fd *sfd, struct file *filp)
+{
+	struct rb_node **rbp, *parent = NULL;
+	struct evl_fd *tmp;
+
+	rbp = &fd_tree.rb_node;
+	while (*rbp) {
+		tmp = rb_entry(*rbp, struct evl_fd, rb);
+		parent = *rbp;
+		if (lean_left(sfd, tmp))
+			rbp = &(*rbp)->rb_left;
+		else if (lean_right(sfd, tmp))
+			rbp = &(*rbp)->rb_right;
+		else
+			return -EEXIST;
+	}
+
+	rb_link_node(&sfd->rb, parent, rbp);
+	rb_insert_color(&sfd->rb, &fd_tree);
+
+	return 0;
+}
+
+static inline
+struct evl_fd *lookup_sfd(unsigned int fd,
+			  struct files_struct *files)
+{
+	struct evl_fd *sfd, tmp;
+	struct rb_node *rb;
+
+	tmp.fd = fd;
+	tmp.files = files;
+	rb = fd_tree.rb_node;
+	while (rb) {
+		sfd = rb_entry(rb, struct evl_fd, rb);
+		if (lean_left(&tmp, sfd))
+			rb = rb->rb_left;
+		else if (lean_right(&tmp, sfd))
+			rb = rb->rb_right;
+		else
+			return sfd;
+	}
+
+	return NULL;
+}
+
+static inline
+struct evl_fd *unindex_sfd(unsigned int fd,
+			   struct files_struct *files)
+{
+	struct evl_fd *sfd = lookup_sfd(fd, files);
+
+	if (sfd)
+		rb_erase(&sfd->rb, &fd_tree);
+
+	return sfd;
+}
+
+void install_inband_fd(unsigned int fd, struct file *filp,
+		       struct files_struct *files) /* in-band */
+{
+	struct evl_fd *sfd;
+	unsigned long flags;
+	int ret = -ENOMEM;
+
+	if (filp->oob_data == NULL)
+		return;
+
+	sfd = kmem_cache_alloc(fd_cache, GFP_KERNEL);
+	if (sfd) {
+		sfd->fd = fd;
+		sfd->files = files;
+		sfd->sfilp = filp->oob_data;
+		raw_spin_lock_irqsave(&fdt_lock, flags);
+		ret = index_sfd(sfd, filp);
+		raw_spin_unlock_irqrestore(&fdt_lock, flags);
+	}
+
+	EVL_WARN_ON(CORE, ret);
+}
+
+void uninstall_inband_fd(unsigned int fd, struct file *filp,
+			 struct files_struct *files) /* in-band */
+{
+	struct evl_fd *sfd;
+	unsigned long flags;
+
+	if (filp->oob_data == NULL)
+		return;
+
+	raw_spin_lock_irqsave(&fdt_lock, flags);
+	sfd = unindex_sfd(fd, files);
+	raw_spin_unlock_irqrestore(&fdt_lock, flags);
+
+	if (sfd)
+		kmem_cache_free(fd_cache, sfd);
+}
+
+void replace_inband_fd(unsigned int fd, struct file *filp,
+		       struct files_struct *files) /* in-band */
+{
+	struct evl_fd *sfd;
+	unsigned long flags;
+
+	if (filp->oob_data == NULL)
+		return;
+
+	raw_spin_lock_irqsave(&fdt_lock, flags);
+
+	sfd = lookup_sfd(fd, files);
+	if (sfd) {
+		sfd->sfilp = filp->oob_data;
+		raw_spin_unlock_irqrestore(&fdt_lock, flags);
+		return;
+	}
+
+	raw_spin_unlock_irqrestore(&fdt_lock, flags);
+
+	install_inband_fd(fd, filp, files);
+}
+
+struct evl_file *evl_get_file(unsigned int fd) /* OOB */
+{
+	struct evl_file *sfilp = NULL;
+	struct evl_fd *sfd;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&fdt_lock, flags);
+	sfd = lookup_sfd(fd, current->files);
+	if (sfd) {
+		sfilp = sfd->sfilp;
+		evl_get_fileref(sfilp);
+	}
+	raw_spin_unlock_irqrestore(&fdt_lock, flags);
+
+	return sfilp;
+}
+
+static void release_oob_ref(struct irq_work *work)
+{
+	struct evl_file *sfilp;
+
+	sfilp = container_of(work, struct evl_file, oob_work);
+	complete(&sfilp->oob_done);
+}
+
+void __evl_put_file(struct evl_file *sfilp)
+{
+	init_irq_work(&sfilp->oob_work, release_oob_ref);
+	irq_work_queue(&sfilp->oob_work);
+}
+
+/**
+ * evl_open_file - Open new file with OOB capabilities
+ *
+ * Called by chrdev with OOB capabilities when a new @sfilp is
+ * opened. @sfilp is paired with the in-band file struct at @filp.
+ */
+int evl_open_file(struct evl_file *sfilp, struct file *filp)
+{
+	sfilp->filp = filp;
+	filp->oob_data = sfilp;	/* mark filp as OOB-capable. */
+	atomic_set(&sfilp->oob_refs, 1);
+	init_completion(&sfilp->oob_done);
+
+	return 0;
+}
+
+/**
+ * evl_release_file - Drop an OOB-capable file
+ *
+ * Called by chrdev with OOB capabilities when @sfilp is about to be
+ * released. Must be called from a fops->release() handler, and paired
+ * with a previous call to evl_open_file() from the fops->open()
+ * handler.
+ */
+void evl_release_file(struct evl_file *sfilp)
+{
+	/*
+	 * Release the original reference on @sfilp. If OOB references
+	 * are still pending (e.g. some thread is still blocked in
+	 * fops->oob_read()), we must wait for them to be dropped
+	 * before allowing the in-band code to dismantle @sfilp->filp.
+	 *
+	 * NOTE: In-band and OOB fds are working together in lockstep
+	 * mode via dovetail_install/uninstall_fd() calls.  Therefore,
+	 * we can't livelock with evl_get_file() as @sfilp was
+	 * removed from the fd tree before fops->release() called us.
+	 */
+	if (atomic_dec_return(&sfilp->oob_refs) > 0)
+		wait_for_completion(&sfilp->oob_done);
+}
+
+void evl_cleanup_files(void)
+{
+	kmem_cache_destroy(fd_cache);
+}
+
+int __init evl_init_files(void)
+{
+	fd_cache = kmem_cache_create("evl_fdcache",
+				     sizeof(struct evl_fd),
+				     0, 0, NULL);
+	if (fd_cache == NULL)
+		return -ENOMEM;
+
+	return 0;
+}
diff --git a/kernel/evenless/init.c b/kernel/evenless/init.c
new file mode 100644
index 000000000000..acf7533cfac8
--- /dev/null
+++ b/kernel/evenless/init.c
@@ -0,0 +1,200 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <evenless/init.h>
+#include <evenless/sched.h>
+#include <evenless/clock.h>
+#include <evenless/timer.h>
+#include <evenless/tick.h>
+#include <evenless/memory.h>
+#include <evenless/file.h>
+#include <evenless/factory.h>
+#include <evenless/control.h>
+#define CREATE_TRACE_POINTS
+#include <trace/events/evenless.h>
+
+static char *oobcpus_arg;
+module_param_named(oobcpus, oobcpus_arg, charp, 0444);
+
+static char init_state_arg[16] = "enabled";
+module_param_string(state, init_state_arg, sizeof(init_state_arg), 0444);
+
+struct cpumask evl_oob_cpus;
+EXPORT_SYMBOL_GPL(evl_oob_cpus);
+
+DEFINE_PER_CPU(struct evl_machine_cpudata, evl_machine_cpudata);
+EXPORT_PER_CPU_SYMBOL_GPL(evl_machine_cpudata);
+
+#ifdef CONFIG_EVENLESS_DEBUG
+#define boot_debug_notice "[DEBUG]"
+#else
+#define boot_debug_notice ""
+#endif
+
+#ifdef CONFIG_ENABLE_DEFAULT_TRACERS
+#define boot_trace_notice "[TRACE]"
+#else
+#define boot_trace_notice ""
+#endif
+
+#define boot_state_notice				\
+	({						\
+		evl_is_stopped() ? "[STOPPED]" : "";	\
+	})
+
+static struct {
+	const char *label;
+	enum evl_run_states state;
+} init_states[] __initdata = {
+	{ "disabled", EVL_STATE_DISABLED },
+	{ "stopped", EVL_STATE_STOPPED },
+	{ "enabled", EVL_STATE_WARMUP },
+};
+
+static void __init setup_init_state(void)
+{
+	static char warn_bad_state[] __initdata =
+		EVL_WARNING "invalid init state '%s'\n";
+	int n;
+
+	for (n = 0; n < ARRAY_SIZE(init_states); n++)
+		if (strcmp(init_states[n].label, init_state_arg) == 0) {
+			set_evl_state(init_states[n].state);
+			return;
+		}
+
+	printk(warn_bad_state, init_state_arg);
+}
+
+#ifdef CONFIG_EVENLESS_DEBUG
+
+void __init evl_warn_init(const char *fn, int level, int status)
+{
+	printk(EVL_ERR "FAILED: %s => [%d]\n", fn, status);
+}
+
+#endif
+
+static __init int init_core(void)
+{
+	int ret;
+
+	enable_oob_stage("Evenless");
+
+	ret = evl_init_memory();
+	if (ret)
+		goto cleanup_stage;
+
+	ret = evl_early_init_factories();
+	if (ret)
+		goto cleanup_memory;
+
+	ret = evl_clock_init();
+	if (ret)
+		goto cleanup_early_factories;
+
+	ret = evl_init_sched();
+	if (ret)
+		goto cleanup_clock;
+
+	ret = evl_init_files();
+	if (ret)
+		goto cleanup_sched;
+
+	/*
+	 * If starting in stopped mode, do all initializations, but do
+	 * not enable the core timer.
+	 */
+	if (evl_is_warming()) {
+		ret = evl_enable_tick();
+		if (ret)
+			goto cleanup_files;
+		set_evl_state(EVL_STATE_RUNNING);
+	}
+
+	ret = dovetail_start();
+	if (ret)
+		goto cleanup_tick;
+
+	/*
+	 * Other factories can clone elements, which would allow users
+	 * to issue Dovetail requests afterwards, so let's expose them
+	 * last.
+	 */
+	ret = evl_late_init_factories();
+	if (ret)
+		goto cleanup_dovetail;
+
+	return 0;
+
+cleanup_dovetail:
+	dovetail_stop();
+cleanup_tick:
+	if (evl_is_running())
+		evl_disable_tick();
+cleanup_files:
+	evl_cleanup_files();
+cleanup_sched:
+	evl_cleanup_sched();
+cleanup_clock:
+	evl_clock_cleanup();
+cleanup_early_factories:
+	evl_early_cleanup_factories();
+cleanup_memory:
+	evl_cleanup_memory();
+cleanup_stage:
+	disable_oob_stage();
+	set_evl_state(EVL_STATE_STOPPED);
+
+	return ret;
+}
+
+static int __init evl_init(void)
+{
+	int ret;
+
+	setup_init_state();
+
+	if (!evl_is_enabled()) {
+		printk(EVL_WARNING "disabled on kernel command line\n");
+		return 0;
+	}
+
+	/*
+	 * Set of CPUs the core knows about (>= set of CPUs running
+	 * EVL threads).
+	 */
+	if (oobcpus_arg && *oobcpus_arg) {
+		if (cpulist_parse(oobcpus_arg, &evl_oob_cpus)) {
+			printk(EVL_WARNING "invalid set of OOB cpus\n");
+			cpumask_copy(&evl_oob_cpus, cpu_online_mask);
+		}
+	} else
+		cpumask_copy(&evl_oob_cpus, cpu_online_mask);
+
+	/* Threads may run on any out-of-band CPU by default. */
+	evl_cpu_affinity = evl_oob_cpus;
+
+	ret = EVL_INIT_CALL(0, init_core());
+	if (ret)
+		goto fail;
+
+	printk(EVL_INFO "core started %s%s%s\n",
+	       boot_debug_notice,
+	       boot_trace_notice,
+	       boot_state_notice);
+
+	return 0;
+fail:
+	set_evl_state(EVL_STATE_DISABLED);
+
+	printk(EVL_ERR "disabling.\n");
+
+	return ret;
+}
+device_initcall(evl_init);
diff --git a/kernel/evenless/irq.c b/kernel/evenless/irq.c
new file mode 100644
index 000000000000..becf7c9395c7
--- /dev/null
+++ b/kernel/evenless/irq.c
@@ -0,0 +1,34 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2017 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <evenless/sched.h>
+
+/* hard IRQs off. */
+void enter_oob_irq(void)
+{
+	struct evl_rq *rq = this_evl_rq();
+
+	rq->lflags |= RQ_IRQ;
+}
+
+/* hard IRQs off. */
+void exit_oob_irq(void)
+{
+	struct evl_rq *rq = this_evl_rq();
+
+	rq->lflags &= ~RQ_IRQ;
+
+	/*
+	 * We are only interested in RQ_SCHED previously set by an OOB
+	 * handler on the current CPU, so there is no cache coherence
+	 * issue. Remote CPUs pair RQ_SCHED requests with an IPI, so
+	 * we don't care about missing them here.
+	 */
+	if (rq->status & RQ_SCHED)
+		evl_schedule();
+}
diff --git a/kernel/evenless/lock.c b/kernel/evenless/lock.c
new file mode 100644
index 000000000000..14e81c7438b2
--- /dev/null
+++ b/kernel/evenless/lock.c
@@ -0,0 +1,102 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Inherited from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2004, 2005 Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>
+ * Copyright (C) 2010, 2018 Philippe Gerum  <rpm@xenomai.org>
+ *
+ * NOTE: this code is on its way out, the ugly superlock is the only
+ * remaining lock of this type. Moving away from the inefficient
+ * single-lock model is planned.
+ */
+
+#include <linux/module.h>
+#include <linux/sched/debug.h>
+#include <evenless/lock.h>
+#include <evenless/clock.h>
+
+DEFINE_XNLOCK(nklock);
+#if defined(CONFIG_SMP) || defined(CONFIG_EVENLESS_DEBUG_LOCKING)
+EXPORT_SYMBOL_GPL(nklock);
+
+int ___xnlock_get(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+{
+	return ____xnlock_get(lock /* , */ T_LOCK_DBG_PASS_CONTEXT);
+}
+EXPORT_SYMBOL_GPL(___xnlock_get);
+
+void ___xnlock_put(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+{
+	____xnlock_put(lock /* , */ T_LOCK_DBG_PASS_CONTEXT);
+}
+EXPORT_SYMBOL_GPL(___xnlock_put);
+#endif /* CONFIG_SMP || CONFIG_EVENLESS_DEBUG_LOCKING */
+
+#ifdef CONFIG_EVENLESS_DEBUG_LOCKING
+
+DEFINE_PER_CPU(struct xnlockinfo, xnlock_stats);
+EXPORT_PER_CPU_SYMBOL_GPL(xnlock_stats);
+
+void xnlock_dbg_prepare_acquire(ktime_t *start)
+{
+	*start = evl_read_clock(&evl_mono_clock);
+}
+EXPORT_SYMBOL_GPL(xnlock_dbg_prepare_acquire);
+
+void xnlock_dbg_acquired(struct xnlock *lock, int cpu, ktime_t *start,
+			 const char *file, int line, const char *function)
+{
+	lock->lock_date = *start;
+	lock->spin_time = ktime_sub(evl_read_clock(&evl_mono_clock), *start);
+	lock->file = file;
+	lock->function = function;
+	lock->line = line;
+	lock->cpu = cpu;
+}
+EXPORT_SYMBOL_GPL(xnlock_dbg_acquired);
+
+int xnlock_dbg_release(struct xnlock *lock,
+		       const char *file, int line, const char *function)
+{
+	struct xnlockinfo *stats;
+	ktime_t lock_time;
+	int cpu;
+
+	lock_time = ktime_sub(evl_read_clock(&evl_mono_clock), lock->lock_date);
+	cpu = raw_smp_processor_id();
+	stats = &per_cpu(xnlock_stats, cpu);
+
+	if (lock->file == NULL) {
+		lock->file = "??";
+		lock->line = 0;
+		lock->function = "invalid";
+	}
+
+	if (unlikely(lock->owner != cpu)) {
+		printk(EVL_ERR "lock %p already unlocked on CPU #%d\n"
+			"          last owner = %s:%u (%s(), CPU #%d)\n",
+		       lock, cpu, lock->file, lock->line, lock->function,
+		       lock->cpu);
+		show_stack(NULL,NULL);
+		return 1;
+	}
+
+	/* File that we released it. */
+	lock->cpu = -lock->cpu;
+	lock->file = file;
+	lock->line = line;
+	lock->function = function;
+
+	if (lock_time > stats->lock_time) {
+		stats->lock_time = lock_time;
+		stats->spin_time = lock->spin_time;
+		stats->file = lock->file;
+		stats->function = lock->function;
+		stats->line = lock->line;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xnlock_dbg_release);
+
+#endif
diff --git a/kernel/evenless/logger.c b/kernel/evenless/logger.c
new file mode 100644
index 000000000000..db40f3ba3030
--- /dev/null
+++ b/kernel/evenless/logger.c
@@ -0,0 +1,230 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <linux/uaccess.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/log2.h>
+#include <linux/mutex.h>
+#include <linux/irq_work.h>
+#include <linux/workqueue.h>
+#include <linux/circ_buf.h>
+#include <linux/atomic.h>
+#include <evenless/factory.h>
+#include <uapi/evenless/logger.h>
+
+struct evl_logger {
+	struct file *outfilp;
+	struct circ_buf circ_buf;
+	atomic_t write_sem;
+	size_t logsz;
+	struct evl_element element;
+	struct irq_work irq_work;
+	struct work_struct work;
+	hard_spinlock_t lock;
+};
+
+static void relay_output(struct work_struct *work)
+{
+	struct evl_logger *logger;
+	int head, tail, rem, len;
+	struct circ_buf *circ;
+	struct file *filp;
+	ssize_t ret;
+	loff_t pos;
+
+	logger = container_of(work, struct evl_logger, work);
+	filp = logger->outfilp;
+	circ = &logger->circ_buf;
+
+	mutex_lock(&filp->f_pos_lock);
+
+	for (;;) {
+		head = smp_load_acquire(&circ->head);
+		tail = circ->tail;
+		rem = CIRC_CNT(head, tail, logger->logsz);
+		if (rem <= 0)
+			break;
+
+		len = min(CIRC_CNT_TO_END(head, tail, logger->logsz), rem);
+
+		pos = filp->f_pos;
+		ret = vfs_write(filp, circ->buf + tail, len, &pos);
+		if (ret >= 0)
+			filp->f_pos = pos;
+
+		smp_store_release(&circ->tail,
+				  (tail + len) & (logger->logsz - 1));
+	}
+
+	mutex_unlock(&filp->f_pos_lock);
+}
+
+static void relay_output_irq(struct irq_work *work)
+{
+	struct evl_logger *logger;
+
+	logger = container_of(work, struct evl_logger, irq_work);
+	schedule_work(&logger->work);
+}
+
+static ssize_t logger_oob_write(struct file *filp,
+				const char __user *u_buf, size_t count)
+{
+	struct evl_logger *logger = element_of(filp, struct evl_logger);
+	struct circ_buf *circ = &logger->circ_buf;
+	ssize_t rem, avail, written = 0;
+	const char __user *u_ptr;
+	int head, tail, len, ret;
+	unsigned long flags;
+	bool kick;
+
+	if (count >= logger->logsz) /* Avail space is logsz - 1. */
+		return -EFBIG;
+retry:
+	u_ptr = u_buf;
+	rem = count;
+
+	raw_spin_lock_irqsave(&logger->lock, flags);
+
+	for (;;) {
+		head = circ->head;
+		tail = READ_ONCE(circ->tail);
+		if (rem == 0 || CIRC_SPACE(head, tail, logger->logsz) <= 0)
+			break;
+
+		avail = CIRC_SPACE_TO_END(head, tail, logger->logsz);
+		len = min(avail, rem);
+
+		raw_spin_unlock_irqrestore(&logger->lock, flags);
+
+		if (atomic_dec_return(&logger->write_sem) < 0) {
+			atomic_inc(&logger->write_sem);
+			goto retry;
+		}
+
+		ret = raw_copy_from_user(circ->buf + head, u_ptr, len);
+		raw_spin_lock_irqsave(&logger->lock, flags);
+		atomic_inc(&logger->write_sem);
+
+		if (ret) {
+			written = -EFAULT;
+			break;
+		}
+
+		smp_store_release(&circ->head,
+				  (head + len) & (logger->logsz - 1));
+		u_ptr += len;
+		rem -= len;
+		written += len;
+	}
+
+	kick = CIRC_CNT(head, tail, logger->logsz) > 0;
+
+	raw_spin_unlock_irqrestore(&logger->lock, flags);
+
+	if (kick)
+		irq_work_queue(&logger->irq_work);
+
+	return written;
+}
+
+static ssize_t logger_write(struct file *filp, const char __user *u_buf,
+			    size_t count, loff_t *ppos)
+{
+	return logger_oob_write(filp, u_buf, count);
+}
+
+static const struct file_operations logger_fops = {
+	.open		= evl_open_element,
+	.release	= evl_close_element,
+	.oob_write	= logger_oob_write,
+	.write		= logger_write,
+};
+
+static struct evl_element *
+logger_factory_build(struct evl_factory *fac, const char *name,
+		     void __user *u_attrs, u32 *state_offp)
+{
+	struct evl_logger_attrs attrs;
+	struct evl_logger *logger;
+	struct file *outfilp;
+	void *bufmem;
+	size_t logsz;
+	int ret;
+
+	ret = copy_from_user(&attrs, u_attrs, sizeof(attrs));
+	if (ret)
+		return ERR_PTR(-EFAULT);
+
+	logsz = roundup_pow_of_two(attrs.logsz);
+	if (order_base_2(logsz) > 30) /* LART */
+		return ERR_PTR(-EINVAL);
+
+	outfilp = fget(attrs.fd);
+	if (outfilp == NULL)
+		return ERR_PTR(-EINVAL);
+
+	logger = kzalloc(sizeof(*logger), GFP_KERNEL);
+	if (logger == NULL) {
+		ret = -ENOMEM;
+		goto fail_logger;
+	}
+
+	bufmem = kzalloc(attrs.logsz, GFP_KERNEL);
+	if (bufmem == NULL) {
+		ret = -ENOMEM;
+		goto fail_bufmem;
+	}
+
+	ret = evl_init_element(&logger->element, &evl_logger_factory);
+	if (ret)
+		goto fail_element;
+
+	logger->outfilp = outfilp;
+	logger->circ_buf.buf = bufmem;
+	logger->logsz = logsz;
+	atomic_set(&logger->write_sem, 1);
+	INIT_WORK(&logger->work, relay_output);
+	init_irq_work(&logger->irq_work, relay_output_irq);
+	raw_spin_lock_init(&logger->lock);
+
+	return &logger->element;
+
+fail_element:
+	kfree(bufmem);
+fail_bufmem:
+	kfree(logger);
+fail_logger:
+	fput(outfilp);
+
+	return ERR_PTR(ret);
+}
+
+static void logger_factory_dispose(struct evl_element *e)
+{
+	struct evl_logger *logger;
+
+	logger = container_of(e, struct evl_logger, element);
+
+	fput(logger->outfilp);
+	kfree(logger->circ_buf.buf);
+	evl_destroy_element(&logger->element);
+
+	kfree_rcu(logger, element.rcu);
+}
+
+struct evl_factory evl_logger_factory = {
+	.name	=	"logger",
+	.fops	=	&logger_fops,
+	.build =	logger_factory_build,
+	.dispose =	logger_factory_dispose,
+	.nrdev	=	CONFIG_EVENLESS_NR_LOGGERS,
+	.flags	=	EVL_FACTORY_CLONE,
+};
diff --git a/kernel/evenless/memory.c b/kernel/evenless/memory.c
new file mode 100644
index 000000000000..96b48e5ad7ac
--- /dev/null
+++ b/kernel/evenless/memory.c
@@ -0,0 +1,723 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/gfp.h>
+#include <linux/slab.h>
+#include <linux/log2.h>
+#include <linux/bitops.h>
+#include <linux/vmalloc.h>
+#include <linux/uaccess.h>
+#include <evenless/memory.h>
+#include <evenless/factory.h>
+#include <evenless/assert.h>
+#include <evenless/init.h>
+#include <uapi/evenless/thread.h>
+#include <uapi/evenless/sem.h>
+#include <uapi/evenless/monitor.h>
+
+static unsigned long sysheap_size_arg;
+module_param_named(sysheap_size, sysheap_size_arg, ulong, 0444);
+
+struct evl_heap evl_system_heap;
+EXPORT_SYMBOL_GPL(evl_system_heap);
+
+struct evl_heap evl_shared_heap;
+
+size_t evl_shm_size;
+
+enum evl_heap_pgtype {
+	page_free =0,
+	page_cont =1,
+	page_list =2
+};
+
+static inline u32 __always_inline
+gen_block_mask(int log2size)
+{
+	return -1U >> (32 - (EVL_HEAP_PAGE_SIZE >> log2size));
+}
+
+static inline  __always_inline
+int addr_to_pagenr(struct evl_heap *heap, void *p)
+{
+	return ((void *)p - heap->membase) >> EVL_HEAP_PAGE_SHIFT;
+}
+
+static inline  __always_inline
+void *pagenr_to_addr(struct evl_heap *heap, int pg)
+{
+	return heap->membase + (pg << EVL_HEAP_PAGE_SHIFT);
+}
+
+#ifdef CONFIG_EVENLESS_DEBUG_MEMORY
+/*
+ * Setting page_cont/page_free in the page map is only required for
+ * enabling full checking of the block address in free requests, which
+ * may be extremely time-consuming when deallocating huge blocks
+ * spanning thousands of pages. We only do such marking when running
+ * in memory debug mode.
+ */
+static inline bool
+page_is_valid(struct evl_heap *heap, int pg)
+{
+	switch (heap->pagemap[pg].type) {
+	case page_free:
+	case page_cont:
+		return false;
+	case page_list:
+	default:
+		return true;
+	}
+}
+
+static void mark_pages(struct evl_heap *heap,
+		       int pg, int nrpages,
+		       enum evl_heap_pgtype type)
+{
+	while (nrpages-- > 0)
+		heap->pagemap[pg].type = type;
+}
+
+#else
+
+static inline bool
+page_is_valid(struct evl_heap *heap, int pg)
+{
+	return true;
+}
+
+static void mark_pages(struct evl_heap *heap,
+		       int pg, int nrpages,
+		       enum evl_heap_pgtype type)
+{ }
+
+#endif
+
+static struct evl_heap_range *
+search_size_ge(struct rb_root *t, size_t size)
+{
+	struct rb_node *rb, *deepest = NULL;
+	struct evl_heap_range *r;
+
+	/*
+	 * We first try to find an exact match. If that fails, we walk
+	 * the tree in logical order by increasing size value from the
+	 * deepest node traversed until we find the first successor to
+	 * that node, or nothing beyond it, whichever comes first.
+	 */
+	rb = t->rb_node;
+	while (rb) {
+		deepest = rb;
+		r = rb_entry(rb, struct evl_heap_range, size_node);
+		if (size < r->size) {
+			rb = rb->rb_left;
+			continue;
+		}
+		if (size > r->size) {
+			rb = rb->rb_right;
+			continue;
+		}
+		return r;
+	}
+
+	rb = deepest;
+	while (rb) {
+		r = rb_entry(rb, struct evl_heap_range, size_node);
+		if (size <= r->size)
+			return r;
+		rb = rb_next(rb);
+	}
+
+	return NULL;
+}
+
+static struct evl_heap_range *
+search_left_mergeable(struct evl_heap *heap, struct evl_heap_range *r)
+{
+	struct rb_node *node = heap->addr_tree.rb_node;
+	struct evl_heap_range *p;
+
+	while (node) {
+		p = rb_entry(node, struct evl_heap_range, addr_node);
+		if ((void *)p + p->size == (void *)r)
+			return p;
+		if (&r->addr_node < node)
+			node = node->rb_left;
+		else
+			node = node->rb_right;
+	}
+
+	return NULL;
+}
+
+static struct evl_heap_range *
+search_right_mergeable(struct evl_heap *heap, struct evl_heap_range *r)
+{
+	struct rb_node *node = heap->addr_tree.rb_node;
+	struct evl_heap_range *p;
+
+	while (node) {
+		p = rb_entry(node, struct evl_heap_range, addr_node);
+		if ((void *)r + r->size == (void *)p)
+			return p;
+		if (&r->addr_node < node)
+			node = node->rb_left;
+		else
+			node = node->rb_right;
+	}
+
+	return NULL;
+}
+
+static void insert_range_bysize(struct evl_heap *heap, struct evl_heap_range *r)
+{
+	struct rb_node **new = &heap->size_tree.rb_node, *parent = NULL;
+	struct evl_heap_range *p;
+
+	while (*new) {
+		p = container_of(*new, struct evl_heap_range, size_node);
+		parent = *new;
+		if (r->size <= p->size)
+			new = &((*new)->rb_left);
+		else
+			new = &((*new)->rb_right);
+	}
+
+	rb_link_node(&r->size_node, parent, new);
+	rb_insert_color(&r->size_node, &heap->size_tree);
+}
+
+static void insert_range_byaddr(struct evl_heap *heap, struct evl_heap_range *r)
+{
+	struct rb_node **new = &heap->addr_tree.rb_node, *parent = NULL;
+	struct evl_heap_range *p;
+
+	while (*new) {
+		p = container_of(*new, struct evl_heap_range, addr_node);
+		parent = *new;
+		if (r < p)
+			new = &((*new)->rb_left);
+		else
+			new = &((*new)->rb_right);
+	}
+
+	rb_link_node(&r->addr_node, parent, new);
+	rb_insert_color(&r->addr_node, &heap->addr_tree);
+}
+
+static int reserve_page_range(struct evl_heap *heap, size_t size)
+{
+	struct evl_heap_range *new, *splitr;
+
+	/* Find a suitable range of pages covering 'size'. */
+	new = search_size_ge(&heap->size_tree, size);
+	if (new == NULL)
+		return -1;
+
+	rb_erase(&new->size_node, &heap->size_tree);
+	if (new->size == size) {
+		rb_erase(&new->addr_node, &heap->addr_tree);
+		return addr_to_pagenr(heap, new);
+	}
+
+	/*
+	 * The free range fetched is larger than what we need: split
+	 * it in two, the upper part is returned to the caller, the
+	 * lower part is sent back to the free list, which makes
+	 * reindexing by address pointless.
+	 */
+	splitr = new;
+	splitr->size -= size;
+	new = (struct evl_heap_range *)((void *)new + splitr->size);
+	insert_range_bysize(heap, splitr);
+
+	return addr_to_pagenr(heap, new);
+}
+
+static void release_page_range(struct evl_heap *heap,
+			       void *page, size_t size)
+{
+	struct evl_heap_range *freed = page, *left, *right;
+	bool addr_linked = false;
+
+	freed->size = size;
+
+	left = search_left_mergeable(heap, freed);
+	if (left) {
+		rb_erase(&left->size_node, &heap->size_tree);
+		left->size += freed->size;
+		freed = left;
+		addr_linked = true;
+	}
+
+	right = search_right_mergeable(heap, freed);
+	if (right) {
+		rb_erase(&right->size_node, &heap->size_tree);
+		freed->size += right->size;
+		if (addr_linked)
+			rb_erase(&right->addr_node, &heap->addr_tree);
+		else
+			rb_replace_node(&right->addr_node, &freed->addr_node,
+					&heap->addr_tree);
+	} else if (!addr_linked)
+		insert_range_byaddr(heap, freed);
+
+	insert_range_bysize(heap, freed);
+	mark_pages(heap, addr_to_pagenr(heap, page),
+		   size >> EVL_HEAP_PAGE_SHIFT, page_free);
+}
+
+static void add_page_front(struct evl_heap *heap,
+			   int pg, int log2size)
+{
+	struct evl_heap_pgentry *new, *head, *next;
+	int ilog;
+
+	/* Insert page at front of the per-bucket page list. */
+
+	ilog = log2size - EVL_HEAP_MIN_LOG2;
+	new = &heap->pagemap[pg];
+	if (heap->buckets[ilog] == -1U) {
+		heap->buckets[ilog] = pg;
+		new->prev = new->next = pg;
+	} else {
+		head = &heap->pagemap[heap->buckets[ilog]];
+		new->prev = heap->buckets[ilog];
+		new->next = head->next;
+		next = &heap->pagemap[new->next];
+		next->prev = pg;
+		head->next = pg;
+		heap->buckets[ilog] = pg;
+	}
+}
+
+static void remove_page(struct evl_heap *heap,
+			int pg, int log2size)
+{
+	struct evl_heap_pgentry *old, *prev, *next;
+	int ilog = log2size - EVL_HEAP_MIN_LOG2;
+
+	/* Remove page from the per-bucket page list. */
+
+	old = &heap->pagemap[pg];
+	if (pg == old->next)
+		heap->buckets[ilog] = -1U;
+	else {
+		if (pg == heap->buckets[ilog])
+			heap->buckets[ilog] = old->next;
+		prev = &heap->pagemap[old->prev];
+		prev->next = old->next;
+		next = &heap->pagemap[old->next];
+		next->prev = old->prev;
+	}
+}
+
+static void move_page_front(struct evl_heap *heap,
+			    int pg, int log2size)
+{
+	int ilog = log2size - EVL_HEAP_MIN_LOG2;
+
+	/* Move page at front of the per-bucket page list. */
+
+	if (heap->buckets[ilog] == pg)
+		return;	 /* Already at front, no move. */
+
+	remove_page(heap, pg, log2size);
+	add_page_front(heap, pg, log2size);
+}
+
+static void move_page_back(struct evl_heap *heap,
+			   int pg, int log2size)
+{
+	struct evl_heap_pgentry *old, *last, *head, *next;
+	int ilog;
+
+	/* Move page at end of the per-bucket page list. */
+
+	old = &heap->pagemap[pg];
+	if (pg == old->next) /* Singleton, no move. */
+		return;
+
+	remove_page(heap, pg, log2size);
+
+	ilog = log2size - EVL_HEAP_MIN_LOG2;
+	head = &heap->pagemap[heap->buckets[ilog]];
+	last = &heap->pagemap[head->prev];
+	old->prev = head->prev;
+	old->next = last->next;
+	next = &heap->pagemap[old->next];
+	next->prev = pg;
+	last->next = pg;
+}
+
+static void *add_free_range(struct evl_heap *heap,
+			    size_t bsize, int log2size)
+{
+	int pg;
+
+	pg = reserve_page_range(heap, ALIGN(bsize, EVL_HEAP_PAGE_SIZE));
+	if (pg < 0)
+		return NULL;
+
+	/*
+	 * Update the page entry.  If @log2size is non-zero
+	 * (i.e. bsize < EVL_HEAP_PAGE_SIZE), bsize is (1 << log2Size)
+	 * between 2^EVL_HEAP_MIN_LOG2 and 2^(EVL_HEAP_PAGE_SHIFT -
+	 * 1).  Save the log2 power into entry.type, then update the
+	 * per-page allocation bitmap to reserve the first block.
+	 *
+	 * Otherwise, we have a larger block which may span multiple
+	 * pages: set entry.type to page_list, indicating the start of
+	 * the page range, and entry.bsize to the overall block size.
+	 */
+	if (log2size) {
+		heap->pagemap[pg].type = log2size;
+		/*
+		 * Mark the first object slot (#0) as busy, along with
+		 * the leftmost bits we won't use for this log2 size.
+		 */
+		heap->pagemap[pg].map = ~gen_block_mask(log2size) | 1;
+		/*
+		 * Insert the new page at front of the per-bucket page
+		 * list, enforcing the assumption that pages with free
+		 * space live close to the head of this list.
+		 */
+		add_page_front(heap, pg, log2size);
+	} else {
+		heap->pagemap[pg].type = page_list;
+		heap->pagemap[pg].bsize = (u32)bsize;
+		mark_pages(heap, pg + 1,
+			   (bsize >> EVL_HEAP_PAGE_SHIFT) - 1, page_cont);
+	}
+
+	heap->used_size += bsize;
+
+	return pagenr_to_addr(heap, pg);
+}
+
+int evl_init_heap(struct evl_heap *heap, void *membase, size_t size)
+{
+	int n, nrpages;
+
+	inband_context_only();
+
+	if (size > EVL_HEAP_MAX_HEAPSZ || !PAGE_ALIGNED(size))
+		return -EINVAL;
+
+	/* Reset bucket page lists, all empty. */
+	for (n = 0; n < EVL_HEAP_MAX_BUCKETS; n++)
+		heap->buckets[n] = -1U;
+
+	raw_spin_lock_init(&heap->lock);
+
+	nrpages = size >> EVL_HEAP_PAGE_SHIFT;
+	heap->pagemap = kzalloc(sizeof(struct evl_heap_pgentry) * nrpages,
+				GFP_KERNEL);
+	if (heap->pagemap == NULL)
+		return -ENOMEM;
+
+	heap->membase = membase;
+	heap->usable_size = size;
+	heap->used_size = 0;
+
+	/*
+	 * The free page pool is maintained as a set of ranges of
+	 * contiguous pages indexed by address and size in rbtrees.
+	 * Initially, we have a single range in those trees covering
+	 * the whole memory we have been given for the heap. Over
+	 * time, that range will be split then possibly re-merged back
+	 * as allocations and deallocations take place.
+	 */
+	heap->size_tree = RB_ROOT;
+	heap->addr_tree = RB_ROOT;
+	release_page_range(heap, membase, size);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(evl_init_heap);
+
+void evl_destroy_heap(struct evl_heap *heap)
+{
+	inband_context_only();
+
+	kfree(heap->pagemap);
+}
+EXPORT_SYMBOL_GPL(evl_destroy_heap);
+
+void *evl_alloc_chunk(struct evl_heap *heap, size_t size)
+{
+	int log2size, ilog, pg, b = -1;
+	unsigned long flags;
+	size_t bsize;
+	void *block;
+
+	if (size == 0)
+		return NULL;
+
+	if (size < EVL_HEAP_MIN_ALIGN) {
+		bsize = size = EVL_HEAP_MIN_ALIGN;
+		log2size = EVL_HEAP_MIN_LOG2;
+	} else {
+		log2size = ilog2(size);
+		if (log2size < EVL_HEAP_PAGE_SHIFT) {
+			if (size & (size - 1))
+				log2size++;
+			bsize = 1 << log2size;
+		} else
+			bsize = ALIGN(size, EVL_HEAP_PAGE_SIZE);
+	}
+
+	/*
+	 * Allocate entire pages directly from the pool whenever the
+	 * block is larger or equal to EVL_HEAP_PAGE_SIZE.  Otherwise,
+	 * use bucketed memory.
+	 *
+	 * NOTE: Fully busy pages from bucketed memory are moved back
+	 * at the end of the per-bucket page list, so that we may
+	 * always assume that either the heading page has some room
+	 * available, or no room is available from any page linked to
+	 * this list, in which case we should immediately add a fresh
+	 * page.
+	 */
+	raw_spin_lock_irqsave(&heap->lock, flags);
+
+	if (bsize >= EVL_HEAP_PAGE_SIZE)
+		/* Add a range of contiguous free pages. */
+		block = add_free_range(heap, bsize, 0);
+	else {
+		ilog = log2size - EVL_HEAP_MIN_LOG2;
+		EVL_WARN_ON(MEMORY, ilog < 0 || ilog >= EVL_HEAP_MAX_BUCKETS);
+		pg = heap->buckets[ilog];
+		/*
+		 * Find a block in the heading page if any. If there
+		 * is none, there won't be any down the list: add a
+		 * new page right away.
+		 */
+		if (pg < 0 || heap->pagemap[pg].map == -1U)
+			block = add_free_range(heap, bsize, log2size);
+		else {
+			b = ffs(~heap->pagemap[pg].map) - 1;
+			/*
+			 * Got one block from the heading per-bucket
+			 * page, tag it as busy in the per-page
+			 * allocation map.
+			 */
+			heap->pagemap[pg].map |= (1U << b);
+			heap->used_size += bsize;
+			block = heap->membase +
+				(pg << EVL_HEAP_PAGE_SHIFT) +
+				(b << log2size);
+			if (heap->pagemap[pg].map == -1U)
+				move_page_back(heap, pg, log2size);
+		}
+	}
+
+	raw_spin_unlock_irqrestore(&heap->lock, flags);
+
+	return block;
+}
+EXPORT_SYMBOL_GPL(evl_alloc_chunk);
+
+void evl_free_chunk(struct evl_heap *heap, void *block)
+{
+	unsigned long pgoff, boff;
+	int log2size, pg, n;
+	unsigned long flags;
+	size_t bsize;
+	u32 oldmap;
+
+	raw_spin_lock_irqsave(&heap->lock, flags);
+
+	/* Compute the heading page number in the page map. */
+	pgoff = block - heap->membase;
+	pg = pgoff >> EVL_HEAP_PAGE_SHIFT;
+
+	if (!page_is_valid(heap, pg))
+		goto bad;
+
+	switch (heap->pagemap[pg].type) {
+	case page_list:
+		bsize = heap->pagemap[pg].bsize;
+		EVL_WARN_ON(MEMORY, (bsize & (EVL_HEAP_PAGE_SIZE - 1)) != 0);
+		release_page_range(heap, pagenr_to_addr(heap, pg), bsize);
+		break;
+
+	default:
+		log2size = heap->pagemap[pg].type;
+		bsize = (1 << log2size);
+		EVL_WARN_ON(MEMORY, bsize >= EVL_HEAP_PAGE_SIZE);
+		boff = pgoff & ~EVL_HEAP_PAGE_MASK;
+		if ((boff & (bsize - 1)) != 0) /* Not at block start? */
+			goto bad;
+
+		n = boff >> log2size; /* Block position in page. */
+		oldmap = heap->pagemap[pg].map;
+		heap->pagemap[pg].map &= ~(1U << n);
+
+		/*
+		 * If the page the block was sitting on is fully idle,
+		 * return it to the pool. Otherwise, check whether
+		 * that page is transitioning from fully busy to
+		 * partially busy state, in which case it should move
+		 * toward the front of the per-bucket page list.
+		 */
+		if (heap->pagemap[pg].map == ~gen_block_mask(log2size)) {
+			remove_page(heap, pg, log2size);
+			release_page_range(heap, pagenr_to_addr(heap, pg),
+					   EVL_HEAP_PAGE_SIZE);
+		} else if (oldmap == -1U)
+			move_page_front(heap, pg, log2size);
+	}
+
+	heap->used_size -= bsize;
+
+	raw_spin_unlock_irqrestore(&heap->lock, flags);
+
+	return;
+bad:
+	raw_spin_unlock_irqrestore(&heap->lock, flags);
+
+	EVL_WARN(MEMORY, 1, "invalid block %p in heap %s",
+		 block, heap == &evl_shared_heap ?
+		 "shared" : "system");
+}
+EXPORT_SYMBOL_GPL(evl_free_chunk);
+
+ssize_t evl_check_chunk(struct evl_heap *heap, void *block)
+{
+	unsigned long pg, pgoff, boff;
+	ssize_t ret = -EINVAL;
+	unsigned long flags;
+	size_t bsize;
+
+	raw_spin_lock_irqsave(&heap->lock, flags);
+
+	/* Calculate the page number from the block address. */
+	pgoff = block - heap->membase;
+	pg = pgoff >> EVL_HEAP_PAGE_SHIFT;
+	if (page_is_valid(heap, pg)) {
+		if (heap->pagemap[pg].type == page_list)
+			bsize = heap->pagemap[pg].bsize;
+		else {
+			bsize = (1 << heap->pagemap[pg].type);
+			boff = pgoff & ~EVL_HEAP_PAGE_MASK;
+			if ((boff & (bsize - 1)) != 0) /* Not at block start? */
+				goto out;
+		}
+		ret = (ssize_t)bsize;
+	}
+out:
+	raw_spin_unlock_irqrestore(&heap->lock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_check_chunk);
+
+static int init_shared_heap(void)
+{
+	size_t size;
+	void *mem;
+	int ret;
+
+	size = CONFIG_EVENLESS_NR_THREADS *
+		sizeof(struct evl_user_window) +
+		CONFIG_EVENLESS_NR_MONITORS *
+		sizeof(struct evl_monitor_state) +
+		CONFIG_EVENLESS_NR_SEMAPHORES *
+		sizeof(struct evl_sem_state);
+	size = PAGE_ALIGN(size);
+	mem = kzalloc(size, GFP_KERNEL);
+	if (mem == NULL)
+		return -ENOMEM;
+
+	ret = evl_init_heap(&evl_shared_heap, mem, size);
+	if (ret) {
+		kfree(mem);
+		return ret;
+	}
+
+	evl_shm_size = size;
+
+	return 0;
+}
+
+static void cleanup_shared_heap(void)
+{
+	void *membase = evl_get_heap_base(&evl_shared_heap);
+
+	evl_destroy_heap(&evl_shared_heap);
+	kfree(membase);
+}
+
+static int init_system_heap(void)
+{
+	size_t size = sysheap_size_arg;
+	void *sysmem;
+	int ret;
+
+	if (size == 0)
+		size = CONFIG_EVENLESS_SYS_HEAPSZ * 1024;
+
+	sysmem = vmalloc(size);
+	if (sysmem == NULL)
+		return -ENOMEM;
+
+	ret = evl_init_heap(&evl_system_heap, sysmem, size);
+	if (ret) {
+		vfree(sysmem);
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void cleanup_system_heap(void)
+{
+	void *membase = evl_get_heap_base(&evl_system_heap);
+
+	evl_destroy_heap(&evl_system_heap);
+	vfree(membase);
+}
+
+int __init evl_init_memory(void)
+{
+	int ret;
+
+	ret = EVL_INIT_CALL(1, init_system_heap());
+	if (ret)
+		return ret;
+
+	ret = EVL_INIT_CALL(1, init_shared_heap());
+	if (ret) {
+		cleanup_system_heap();
+		return ret;
+	}
+
+	return 0;
+}
+
+void evl_cleanup_memory(void)
+{
+	cleanup_shared_heap();
+	cleanup_system_heap();
+}
+
+void *evl_alloc_irq_work(size_t size)
+{
+	return evl_alloc(size);
+}
+
+void evl_free_irq_work(void *p)
+{
+	evl_free(p);
+}
diff --git a/kernel/evenless/monitor.c b/kernel/evenless/monitor.c
new file mode 100644
index 000000000000..ca65075ef40d
--- /dev/null
+++ b/kernel/evenless/monitor.c
@@ -0,0 +1,601 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <evenless/thread.h>
+#include <evenless/synch.h>
+#include <evenless/clock.h>
+#include <evenless/monitor.h>
+#include <evenless/thread.h>
+#include <evenless/memory.h>
+#include <evenless/lock.h>
+#include <evenless/sched.h>
+#include <evenless/factory.h>
+#include <evenless/syscall.h>
+#include <asm/evenless/syscall.h>
+#include <uapi/evenless/monitor.h>
+#include <trace/events/evenless.h>
+
+struct evl_monitor {
+	struct evl_element element;
+	struct evl_monitor_state *state;
+	int type;
+	union {
+		struct {
+			struct evl_syn lock;
+			struct list_head events;
+		};
+		struct {
+			struct evl_syn wait_queue;
+			struct evl_monitor *gate;
+			struct list_head next; /* in ->events */
+		};
+	};
+};
+
+static const struct file_operations monitor_fops;
+
+struct evl_monitor *get_monitor_by_fd(int efd, struct evl_file **sfilpp)
+{
+	struct evl_file *sfilp = evl_get_file(efd);
+
+	if (sfilp && sfilp->filp->f_op == &monitor_fops) {
+		*sfilpp = sfilp;
+		return element_of(sfilp->filp, struct evl_monitor);
+	}
+
+	return NULL;
+}
+
+int evl_signal_monitor_targeted(struct evl_thread *target, int monfd)
+{
+	struct evl_monitor *event;
+	struct evl_file *sfilp;
+	unsigned long flags;
+	int ret = -EAGAIN;
+
+	event = get_monitor_by_fd(monfd, &sfilp);
+	if (event == NULL)
+		return -EINVAL;
+
+	if (event->type != EVL_MONITOR_EV) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	/*
+	 * Current ought to hold the gate lock before calling us; if
+	 * not, we might race updating the state flags, possibly
+	 * loosing events. Too bad.
+	 */
+	if (target->wchan == &event->wait_queue) {
+		target->info |= T_SIGNAL;
+		event->state->flags |= EVL_MONITOR_TARGETED;
+		ret = 0;
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+out:
+	evl_put_file(sfilp);
+
+	return ret;
+}
+
+void __evl_commit_monitor_ceiling(struct evl_thread *curr)  /* nklock held, irqs off, OOB */
+{
+	struct evl_monitor *gate;
+
+	/*
+	 * curr->u_window has to be valid since curr bears T_USER.  If
+	 * pp_pending is a bad handle, just skip ceiling.
+	 */
+	gate = evl_get_element_by_fundle(&evl_monitor_factory,
+					 curr->u_window->pp_pending,
+					 struct evl_monitor);
+	if (gate == NULL)
+		goto out;
+
+	if (gate->type == EVL_MONITOR_PP)
+		evl_commit_syn_ceiling(&gate->lock, curr);
+
+	evl_put_element(&gate->element);
+out:
+	curr->u_window->pp_pending = EVL_NO_HANDLE;
+}
+
+/* nklock held, irqs off */
+static void wakeup_waiters(struct evl_monitor *event)
+{
+	struct evl_monitor_state *state = event->state;
+	struct evl_thread *waiter, *n;
+	bool bcast;
+
+	bcast = !!(state->flags & EVL_MONITOR_BROADCAST);
+
+	/*
+	 * Unblock threads for which an event is pending, either one
+	 * or all of them, depending on the broadcast flag.
+	 *
+	 * NOTES:
+	 *
+	 * 1. event delivery is postponed until the poster releases
+	 * the gate, so a signal may be pending in absence of any
+	 * waiter for it.
+	 *
+	 * 2. targeted events have precedence over un-targeted ones:
+	 * if at some point userland sent a targeted event to a
+	 * (valid) waiter during a transaction, the kernel will only
+	 * look for targeted waiters in the wake up loop. We don't
+	 * default to calling evl_wake_up_syn() in case those
+	 * waiters have aborted wait in the meantime.
+	 *
+	 * CAUTION: we must keep the wake up ops rescheduling-free, so
+	 * that low priority threads cannot preempt us before high
+	 * priority ones have been readied. For the moment, we are
+	 * covered by disabling IRQs when grabbing the nklock: be
+	 * careful when killing the latter.
+	 */
+	if ((state->flags & EVL_MONITOR_SIGNALED) &&
+	    evl_syn_has_waiter(&event->wait_queue)) {
+		if (bcast)
+			evl_flush_syn(&event->wait_queue, 0);
+		else if (state->flags & EVL_MONITOR_TARGETED) {
+			evl_for_each_syn_waiter_safe(waiter, n,
+						     &event->wait_queue) {
+				if (waiter->info & T_SIGNAL)
+					evl_wake_up_targeted_syn(&event->wait_queue,
+								 waiter);
+			}
+		} else
+			evl_wake_up_syn(&event->wait_queue);
+	}
+
+	state->flags &= ~(EVL_MONITOR_SIGNALED|
+			  EVL_MONITOR_BROADCAST|
+			  EVL_MONITOR_TARGETED);
+}
+
+/* nklock held, irqs off */
+static int __enter_monitor(struct evl_monitor *gate,
+			   struct evl_thread *curr)
+{
+	int info;
+
+	evl_commit_monitor_ceiling(curr);
+
+	info = evl_acquire_syn(&gate->lock, EVL_INFINITE, EVL_REL);
+	if (info)
+		/* Break or error, no timeout possible. */
+		return info & T_BREAK ? -EINTR : -EINVAL;
+
+	return 0;
+}
+
+/* nklock held, irqs off */
+static int enter_monitor(struct evl_monitor *gate)
+{
+	struct evl_thread *curr = evl_current_thread();
+
+	if (gate->type == EVL_MONITOR_EV)
+		return -EINVAL;
+
+	if (evl_is_syn_owner(gate->lock.fastlock, fundle_of(curr)))
+		return -EDEADLK;	/* Deny recursive locking. */
+
+	return __enter_monitor(gate, curr);
+}
+
+/* nklock held, irqs off */
+static void __exit_monitor(struct evl_monitor *gate,
+			   struct evl_thread *curr)
+{
+	/*
+	 * If we are about to release the lock which is still pending
+	 * PP (i.e. we never got scheduled out while holding it),
+	 * clear the lazy handle.
+	 */
+	if (fundle_of(gate) == curr->u_window->pp_pending)
+		curr->u_window->pp_pending = EVL_NO_HANDLE;
+
+	evl_release_syn(&gate->lock, curr);
+}
+
+/* nklock held, irqs off */
+static int exit_monitor(struct evl_monitor *gate)
+{
+	struct evl_thread *curr = evl_current_thread();
+	struct evl_monitor_state *state = gate->state;
+	struct evl_monitor *event, *n;
+
+	if (gate->type == EVL_MONITOR_EV)
+		return -EINVAL;
+
+	if (!evl_is_syn_owner(gate->lock.fastlock, fundle_of(curr)))
+		return -EPERM;
+
+	if (state->flags & EVL_MONITOR_SIGNALED) {
+		state->flags &= ~EVL_MONITOR_SIGNALED;
+		if (!list_empty(&gate->events)) {
+			list_for_each_entry_safe(event, n, &gate->events, next) {
+				if (event->state->flags & EVL_MONITOR_SIGNALED) {
+					list_del(&event->next);
+					wakeup_waiters(event);
+				}
+			}
+		}
+	}
+
+	__exit_monitor(gate, curr);
+	evl_schedule();
+
+	return 0;
+}
+
+/* nklock held, irqs off */
+static void untrack_event(struct evl_monitor *event,
+			  struct evl_monitor *gate)
+{
+	if (event->gate == gate &&
+	    !evl_syn_has_waiter(&event->wait_queue)) {
+		event->state->u.gate_offset = EVL_MONITOR_NOGATE;
+		list_del(&event->next);
+		event->gate = NULL;
+	}
+}
+
+/* nklock held, irqs off */
+static int wait_monitor(struct evl_monitor *event,
+			struct evl_monitor_waitreq *req,
+			__s32 *r_op_ret)
+{
+	struct evl_thread *curr = evl_current_thread();
+	int ret = 0, op_ret = 0, info;
+	struct evl_monitor *gate;
+	struct evl_file *sfilp;
+	enum evl_tmode tmode;
+	ktime_t timeout;
+
+	if (event->type != EVL_MONITOR_EV) {
+		op_ret = -EINVAL;
+		goto out;
+	}
+
+	/* Find the gate monitor protecting us. */
+	gate = get_monitor_by_fd(req->gatefd, &sfilp);
+	if (gate == NULL) {
+		op_ret = -EINVAL;
+		goto out;
+	}
+
+	if (gate->type == EVL_MONITOR_EV) {
+		op_ret = -EINVAL;
+		goto put;
+	}
+
+	/* Make sure we actually passed the gate. */
+	if (!evl_is_syn_owner(gate->lock.fastlock, fundle_of(curr))) {
+		op_ret = -EPERM;
+		goto put;
+	}
+
+	/*
+	 * Track event monitors the gate monitor protects. When
+	 * multiple threads issue concurrent wait requests on the same
+	 * event monitor, they must use the same gate to serialize.
+	 */
+	if (event->gate == NULL) {
+		list_add_tail(&event->next, &gate->events);
+		event->gate = gate;
+		event->state->u.gate_offset = evl_shared_offset(gate->state);
+	} else if (event->gate != gate) {
+		op_ret = -EINVAL;
+		goto put;
+	}
+
+	curr->info &= ~T_SIGNAL; /* CAUTION: depends on nklock held ATM */
+
+	__exit_monitor(gate, curr);
+
+	/*
+	 * Wait on the event. If a break condition is raised such as
+	 * an inband signal pending, do not attempt to reacquire the
+	 * gate lock just yet as this might block indefinitely (in
+	 * theory). Exit to user mode, allowing any pending signal to
+	 * be handled in the meantime, then expect userland to issue
+	 * MONUNWAIT to recover.
+	 */
+	timeout = timespec_to_ktime(req->timeout);
+	tmode = timeout ? EVL_ABS : EVL_REL;
+	info = evl_sleep_on_syn(&event->wait_queue, timeout, tmode);
+	if (info) {
+		if (info & T_BREAK) {
+			ret = -EINTR;
+			goto put;
+		}
+		if (info & T_TIMEO)
+			op_ret = -ETIMEDOUT;
+	}
+
+	ret = __enter_monitor(gate, curr);
+
+	untrack_event(event, gate);
+put:
+	evl_put_file(sfilp);
+out:
+	*r_op_ret = op_ret;
+
+	return ret;
+}
+
+/* nklock held, irqs off */
+static int unwait_monitor(struct evl_monitor *event,
+			  struct evl_monitor_unwaitreq *req)
+{
+	struct evl_monitor *gate;
+	struct evl_file *sfilp;
+	int ret;
+
+	if (event->type != EVL_MONITOR_EV)
+		return -EINVAL;
+
+	/* Find the gate monitor we need to re-acquire. */
+	gate = get_monitor_by_fd(req->gatefd, &sfilp);
+	if (gate == NULL)
+		return -EINVAL;
+
+	ret = enter_monitor(gate);
+	if (ret != -EINTR)
+		untrack_event(event, gate);
+
+	evl_put_file(sfilp);
+
+	return ret;
+}
+
+static long monitor_ioctl(struct file *filp, unsigned int cmd,
+			  unsigned long arg)
+{
+	struct evl_monitor *mon = element_of(filp, struct evl_monitor);
+	struct evl_monitor_binding bind, __user *u_bind;
+
+	if (cmd != EVL_MONIOC_BIND)
+		return -ENOTTY;
+
+	bind.type = mon->type;
+	bind.eids.minor = mon->element.minor;
+	bind.eids.state_offset = evl_shared_offset(mon->state);
+	bind.eids.fundle = fundle_of(mon);
+	u_bind = (typeof(u_bind))arg;
+
+	return copy_to_user(u_bind, &bind, sizeof(bind)) ? -EFAULT : 0;
+}
+
+static long monitor_oob_ioctl(struct file *filp, unsigned int cmd,
+			      unsigned long arg)
+{
+	struct evl_monitor *mon = element_of(filp, struct evl_monitor);
+	struct evl_monitor_unwaitreq uwreq, __user *u_uwreq;
+	struct evl_monitor_waitreq wreq, __user *u_wreq;
+	unsigned long flags;
+	__s32 op_ret;
+	long ret;
+
+	if (cmd == EVL_MONIOC_WAIT) {
+		u_wreq = (typeof(u_wreq))arg;
+		ret = raw_copy_from_user(&wreq, u_wreq, sizeof(wreq));
+		if (ret)
+			return -EFAULT;
+		xnlock_get_irqsave(&nklock, flags);
+		ret = wait_monitor(mon, &wreq, &op_ret);
+		xnlock_put_irqrestore(&nklock, flags);
+		raw_put_user(op_ret, &u_wreq->status);
+		return ret;
+	} else if (cmd == EVL_MONIOC_UNWAIT) {
+		u_uwreq = (typeof(u_uwreq))arg;
+		ret = raw_copy_from_user(&uwreq, u_uwreq, sizeof(uwreq));
+		if (ret)
+			return -EFAULT;
+		xnlock_get_irqsave(&nklock, flags);
+		ret = unwait_monitor(mon, &uwreq);
+		xnlock_put_irqrestore(&nklock, flags);
+		return ret;
+	}
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	switch (cmd) {
+	case EVL_MONIOC_ENTER:
+		ret = enter_monitor(mon);
+		break;
+	case EVL_MONIOC_EXIT:
+		ret = exit_monitor(mon);
+		break;
+	default:
+		ret = -ENOTTY;
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+
+static const struct file_operations monitor_fops = {
+	.open		= evl_open_element,
+	.release	= evl_close_element,
+	.unlocked_ioctl	= monitor_ioctl,
+	.oob_ioctl	= monitor_oob_ioctl,
+};
+
+static struct evl_element *
+monitor_factory_build(struct evl_factory *fac, const char *name,
+		      void __user *u_attrs, u32 *state_offp)
+{
+	struct evl_monitor_state *state;
+	struct evl_monitor_attrs attrs;
+	struct evl_monitor *mon;
+	struct evl_clock *clock;
+	int ret;
+
+	ret = copy_from_user(&attrs, u_attrs, sizeof(attrs));
+	if (ret)
+		return ERR_PTR(-EFAULT);
+
+	switch (attrs.type) {
+	case EVL_MONITOR_PP:
+		if (attrs.ceiling == 0 ||
+		    attrs.ceiling > EVL_CORE_MAX_PRIO)
+			return ERR_PTR(-EINVAL);
+		break;
+	case EVL_MONITOR_PI:
+		if (attrs.ceiling)
+			return ERR_PTR(-EINVAL);
+		break;
+	case EVL_MONITOR_EV:
+		break;
+	default:
+		return ERR_PTR(-EINVAL);
+	}
+
+	clock = evl_get_clock_by_fd(attrs.clockfd);
+	if (clock == NULL)
+		return ERR_PTR(-EINVAL);
+
+	mon = kzalloc(sizeof(*mon), GFP_KERNEL);
+	if (mon == NULL) {
+		ret = -ENOMEM;
+		goto fail_alloc;
+	}
+
+	ret = evl_init_element(&mon->element, &evl_monitor_factory);
+	if (ret)
+		goto fail_element;
+
+	state = evl_zalloc_chunk(&evl_shared_heap, sizeof(*state));
+	if (state == NULL) {
+		ret = -ENOMEM;
+		goto fail_heap;
+	}
+
+	switch (attrs.type) {
+	case EVL_MONITOR_PP:
+		state->u.gate.ceiling = attrs.ceiling;
+		evl_init_syn_protect(&mon->lock, clock,
+				     &state->u.gate.owner,
+				     &state->u.gate.ceiling);
+		INIT_LIST_HEAD(&mon->events);
+		break;
+	case EVL_MONITOR_PI:
+		evl_init_syn(&mon->lock, EVL_SYN_PI, clock,
+			     &state->u.gate.owner);
+		INIT_LIST_HEAD(&mon->events);
+		break;
+	case EVL_MONITOR_EV:
+	default:
+		evl_init_syn(&mon->wait_queue, EVL_SYN_PRIO,
+			     clock, NULL);
+		state->u.gate_offset = EVL_MONITOR_NOGATE;
+	}
+
+	/*
+	 * The type information is critical for the kernel sanity,
+	 * don't allow userland to mess with it, so don't trust the
+	 * shared state for this.
+	 */
+	mon->type = attrs.type;
+	mon->state = state;
+	*state_offp = evl_shared_offset(state);
+	evl_index_element(&mon->element);
+
+	return &mon->element;
+
+fail_heap:
+	evl_destroy_element(&mon->element);
+fail_element:
+	kfree(mon);
+fail_alloc:
+	evl_put_clock(clock);
+
+	return ERR_PTR(ret);
+}
+
+static void monitor_factory_dispose(struct evl_element *e)
+{
+	struct evl_monitor *mon;
+	unsigned long flags;
+
+	mon = container_of(e, struct evl_monitor, element);
+
+	evl_unindex_element(&mon->element);
+
+	if (mon->type == EVL_MONITOR_EV) {
+		evl_put_clock(mon->wait_queue.clock);
+		evl_destroy_syn(&mon->wait_queue);
+		if (mon->gate) {
+			xnlock_get_irqsave(&nklock, flags);
+			list_del(&mon->next);
+			xnlock_put_irqrestore(&nklock, flags);
+		}
+	} else {
+		evl_put_clock(mon->lock.clock);
+		evl_destroy_syn(&mon->lock);
+	}
+
+	evl_free_chunk(&evl_shared_heap, mon->state);
+	evl_destroy_element(&mon->element);
+	kfree_rcu(mon, element.rcu);
+}
+
+static ssize_t state_show(struct device *dev,
+			  struct device_attribute *attr,
+			  char *buf)
+{
+	struct evl_thread *owner = NULL;
+	struct evl_monitor *mon;
+	fundle_t fun;
+	ssize_t ret;
+
+	mon = evl_get_element_by_dev(dev, struct evl_monitor);
+
+	if (mon->type == EVL_MONITOR_EV)
+		ret = snprintf(buf, PAGE_SIZE, "%#x\n",
+			       mon->state->flags);
+	else {
+		fun = atomic_read(&mon->state->u.gate.owner);
+		if (fun != EVL_NO_HANDLE)
+			owner = evl_get_element_by_fundle(&evl_thread_factory,
+						  fun, struct evl_thread);
+		ret = snprintf(buf, PAGE_SIZE, "%d %u\n",
+			       owner ? evl_get_inband_pid(owner) : -1,
+			       mon->state->u.gate.ceiling);
+		if (owner)
+			evl_put_element(&owner->element);
+	}
+
+	evl_put_element(&mon->element);
+
+	return ret;
+}
+static DEVICE_ATTR_RO(state);
+
+static struct attribute *monitor_attrs[] = {
+	&dev_attr_state.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(monitor);
+
+struct evl_factory evl_monitor_factory = {
+	.name	=	"monitor",
+	.fops	=	&monitor_fops,
+	.build =	monitor_factory_build,
+	.dispose =	monitor_factory_dispose,
+	.nrdev	=	CONFIG_EVENLESS_NR_MONITORS,
+	.attrs	=	monitor_groups,
+	.flags	=	EVL_FACTORY_CLONE,
+};
diff --git a/kernel/evenless/poller.c b/kernel/evenless/poller.c
new file mode 100644
index 000000000000..543282a924ae
--- /dev/null
+++ b/kernel/evenless/poller.c
@@ -0,0 +1,559 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/types.h>
+#include <linux/err.h>
+#include <linux/fs.h>
+#include <linux/slab.h>
+#include <linux/rbtree.h>
+#include <linux/poll.h>
+#include <linux/module.h>
+#include <evenless/file.h>
+#include <evenless/thread.h>
+#include <evenless/memory.h>
+#include <evenless/poller.h>
+#include <evenless/sched.h>
+#include <asm/evenless/syscall.h>
+
+struct event_poller {
+	struct rb_root node_index;  /* struct poll_node */
+	struct list_head node_list;  /* struct poll_node */
+	struct evl_syn wait_queue;
+	struct evl_element element;
+	hard_spinlock_t lock;
+	int nodenr;
+	unsigned int generation;
+	bool readied;
+	struct list_head next;	/* in private wake up list */
+};
+
+struct poll_node {
+	unsigned int fd;
+	struct evl_file *sfilp;
+	int events_polled;
+	struct event_poller *poller;
+	struct rb_node rb;	/* in poller->node_index */
+	struct list_head next;	/* in poller->node_list */
+};
+
+/*
+ * The watchpoint struct linked to poll heads by drivers. This watches
+ * files not elements, so that we can monitor any type of EVL files.
+ */
+struct evl_poll_watchpoint {
+	struct poll_node node;
+	int events_received;
+	struct oob_poll_wait wait;
+	struct evl_poll_head *head;
+};
+
+void evl_poll_watch(struct evl_poll_head *head,
+		    struct oob_poll_wait *wait)
+{
+	struct evl_poll_watchpoint *wpt;
+	unsigned long flags;
+
+	wpt = container_of(wait, struct evl_poll_watchpoint, wait);
+	wpt->head = head;
+	raw_spin_lock_irqsave(&head->lock, flags);
+	wpt->events_received = 0;
+	list_add(&wait->next, &head->watchpoints);
+	raw_spin_unlock_irqrestore(&head->lock, flags);
+}
+EXPORT_SYMBOL_GPL(evl_poll_watch);
+
+void __evl_signal_poll_events(struct evl_poll_head *head,
+			      int events)
+{
+	struct evl_poll_watchpoint *wpt, *n;
+	struct event_poller *poller;
+	LIST_HEAD(wakeup_list);
+	unsigned long flags;
+	int ready;
+
+	raw_spin_lock_irqsave(&head->lock, flags);
+
+	if (!list_empty(&head->watchpoints)) {
+		list_for_each_entry_safe(wpt, n, &head->watchpoints, wait.next) {
+			ready = events & wpt->node.events_polled;
+			if (ready) {
+				wpt->events_received |= ready;
+				list_add(&wpt->node.poller->next, &wakeup_list);
+			}
+		}
+	}
+
+	raw_spin_unlock_irqrestore(&head->lock, flags);
+
+	if (!list_empty(&wakeup_list)) {
+		list_for_each_entry(poller, &wakeup_list, next) {
+			xnlock_get_irqsave(&nklock, flags);
+			poller->readied = true;
+			evl_wake_up_syn(&poller->wait_queue);
+			xnlock_put_irqrestore(&nklock, flags);
+		}
+	}
+
+	evl_schedule();
+}
+EXPORT_SYMBOL_GPL(__evl_signal_poll_events);
+
+void __evl_clear_poll_events(struct evl_poll_head *head,
+			     int events)
+{
+	struct evl_poll_watchpoint *wpt;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&head->lock, flags);
+
+	if (!list_empty(&head->watchpoints))
+		list_for_each_entry(wpt, &head->watchpoints, wait.next)
+			wpt->events_received &= ~events;
+
+	raw_spin_unlock_irqrestore(&head->lock, flags);
+}
+EXPORT_SYMBOL_GPL(__evl_clear_poll_events);
+
+static inline
+int __index_node(struct rb_root *root, struct poll_node *node)
+{
+	struct rb_node **rbp, *parent = NULL;
+	struct poll_node *tmp;
+
+	rbp = &root->rb_node;
+	while (*rbp) {
+		tmp = rb_entry(*rbp, struct poll_node, rb);
+		parent = *rbp;
+		if (node->fd < tmp->fd)
+			rbp = &(*rbp)->rb_left;
+		else if (node->fd > tmp->fd)
+			rbp = &(*rbp)->rb_right;
+		else
+			return -EEXIST;
+	}
+
+	rb_link_node(&node->rb, parent, rbp);
+	rb_insert_color(&node->rb, root);
+
+	return 0;
+}
+
+static int add_node(struct event_poller *poller,
+		    struct evl_poller_ctlreq *creq)
+{
+	struct poll_node *node;
+	unsigned long flags;
+	int ret;
+
+	node = evl_alloc(sizeof(*node));
+	if (node == NULL)
+		return -ENOMEM;
+
+	node->fd = creq->fd;
+	node->events_polled = creq->events;
+
+	node->sfilp = evl_get_file(creq->fd);
+	if (node->sfilp == NULL) {
+		ret = -EBADF;
+		goto fail_get;
+	}
+
+	raw_spin_lock_irqsave(&poller->lock, flags);
+
+	ret = __index_node(&poller->node_index, node);
+	if (ret)
+		goto fail_add;
+
+	list_add(&node->next, &poller->node_list);
+	poller->nodenr++;
+	if (++poller->generation == 0) /* Keep zero for init state. */
+		poller->generation = 1;
+
+	raw_spin_unlock_irqrestore(&poller->lock, flags);
+
+	return 0;
+
+fail_add:
+	raw_spin_unlock_irqrestore(&poller->lock, flags);
+	evl_put_file(node->sfilp);
+fail_get:
+	evl_free(node);
+
+	return ret;
+}
+
+static struct poll_node *
+lookup_node(struct rb_root *root, unsigned int fd)
+{
+	struct poll_node *node;
+	struct rb_node *rb;
+
+	rb = root->rb_node;
+	while (rb) {
+		node = rb_entry(rb, struct poll_node, rb);
+		if (fd < node->fd)
+			rb = rb->rb_left;
+		else if (fd > node->fd)
+			rb = rb->rb_right;
+		else
+			return node;
+	}
+
+	return NULL;
+}
+
+static void __del_node(struct poll_node *node)
+{
+	evl_put_file(node->sfilp);
+	evl_free(node);
+}
+
+static int del_node(struct event_poller *poller,
+		    struct evl_poller_ctlreq *creq)
+{
+	struct poll_node *node;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&poller->lock, flags);
+
+	node = lookup_node(&poller->node_index, creq->fd);
+	if (node == NULL) {
+		raw_spin_unlock_irqrestore(&poller->lock, flags);
+		return -EBADF;
+	}
+
+	rb_erase(&node->rb, &poller->node_index);
+	list_del(&node->next);
+	poller->nodenr--;
+	poller->generation++;
+
+	raw_spin_unlock_irqrestore(&poller->lock, flags);
+
+	__del_node(node);
+
+	return 0;
+}
+
+static inline
+int mod_node(struct event_poller *poller,
+	     struct evl_poller_ctlreq *creq)
+{
+	struct poll_node *node;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&poller->lock, flags);
+
+	node = lookup_node(&poller->node_index, creq->fd);
+	if (node == NULL) {
+		raw_spin_unlock_irqrestore(&poller->lock, flags);
+		return -EBADF;
+	}
+
+	node->events_polled = creq->events;
+	poller->generation++;
+
+	raw_spin_unlock_irqrestore(&poller->lock, flags);
+
+	return 0;
+}
+
+static inline
+int setup_node(struct event_poller *poller,
+	       struct evl_poller_ctlreq *creq)
+{
+	int ret;
+
+	switch (creq->action) {
+	case EVL_POLLER_CTLADD:
+		ret = add_node(poller, creq);
+		break;
+	case EVL_POLLER_CTLDEL:
+		ret = del_node(poller, creq);
+		break;
+	case EVL_POLLER_CTLMOD:
+		ret = mod_node(poller, creq);
+		break;
+	}
+
+	return -EINVAL;
+}
+
+static int collect_events(struct event_poller *poller,
+			  struct evl_poll_event __user *u_ev,
+			  int maxevents, bool do_poll)
+{
+	struct evl_thread *curr = evl_current_thread();
+	struct evl_poll_watchpoint *wpt, *table;
+	int ret, n, nr, count = 0, ready;
+	struct evl_poll_event ev;
+	struct poll_node *node;
+	unsigned int generation;
+	unsigned long flags;
+	struct file *filp;
+
+	raw_spin_lock_irqsave(&poller->lock, flags);
+
+	nr = poller->nodenr;
+	if (nr == 0) {
+		raw_spin_unlock_irqrestore(&poller->lock, flags);
+		return -EINVAL;
+	}
+
+	/*
+	 * Check whether the registered nodes are in sync with the
+	 * caller's registered watchpoints (if any). Go polling
+	 * directly using those watchpoints if so, otherwise resync.
+	 */
+	table = curr->poll_context.table;
+	generation = poller->generation;
+	if (likely(generation == curr->poll_context.generation || !do_poll))
+		goto collect;
+
+	/* Need to resync. */
+	do {
+		generation = poller->generation;
+		raw_spin_unlock_irqrestore(&poller->lock, flags);
+		if (table)
+			evl_free(table);
+		table = evl_alloc(sizeof(*wpt) * nr);
+		if (table == NULL) {
+			curr->poll_context.nr = 0;
+			curr->poll_context.table = NULL;
+			curr->poll_context.generation = 0;
+			return -ENOMEM;
+		}
+		raw_spin_lock_irqsave(&poller->lock, flags);
+	} while (generation != poller->generation);
+
+	curr->poll_context.table = table;
+	curr->poll_context.nr = nr;
+	curr->poll_context.generation = generation;
+
+	wpt = table;
+	list_for_each_entry(node, &poller->node_list, next) {
+		evl_get_fileref(node->sfilp);
+		wpt->node = *node;
+		wpt++;
+	}
+
+collect:
+	raw_spin_unlock_irqrestore(&poller->lock, flags);
+
+	/*
+	 * Provided that each f_op->release of the OOB drivers maintaining
+	 * wpt->node.sfilp is properly calling evl_release_file()
+	 * before it dismantles the file, having a reference on
+	 * wpt->sfilp guarantees us that wpt->sfilp->filp is stable
+	 * until the last ref. is dropped via evl_put_file().
+	 */
+	for (n = 0, wpt = table; n < nr; n++, wpt++) {
+		if (do_poll) {
+			ready = POLLIN|POLLOUT|POLLRDNORM|POLLWRNORM; /* Default. */
+			filp = wpt->node.sfilp->filp;
+			if (filp->f_op->oob_poll)
+				ready = filp->f_op->oob_poll(filp, &wpt->wait);
+		} else
+			ready = wpt->events_received & wpt->node.events_polled;
+
+		if (ready) {
+			ev.fd = wpt->node.fd;
+			ev.events = ready;
+			ret = raw_copy_to_user(u_ev, &ev, sizeof(ev));
+			if (ret)
+				return -EFAULT;
+			u_ev++;
+			if (++count >= maxevents)
+				break;
+		}
+	}
+
+	return count;
+}
+
+static inline void clear_wait(void)
+{
+	struct evl_thread *curr = evl_current_thread();
+	struct evl_poll_watchpoint *wpt;
+	unsigned long flags;
+	int n;
+
+	/*
+	 * Current stopped waiting for events, remove the watchpoints
+	 * we have been monitoring so far from their poll heads.
+	 * wpt->head->lock serializes with __evl_signal_poll_events().
+	 */
+	for (n = 0, wpt = curr->poll_context.table;
+	     n < curr->poll_context.nr; n++, wpt++) {
+		raw_spin_lock_irqsave(&wpt->head->lock, flags);
+		list_del(&wpt->wait.next);
+		raw_spin_unlock_irqrestore(&wpt->head->lock, flags);
+		evl_put_file(wpt->node.sfilp);
+	}
+}
+
+static inline
+int wait_events(struct file *filp,
+		struct event_poller *poller,
+		struct evl_poller_waitreq *wreq)
+{
+	enum evl_tmode tmode;
+	unsigned long flags;
+	ktime_t timeout;
+	int info, count;
+
+	if (wreq->nrevents < 0 || wreq->nrevents > poller->nodenr)
+		return -EINVAL;
+
+	if ((unsigned long)wreq->timeout.tv_nsec >= ONE_BILLION)
+		return -EINVAL;
+
+	if (wreq->nrevents == 0)
+		return 0;
+
+	count = collect_events(poller, wreq->events,
+			       wreq->nrevents, true);
+	if (count > 0 || count == -EFAULT)
+		goto unwait;
+	if (count < 0)
+		return count;
+
+	if (filp->f_flags & O_NONBLOCK) {
+		count = -EAGAIN;
+		goto unwait;
+	}
+
+	timeout = timespec_to_ktime(wreq->timeout);
+	tmode = timeout ? EVL_ABS : EVL_REL;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	info = 0;
+	if (!poller->readied)
+		info = evl_sleep_on_syn(&poller->wait_queue, timeout, tmode);
+
+	xnlock_put_irqrestore(&nklock, flags);
+	if (info)
+		/* No way we could have received T_RMID. */
+		count = info & T_BREAK ? -EINTR : -ETIMEDOUT;
+	else
+		count = collect_events(poller, wreq->events,
+				       wreq->nrevents, false);
+unwait:
+	clear_wait();
+
+	return count;
+}
+
+static long poller_oob_ioctl(struct file *filp, unsigned int cmd,
+			     unsigned long arg)
+{
+	struct event_poller *poller = element_of(filp, struct event_poller);
+	struct evl_poller_waitreq wreq, __user *u_wreq;
+	struct evl_poller_ctlreq creq, __user *u_creq;
+	int ret;
+
+	switch (cmd) {
+	case EVL_POLIOC_CTL:
+		u_creq = (typeof(u_creq))arg;
+		ret = raw_copy_from_user(&creq, u_creq, sizeof(creq));
+		if (ret)
+			return -EFAULT;
+		ret = setup_node(poller, &creq);
+		break;
+	case EVL_POLIOC_WAIT:
+		u_wreq = (typeof(u_wreq))arg;
+		ret = raw_copy_from_user(&wreq, u_wreq, sizeof(wreq));
+		if (ret)
+			return -EFAULT;
+		ret = wait_events(filp, poller, &wreq);
+		if (ret >= 0 && raw_put_user(ret, &u_wreq->nrevents))
+			return -EFAULT;
+		ret = 0;
+		break;
+	default:
+		ret = -ENOTTY;
+	}
+
+	return ret;
+}
+
+static const struct file_operations poller_fops = {
+	.open		= evl_open_element,
+	.release	= evl_close_element,
+	.oob_ioctl	= poller_oob_ioctl,
+};
+
+static struct evl_element *
+poller_factory_build(struct evl_factory *fac, const char *name,
+		     void __user *u_attrs, u32 *state_offp)
+{
+	struct evl_poller_attrs attrs;
+	struct event_poller *poller;
+	struct evl_clock *clock;
+	int ret;
+
+	ret = copy_from_user(&attrs, u_attrs, sizeof(attrs));
+	if (ret)
+		return ERR_PTR(-EFAULT);
+
+	clock = evl_get_clock_by_fd(attrs.clockfd);
+	if (clock == NULL)
+		return ERR_PTR(-EINVAL);
+
+	poller = kzalloc(sizeof(*poller), GFP_KERNEL);
+	if (poller == NULL) {
+		ret = -ENOMEM;
+		goto fail_alloc;
+	}
+
+	ret = evl_init_element(&poller->element, &evl_poller_factory);
+	if (ret)
+		goto fail_element;
+
+	poller->node_index = RB_ROOT;
+	INIT_LIST_HEAD(&poller->node_list);
+	evl_init_syn(&poller->wait_queue, EVL_SYN_PRIO, clock, NULL);
+	raw_spin_lock_init(&poller->lock);
+
+	return &poller->element;
+
+fail_element:
+	kfree(poller);
+fail_alloc:
+	evl_put_clock(clock);
+
+	return ERR_PTR(ret);
+}
+
+static inline void flush_nodes(struct event_poller *poller)
+{
+	struct poll_node *node;
+
+	if (!list_empty(&poller->node_list))
+		list_for_each_entry(node, &poller->node_list, next)
+			__del_node(node);
+}
+
+static void poller_factory_dispose(struct evl_element *e)
+{
+	struct event_poller *poller;
+
+	poller = container_of(e, struct event_poller, element);
+
+	flush_nodes(poller);
+	evl_put_clock(poller->wait_queue.clock);
+	evl_destroy_element(&poller->element);
+	kfree_rcu(poller, element.rcu);
+}
+
+struct evl_factory evl_poller_factory = {
+	.name	=	"poller",
+	.fops	=	&poller_fops,
+	.build =	poller_factory_build,
+	.dispose =	poller_factory_dispose,
+	.nrdev	=	CONFIG_EVENLESS_NR_POLLERS,
+	.flags	=	EVL_FACTORY_CLONE,
+};
diff --git a/kernel/evenless/sched/Makefile b/kernel/evenless/sched/Makefile
new file mode 100644
index 000000000000..a54f368cfa9c
--- /dev/null
+++ b/kernel/evenless/sched/Makefile
@@ -0,0 +1,11 @@
+obj-$(CONFIG_EVENLESS) += evenless.o
+
+ccflags-y += -Ikernel
+
+evenless-y :=	\
+	core.o	\
+	rt.o	\
+	idle.o	\
+	weak.o
+
+evenless-$(CONFIG_EVENLESS_SCHED_QUOTA) += quota.o
diff --git a/kernel/evenless/sched/core.c b/kernel/evenless/sched/core.c
new file mode 100644
index 000000000000..c1e08d9e126d
--- /dev/null
+++ b/kernel/evenless/sched/core.c
@@ -0,0 +1,1064 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2001, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/signal.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <linux/cpuidle.h>
+#include <linux/mmu_context.h>
+#include <asm/div64.h>
+#include <asm/switch_to.h>
+#include <evenless/sched.h>
+#include <evenless/thread.h>
+#include <evenless/timer.h>
+#include <evenless/memory.h>
+#include <evenless/clock.h>
+#include <evenless/tick.h>
+#include <evenless/monitor.h>
+#include <uapi/evenless/signal.h>
+#include <trace/events/evenless.h>
+
+DEFINE_PER_CPU(struct evl_rq, evl_runqueues);
+EXPORT_PER_CPU_SYMBOL_GPL(evl_runqueues);
+
+struct cpumask evl_cpu_affinity = CPU_MASK_ALL;
+EXPORT_SYMBOL_GPL(evl_cpu_affinity);
+
+LIST_HEAD(evl_thread_list);
+
+int evl_nrthreads;
+
+static struct evl_sched_class *evl_sched_highest;
+
+#define for_each_evl_sched_class(p)			\
+	for (p = evl_sched_highest; p; p = p->next)
+
+static void register_one_class(struct evl_sched_class *sched_class)
+{
+	sched_class->next = evl_sched_highest;
+	evl_sched_highest = sched_class;
+
+	/*
+	 * Classes shall be registered by increasing priority order,
+	 * idle first and up.
+	 */
+	EVL_WARN_ON(CORE, sched_class->next &&
+		    sched_class->next->weight > sched_class->weight);
+}
+
+static void register_classes(void)
+{
+	register_one_class(&evl_sched_idle);
+	register_one_class(&evl_sched_weak);
+#ifdef CONFIG_EVENLESS_SCHED_QUOTA
+	register_one_class(&evl_sched_quota);
+#endif
+	register_one_class(&evl_sched_rt);
+}
+
+#ifdef CONFIG_EVENLESS_WATCHDOG
+
+static unsigned long wd_timeout_arg = CONFIG_EVENLESS_WATCHDOG_TIMEOUT;
+module_param_named(watchdog_timeout, wd_timeout_arg, ulong, 0644);
+
+static inline ktime_t get_watchdog_timeout(void)
+{
+	return ns_to_ktime(wd_timeout_arg * 1000000000ULL);
+}
+
+static void watchdog_handler(struct evl_timer *timer) /* hard irqs off */
+{
+	struct evl_rq *this_rq = this_evl_rq();
+	struct evl_thread *curr = this_rq->curr;
+
+	trace_evl_watchdog_signal(curr);
+
+	/*
+	 * CAUTION: The watchdog tick might have been delayed while we
+	 * were busy switching the CPU to in-band context at the
+	 * trigger date eventually. Make sure that we are not about to
+	 * kick the incoming root thread.
+	 */
+	if (curr->state & T_ROOT)
+		return;
+
+	if (curr->state & T_USER) {
+		printk(EVL_WARNING "watchdog triggered on CPU #%d -- runaway thread "
+		       "'%s' signaled\n", evl_rq_cpu(this_rq), curr->name);
+		evl_call_mayday(curr, SIGDEBUG_WATCHDOG);
+	} else {
+		printk(EVL_WARNING "watchdog triggered on CPU #%d -- runaway thread "
+		       "'%s' canceled\n", evl_rq_cpu(this_rq), curr->name);
+		/*
+		 * On behalf on an IRQ handler, evl_cancel_thread()
+		 * would go half way cancelling the preempted
+		 * thread. Therefore we manually raise T_KICKED to
+		 * cause the next call to evl_suspend_thread() to
+		 * return early in T_BREAK condition, and T_CANCELD so
+		 * that @thread exits next time it invokes
+		 * evl_test_cancel().
+		 */
+		xnlock_get(&nklock);
+		curr->info |= (T_KICKED|T_CANCELD);
+		xnlock_put(&nklock);
+	}
+}
+
+#endif /* CONFIG_EVENLESS_WATCHDOG */
+
+static void roundrobin_handler(struct evl_timer *timer) /* hard irqs off */
+{
+	struct evl_rq *this_rq;
+
+	this_rq = container_of(timer, struct evl_rq, rrbtimer);
+	xnlock_get(&nklock);
+	evl_sched_tick(this_rq);
+	xnlock_put(&nklock);
+}
+
+static void proxy_tick_handler(struct evl_timer *timer) /* hard irqs off */
+{
+	struct evl_rq *this_rq;
+
+	/*
+	 * Propagating the proxy tick to the host kernel is a low
+	 * priority duty: postpone this until the end of the core tick
+	 * handler.
+	 */
+	this_rq = container_of(timer, struct evl_rq, htimer);
+	this_rq->lflags |= RQ_TPROXY;
+	this_rq->lflags &= ~RQ_TDEFER;
+}
+
+static void init_rq(struct evl_rq *rq, int cpu)
+{
+	struct evl_sched_class *sched_class;
+	struct evl_init_thread_attr iattr;
+	const char *name_fmt;
+
+#ifdef CONFIG_SMP
+	rq->cpu = cpu;
+	name_fmt = "ROOT/%u";
+	rq->proxy_timer_name = kasprintf(GFP_KERNEL, "[proxy-timer/%u]", cpu);
+	rq->rrb_timer_name = kasprintf(GFP_KERNEL, "[rrb-timer/%u]", cpu);
+	cpumask_clear(&rq->resched);
+#else
+	name_fmt = "ROOT";
+	rq->proxy_timer_name = kstrdup("[proxy-timer]", GFP_KERNEL);
+	rq->rrb_timer_name = kstrdup("[rrb-timer]", GFP_KERNEL);
+#endif
+	for_each_evl_sched_class(sched_class) {
+		if (sched_class->sched_init)
+			sched_class->sched_init(rq);
+	}
+
+	rq->status = 0;
+	rq->lflags = RQ_IDLE;
+	rq->curr = &rq->root_thread;
+
+	/*
+	 * No direct handler here since proxy timer events are handled
+	 * specifically by the generic timer code.
+	 */
+	evl_init_timer(&rq->htimer, &evl_mono_clock, proxy_tick_handler,
+		       rq, EVL_TIMER_IGRAVITY);
+	evl_set_timer_priority(&rq->htimer, EVL_TIMER_LOPRIO);
+	evl_set_timer_name(&rq->htimer, rq->proxy_timer_name);
+	evl_init_timer(&rq->rrbtimer, &evl_mono_clock, roundrobin_handler,
+		       rq, EVL_TIMER_IGRAVITY);
+	evl_set_timer_name(&rq->rrbtimer, rq->rrb_timer_name);
+	evl_set_timer_priority(&rq->rrbtimer, EVL_TIMER_LOPRIO);
+#ifdef CONFIG_EVENLESS_WATCHDOG
+	evl_init_timer(&rq->wdtimer, &evl_mono_clock, watchdog_handler,
+		       rq, EVL_TIMER_IGRAVITY);
+	evl_set_timer_name(&rq->wdtimer, "[watchdog]");
+	evl_set_timer_priority(&rq->wdtimer, EVL_TIMER_LOPRIO);
+#endif /* CONFIG_EVENLESS_WATCHDOG */
+
+	evl_set_current_account(rq, &rq->root_thread.stat.account);
+
+	/*
+	 * Postpone evl_init_thread() - which sets RQ_SCHED upon
+	 * setting the schedparams for the root thread - until we have
+	 * enough of the runqueue initialized, so that attempting to
+	 * reschedule from exit_oob_irq() later on is harmless.
+	 */
+	iattr.flags = T_ROOT;
+	iattr.affinity = *cpumask_of(cpu);
+	iattr.sched_class = &evl_sched_idle;
+	iattr.sched_param.idle.prio = EVL_IDLE_PRIO;
+	evl_init_thread(&rq->root_thread, &iattr, rq, name_fmt, cpu);
+
+	evl_init_syn(&rq->yield_sync, EVL_SYN_FIFO,
+		     &evl_mono_clock, NULL);
+
+	dovetail_init_altsched(&rq->root_thread.altsched);
+
+	list_add_tail(&rq->root_thread.next, &evl_thread_list);
+	evl_nrthreads++;
+}
+
+static void destroy_rq(struct evl_rq *rq) /* nklock held, irqs off */
+{
+	evl_destroy_timer(&rq->htimer);
+	evl_destroy_timer(&rq->rrbtimer);
+	kfree(rq->proxy_timer_name);
+	kfree(rq->rrb_timer_name);
+	evl_destroy_timer(&rq->root_thread.ptimer);
+	evl_destroy_timer(&rq->root_thread.rtimer);
+#ifdef CONFIG_EVENLESS_WATCHDOG
+	evl_destroy_timer(&rq->wdtimer);
+#endif /* CONFIG_EVENLESS_WATCHDOG */
+}
+
+static inline void set_thread_running(struct evl_rq *rq,
+				      struct evl_thread *thread)
+{
+	thread->state &= ~T_READY;
+	if (thread->state & T_RRB)
+		evl_start_timer(&rq->rrbtimer,
+				evl_abs_timeout(&rq->rrbtimer, thread->rrperiod),
+				EVL_INFINITE);
+	else
+		evl_stop_timer(&rq->rrbtimer);
+}
+
+/* Must be called with nklock locked, interrupts off. */
+struct evl_thread *evl_pick_thread(struct evl_rq *rq)
+{
+	struct evl_sched_class *sched_class __maybe_unused;
+	struct evl_thread *curr = rq->curr;
+	struct evl_thread *thread;
+
+	if (!(curr->state & (EVL_THREAD_BLOCK_BITS | T_ZOMBIE))) {
+		/*
+		 * Do not preempt the current thread if it holds the
+		 * scheduler lock. However, such lock is never
+		 * considered for the root thread which may never
+		 * defer scheduling.
+		 */
+		if (curr->lock_count > 0 && !(curr->state & T_ROOT)) {
+			evl_set_self_resched(rq);
+			return curr;
+		}
+		/*
+		 * Push the current thread back to the run queue of
+		 * the scheduling class it belongs to, if not yet
+		 * linked to it (T_READY tells us if it is).
+		 */
+		if (!(curr->state & T_READY)) {
+			evl_requeue_thread(curr);
+			curr->state |= T_READY;
+		}
+	}
+
+	/*
+	 * Find the runnable thread having the highest priority among
+	 * all scheduling classes, scanned by decreasing priority.
+	 */
+	for_each_evl_sched_class(sched_class) {
+		thread = sched_class->sched_pick(rq);
+		if (thread) {
+			set_thread_running(rq, thread);
+			return thread;
+		}
+	}
+
+	return NULL; /* Never executed because of the idle class. */
+}
+
+#ifdef CONFIG_EVENLESS_DEBUG_LOCKING
+
+void evl_disable_preempt(void)
+{
+	struct evl_rq *rq = this_evl_rq();
+	struct evl_thread *curr = rq->curr;
+
+	/* If RQ_IRQ is set, the scheduler is already locked. */
+
+	if (rq->lflags & RQ_IRQ)
+		return;
+
+	/*
+	 * The fast evl_current_thread() accessor carries the
+	 * relevant lock nesting count only if current runs in OOB
+	 * context. Otherwise, if the caller is running in-band,
+	 * we fall back to the root thread on the current rq, which
+	 * must be done with IRQs off to prevent CPU migration.
+	 * Either way, we don't need to grab the super lock.
+	 */
+	EVL_WARN_ON_ONCE(CORE, (curr->state & T_ROOT) &&
+			 !hard_irqs_disabled());
+
+	curr->lock_count++;
+}
+EXPORT_SYMBOL(evl_disable_preempt);
+
+void evl_enable_preempt(void)
+{
+	struct evl_rq *rq = this_evl_rq();
+	struct evl_thread *curr = rq->curr;
+
+	if (rq->lflags & RQ_IRQ)
+		return;
+
+	if (!EVL_ASSERT(CORE, curr->lock_count > 0))
+		return;
+
+	if (--curr->lock_count == 0)
+		evl_schedule();
+}
+EXPORT_SYMBOL(evl_enable_preempt);
+
+#endif /* CONFIG_EVENLESS_DEBUG_LOCKING */
+
+/* nklock locked, interrupts off. */
+void evl_putback_thread(struct evl_thread *thread)
+{
+	if (thread->state & T_READY)
+		evl_dequeue_thread(thread);
+	else
+		thread->state |= T_READY;
+
+	evl_enqueue_thread(thread);
+	evl_set_resched(thread->rq);
+}
+
+/* nklock locked, interrupts off. */
+int evl_set_thread_policy(struct evl_thread *thread,
+			  struct evl_sched_class *sched_class,
+			  const union evl_sched_param *p)
+{
+	struct evl_sched_class *orig_effective_class __maybe_unused;
+	bool effective;
+	int ret;
+
+	/* Check parameters early on. */
+	ret = evl_check_schedparams(sched_class, thread, p);
+	if (ret)
+		return ret;
+
+	/*
+	 * Declaring a thread to a new scheduling class may fail, so
+	 * we do that early, while the thread is still a member of the
+	 * previous class. However, this also means that the
+	 * declaration callback shall not do anything that might
+	 * affect the previous class (such as touching thread->rq_next
+	 * for instance).
+	 */
+	if (sched_class != thread->base_class) {
+		ret = evl_declare_thread(sched_class, thread, p);
+		if (ret)
+			return ret;
+	}
+
+	/*
+	 * As a special case, we may be called from evl_init_thread()
+	 * with no previous scheduling class at all.
+	 */
+	if (likely(thread->base_class != NULL)) {
+		if (thread->state & T_READY)
+			evl_dequeue_thread(thread);
+
+		if (sched_class != thread->base_class)
+			evl_forget_thread(thread);
+	}
+
+	/*
+	 * Set the base and effective scheduling parameters. However,
+	 * evl_set_schedparam() will deny lowering the effective
+	 * priority if a boost is undergoing, only recording the
+	 * change into the base priority field in such situation.
+	 */
+	thread->base_class = sched_class;
+	/*
+	 * Referring to the effective class from a setparam() handler
+	 * is wrong: make sure to break if so.
+	 */
+	if (EVL_DEBUG(CORE)) {
+		orig_effective_class = thread->sched_class;
+		thread->sched_class = NULL;
+	}
+
+	/*
+	 * This is the ONLY place where calling
+	 * evl_set_schedparam() is legit, sane and safe.
+	 */
+	effective = evl_set_schedparam(thread, p);
+	if (effective) {
+		thread->sched_class = sched_class;
+		thread->wprio = evl_calc_weighted_prio(sched_class, thread->cprio);
+	} else if (EVL_DEBUG(CORE))
+		thread->sched_class = orig_effective_class;
+
+	if (thread->state & T_READY)
+		evl_enqueue_thread(thread);
+
+	/*
+	 * Make sure not to raise RQ_SCHED when setting up the root
+	 * thread, so that we can't start rescheduling from
+	 * exit_oob_irq() before all CPUs have their runqueue fully
+	 * built. Filtering on T_ROOT here is correct because the root
+	 * thread enters the idle class once as part of the runqueue
+	 * setup process and never leaves it afterwards.
+	 */
+	if (!(thread->state & (T_DORMANT|T_ROOT)))
+		evl_set_resched(thread->rq);
+	else
+		EVL_WARN_ON(CORE, (thread->state & T_ROOT) &&
+			    sched_class != &evl_sched_idle);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(evl_set_thread_policy);
+
+/* nklock locked, interrupts off. */
+bool evl_set_effective_thread_priority(struct evl_thread *thread, int prio)
+{
+	int wprio = evl_calc_weighted_prio(thread->base_class, prio);
+
+	thread->bprio = prio;
+	if (wprio == thread->wprio)
+		return true;
+
+	/*
+	 * We may not lower the effective/current priority of a
+	 * boosted thread when changing the base scheduling
+	 * parameters. Only evl_track_thread_policy() and
+	 * evl_protect_thread_priority() may do so when dealing with PI
+	 * and PP synchs resp.
+	 */
+	if (wprio < thread->wprio && (thread->state & T_BOOST))
+		return false;
+
+	thread->cprio = prio;
+
+	trace_evl_thread_set_current_prio(thread);
+
+	return true;
+}
+
+/* nklock locked, interrupts off. */
+void evl_track_thread_policy(struct evl_thread *thread,
+			     struct evl_thread *target)
+{
+	union evl_sched_param param;
+
+	/*
+	 * Inherit (or reset) the effective scheduling class and
+	 * priority of a thread. Unlike evl_set_thread_policy(), this
+	 * routine is allowed to lower the weighted priority with no
+	 * restriction, even if a boost is undergoing.
+	 */
+	if (thread->state & T_READY)
+		evl_dequeue_thread(thread);
+	/*
+	 * Self-targeting means to reset the scheduling policy and
+	 * parameters to the base settings. Otherwise, make thread
+	 * inherit the scheduling parameters from target.
+	 */
+	if (target == thread) {
+		thread->sched_class = thread->base_class;
+		evl_track_priority(thread, NULL);
+		/*
+		 * Per SuSv2, resetting the base scheduling parameters
+		 * should not move the thread to the tail of its
+		 * priority group.
+		 */
+		if (thread->state & T_READY)
+			evl_requeue_thread(thread);
+
+	} else {
+		evl_get_schedparam(target, &param);
+		thread->sched_class = target->sched_class;
+		evl_track_priority(thread, &param);
+		if (thread->state & T_READY)
+			evl_enqueue_thread(thread);
+	}
+
+	trace_evl_thread_set_current_prio(thread);
+
+	evl_set_resched(thread->rq);
+}
+
+/* nklock locked, interrupts off. */
+void evl_protect_thread_priority(struct evl_thread *thread, int prio)
+{
+	/*
+	 * Apply a PP boost by changing the effective priority of a
+	 * thread, forcing it to the RT class. Like
+	 * evl_track_thread_policy(), this routine is allowed to lower
+	 * the weighted priority with no restriction, even if a boost
+	 * is undergoing.
+	 *
+	 * This routine only deals with active boosts, resetting the
+	 * base priority when leaving a PP boost is obtained by a call
+	 * to evl_track_thread_policy().
+	 */
+	if (thread->state & T_READY)
+		evl_dequeue_thread(thread);
+
+	thread->sched_class = &evl_sched_rt;
+	evl_ceil_priority(thread, prio);
+
+	if (thread->state & T_READY)
+		evl_enqueue_thread(thread);
+
+	trace_evl_thread_set_current_prio(thread);
+
+	evl_set_resched(thread->rq);
+}
+
+static void migrate_thread(struct evl_thread *thread, struct evl_rq *rq)
+{
+	struct evl_sched_class *sched_class = thread->sched_class;
+
+	if (thread->state & T_READY) {
+		evl_dequeue_thread(thread);
+		thread->state &= ~T_READY;
+	}
+
+	if (sched_class->sched_migrate)
+		sched_class->sched_migrate(thread, rq);
+	/*
+	 * WARNING: the scheduling class may have just changed as a
+	 * result of calling the per-class migration hook.
+	 */
+	thread->rq = rq;
+}
+
+/*
+ * nklock locked, interrupts off. Thread may be blocked.
+ */
+void evl_migrate_rq(struct evl_thread *thread, struct evl_rq *rq)
+{
+	struct evl_rq *last_rq = thread->rq;
+
+	migrate_thread(thread, rq);
+
+	if (!(thread->state & EVL_THREAD_BLOCK_BITS)) {
+		evl_requeue_thread(thread);
+		thread->state |= T_READY;
+		evl_set_resched(last_rq);
+	}
+}
+
+void evl_init_schedq(struct evl_multilevel_queue *q)
+{
+	int prio;
+
+	q->elems = 0;
+	bitmap_zero(q->prio_map, EVL_MLQ_LEVELS);
+
+	for (prio = 0; prio < EVL_MLQ_LEVELS; prio++)
+		INIT_LIST_HEAD(q->heads + prio);
+}
+
+static inline int get_qindex(struct evl_multilevel_queue *q, int prio)
+{
+	/*
+	 * BIG FAT WARNING: We need to rescale the priority level to a
+	 * 0-based range. We use find_first_bit() to scan the bitmap
+	 * which is a bit scan forward operation. Therefore, the lower
+	 * the index value, the higher the priority (since least
+	 * significant bits will be found first when scanning the
+	 * bitmap).
+	 */
+	return EVL_MLQ_LEVELS - prio - 1;
+}
+
+static struct list_head *add_q(struct evl_multilevel_queue *q, int prio)
+{
+	struct list_head *head;
+	int idx;
+
+	idx = get_qindex(q, prio);
+	head = q->heads + idx;
+	q->elems++;
+
+	/* New item is not linked yet. */
+	if (list_empty(head))
+		__set_bit(idx, q->prio_map);
+
+	return head;
+}
+
+void evl_add_schedq(struct evl_multilevel_queue *q,
+		    struct evl_thread *thread)
+{
+	struct list_head *head = add_q(q, thread->cprio);
+	list_add(&thread->rq_next, head);
+}
+
+void evl_add_schedq_tail(struct evl_multilevel_queue *q,
+			 struct evl_thread *thread)
+{
+	struct list_head *head = add_q(q, thread->cprio);
+	list_add_tail(&thread->rq_next, head);
+}
+
+static void del_q(struct evl_multilevel_queue *q,
+		  struct list_head *entry, int idx)
+{
+	struct list_head *head = q->heads + idx;
+
+	list_del(entry);
+	q->elems--;
+
+	if (list_empty(head))
+		__clear_bit(idx, q->prio_map);
+}
+
+void evl_del_schedq(struct evl_multilevel_queue *q,
+		    struct evl_thread *thread)
+{
+	del_q(q, &thread->rq_next, get_qindex(q, thread->cprio));
+}
+
+struct evl_thread *evl_get_schedq(struct evl_multilevel_queue *q)
+{
+	struct evl_thread *thread;
+	struct list_head *head;
+	int idx;
+
+	if (q->elems == 0)
+		return NULL;
+
+	idx = evl_get_schedq_weight(q);
+	head = q->heads + idx;
+	thread = list_first_entry(head, struct evl_thread, rq_next);
+	del_q(q, &thread->rq_next, idx);
+
+	return thread;
+}
+
+struct evl_thread *
+evl_lookup_schedq(struct evl_multilevel_queue *q, int prio)
+{
+	struct list_head *head;
+	int idx;
+
+	idx = get_qindex(q, prio);
+	head = q->heads + idx;
+	if (list_empty(head))
+		return NULL;
+
+	return list_first_entry(head, struct evl_thread, rq_next);
+}
+
+struct evl_thread *evl_rt_pick(struct evl_rq *rq)
+{
+	struct evl_multilevel_queue *q = &rq->rt.runnable;
+	struct evl_thread *thread;
+	struct list_head *head;
+	int idx;
+
+	if (q->elems == 0)
+		return NULL;
+
+	/*
+	 * Some scheduling policies may be implemented as variants of
+	 * the core SCHED_FIFO class, sharing its runqueue
+	 * (e.g. SCHED_QUOTA). This means that we have to do some
+	 * cascading to call the right pick handler eventually.
+	 */
+	idx = evl_get_schedq_weight(q);
+	head = q->heads + idx;
+
+	/*
+	 * The active class (i.e. ->sched_class) is the one currently
+	 * queuing the thread, reflecting any priority boost due to
+	 * PI.
+	 */
+	thread = list_first_entry(head, struct evl_thread, rq_next);
+	if (unlikely(thread->sched_class != &evl_sched_rt))
+		return thread->sched_class->sched_pick(rq);
+
+	del_q(q, &thread->rq_next, idx);
+
+	return thread;
+}
+
+static inline int test_resched(struct evl_rq *rq)
+{
+	int resched = evl_need_resched(rq);
+#ifdef CONFIG_SMP
+	/* Send resched IPI to remote CPU(s). */
+	if (unlikely(!cpumask_empty(&rq->resched))) {
+		smp_mb();
+		irq_pipeline_send_remote(RESCHEDULE_OOB_IPI, &rq->resched);
+		cpumask_clear(&rq->resched);
+	}
+#endif
+	rq->status &= ~RQ_SCHED;
+
+	return resched;
+}
+
+static inline void enter_root(struct evl_thread *root)
+{
+#ifdef CONFIG_EVENLESS_WATCHDOG
+	evl_stop_timer(&evl_thread_rq(root)->wdtimer);
+#endif
+}
+
+static inline void leave_root(struct evl_thread *root)
+{
+	dovetail_resume_oob(&root->altsched);
+
+#ifdef CONFIG_EVENLESS_WATCHDOG
+	evl_start_timer(&evl_thread_rq(root)->wdtimer,
+			evl_abs_timeout(&evl_thread_rq(root)->wdtimer,
+					get_watchdog_timeout()),
+			EVL_INFINITE);
+#endif
+}
+
+irqreturn_t __evl_schedule_handler(int irq, void *dev_id)
+{
+	/* hw interrupts are off. */
+	trace_evl_schedule_remote(this_evl_rq());
+	evl_schedule();
+
+	return IRQ_HANDLED;
+}
+
+bool ___evl_schedule(struct evl_rq *this_rq)
+{
+	struct evl_thread *prev, *next, *curr;
+	bool switched, shadow;
+	unsigned long flags;
+
+	trace_evl_schedule(this_rq);
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	curr = this_rq->curr;
+	/*
+	 * CAUTION: curr->altsched.task may be unsynced and even stale
+	 * if curr == &this_rq->root_thread, since the task logged by
+	 * leave_root() may not still be the current one. Use
+	 * "current" for disambiguating.
+	 */
+	if (curr->state & T_USER)
+		evl_commit_monitor_ceiling(curr);
+
+	switched = false;
+	if (!test_resched(this_rq))
+		goto out;
+
+	next = evl_pick_thread(this_rq);
+	if (next == curr) {
+		if (unlikely(next->state & T_ROOT)) {
+			if (this_rq->lflags & RQ_TPROXY)
+				evl_notify_proxy_tick(this_rq);
+			if (this_rq->lflags & RQ_TDEFER)
+				evl_program_local_tick(&evl_mono_clock);
+		}
+		goto out;
+	}
+
+	prev = curr;
+
+	trace_evl_switch_context(prev, next);
+
+	this_rq->curr = next;
+	shadow = true;
+
+	if (prev->state & T_ROOT) {
+		leave_root(prev);
+		shadow = false;
+	} else if (next->state & T_ROOT) {
+		if (this_rq->lflags & RQ_TPROXY)
+			evl_notify_proxy_tick(this_rq);
+		if (this_rq->lflags & RQ_TDEFER)
+			evl_program_local_tick(&evl_mono_clock);
+		enter_root(next);
+	}
+
+	evl_switch_account(this_rq, &next->stat.account);
+	evl_inc_counter(&next->stat.csw);
+	dovetail_context_switch(&prev->altsched, &next->altsched);
+
+	/*
+	 * Hard interrupts must be disabled here (has to be done on
+	 * entry of the host kernel's switch_to() function), but it is
+	 * what callers expect, particularly the reschedule of an IRQ
+	 * handler that hit before we call evl_schedule() from
+	 * evl_suspend_thread() when switching a thread to in-band
+	 * context.
+	 */
+	if (EVL_WARN_ON_ONCE(CORE, !hard_irqs_disabled()))
+		hard_irqs_disabled();
+
+	/*
+	 * Refresh the current rq and thread pointers, this is
+	 * required to cope with in-band<->oob stage transitions.
+	 */
+	this_rq = this_evl_rq();
+	curr = this_rq->curr;
+
+	EVL_WARN_ON(CORE, curr->state & EVL_THREAD_BLOCK_BITS);
+
+	if (shadow && !test_thread_local_flags(_TLF_OOB))
+		goto inband;
+
+	switched = true;
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return switched;
+
+inband:
+	/*
+	 * This represents the switch tail code of the regular
+	 * schedule() routine for a thread re-entering the in-band stage,
+	 * as a result of an earlier call to evl_switch_inband():
+	 * post_event() -> ... IRQ:wake_up_process() -> schedule()
+	 */
+	EVL_WARN_ON_ONCE(CORE, !(preempt_count() & STAGE_MASK));
+	preempt_count_sub(STAGE_OFFSET);
+
+	return true;
+}
+EXPORT_SYMBOL_GPL(___evl_schedule);
+
+struct evl_sched_class *
+evl_find_sched_class(union evl_sched_param *param,
+		     const struct evl_sched_attrs *attrs,
+		     ktime_t *tslice_r)
+{
+	struct evl_sched_class *sched_class;
+	int prio, policy;
+	ktime_t tslice;
+
+	policy = attrs->sched_policy;
+	prio = attrs->sched_priority;
+	tslice = EVL_INFINITE;
+
+	/*
+	 * NOTE: The user-defined policy may be different than ours,
+	 * e.g. SCHED_FIFO,prio=-7 from userland would be interpreted
+	 * as SCHED_WEAK,prio=7 in kernel space.
+	 */
+	if (prio < 0) {
+		prio = -prio;
+		policy = SCHED_WEAK;
+	}
+	sched_class = &evl_sched_rt;
+	param->rt.prio = prio;
+
+	switch (policy) {
+	case SCHED_NORMAL:
+		if (prio)
+			return NULL;
+		/* Fallback wanted */
+	case SCHED_WEAK:
+		if (prio < EVL_WEAK_MIN_PRIO ||
+		    prio > EVL_WEAK_MAX_PRIO)
+			return NULL;
+		param->weak.prio = prio;
+		sched_class = &evl_sched_weak;
+		break;
+	case SCHED_RR:
+		/* if unspecified, use current one. */
+		tslice = timespec_to_ktime(attrs->sched_rr_quantum);
+		if (timeout_infinite(tslice) && tslice_r)
+			tslice = *tslice_r;
+		/* falldown wanted */
+	case SCHED_FIFO:
+		if (prio < EVL_FIFO_MIN_PRIO ||
+		    prio > EVL_FIFO_MAX_PRIO)
+			return NULL;
+		break;
+	case SCHED_EVL:
+		if (prio < EVL_CORE_MIN_PRIO ||
+		    prio > EVL_CORE_MAX_PRIO)
+			return NULL;
+		break;
+#ifdef CONFIG_EVENLESS_SCHED_QUOTA
+	case SCHED_QUOTA:
+		param->quota.prio = attrs->sched_priority;
+		param->quota.tgid = attrs->sched_quota_group;
+		sched_class = &evl_sched_quota;
+		break;
+#endif
+	default:
+		return NULL;
+	}
+
+	*tslice_r = tslice;
+
+	return sched_class;
+}
+
+void evl_notify_inband_yield(void) /* In-band only */
+{
+	struct evl_rq *this_rq;
+	unsigned long flags;
+
+	flags = hard_local_irq_save();
+
+	this_rq = this_evl_rq();
+
+	if (evl_syn_has_waiter(&this_rq->yield_sync)) {
+		evl_flush_syn(&this_rq->yield_sync, 0);
+		evl_schedule();
+	}
+
+	hard_local_irq_restore(flags);
+}
+
+static int yield_inband(void)	/* OOB only */
+{
+	struct evl_rq *this_rq = this_evl_rq();
+	ktime_t start;
+	int ret;
+
+	/*
+	 * We need to limit the wait time in order to prevent a
+	 * SCHED_FIFO non-rt thread stuck in a tight loop from
+	 * blocking the caller from waking up, since no
+	 * linux-originated schedule event would happen for unblocking
+	 * it on the current CPU. For this reason, we wait for
+	 * TICK_NSEC.
+	 */
+	start = evl_read_clock(&evl_mono_clock);
+
+	do {
+		ret = evl_sleep_on_syn(&this_rq->yield_sync,
+				       TICK_NSEC, EVL_ABS);
+		if (ret)
+			break;
+	} while (ktime_before(evl_read_clock(&evl_mono_clock),
+			      TICK_NSEC));
+
+	return ret & T_BREAK ? -EINTR : 0;
+}
+
+int evl_sched_yield(void)
+{
+	struct evl_thread *curr = evl_current_thread();
+
+	oob_context_only();
+
+	evl_resume_thread(curr, 0);
+	if (evl_schedule())
+		return 0;
+
+	/*
+	 * If the round-robin move did not beget any context switch to
+	 * a thread running in OOB context, then wait for the next
+	 * linux context switch to happen.
+	 *
+	 * Rationale: evl_sched_yield() not causing any context
+	 * switch is most likely unwanted, since this service is
+	 * commonly used for implementing a poor man's cooperative
+	 * scheduling. By waiting for an inband context switch to
+	 * happen, we guarantee that the CPU has been relinquished for
+	 * a while.
+	 *
+	 * Typically, this behavior allows a thread running in OOB
+	 * context to effectively yield the CPU to a thread of
+	 * same/higher priority stuck running in-band.
+	 */
+	return yield_inband();
+}
+EXPORT_SYMBOL_GPL(evl_sched_yield);
+
+#ifdef CONFIG_TRACING
+
+const char *evl_trace_sched_attrs(struct trace_seq *p,
+				  struct evl_sched_attrs *attrs)
+{
+	const char *ret = trace_seq_buffer_ptr(p);
+
+	switch (attrs->sched_policy) {
+	case SCHED_QUOTA:
+		trace_seq_printf(p, "priority=%d, group=%d",
+				 attrs->sched_priority,
+				 attrs->sched_quota_group);
+		break;
+	case SCHED_NORMAL:
+		break;
+	case SCHED_RR:
+	case SCHED_FIFO:
+	case SCHED_EVL:
+	case SCHED_WEAK:
+	default:
+		trace_seq_printf(p, "priority=%d", attrs->sched_priority);
+		break;
+	}
+	trace_seq_putc(p, '\0');
+
+	return ret;
+}
+
+#endif /* CONFIG_TRACING */
+
+/* in-band stage, hard_irqs_disabled() */
+bool irq_cpuidle_control(struct cpuidle_device *dev,
+			 struct cpuidle_state *state)
+{
+	/*
+	 * Deny entering sleep state if this entails stopping the
+	 * timer (i.e. C3STOP misfeature).
+	 */
+	if (state && (state->flags & CPUIDLE_FLAG_TIMER_STOP))
+		return false;
+
+	return true;
+}
+
+int __init evl_init_sched(void)
+{
+	struct evl_rq *rq;
+	int ret, cpu;
+
+	register_classes();
+
+	for_each_online_cpu(cpu) {
+		rq = &per_cpu(evl_runqueues, cpu);
+		init_rq(rq, cpu);
+	}
+
+	if (IS_ENABLED(CONFIG_SMP)) {
+		ret = __request_percpu_irq(RESCHEDULE_OOB_IPI,
+					   __evl_schedule_handler,
+					   IRQF_OOB,
+					   "Evenless reschedule",
+					   &evl_machine_cpudata);
+		if (ret)
+			goto cleanup_rq;
+	}
+
+	return 0;
+
+cleanup_rq:
+	for_each_online_cpu(cpu) {
+		rq = evl_cpu_rq(cpu);
+		destroy_rq(rq);
+	}
+
+	return ret;
+}
+
+void __init evl_cleanup_sched(void)
+{
+	struct evl_rq *rq;
+	int cpu;
+
+	if (IS_ENABLED(CONFIG_SMP))
+		free_percpu_irq(RESCHEDULE_OOB_IPI, &evl_machine_cpudata);
+
+	for_each_online_cpu(cpu) {
+		rq = evl_cpu_rq(cpu);
+		destroy_rq(rq);
+	}
+}
diff --git a/kernel/evenless/sched/idle.c b/kernel/evenless/sched/idle.c
new file mode 100644
index 000000000000..78e61db5d634
--- /dev/null
+++ b/kernel/evenless/sched/idle.c
@@ -0,0 +1,47 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2008, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <evenless/sched.h>
+
+static struct evl_thread *evl_idle_pick(struct evl_rq *rq)
+{
+	return &rq->root_thread;
+}
+
+static bool evl_idle_setparam(struct evl_thread *thread,
+			      const union evl_sched_param *p)
+{
+	return __evl_set_idle_schedparam(thread, p);
+}
+
+static void evl_idle_getparam(struct evl_thread *thread,
+			      union evl_sched_param *p)
+{
+	__evl_get_idle_schedparam(thread, p);
+}
+
+static void evl_idle_trackprio(struct evl_thread *thread,
+			       const union evl_sched_param *p)
+{
+	__evl_track_idle_priority(thread, p);
+}
+
+static void evl_idle_ceilprio(struct evl_thread *thread, int prio)
+{
+	__evl_ceil_idle_priority(thread, prio);
+}
+
+struct evl_sched_class evl_sched_idle = {
+	.sched_pick		=	evl_idle_pick,
+	.sched_setparam		=	evl_idle_setparam,
+	.sched_getparam		=	evl_idle_getparam,
+	.sched_trackprio	=	evl_idle_trackprio,
+	.sched_ceilprio		=	evl_idle_ceilprio,
+	.weight			=	EVL_CLASS_WEIGHT(0),
+	.policy			=	SCHED_IDLE,
+	.name			=	"idle"
+};
diff --git a/kernel/evenless/sched/quota.c b/kernel/evenless/sched/quota.c
new file mode 100644
index 000000000000..fff24cba77cb
--- /dev/null
+++ b/kernel/evenless/sched/quota.c
@@ -0,0 +1,660 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2013, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/bitmap.h>
+#include <asm/div64.h>
+#include <evenless/sched.h>
+#include <evenless/memory.h>
+#include <uapi/evenless/sched.h>
+
+/*
+ * With this policy, each per-CPU runqueue maintains a list of active
+ * thread groups for the sched_rt class.
+ *
+ * Each time a thread is picked from the runqueue, we check whether we
+ * still have budget for running it, looking at the group it belongs
+ * to. If so, a timer is armed to elapse when that group has no more
+ * budget, would the incoming thread run unpreempted until then
+ * (i.e. evl_quota->limit_timer).
+ *
+ * Otherwise, if no budget remains in the group for running the
+ * candidate thread, we move the latter to a local expiry queue
+ * maintained by the group. This process is done on the fly as we pull
+ * from the runqueue.
+ *
+ * Updating the remaining budget is done each time the EVL core asks
+ * for replacing the current thread with the next runnable one,
+ * i.e. evl_quota_pick(). There we charge the elapsed run time of the
+ * outgoing thread to the relevant group, and conversely, we check
+ * whether the incoming thread has budget.
+ *
+ * Finally, a per-CPU timer (evl_quota->refill_timer) periodically
+ * ticks in the background, in accordance to the defined quota
+ * interval. Thread group budgets get replenished by its handler in
+ * accordance to their respective share, pushing all expired threads
+ * back to the run queue in the same move.
+ *
+ * NOTE: since the core logic enforcing the budget entirely happens in
+ * evl_quota_pick(), applying a budget change can be done as simply as
+ * forcing the rescheduling procedure to be invoked asap. As a result
+ * of this, the EVL core will ask for the next thread to run, which
+ * means calling evl_quota_pick() eventually.
+ *
+ * CAUTION: evl_quota_group->nr_active does count both the threads
+ * from that group linked to the sched_rt runqueue, _and_ the threads
+ * moved to the local expiry queue. As a matter of fact, the expired
+ * threads - those for which we consumed all the per-group budget -
+ * are still seen as runnable (i.e. not blocked/suspended) by the EVL
+ * core. This only means that the SCHED_QUOTA policy won't pick them
+ * until the corresponding budget is replenished.
+ */
+static DECLARE_BITMAP(group_map, CONFIG_EVENLESS_SCHED_QUOTA_NR_GROUPS);
+
+static inline int group_is_active(struct evl_quota_group *tg)
+{
+	struct evl_thread *curr = tg->rq->curr;
+
+	if (tg->nr_active)
+		return 1;
+
+	/*
+	 * Check whether the current thread belongs to the group, and
+	 * is still in running state (T_READY denotes a thread linked
+	 * to the runqueue, in which case tg->nr_active already
+	 * accounts for it).
+	 */
+	if (curr->quota == tg &&
+	    (curr->state & (T_READY|EVL_THREAD_BLOCK_BITS)) == 0)
+		return 1;
+
+	return 0;
+}
+
+static inline void replenish_budget(struct evl_sched_quota *qs,
+				    struct evl_quota_group *tg)
+{
+	ktime_t budget, credit;
+
+	if (tg->quota == tg->quota_peak) {
+		/*
+		 * Fast path: we don't accumulate runtime credit.
+		 * This includes groups with no runtime limit
+		 * (i.e. quota off: quota >= period && quota == peak).
+		 */
+		tg->run_budget = tg->quota;
+		return;
+	}
+
+	/*
+	 * We have to deal with runtime credit accumulation, as the
+	 * group may consume more than its base quota during a single
+	 * interval, up to a peak duration though (not to monopolize
+	 * the CPU).
+	 *
+	 * - In the simplest case, a group is allotted a new full
+	 * budget plus the unconsumed portion of the previous budget,
+	 * provided the sum does not exceed the peak quota.
+	 *
+	 * - When there is too much budget for a single interval
+	 * (i.e. above peak quota), we spread the extra time over
+	 * multiple intervals through a credit accumulation mechanism.
+	 *
+	 * - The accumulated credit is dropped whenever a group has no
+	 * runnable threads.
+	 */
+	if (!group_is_active(tg)) {
+		/* Drop accumulated credit. */
+		tg->run_credit = 0;
+		tg->run_budget = tg->quota;
+		return;
+	}
+
+	budget = ktime_add(tg->run_budget, tg->quota);
+	if (budget > tg->quota_peak) {
+		/* Too much budget, spread it over intervals. */
+		tg->run_credit =
+			ktime_add(tg->run_credit,
+				  ktime_sub(budget, tg->quota_peak));
+		tg->run_budget = tg->quota_peak;
+	} else if (tg->run_credit) {
+		credit = ktime_sub(tg->quota_peak, budget);
+		/* Consume the accumulated credit. */
+		if (tg->run_credit >= credit)
+			tg->run_credit =
+				ktime_sub(tg->run_credit, credit);
+		else {
+			credit = tg->run_credit;
+			tg->run_credit = 0;
+		}
+		/* Allot extended budget, limited to peak quota. */
+		tg->run_budget = ktime_add(budget, credit);
+	} else
+		/* No credit, budget was below peak quota. */
+		tg->run_budget = budget;
+}
+
+static void quota_refill_handler(struct evl_timer *timer) /* hard irqs off */
+{
+	struct evl_quota_group *tg;
+	struct evl_thread *thread, *tmp;
+	struct evl_sched_quota *qs;
+	struct evl_rq *rq;
+
+	qs = container_of(timer, struct evl_sched_quota, refill_timer);
+	rq = container_of(qs, struct evl_rq, quota);
+
+	xnlock_get(&nklock);
+
+	list_for_each_entry(tg, &qs->groups, next) {
+		/* Allot a new runtime budget for the group. */
+		replenish_budget(qs, tg);
+
+		if (tg->run_budget == 0 || list_empty(&tg->expired))
+			continue;
+		/*
+		 * For each group living on this CPU, move all expired
+		 * threads back to the runqueue. Since those threads
+		 * were moved out of the runqueue as we were
+		 * considering them for execution, we push them back
+		 * in LIFO order to their respective priority group.
+		 * The expiry queue is FIFO to keep ordering right
+		 * among expired threads.
+		 */
+		list_for_each_entry_safe_reverse(thread, tmp, &tg->expired, quota_expired) {
+			list_del_init(&thread->quota_expired);
+			evl_add_schedq(&rq->rt.runnable, thread);
+		}
+	}
+
+	evl_set_self_resched(evl_get_timer_rq(timer));
+
+	xnlock_put(&nklock);
+}
+
+static void quota_limit_handler(struct evl_timer *timer) /* hard irqs off */
+{
+	struct evl_rq *rq;
+
+	rq = container_of(timer, struct evl_rq, quota.limit_timer);
+	/*
+	 * Force a rescheduling on the return path of the current
+	 * interrupt, so that the budget is re-evaluated for the
+	 * current group in evl_quota_pick().
+	 */
+	xnlock_get(&nklock);
+	evl_set_self_resched(rq);
+	xnlock_put(&nklock);
+}
+
+static int quota_sum_all(struct evl_sched_quota *qs)
+{
+	struct evl_quota_group *tg;
+	int sum;
+
+	if (list_empty(&qs->groups))
+		return 0;
+
+	sum = 0;
+	list_for_each_entry(tg, &qs->groups, next)
+		sum += tg->quota_percent;
+
+	return sum;
+}
+
+static void quota_init(struct evl_rq *rq)
+{
+	struct evl_sched_quota *qs = &rq->quota;
+
+	qs->period = CONFIG_EVENLESS_SCHED_QUOTA_PERIOD * 1000ULL;
+	INIT_LIST_HEAD(&qs->groups);
+
+	evl_init_timer(&qs->refill_timer,
+		       &evl_mono_clock, quota_refill_handler, rq,
+		       EVL_TIMER_IGRAVITY);
+	evl_set_timer_name(&qs->refill_timer, "[quota-refill]");
+
+	evl_init_timer(&qs->limit_timer,
+		       &evl_mono_clock, quota_limit_handler, rq,
+		       EVL_TIMER_IGRAVITY);
+	evl_set_timer_name(&qs->limit_timer, "[quota-limit]");
+}
+
+static bool quota_setparam(struct evl_thread *thread,
+			   const union evl_sched_param *p)
+{
+	struct evl_quota_group *tg;
+	struct evl_sched_quota *qs;
+	bool effective;
+
+	thread->state &= ~T_WEAK;
+	effective = evl_set_effective_thread_priority(thread, p->quota.prio);
+
+	qs = &thread->rq->quota;
+	list_for_each_entry(tg, &qs->groups, next) {
+		if (tg->tgid != p->quota.tgid)
+			continue;
+		if (thread->quota) {
+			/* Dequeued earlier by our caller. */
+			list_del(&thread->quota_next);
+			thread->quota->nr_threads--;
+		}
+		thread->quota = tg;
+		list_add(&thread->quota_next, &tg->members);
+		tg->nr_threads++;
+		return effective;
+	}
+
+	return false;		/* not reached. */
+}
+
+static void quota_getparam(struct evl_thread *thread,
+			   union evl_sched_param *p)
+{
+	p->quota.prio = thread->cprio;
+	p->quota.tgid = thread->quota->tgid;
+}
+
+static void quota_trackprio(struct evl_thread *thread,
+			    const union evl_sched_param *p)
+{
+	if (p) {
+		/* We should not cross groups during PI boost. */
+		EVL_WARN_ON(CORE,
+			    thread->base_class == &evl_sched_quota &&
+			    thread->quota->tgid != p->quota.tgid);
+		thread->cprio = p->quota.prio;
+	} else
+		thread->cprio = thread->bprio;
+}
+
+static void quota_ceilprio(struct evl_thread *thread, int prio)
+{
+	if (prio > EVL_QUOTA_MAX_PRIO)
+		prio = EVL_QUOTA_MAX_PRIO;
+
+	thread->cprio = prio;
+}
+
+static int quota_chkparam(struct evl_thread *thread,
+			  const union evl_sched_param *p)
+{
+	struct evl_quota_group *tg;
+	struct evl_sched_quota *qs;
+	int tgid;
+
+	if (p->quota.prio < EVL_QUOTA_MIN_PRIO ||
+	    p->quota.prio > EVL_QUOTA_MAX_PRIO)
+		return -EINVAL;
+
+	tgid = p->quota.tgid;
+	if (tgid < 0 || tgid >= CONFIG_EVENLESS_SCHED_QUOTA_NR_GROUPS)
+		return -EINVAL;
+
+	/*
+	 * The group must be managed on the same CPU the thread
+	 * currently runs on.
+	 */
+	qs = &thread->rq->quota;
+	list_for_each_entry(tg, &qs->groups, next) {
+		if (tg->tgid == tgid)
+			return 0;
+	}
+
+	/*
+	 * If that group exists nevertheless, we give userland a
+	 * specific error code.
+	 */
+	if (test_bit(tgid, group_map))
+		return -EPERM;
+
+	return -EINVAL;
+}
+
+static void quota_forget(struct evl_thread *thread)
+{
+	thread->quota->nr_threads--;
+	EVL_WARN_ON_ONCE(CORE, thread->quota->nr_threads < 0);
+	list_del(&thread->quota_next);
+	thread->quota = NULL;
+}
+
+static void quota_kick(struct evl_thread *thread)
+{
+	struct evl_quota_group *tg = thread->quota;
+	struct evl_rq *rq = thread->rq;
+
+	/*
+	 * Allow a kicked thread to be elected for running until it
+	 * switches to in-band context, even if the group it belongs
+	 * to lacks runtime budget.
+	 */
+	if (tg->run_budget == 0 && !list_empty(&thread->quota_expired)) {
+		list_del_init(&thread->quota_expired);
+		evl_add_schedq_tail(&rq->rt.runnable, thread);
+	}
+}
+
+static inline int thread_is_runnable(struct evl_thread *thread)
+{
+	return thread->quota->run_budget > 0 || (thread->info & T_KICKED);
+}
+
+static void quota_enqueue(struct evl_thread *thread)
+{
+	struct evl_quota_group *tg = thread->quota;
+	struct evl_rq *rq = thread->rq;
+
+	if (!thread_is_runnable(thread))
+		list_add_tail(&thread->quota_expired, &tg->expired);
+	else
+		evl_add_schedq_tail(&rq->rt.runnable, thread);
+
+	tg->nr_active++;
+}
+
+static void quota_dequeue(struct evl_thread *thread)
+{
+	struct evl_quota_group *tg = thread->quota;
+	struct evl_rq *rq = thread->rq;
+
+	if (!list_empty(&thread->quota_expired))
+		list_del_init(&thread->quota_expired);
+	else
+		evl_del_schedq(&rq->rt.runnable, thread);
+
+	tg->nr_active--;
+}
+
+static void quota_requeue(struct evl_thread *thread)
+{
+	struct evl_quota_group *tg = thread->quota;
+	struct evl_rq *rq = thread->rq;
+
+	if (!thread_is_runnable(thread))
+		list_add(&thread->quota_expired, &tg->expired);
+	else
+		evl_add_schedq(&rq->rt.runnable, thread);
+
+	tg->nr_active++;
+}
+
+static struct evl_thread *quota_pick(struct evl_rq *rq)
+{
+	struct evl_thread *next, *curr = rq->curr;
+	struct evl_sched_quota *qs = &rq->quota;
+	struct evl_quota_group *otg, *tg;
+	ktime_t now, elapsed;
+
+	now = evl_read_clock(&evl_mono_clock);
+	otg = curr->quota;
+	if (otg == NULL)
+		goto pick;
+	/*
+	 * Charge the time consumed by the outgoing thread to the
+	 * group it belongs to.
+	 */
+	elapsed = ktime_sub(now, otg->run_start);
+	if (elapsed < otg->run_budget)
+		otg->run_budget = ktime_sub(otg->run_budget, elapsed);
+	else
+		otg->run_budget = 0;
+pick:
+	next = evl_get_schedq(&rq->rt.runnable);
+	if (next == NULL) {
+		evl_stop_timer(&qs->limit_timer);
+		return NULL;
+	}
+
+	/*
+	 * As we basically piggyback on the SCHED_FIFO runqueue, make
+	 * sure to detect non-quota threads.
+	 */
+	tg = next->quota;
+	if (tg == NULL)
+		return next;
+
+	tg->run_start = now;
+
+	/*
+	 * Don't consider budget if kicked, we have to allow this
+	 * thread to run until it eventually switches to in-band
+	 * context.
+	 */
+	if (next->info & T_KICKED) {
+		evl_stop_timer(&qs->limit_timer);
+		goto out;
+	}
+
+	if (ktime_to_ns(tg->run_budget) == 0) {
+		/* Flush expired group members as we go. */
+		list_add_tail(&next->quota_expired, &tg->expired);
+		goto pick;
+	}
+
+	if (otg == tg && evl_timer_is_running(&qs->limit_timer))
+		/* Same group, leave the running timer untouched. */
+		goto out;
+
+	/* Arm limit timer for the new running group. */
+	evl_start_timer(&qs->limit_timer,
+			ktime_add(now, tg->run_budget),
+			EVL_INFINITE);
+out:
+	tg->nr_active--;
+
+	return next;
+}
+
+static void quota_migrate(struct evl_thread *thread, struct evl_rq *rq)
+{
+	union evl_sched_param param;
+	/*
+	 * Runtime quota groups are defined per-CPU, so leaving the
+	 * current CPU means exiting the group. We do this by moving
+	 * the target thread to the plain RT class.
+	 */
+	param.rt.prio = thread->cprio;
+	__evl_set_thread_schedparam(thread, &evl_sched_rt, &param);
+}
+
+static ssize_t quota_show(struct evl_thread *thread,
+			  char *buf, ssize_t count)
+{
+	return snprintf(buf, count, "%d\n",
+			thread->quota->tgid);
+}
+
+int evl_quota_create_group(struct evl_quota_group *tg,
+			   struct evl_rq *rq,
+			   int *quota_sum_r)
+{
+	int tgid, nr_groups = CONFIG_EVENLESS_SCHED_QUOTA_NR_GROUPS;
+	struct evl_sched_quota *qs = &rq->quota;
+
+	atomic_only();
+
+	tgid = find_first_zero_bit(group_map, nr_groups);
+	if (tgid >= nr_groups)
+		return -ENOSPC;
+
+	__set_bit(tgid, group_map);
+	tg->tgid = tgid;
+	tg->rq = rq;
+	tg->run_budget = qs->period;
+	tg->run_credit = 0;
+	tg->quota_percent = 100;
+	tg->quota_peak_percent = 100;
+	tg->quota = qs->period;
+	tg->quota_peak = qs->period;
+	tg->nr_active = 0;
+	tg->nr_threads = 0;
+	INIT_LIST_HEAD(&tg->members);
+	INIT_LIST_HEAD(&tg->expired);
+
+	if (list_empty(&qs->groups))
+		evl_start_timer(&qs->refill_timer,
+				evl_abs_timeout(&qs->refill_timer, qs->period),
+				qs->period);
+
+	list_add(&tg->next, &qs->groups);
+	*quota_sum_r = quota_sum_all(qs);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(evl_quota_create_group);
+
+int evl_quota_destroy_group(struct evl_quota_group *tg,
+			    int force, int *quota_sum_r)
+{
+	struct evl_sched_quota *qs = &tg->rq->quota;
+	struct evl_thread *thread, *tmp;
+	union evl_sched_param param;
+
+	atomic_only();
+
+	if (!list_empty(&tg->members)) {
+		if (!force)
+			return -EBUSY;
+		/* Move group members to the rt class. */
+		list_for_each_entry_safe(thread, tmp, &tg->members, quota_next) {
+			param.rt.prio = thread->cprio;
+			__evl_set_thread_schedparam(thread, &evl_sched_rt, &param);
+		}
+	}
+
+	list_del(&tg->next);
+	__clear_bit(tg->tgid, group_map);
+
+	if (list_empty(&qs->groups))
+		evl_stop_timer(&qs->refill_timer);
+
+	*quota_sum_r = quota_sum_all(qs);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(evl_quota_destroy_group);
+
+void evl_quota_set_limit(struct evl_quota_group *tg,
+			 int quota_percent, int quota_peak_percent,
+			 int *quota_sum_r)
+{
+	struct evl_sched_quota *qs = &tg->rq->quota;
+	ktime_t now, elapsed, consumed;
+	ktime_t old_quota = tg->quota;
+	u64 n;
+
+	atomic_only();
+
+	if (quota_percent < 0 || quota_percent > 100) { /* Quota off. */
+		quota_percent = 100;
+		tg->quota = qs->period;
+	} else {
+		n = qs->period * quota_percent;
+		do_div(n, 100);
+		tg->quota = n;
+	}
+
+	if (quota_peak_percent < quota_percent)
+		quota_peak_percent = quota_percent;
+
+	if (quota_peak_percent < 0 || quota_peak_percent > 100) {
+		quota_peak_percent = 100;
+		tg->quota_peak = qs->period;
+	} else {
+		n = qs->period * quota_peak_percent;
+		do_div(n, 100);
+		tg->quota_peak = n;
+	}
+
+	tg->quota_percent = quota_percent;
+	tg->quota_peak_percent = quota_peak_percent;
+
+	if (group_is_active(tg)) {
+		now = evl_read_clock(&evl_mono_clock);
+
+		elapsed = now - tg->run_start;
+		if (elapsed < tg->run_budget)
+			tg->run_budget -= elapsed;
+		else
+			tg->run_budget = 0;
+
+		tg->run_start = now;
+		evl_stop_timer(&qs->limit_timer);
+	}
+
+	if (tg->run_budget <= old_quota)
+		consumed = old_quota - tg->run_budget;
+	else
+		consumed = 0;
+
+	if (tg->quota >= consumed)
+		tg->run_budget = tg->quota - consumed;
+	else
+		tg->run_budget = 0;
+
+	tg->run_credit = 0;	/* Drop accumulated credit. */
+
+	*quota_sum_r = quota_sum_all(qs);
+
+	/*
+	 * Apply the new budget immediately, in case a member of this
+	 * group is currently running.
+	 */
+	evl_set_resched(tg->rq);
+	evl_schedule();
+}
+EXPORT_SYMBOL_GPL(evl_quota_set_limit);
+
+struct evl_quota_group *
+evl_quota_find_group(struct evl_rq *rq, int tgid)
+{
+	struct evl_quota_group *tg;
+
+	atomic_only();
+
+	if (list_empty(&rq->quota.groups))
+		return NULL;
+
+	list_for_each_entry(tg, &rq->quota.groups, next) {
+		if (tg->tgid == tgid)
+			return tg;
+	}
+
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(evl_quota_find_group);
+
+int evl_quota_sum_all(struct evl_rq *rq)
+{
+	struct evl_sched_quota *qs = &rq->quota;
+
+	atomic_only();
+
+	return quota_sum_all(qs);
+}
+EXPORT_SYMBOL_GPL(evl_quota_sum_all);
+
+struct evl_sched_class evl_sched_quota = {
+	.sched_init		=	quota_init,
+	.sched_enqueue		=	quota_enqueue,
+	.sched_dequeue		=	quota_dequeue,
+	.sched_requeue		=	quota_requeue,
+	.sched_pick		=	quota_pick,
+	.sched_migrate		=	quota_migrate,
+	.sched_chkparam		=	quota_chkparam,
+	.sched_setparam		=	quota_setparam,
+	.sched_getparam		=	quota_getparam,
+	.sched_trackprio	=	quota_trackprio,
+	.sched_ceilprio		=	quota_ceilprio,
+	.sched_forget		=	quota_forget,
+	.sched_kick		=	quota_kick,
+	.sched_show		=	quota_show,
+	.weight			=	EVL_CLASS_WEIGHT(2),
+	.policy			=	SCHED_QUOTA,
+	.name			=	"quota"
+};
+EXPORT_SYMBOL_GPL(evl_sched_quota);
diff --git a/kernel/evenless/sched/rt.c b/kernel/evenless/sched/rt.c
new file mode 100644
index 000000000000..be2307dc0005
--- /dev/null
+++ b/kernel/evenless/sched/rt.c
@@ -0,0 +1,135 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2008, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <evenless/sched.h>
+
+static void evl_rt_init(struct evl_rq *rq)
+{
+	evl_init_schedq(&rq->rt.runnable);
+}
+
+static void evl_rt_requeue(struct evl_thread *thread)
+{
+	/*
+	 * Put back at same place: i.e. requeue to head of current
+	 * priority group (i.e. LIFO, used for preemption handling).
+	 */
+	__evl_requeue_rt_thread(thread);
+}
+
+static void evl_rt_enqueue(struct evl_thread *thread)
+{
+	/*
+	 * Enqueue for next pick: i.e. move to end of current priority
+	 * group (i.e. FIFO).
+	 */
+	__evl_enqueue_rt_thread(thread);
+}
+
+static void evl_rt_dequeue(struct evl_thread *thread)
+{
+	/*
+	 * Pull from the runnable thread queue.
+	 */
+	__evl_dequeue_rt_thread(thread);
+}
+
+static void evl_rt_rotate(struct evl_rq *rq,
+			  const union evl_sched_param *p)
+{
+	struct evl_thread *thread, *curr;
+
+	if (evl_schedq_is_empty(&rq->rt.runnable))
+		return;	/* No runnable thread in this class. */
+
+	curr = rq->curr;
+	thread = evl_lookup_schedq(&rq->rt.runnable, p->rt.prio);
+	if (thread == NULL)
+		return;
+
+	/*
+	 * In case we picked the current thread, we have to make sure
+	 * not to move it back to the run queue if it was blocked
+	 * before we were called. The same goes if the current thread
+	 * holds the scheduler lock.
+	 */
+	if (thread != curr ||
+	    (!(curr->state & EVL_THREAD_BLOCK_BITS) &&
+	     curr->lock_count == 0))
+		evl_putback_thread(thread);
+}
+
+static void evl_rt_tick(struct evl_rq *rq)
+{
+	/*
+	 * The round-robin time credit is only consumed by a running
+	 * thread that neither holds the scheduler lock nor was
+	 * blocked before entering this callback. As the time slice is
+	 * exhausted for the running thread, move it back to the
+	 * run queue at the end of its priority group.
+	 */
+	evl_putback_thread(rq->curr);
+}
+
+static int evl_rt_chkparam(struct evl_thread *thread,
+			   const union evl_sched_param *p)
+{
+	return __evl_chk_rt_schedparam(thread, p);
+}
+
+static bool evl_rt_setparam(struct evl_thread *thread,
+			    const union evl_sched_param *p)
+{
+	return __evl_set_rt_schedparam(thread, p);
+}
+
+static void evl_rt_getparam(struct evl_thread *thread,
+			    union evl_sched_param *p)
+{
+	__evl_get_rt_schedparam(thread, p);
+}
+
+static void evl_rt_trackprio(struct evl_thread *thread,
+			     const union evl_sched_param *p)
+{
+	__evl_track_rt_priority(thread, p);
+}
+
+static void evl_rt_ceilprio(struct evl_thread *thread, int prio)
+{
+	__evl_ceil_rt_priority(thread, prio);
+}
+
+static ssize_t evl_rt_show(struct evl_thread *thread,
+			   char *buf, ssize_t count)
+{
+	if (thread->state & T_RRB)
+		return snprintf(buf, count, "%Ld\n",
+				ktime_to_ns(thread->rrperiod));
+
+	return 0;
+}
+
+struct evl_sched_class evl_sched_rt = {
+	.sched_init		=	evl_rt_init,
+	.sched_enqueue		=	evl_rt_enqueue,
+	.sched_dequeue		=	evl_rt_dequeue,
+	.sched_requeue		=	evl_rt_requeue,
+	.sched_pick		=	evl_rt_pick,
+	.sched_tick		=	evl_rt_tick,
+	.sched_rotate		=	evl_rt_rotate,
+	.sched_chkparam		=	evl_rt_chkparam,
+	.sched_setparam		=	evl_rt_setparam,
+	.sched_trackprio	=	evl_rt_trackprio,
+	.sched_ceilprio		=	evl_rt_ceilprio,
+	.sched_getparam		=	evl_rt_getparam,
+	.sched_show		=	evl_rt_show,
+	.weight			=	EVL_CLASS_WEIGHT(3),
+	.policy			=	SCHED_FIFO,
+	.name			=	"rt"
+};
+EXPORT_SYMBOL_GPL(evl_sched_rt);
diff --git a/kernel/evenless/sched/weak.c b/kernel/evenless/sched/weak.c
new file mode 100644
index 000000000000..a8d94daa8695
--- /dev/null
+++ b/kernel/evenless/sched/weak.c
@@ -0,0 +1,104 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2013, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <evenless/sched.h>
+#include <uapi/evenless/sched.h>
+
+static void weak_init(struct evl_rq *rq)
+{
+	evl_init_schedq(&rq->weak.runnable);
+}
+
+static void weak_requeue(struct evl_thread *thread)
+{
+	evl_add_schedq(&thread->rq->weak.runnable, thread);
+}
+
+static void weak_enqueue(struct evl_thread *thread)
+{
+	evl_add_schedq_tail(&thread->rq->weak.runnable, thread);
+}
+
+static void weak_dequeue(struct evl_thread *thread)
+{
+	evl_del_schedq(&thread->rq->weak.runnable, thread);
+}
+
+static struct evl_thread *weak_pick(struct evl_rq *rq)
+{
+	return evl_get_schedq(&rq->weak.runnable);
+}
+
+static int weak_chkparam(struct evl_thread *thread,
+			 const union evl_sched_param *p)
+{
+	if (p->weak.prio < EVL_WEAK_MIN_PRIO ||
+	    p->weak.prio > EVL_WEAK_MAX_PRIO)
+		return -EINVAL;
+
+	return 0;
+}
+
+static bool weak_setparam(struct evl_thread *thread,
+			  const union evl_sched_param *p)
+{
+	if (!(thread->state & T_BOOST))
+		thread->state |= T_WEAK;
+
+	return evl_set_effective_thread_priority(thread, p->weak.prio);
+}
+
+static void weak_getparam(struct evl_thread *thread,
+			  union evl_sched_param *p)
+{
+	p->weak.prio = thread->cprio;
+}
+
+static void weak_trackprio(struct evl_thread *thread,
+			   const union evl_sched_param *p)
+{
+	if (p)
+		thread->cprio = p->weak.prio;
+	else
+		thread->cprio = thread->bprio;
+}
+
+static void weak_ceilprio(struct evl_thread *thread, int prio)
+{
+	if (prio > EVL_WEAK_MAX_PRIO)
+		prio = EVL_WEAK_MAX_PRIO;
+
+	thread->cprio = prio;
+}
+
+static int weak_declare(struct evl_thread *thread,
+			const union evl_sched_param *p)
+{
+	if (p->weak.prio < EVL_WEAK_MIN_PRIO ||
+	    p->weak.prio > EVL_WEAK_MAX_PRIO)
+		return -EINVAL;
+
+	return 0;
+}
+
+struct evl_sched_class evl_sched_weak = {
+	.sched_init		=	weak_init,
+	.sched_enqueue		=	weak_enqueue,
+	.sched_dequeue		=	weak_dequeue,
+	.sched_requeue		=	weak_requeue,
+	.sched_pick		=	weak_pick,
+	.sched_declare		=	weak_declare,
+	.sched_chkparam		=	weak_chkparam,
+	.sched_setparam		=	weak_setparam,
+	.sched_trackprio	=	weak_trackprio,
+	.sched_ceilprio		=	weak_ceilprio,
+	.sched_getparam		=	weak_getparam,
+	.weight			=	EVL_CLASS_WEIGHT(1),
+	.policy			=	SCHED_WEAK,
+	.name			=	"weak"
+};
+EXPORT_SYMBOL_GPL(evl_sched_weak);
diff --git a/kernel/evenless/sem.c b/kernel/evenless/sem.c
new file mode 100644
index 000000000000..53722c0edca6
--- /dev/null
+++ b/kernel/evenless/sem.c
@@ -0,0 +1,310 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <evenless/synch.h>
+#include <evenless/thread.h>
+#include <evenless/clock.h>
+#include <evenless/sem.h>
+#include <evenless/memory.h>
+#include <evenless/lock.h>
+#include <evenless/factory.h>
+#include <evenless/sched.h>
+#include <asm/evenless/syscall.h>
+#include <trace/events/evenless.h>
+
+struct evl_sem {
+	struct evl_element element;
+	struct evl_sem_state *state;
+	struct evl_syn wait_queue;
+};
+
+struct sem_wait_data {
+	int count;
+};
+
+static int acquire_sem(struct evl_sem *sem,
+		       struct evl_sem_waitreq *req)
+{
+	struct evl_thread *curr = evl_current_thread();
+	struct evl_sem_state *state = sem->state;
+	struct sem_wait_data wda;
+	enum evl_tmode tmode;
+	int info, ret = 0;
+	ktime_t timeout;
+
+	if (req->count <= 0)
+		return -EINVAL;
+
+	if ((unsigned long)req->timeout.tv_nsec >= ONE_BILLION)
+		return -EINVAL;
+
+	if (state->flags & EVL_SEM_PULSE)
+		req->count = 1;
+
+	if (atomic_sub_return(req->count, &state->value) >= 0)
+		return 0;
+
+	wda.count = req->count;
+	curr->wait_data = &wda;
+
+	timeout = timespec_to_ktime(req->timeout);
+	tmode = timeout ? EVL_ABS : EVL_REL;
+	info = evl_sleep_on_syn(&sem->wait_queue, timeout, tmode);
+	if (info & (T_BREAK|T_BCAST|T_TIMEO)) {
+		atomic_add(req->count, &state->value);
+		ret = -ETIMEDOUT;
+		if (info & T_BREAK)
+			ret = -EINTR;
+		else if (info & T_BCAST)
+			ret = -EAGAIN;
+	} /* No way we could receive T_RMID */
+
+	return ret;
+}
+
+static int release_sem(struct evl_sem *sem, int count)
+{
+	struct evl_sem_state *state = sem->state;
+	struct evl_thread *waiter, *n;
+	struct sem_wait_data *wda;
+	unsigned long flags;
+	int oldval;
+
+	if (count <= 0)
+		return -EINVAL;
+
+	oldval = atomic_read(&state->value);
+	if (oldval + count < 0)
+		return -EINVAL;
+
+	if (state->flags & EVL_SEM_PULSE)
+		count = 1;
+
+	if (atomic_add_return(count, &state->value) >= count) {
+		/* Old value >= 0, nobody is waiting. */
+		if (state->flags & EVL_SEM_PULSE)
+			atomic_set(&state->value, 0);
+		return 0;
+	}
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (!evl_syn_has_waiter(&sem->wait_queue))
+		goto out;
+
+	/*
+	 * Try waking up waiters. The top waiter must progress, do not
+	 * serve other waiters down the queue until this one is
+	 * satisfied.
+	 */
+	evl_for_each_syn_waiter_safe(waiter, n, &sem->wait_queue) {
+		wda = waiter->wait_data;
+		if (atomic_sub_return(wda->count, &state->value) < 0) {
+			atomic_add(wda->count, &state->value); /* Nope, undo. */
+			break;
+		}
+		evl_wake_up_targeted_syn(&sem->wait_queue, waiter);
+	}
+
+	evl_schedule();
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return 0;
+}
+
+static int broadcast_sem(struct evl_sem *sem)
+{
+	if (evl_flush_syn(&sem->wait_queue, T_BCAST))
+		evl_schedule();
+
+	return 0;
+}
+
+static long sem_common_ioctl(struct evl_sem *sem,
+			     unsigned int cmd, unsigned long arg)
+{
+	__s32 count;
+	long ret;
+
+	switch (cmd) {
+	case EVL_SEMIOC_PUT:
+		ret = raw_get_user(count, (__s32 *)arg);
+		if (ret)
+			return -EFAULT;
+		ret = release_sem(sem, count);
+		break;
+	case EVL_SEMIOC_BCAST:
+		ret = broadcast_sem(sem);
+		break;
+	default:
+		ret = -ENOTTY;
+	}
+
+	return ret;
+}
+
+static long sem_oob_ioctl(struct file *filp, unsigned int cmd,
+			  unsigned long arg)
+{
+	struct evl_sem *sem = element_of(filp, struct evl_sem);
+	struct evl_sem_waitreq wreq, __user *u_wreq;
+	long ret;
+
+	switch (cmd) {
+	case EVL_SEMIOC_GET:
+		u_wreq = (typeof(u_wreq))arg;
+		ret = raw_copy_from_user(&wreq, u_wreq, sizeof(wreq));
+		if (ret)
+			return -EFAULT;
+		ret = acquire_sem(sem, &wreq);
+		break;
+	default:
+		ret = sem_common_ioctl(sem, cmd, arg);
+	}
+
+	return ret;
+}
+
+static long sem_ioctl(struct file *filp, unsigned int cmd,
+		      unsigned long arg)
+{
+	struct evl_sem *sem = element_of(filp, struct evl_sem);
+	struct evl_element_ids eids, __user *u_eids;
+
+	if (cmd != EVL_SEMIOC_BIND)
+		return sem_common_ioctl(sem, cmd, arg);
+
+	eids.minor = sem->element.minor;
+	eids.state_offset = evl_shared_offset(sem->state);
+	eids.fundle = fundle_of(sem);
+	u_eids = (typeof(u_eids))arg;
+
+	return copy_to_user(u_eids, &eids, sizeof(eids)) ? -EFAULT : 0;
+}
+
+static const struct file_operations sem_fops = {
+	.open		= evl_open_element,
+	.release	= evl_close_element,
+	.unlocked_ioctl	= sem_ioctl,
+	.oob_ioctl	= sem_oob_ioctl,
+};
+
+static struct evl_element *
+sem_factory_build(struct evl_factory *fac, const char *name,
+		  void __user *u_attrs, u32 *state_offp)
+{
+	struct evl_sem_state *state;
+	struct evl_sem_attrs attrs;
+	struct evl_clock *clock;
+	int synflags = 0, ret;
+	struct evl_sem *sem;
+
+	ret = copy_from_user(&attrs, u_attrs, sizeof(attrs));
+	if (ret)
+		return ERR_PTR(-EFAULT);
+
+	if (attrs.flags & ~(EVL_SEM_PRIO|EVL_SEM_PULSE))
+		return ERR_PTR(-EINVAL);
+
+	if (attrs.initval < 0)
+		return ERR_PTR(-EINVAL);
+
+	if ((attrs.flags & EVL_SEM_PULSE) && attrs.initval > 0)
+		return ERR_PTR(-EINVAL);
+
+	clock = evl_get_clock_by_fd(attrs.clockfd);
+	if (clock == NULL)
+		return ERR_PTR(-EINVAL);
+
+	sem = kzalloc(sizeof(*sem), GFP_KERNEL);
+	if (sem == NULL) {
+		ret = -ENOMEM;
+		goto fail_alloc;
+	}
+
+	ret = evl_init_element(&sem->element, &evl_sem_factory);
+	if (ret)
+		goto fail_element;
+
+	state = evl_alloc_chunk(&evl_shared_heap, sizeof(*state));
+	if (state == NULL) {
+		ret = -ENOMEM;
+		goto fail_heap;
+	}
+
+	if (attrs.flags & EVL_SEM_PRIO)
+		synflags |= EVL_SYN_PRIO;
+
+	evl_init_syn(&sem->wait_queue, synflags, clock, NULL);
+	atomic_set(&state->value, attrs.initval);
+	state->flags = attrs.flags;
+	sem->state = state;
+
+	*state_offp = evl_shared_offset(state);
+	evl_index_element(&sem->element);
+
+	return &sem->element;
+
+fail_heap:
+	evl_destroy_element(&sem->element);
+fail_element:
+	kfree(sem);
+fail_alloc:
+	evl_put_clock(clock);
+
+	return ERR_PTR(ret);
+}
+
+static void sem_factory_dispose(struct evl_element *e)
+{
+	struct evl_sem *sem;
+
+	sem = container_of(e, struct evl_sem, element);
+
+	evl_unindex_element(&sem->element);
+	evl_put_clock(sem->wait_queue.clock);
+	evl_destroy_syn(&sem->wait_queue);
+	evl_free_chunk(&evl_shared_heap, sem->state);
+	evl_destroy_element(&sem->element);
+	kfree_rcu(sem, element.rcu);
+}
+
+static ssize_t value_show(struct device *dev,
+			  struct device_attribute *attr,
+			  char *buf)
+{
+	struct evl_sem *sem;
+	ssize_t ret;
+
+	sem = evl_get_element_by_dev(dev, struct evl_sem);
+	ret = snprintf(buf, PAGE_SIZE, "%d\n",
+		       atomic_read(&sem->state->value));
+	evl_put_element(&sem->element);
+
+	return ret;
+}
+static DEVICE_ATTR_RO(value);
+
+static struct attribute *sem_attrs[] = {
+	&dev_attr_value.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(sem);
+
+struct evl_factory evl_sem_factory = {
+	.name	=	"semaphore",
+	.fops	=	&sem_fops,
+	.build =	sem_factory_build,
+	.dispose =	sem_factory_dispose,
+	.nrdev	=	CONFIG_EVENLESS_NR_SEMAPHORES,
+	.attrs	=	sem_groups,
+	.flags	=	EVL_FACTORY_CLONE,
+};
diff --git a/kernel/evenless/synch.c b/kernel/evenless/synch.c
new file mode 100644
index 000000000000..28513e2be87a
--- /dev/null
+++ b/kernel/evenless/synch.c
@@ -0,0 +1,896 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2001, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <stdarg.h>
+#include <linux/signal.h>
+#include <linux/kernel.h>
+#include <evenless/sched.h>
+#include <evenless/synch.h>
+#include <evenless/thread.h>
+#include <evenless/monitor.h>
+#include <evenless/clock.h>
+#include <uapi/evenless/signal.h>
+#include <trace/events/evenless.h>
+
+static inline int get_ceiling_value(struct evl_syn *synch)
+{
+	/*
+	 * The ceiling priority value is stored in user-writable
+	 * memory, make sure to constrain it within valid bounds for
+	 * evl_sched_rt before using it.
+	 */
+	return clamp(*synch->ceiling_ref, 1U, (u32)EVL_CORE_MAX_PRIO);
+}
+
+static inline void get_thread_resource(struct evl_thread *curr)
+{
+	/*
+	 * Track resource locking depth, to prevent weak threads from
+	 * being switched back to in-band mode on return from OOB
+	 * syscalls.
+	 */
+	if (curr->state & (T_WEAK|T_DEBUG))
+		curr->res_count++;
+}
+
+static inline bool put_thread_resource(struct evl_thread *curr)
+{
+	if ((curr->state & T_WEAK) ||
+	    IS_ENABLED(CONFIG_EVENLESS_DEBUG_MONITOR_SLEEP)) {
+		if (unlikely(curr->res_count == 0)) {
+			if (curr->state & T_WARN)
+				evl_signal_thread(curr, SIGDEBUG,
+						  SIGDEBUG_RESCNT_IMBALANCE);
+			return false;
+		}
+		curr->res_count--;
+	}
+
+	return true;
+}
+
+void evl_init_syn(struct evl_syn *synch, int flags,
+		  struct evl_clock *clock, atomic_t *fastlock)
+{
+	if (flags & (EVL_SYN_PI|EVL_SYN_PP))
+		flags |= EVL_SYN_PRIO | EVL_SYN_OWNER;
+
+	synch->status = flags & ~EVL_SYN_CLAIMED;
+	synch->owner = NULL;
+	synch->wprio = -1;
+	synch->ceiling_ref = NULL;
+	synch->clock = clock;
+	INIT_LIST_HEAD(&synch->wait_list);
+
+	if (flags & EVL_SYN_OWNER) {
+		synch->fastlock = fastlock;
+		atomic_set(fastlock, EVL_NO_HANDLE);
+	} else
+		synch->fastlock = NULL;
+}
+EXPORT_SYMBOL_GPL(evl_init_syn);
+
+void evl_init_syn_protect(struct evl_syn *synch,
+			  struct evl_clock *clock,
+			  atomic_t *fastlock, u32 *ceiling_ref)
+{
+	evl_init_syn(synch, EVL_SYN_PP, clock, fastlock);
+	synch->ceiling_ref = ceiling_ref;
+}
+
+bool evl_destroy_syn(struct evl_syn *synch)
+{
+	bool ret;
+
+	ret = evl_flush_syn(synch, T_RMID);
+	EVL_WARN_ON(CORE, synch->status & EVL_SYN_CLAIMED);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_destroy_syn);
+
+static inline
+int block_thread_timed(ktime_t timeout, enum evl_tmode timeout_mode,
+		       struct evl_clock *clock,
+		       struct evl_syn *wchan)
+{
+	struct evl_thread *curr = evl_current_thread();
+
+	evl_suspend_thread(curr, T_PEND, timeout,
+			   timeout_mode, clock, wchan);
+
+	return curr->info & (T_RMID|T_TIMEO|T_BREAK);
+}
+
+int evl_sleep_on_syn(struct evl_syn *synch, ktime_t timeout,
+		     enum evl_tmode timeout_mode)
+{
+	struct evl_thread *curr;
+	unsigned long flags;
+	int ret;
+
+	oob_context_only();
+
+	if (EVL_WARN_ON(CORE, synch->status & EVL_SYN_OWNER))
+		return T_BREAK;
+
+	curr = evl_current_thread();
+
+	if (IS_ENABLED(CONFIG_EVENLESS_DEBUG_MONITOR_SLEEP) &&
+	    curr->res_count > 0 && (curr->state & T_WARN))
+		evl_signal_thread(curr, SIGDEBUG, SIGDEBUG_MONITOR_SLEEP);
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	trace_evl_synch_sleepon(synch);
+
+	if (!(synch->status & EVL_SYN_PRIO)) /* i.e. FIFO */
+		list_add_tail(&curr->syn_next, &synch->wait_list);
+	else /* i.e. priority-sorted */
+		list_add_priff(curr, &synch->wait_list, wprio, syn_next);
+
+	ret = block_thread_timed(timeout, timeout_mode, synch->clock, synch);
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_sleep_on_syn);
+
+struct evl_thread *evl_wake_up_syn(struct evl_syn *synch)
+{
+	struct evl_thread *thread;
+	unsigned long flags;
+
+	if (EVL_WARN_ON(CORE, synch->status & EVL_SYN_OWNER))
+		return NULL;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (list_empty(&synch->wait_list)) {
+		thread = NULL;
+		goto out;
+	}
+
+	trace_evl_synch_wakeup(synch);
+	thread = list_first_entry(&synch->wait_list, struct evl_thread, syn_next);
+	list_del(&thread->syn_next);
+	thread->wchan = NULL;
+	evl_resume_thread(thread, T_PEND);
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return thread;
+}
+EXPORT_SYMBOL_GPL(evl_wake_up_syn);
+
+int evl_wake_up_nr_syn(struct evl_syn *synch, int nr)
+{
+	struct evl_thread *thread, *tmp;
+	unsigned long flags;
+	int nwakeups = 0;
+
+	if (EVL_WARN_ON(CORE, synch->status & EVL_SYN_OWNER))
+		return 0;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (list_empty(&synch->wait_list))
+		goto out;
+
+	trace_evl_synch_wakeup_many(synch);
+
+	list_for_each_entry_safe(thread, tmp, &synch->wait_list, syn_next) {
+		if (nwakeups++ >= nr)
+			break;
+		list_del(&thread->syn_next);
+		thread->wchan = NULL;
+		evl_resume_thread(thread, T_PEND);
+	}
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return nwakeups;
+}
+EXPORT_SYMBOL_GPL(evl_wake_up_nr_syn);
+
+void evl_wake_up_targeted_syn(struct evl_syn *synch,
+			      struct evl_thread *waiter)
+{
+	unsigned long flags;
+
+	if (EVL_WARN_ON(CORE, synch->status & EVL_SYN_OWNER))
+		return;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	trace_evl_synch_wakeup(synch);
+	list_del(&waiter->syn_next);
+	waiter->wchan = NULL;
+	evl_resume_thread(waiter, T_PEND);
+
+	xnlock_put_irqrestore(&nklock, flags);
+}
+EXPORT_SYMBOL_GPL(evl_wake_up_targeted_syn);
+
+static inline void raise_boost_flag(struct evl_thread *owner)
+{
+	/* Backup the base priority at first boost only. */
+	if (!(owner->state & T_BOOST)) {
+		owner->bprio = owner->cprio;
+		owner->state |= T_BOOST;
+	}
+}
+
+static void inherit_thread_priority(struct evl_thread *owner,
+				    struct evl_thread *target)
+{
+	if (owner->state & T_ZOMBIE)
+		return;
+
+	/* Apply the scheduling policy of "target" to "thread" */
+	evl_track_thread_policy(owner, target);
+
+	/*
+	 * Owner may be sleeping, propagate priority update through
+	 * the PI chain if needed.
+	 */
+	if (owner->wchan)
+		evl_requeue_syn_waiter(owner);
+}
+
+static void __ceil_owner_priority(struct evl_thread *owner, int prio)
+{
+	if (owner->state & T_ZOMBIE)
+		return;
+	/*
+	 * Raise owner priority to the ceiling value, this implicitly
+	 * selects SCHED_FIFO for the owner.
+	 */
+	evl_protect_thread_priority(owner, prio);
+
+	if (owner->wchan)
+		evl_requeue_syn_waiter(owner);
+}
+
+static void adjust_boost(struct evl_thread *owner, struct evl_thread *target)
+{
+	struct evl_syn *synch;
+
+	/*
+	 * CAUTION: we may have PI and PP-enabled objects among the
+	 * boosters, considering the leader of synch->wait_list is
+	 * therefore NOT enough for determining the next boost
+	 * priority, since PP is tracked on acquisition, not on
+	 * contention. Check the head of the booster list instead.
+	 */
+	synch = list_first_entry(&owner->boosters, struct evl_syn, next);
+	if (synch->wprio == owner->wprio)
+		return;
+
+	if (synch->status & EVL_SYN_PP)
+		__ceil_owner_priority(owner, get_ceiling_value(synch));
+	else {
+		if (EVL_WARN_ON(CORE, list_empty(&synch->wait_list)))
+			return;
+		if (target == NULL)
+			target = list_first_entry(&synch->wait_list,
+						  struct evl_thread, syn_next);
+		inherit_thread_priority(owner, target);
+	}
+}
+
+static void ceil_owner_priority(struct evl_syn *synch)
+{
+	struct evl_thread *owner = synch->owner;
+	int wprio;
+
+	/* PP ceiling values are implicitly based on the RT class. */
+	wprio = evl_calc_weighted_prio(&evl_sched_rt,
+				       get_ceiling_value(synch));
+	synch->wprio = wprio;
+	list_add_priff(synch, &owner->boosters, wprio, next);
+	raise_boost_flag(owner);
+	synch->status |= EVL_SYN_CEILING;
+
+	/*
+	 * If the ceiling value is lower than the current effective
+	 * priority, we must not adjust the latter.  BEWARE: not only
+	 * this restriction is required to keep the PP logic right,
+	 * but this is also a basic assumption made by all
+	 * evl_commit_monitor_ceiling() callers which won't check for any
+	 * rescheduling opportunity upon return.
+	 *
+	 * However we do want the object to be linked to the booster
+	 * list, and T_BOOST must appear in the current thread status.
+	 *
+	 * This way, setparam() won't be allowed to decrease the
+	 * current weighted priority below the ceiling value, until we
+	 * eventually release this object.
+	 */
+	if (wprio > owner->wprio)
+		adjust_boost(owner, NULL);
+}
+
+static inline
+void track_owner(struct evl_syn *synch, struct evl_thread *owner)
+{
+	synch->owner = owner;
+}
+
+static inline  /* nklock held, irqs off */
+void set_current_owner_locked(struct evl_syn *synch, struct evl_thread *owner)
+{
+	/*
+	 * Update the owner information, and apply priority protection
+	 * for PP objects. We may only get there if owner is current,
+	 * or blocked.
+	 */
+	track_owner(synch, owner);
+	if (synch->status & EVL_SYN_PP)
+		ceil_owner_priority(synch);
+}
+
+static inline
+void set_current_owner(struct evl_syn *synch, struct evl_thread *owner)
+{
+	unsigned long flags;
+
+	track_owner(synch, owner);
+	if (synch->status & EVL_SYN_PP) {
+		xnlock_get_irqsave(&nklock, flags);
+		ceil_owner_priority(synch);
+		xnlock_put_irqrestore(&nklock, flags);
+	}
+}
+
+static inline
+fundle_t get_owner_handle(fundle_t ownerh, struct evl_syn *synch)
+{
+	/*
+	 * On acquisition from kernel space, the fast lock handle
+	 * should bear the FLCEIL bit for PP objects, so that userland
+	 * takes the slow path on release, jumping to the kernel for
+	 * dropping the ceiling priority boost.
+	 */
+	if (synch->status & EVL_SYN_PP)
+		ownerh = evl_syn_fast_ceil(ownerh);
+
+	return ownerh;
+}
+
+void evl_commit_syn_ceiling(struct evl_syn *synch,
+			    struct evl_thread *curr)
+{
+	fundle_t oldh, h;
+	atomic_t *lockp;
+
+	/*
+	 * For PP locks, userland does, in that order:
+	 *
+	 * -- LOCK
+	 * 1. curr->u_window->pp_pending = fundle_of(monitor)
+	 *    barrier();
+	 * 2. atomic_cmpxchg(lockp, EVL_NO_HANDLE, fundle_of(curr));
+	 *
+	 * -- UNLOCK
+	 * 1. atomic_cmpxchg(lockp, fundle_of(curr), EVL_NO_HANDLE); [unclaimed]
+	 *    barrier();
+	 * 2. curr->u_window->pp_pending = EVL_NO_HANDLE
+	 *
+	 * Make sure we have not been caught in a rescheduling in
+	 * between those steps. If we did, then we won't be holding
+	 * the lock as we schedule away, therefore no priority update
+	 * must take place.
+	 *
+	 * Hypothetical case: we might be called multiple times for
+	 * committing a lazy ceiling for the same object, e.g. if
+	 * userland is preempted in the middle of a recursive locking
+	 * sequence. Since the only synchronization object we have is
+	 * the monitor, and that one does not support recursive
+	 * locking, what has just been described cannot happen yet
+	 * though.
+	 *
+	 * This would stem from the fact that userland has to update
+	 * ->pp_pending prior to trying to grab the lock atomically,
+	 * at which point it can figure out whether a recursive
+	 * locking happened. We get out of this trap by testing the
+	 * EVL_SYN_CEILING flag.
+	 */
+	if (!evl_is_syn_owner(synch->fastlock, fundle_of(curr)) ||
+	    (synch->status & EVL_SYN_CEILING))
+		return;
+
+	track_owner(synch, curr);
+	ceil_owner_priority(synch);
+	/*
+	 * Raise FLCEIL, which indicates a kernel entry will be
+	 * required for releasing this resource.
+	 */
+	lockp = synch->fastlock;
+	do {
+		h = atomic_read(lockp);
+		oldh = atomic_cmpxchg(lockp, h, evl_syn_fast_ceil(h));
+	} while (oldh != h);
+}
+
+int evl_try_acquire_syn(struct evl_syn *synch)
+{
+	struct evl_thread *curr;
+	atomic_t *lockp;
+	fundle_t h;
+
+	oob_context_only();
+
+	if (EVL_WARN_ON(CORE, !(synch->status & EVL_SYN_OWNER)))
+		return -EINVAL;
+
+	curr = evl_current_thread();
+	lockp = synch->fastlock;
+	trace_evl_synch_try_acquire(synch);
+
+	h = atomic_cmpxchg(lockp, EVL_NO_HANDLE,
+			   get_owner_handle(fundle_of(curr), synch));
+	if (h != EVL_NO_HANDLE)
+		return evl_get_index(h) == fundle_of(curr) ?
+			-EDEADLK : -EBUSY;
+
+	set_current_owner(synch, curr);
+	get_thread_resource(curr);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(evl_try_acquire_syn);
+
+#ifdef CONFIG_EVENLESS_DEBUG_MONITOR_INBAND
+
+/*
+ * Detect when a thread is about to wait on a synchronization
+ * object currently owned by someone running in-band.
+ */
+static void detect_inband_owner(struct evl_syn *synch,
+				struct evl_thread *waiter)
+{
+	if ((waiter->state & T_WARN) &&
+	    !(waiter->info & T_PIALERT) &&
+	    (synch->owner->state & T_INBAND)) {
+		waiter->info |= T_PIALERT;
+		evl_signal_thread(waiter, SIGDEBUG,
+				  SIGDEBUG_MIGRATE_PRIOINV);
+	} else
+		waiter->info &= ~T_PIALERT;
+}
+
+/*
+ * Detect when a thread is about to switch to in-band context while
+ * holding booster(s) (claimed PI or active PP object), which denotes
+ * a potential priority inversion. In such an event, any waiter
+ * bearing the T_WARN bit will receive a SIGDEBUG notification.
+ */
+void evl_detect_boost_drop(struct evl_thread *owner)
+{
+	struct evl_thread *waiter;
+	struct evl_syn *synch;
+	unsigned long flags;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	for_each_evl_booster(synch, owner) {
+		evl_for_each_syn_waiter(waiter, synch) {
+			if (waiter->state & T_WARN) {
+				waiter->info |= T_PIALERT;
+				evl_signal_thread(waiter, SIGDEBUG,
+						  SIGDEBUG_MIGRATE_PRIOINV);
+			}
+		}
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+}
+
+#else /* !CONFIG_EVENLESS_DEBUG_MONITOR_INBAND */
+
+static inline
+void detect_inband_owner(struct evl_syn *synch,
+			 struct evl_thread *waiter) { }
+
+#endif /* !CONFIG_EVENLESS_DEBUG_MONITOR_INBAND */
+
+int evl_acquire_syn(struct evl_syn *synch, ktime_t timeout,
+		    enum evl_tmode timeout_mode)
+{
+	struct evl_thread *curr, *owner;
+	fundle_t currh, h, oldh;
+	unsigned long flags;
+	atomic_t *lockp;
+	int ret;
+
+	oob_context_only();
+
+	if (EVL_WARN_ON(CORE, !(synch->status & EVL_SYN_OWNER)))
+		return T_BREAK;
+
+	curr = evl_current_thread();
+	currh = fundle_of(curr);
+	lockp = synch->fastlock;
+	trace_evl_synch_acquire(synch);
+redo:
+	/* Basic form of evl_try_acquire_syn(). */
+	h = atomic_cmpxchg(lockp, EVL_NO_HANDLE,
+			   get_owner_handle(currh, synch));
+	if (likely(h == EVL_NO_HANDLE)) {
+		set_current_owner(synch, curr);
+		get_thread_resource(curr);
+		return 0;
+	}
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	/*
+	 * Set claimed bit.  In case it appears to be set already,
+	 * re-read its state under nklock so that we don't miss any
+	 * change between the lock-less read and here. But also try to
+	 * avoid cmpxchg where possible. Only if it appears not to be
+	 * set, start with cmpxchg directly.
+	 */
+	if (evl_fast_syn_is_claimed(h)) {
+		oldh = atomic_read(lockp);
+		goto test_no_owner;
+	}
+
+	do {
+		oldh = atomic_cmpxchg(lockp, h, evl_syn_fast_claim(h));
+		if (likely(oldh == h))
+			break;
+	test_no_owner:
+		if (oldh == EVL_NO_HANDLE) {
+			/* Lock released from another cpu. */
+			xnlock_put_irqrestore(&nklock, flags);
+			goto redo;
+		}
+		h = oldh;
+	} while (!evl_fast_syn_is_claimed(h));
+
+	owner = evl_get_element_by_fundle(&evl_thread_factory, h,
+					  struct evl_thread);
+	/*
+	 * If the handle is broken, pretend that the synch object was
+	 * deleted to signal an error.
+	 */
+	if (owner == NULL) {
+		xnlock_put_irqrestore(&nklock, flags);
+		return T_RMID;
+	}
+
+	/*
+	 * This is the contended path. We just detected an earlier
+	 * syscall-less fast locking from userland, fix up the
+	 * in-kernel state information accordingly.
+	 *
+	 * The consistency of the state information is guaranteed,
+	 * because we just raised the claim bit atomically for this
+	 * contended lock, therefore userland will have to jump to the
+	 * kernel when releasing it, instead of doing a fast
+	 * unlock. Since we currently own the superlock, consistency
+	 * wrt transfer_ownership() is guaranteed through
+	 * serialization.
+	 *
+	 * CAUTION: in this particular case, the only assumptions we
+	 * can safely make is that *owner is valid but not current on
+	 * this CPU.
+	 */
+	track_owner(synch, owner);
+	detect_inband_owner(synch, curr);
+
+	if (!(synch->status & EVL_SYN_PRIO)) { /* i.e. FIFO */
+		list_add_tail(&curr->syn_next, &synch->wait_list);
+		goto block;
+	}
+
+	if (curr->wprio > owner->wprio) {
+		if ((owner->info & T_WAKEN) && owner->wwake == synch) {
+			/* Ownership is still pending, steal the resource. */
+			set_current_owner_locked(synch, curr);
+			owner->info |= T_ROBBED;
+			ret = 0;
+			goto grab;
+		}
+
+		list_add_priff(curr, &synch->wait_list, wprio, syn_next);
+
+		if (synch->status & EVL_SYN_PI) {
+			raise_boost_flag(owner);
+
+			if (synch->status & EVL_SYN_CLAIMED)
+				list_del(&synch->next); /* owner->boosters */
+			else
+				synch->status |= EVL_SYN_CLAIMED;
+
+			synch->wprio = curr->wprio;
+			list_add_priff(synch, &owner->boosters, wprio, next);
+			/*
+			 * curr->wprio > owner->wprio implies that
+			 * synch must be leading the booster list
+			 * after insertion, so we may call
+			 * inherit_thread_priority() for tracking
+			 * current's priority directly without going
+			 * through adjust_boost().
+			 */
+			inherit_thread_priority(owner, curr);
+		}
+	} else
+		list_add_priff(curr, &synch->wait_list, wprio, syn_next);
+block:
+	ret = block_thread_timed(timeout, timeout_mode, synch->clock, synch);
+	curr->wwake = NULL;
+	curr->info &= ~T_WAKEN;
+
+	if (ret)
+		goto out;
+
+	if (curr->info & T_ROBBED) {
+		/*
+		 * Somebody stole us the ownership while we were ready
+		 * to run, waiting for the CPU: we need to wait again
+		 * for the resource.
+		 */
+		if (timeout_mode != EVL_REL || timeout_infinite(timeout)) {
+			xnlock_put_irqrestore(&nklock, flags);
+			goto redo;
+		}
+		timeout = evl_get_stopped_timer_delta(&curr->rtimer);
+		if (timeout) { /* Otherwise, it's too late. */
+			xnlock_put_irqrestore(&nklock, flags);
+			goto redo;
+		}
+		ret = T_TIMEO;
+		goto out;
+	}
+grab:
+	get_thread_resource(curr);
+
+	if (evl_syn_has_waiter(synch))
+		currh = evl_syn_fast_claim(currh);
+
+	/* Set new ownership for this object. */
+	atomic_set(lockp, get_owner_handle(currh, synch));
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	evl_put_element(&owner->element);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_acquire_syn);
+
+static void drop_booster(struct evl_syn *synch, struct evl_thread *owner)
+{
+	list_del(&synch->next);	/* owner->boosters */
+
+	if (list_empty(&owner->boosters)) {
+		owner->state &= ~T_BOOST;
+		inherit_thread_priority(owner, owner);
+	} else
+		adjust_boost(owner, NULL);
+}
+
+static inline void clear_pi_boost(struct evl_syn *synch,
+				  struct evl_thread *owner)
+{	/* nklock held, irqs off */
+	synch->status &= ~EVL_SYN_CLAIMED;
+	drop_booster(synch, owner);
+}
+
+static inline void clear_pp_boost(struct evl_syn *synch,
+				  struct evl_thread *owner)
+{	/* nklock held, irqs off */
+	synch->status &= ~EVL_SYN_CEILING;
+	drop_booster(synch, owner);
+}
+
+static bool transfer_ownership(struct evl_syn *synch,
+			       struct evl_thread *lastowner)
+{				/* nklock held, irqs off */
+	struct evl_thread *n_owner;
+	fundle_t n_ownerh;
+	atomic_t *lockp;
+
+	lockp = synch->fastlock;
+
+	/*
+	 * Our caller checked for contention locklessly, so we do have
+	 * to check again under lock in a different way.
+	 */
+	if (list_empty(&synch->wait_list)) {
+		synch->owner = NULL;
+		atomic_set(lockp, EVL_NO_HANDLE);
+		return false;
+	}
+
+	n_owner = list_first_entry(&synch->wait_list, struct evl_thread, syn_next);
+	list_del(&n_owner->syn_next);
+	n_owner->wchan = NULL;
+	n_owner->wwake = synch;
+	set_current_owner_locked(synch, n_owner);
+	n_owner->info |= T_WAKEN;
+	evl_resume_thread(n_owner, T_PEND);
+
+	if (synch->status & EVL_SYN_CLAIMED)
+		clear_pi_boost(synch, lastowner);
+
+	n_ownerh = get_owner_handle(fundle_of(n_owner), synch);
+	if (evl_syn_has_waiter(synch))
+		n_ownerh = evl_syn_fast_claim(n_ownerh);
+
+	atomic_set(lockp, n_ownerh);
+
+	return true;
+}
+
+bool evl_release_syn(struct evl_syn *synch, struct evl_thread *curr)
+{
+	bool need_resched = false;
+	unsigned long flags;
+	fundle_t currh, h;
+	atomic_t *lockp;
+
+	if (EVL_WARN_ON(CORE, !(synch->status & EVL_SYN_OWNER)))
+		return false;
+
+	trace_evl_synch_release(synch);
+
+	if (!put_thread_resource(curr))
+		return false;
+
+	lockp = synch->fastlock;
+	currh = fundle_of(curr);
+	/*
+	 * FLCEIL may only be raised by the owner, or when the owner
+	 * is blocked waiting for the synch (ownership transfer). In
+	 * addition, only the current owner of a synch may release it,
+	 * therefore we can't race while testing FLCEIL locklessly.
+	 * All updates to FLCLAIM are covered by the superlock.
+	 *
+	 * Therefore, clearing the fastlock racelessly in this routine
+	 * without leaking FLCEIL/FLCLAIM updates can be achieved by
+	 * holding the superlock.
+	 */
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (synch->status & EVL_SYN_CEILING) {
+		clear_pp_boost(synch, curr);
+		need_resched = true;
+	}
+
+	h = atomic_cmpxchg(lockp, currh, EVL_NO_HANDLE);
+	if ((h & ~EVL_SYN_FLCEIL) != currh)
+		/* FLCLAIM set, synch is contended. */
+		need_resched = transfer_ownership(synch, curr);
+	else if (h != currh)	/* FLCEIL set, FLCLAIM clear. */
+		atomic_set(lockp, EVL_NO_HANDLE);
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return need_resched;
+}
+EXPORT_SYMBOL_GPL(evl_release_syn);
+
+void evl_requeue_syn_waiter(struct evl_thread *thread)
+{				/* nklock held, irqs off */
+	struct evl_syn *synch = thread->wchan;
+	struct evl_thread *owner;
+
+	if (EVL_WARN_ON(CORE, !(synch->status & EVL_SYN_PRIO)))
+		return;
+
+	/*
+	 * Update the position in the pend queue of a thread waiting
+	 * for a lock. This routine propagates the change throughout
+	 * the PI chain if required.
+	 */
+	list_del(&thread->syn_next);
+	list_add_priff(thread, &synch->wait_list, wprio, syn_next);
+	owner = synch->owner;
+
+	/* Only PI-enabled objects are of interest here. */
+	if (!(synch->status & EVL_SYN_PI))
+		return;
+
+	synch->wprio = thread->wprio;
+	if (synch->status & EVL_SYN_CLAIMED)
+		list_del(&synch->next);
+	else {
+		synch->status |= EVL_SYN_CLAIMED;
+		raise_boost_flag(owner);
+	}
+
+	list_add_priff(synch, &owner->boosters, wprio, next);
+	adjust_boost(owner, thread);
+}
+EXPORT_SYMBOL_GPL(evl_requeue_syn_waiter);
+
+struct evl_thread *evl_syn_wait_head(struct evl_syn *synch)
+{
+	return list_first_entry_or_null(&synch->wait_list,
+					struct evl_thread, syn_next);
+}
+EXPORT_SYMBOL_GPL(evl_syn_wait_head);
+
+bool evl_flush_syn(struct evl_syn *synch, int reason)
+{
+	struct evl_thread *waiter, *tmp;
+	unsigned long flags;
+	bool ret;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	trace_evl_synch_flush(synch);
+
+	if (list_empty(&synch->wait_list)) {
+		EVL_WARN_ON(CORE, synch->status & EVL_SYN_CLAIMED);
+		ret = false;
+	} else {
+		ret = true;
+		list_for_each_entry_safe(waiter, tmp, &synch->wait_list, syn_next) {
+			list_del(&waiter->syn_next);
+			waiter->info |= reason;
+			waiter->wchan = NULL;
+			evl_resume_thread(waiter, T_PEND);
+		}
+		if (synch->status & EVL_SYN_CLAIMED)
+			clear_pi_boost(synch, synch->owner);
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_flush_syn);
+
+void evl_forget_syn_waiter(struct evl_thread *thread)
+{				/* nklock held, irqs off */
+	struct evl_syn *synch = thread->wchan;
+	struct evl_thread *owner, *target;
+
+	/*
+	 * Do all the necessary housekeeping chores to stop a thread
+	 * from waiting on a given synchronization object. Doing so
+	 * may require to update a PI chain.
+	 */
+	trace_evl_synch_forget(synch);
+
+	thread->state &= ~T_PEND;
+	thread->wchan = NULL;
+	list_del(&thread->syn_next); /* synch->wait_list */
+
+	/*
+	 * Only a waiter leaving a PI chain triggers an update.
+	 * NOTE: PP objects never bear the CLAIMED bit.
+	 */
+	if (!(synch->status & EVL_SYN_CLAIMED))
+		return;
+
+	owner = synch->owner;
+
+	if (list_empty(&synch->wait_list)) {
+		/* No more waiters: clear the PI boost. */
+		clear_pi_boost(synch, owner);
+		return;
+	}
+
+	/*
+	 * Reorder the booster queue of the current owner after we
+	 * left the wait list, then set its priority to the new
+	 * required minimum required to prevent priority inversion.
+	 */
+	target = list_first_entry(&synch->wait_list, struct evl_thread, syn_next);
+	synch->wprio = target->wprio;
+	list_del(&synch->next);	/* owner->boosters */
+	list_add_priff(synch, &owner->boosters, wprio, next);
+	adjust_boost(owner, target);
+}
+EXPORT_SYMBOL_GPL(evl_forget_syn_waiter);
diff --git a/kernel/evenless/syscall.c b/kernel/evenless/syscall.c
new file mode 100644
index 000000000000..603bae6e95e1
--- /dev/null
+++ b/kernel/evenless/syscall.c
@@ -0,0 +1,341 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Context handling logic derived from Xenomai Cobalt
+ * (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2005, 2018 Philippe Gerum  <rpm@xenomai.org>
+ * Copyright (C) 2005 Gilles Chanteperdrix  <gilles.chanteperdrix@xenomai.org>
+ */
+
+#include <linux/types.h>
+#include <linux/err.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/dovetail.h>
+#include <linux/kconfig.h>
+#include <linux/kernel.h>
+#include <linux/sched/task_stack.h>
+#include <linux/sched/signal.h>
+#include <evenless/control.h>
+#include <evenless/thread.h>
+#include <evenless/timer.h>
+#include <evenless/monitor.h>
+#include <evenless/clock.h>
+#include <evenless/sched.h>
+#include <evenless/file.h>
+#include <trace/events/evenless.h>
+#include <uapi/evenless/syscall.h>
+#include <asm/evenless/syscall.h>
+#ifdef CONFIG_FTRACE
+#include <trace/trace.h>
+#endif
+
+#define EVENLESS_SYSCALL(__name, __args)	\
+	long EvEnLeSs_ ## __name __args
+
+#define SYSCALL_PROPAGATE   0
+#define SYSCALL_STOP        1
+
+typedef long (*evl_syshand)(unsigned long arg1, unsigned long arg2,
+			    unsigned long arg3, unsigned long arg4,
+			    unsigned long arg5);
+
+static const evl_syshand evl_syscalls[__NR_EVENLESS_SYSCALLS];
+
+static inline void do_oob_request(int nr, struct pt_regs *regs)
+{
+	evl_syshand handler;
+	long ret;
+
+	handler = evl_syscalls[nr];
+	ret = handler(oob_arg1(regs),
+		      oob_arg2(regs),
+		      oob_arg3(regs),
+		      oob_arg4(regs),
+		      oob_arg5(regs));
+
+	set_oob_retval(regs, ret);
+}
+
+static void prepare_for_signal(struct task_struct *p,
+			       struct evl_thread *curr,
+			       struct pt_regs *regs)
+{
+	int cause = SIGDEBUG_UNDEFINED;
+	unsigned long flags;
+
+	/*
+	 * FIXME: no restart mode flag for setting -EINTR instead of
+	 * -ERESTARTSYS should be obtained from curr->local_info on a
+	 * per-invocation basis, not on a per-call one (since we have
+	 * 3 generic calls only).
+	 */
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (curr->info & T_KICKED) {
+		if (signal_pending(p)) {
+			set_oob_error(regs, -ERESTARTSYS);
+			if (!(curr->state & T_SSTEP))
+				cause = SIGDEBUG_MIGRATE_SIGNAL;
+			curr->info &= ~T_BREAK;
+		}
+		curr->info &= ~T_KICKED;
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	evl_test_cancel();
+
+	evl_switch_inband(cause);
+}
+
+static int do_oob_syscall(struct irq_stage *stage, struct pt_regs *regs)
+{
+	struct evl_thread *curr;
+	struct task_struct *p;
+	unsigned int nr;
+
+	if (!is_oob_syscall(regs))
+		goto do_inband;
+
+	nr = oob_syscall_nr(regs);
+	curr = evl_current_thread();
+	if (curr == NULL || !cap_raised(current_cap(), CAP_SYS_NICE)) {
+		if (EVL_DEBUG(CORE))
+			printk(EVL_WARNING
+			       "OOB syscall <%d> denied to %s[%d]\n",
+			       nr, current->comm, task_pid_nr(current));
+		set_oob_error(regs, -EPERM);
+		return SYSCALL_STOP;
+	}
+
+	if (nr >= ARRAY_SIZE(evl_syscalls))
+		goto bad_syscall;
+
+	/*
+	 * If the syscall originates from in-band context, hand it
+	 * over to handle_inband_syscall() where the caller would be
+	 * switched to OOB context prior to handling the request.
+	 */
+	if (stage != &oob_stage)
+		return SYSCALL_PROPAGATE;
+
+	trace_evl_oob_sysentry(nr);
+
+	do_oob_request(nr, regs);
+
+	/* Syscall might have switched in-band, recheck. */
+	if (!evl_is_inband()) {
+		p = current;
+		if (signal_pending(p) || (curr->info & T_KICKED))
+			prepare_for_signal(p, curr, regs);
+		else if ((curr->state & T_WEAK) && curr->res_count == 0)
+			evl_switch_inband(SIGDEBUG_UNDEFINED);
+	}
+
+	/* Update the stats and user visible info. */
+	evl_inc_counter(&curr->stat.sc);
+	evl_sync_uwindow(curr);
+
+	trace_evl_oob_sysexit(oob_retval(regs));
+
+	return SYSCALL_STOP;
+
+do_inband:
+	if (evl_is_inband())
+		return SYSCALL_PROPAGATE;
+
+	/*
+	 * If this is a legit in-band syscall issued from OOB context,
+	 * switch to in-band mode before propagating the syscall down
+	 * the pipeline.
+	 */
+	if (inband_syscall_nr(regs, &nr)) {
+		evl_switch_inband(SIGDEBUG_MIGRATE_SYSCALL);
+		return SYSCALL_PROPAGATE;
+	}
+
+bad_syscall:
+	printk(EVL_WARNING "bad OOB syscall <%#x>\n", nr);
+
+	set_oob_error(regs, -ENOSYS);
+
+	return SYSCALL_STOP;
+}
+
+static int do_inband_syscall(struct irq_stage *stage, struct pt_regs *regs)
+{
+	struct evl_thread *curr = evl_current_thread(); /* Always valid. */
+	struct task_struct *p;
+	unsigned int nr;
+	int ret;
+
+	/*
+	 * Catch cancellation requests pending for threads undergoing
+	 * the weak scheduling policy, which won't cross
+	 * prepare_for_signal() frequently as they run mostly in-band.
+	 */
+	evl_test_cancel();
+
+	/* Handle lazy schedparam updates before switching. */
+	evl_propagate_schedparam_change(curr);
+
+	/* Propagate in-band syscalls. */
+	if (!is_oob_syscall(regs))
+		return SYSCALL_PROPAGATE;
+
+	/*
+	 * Process an OOB syscall after switching current to OOB
+	 * context.  do_oob_syscall() already checked the syscall
+	 * number.
+	 */
+	nr = oob_syscall_nr(regs);
+
+	trace_evl_inband_sysentry(nr);
+
+	ret = evl_switch_oob();
+	if (ret) {
+		set_oob_error(regs, ret);
+		goto done;
+	}
+
+	do_oob_request(nr, regs);
+
+	if (!evl_is_inband()) {
+		p = current;
+		if (signal_pending(p))
+			prepare_for_signal(p, curr, regs);
+		else if ((curr->state & T_WEAK) && curr->res_count == 0)
+			evl_switch_inband(SIGDEBUG_UNDEFINED);
+	}
+done:
+	curr->local_info &= ~T_HICCUP;
+	evl_inc_counter(&curr->stat.sc);
+	evl_sync_uwindow(curr);
+
+	trace_evl_inband_sysexit(oob_retval(regs));
+
+	return SYSCALL_STOP;
+}
+
+int handle_pipelined_syscall(struct irq_stage *stage, struct pt_regs *regs)
+{
+	if (unlikely(running_inband()))
+		return do_inband_syscall(stage, regs);
+
+	return do_oob_syscall(stage, regs);
+}
+
+int handle_oob_syscall(struct pt_regs *regs)
+{
+	int ret;
+
+	ret = do_oob_syscall(&oob_stage, regs);
+	if (EVL_WARN_ON(CORE, ret == SYSCALL_PROPAGATE))
+		ret = SYSCALL_STOP;
+
+	return ret;
+}
+
+static EVENLESS_SYSCALL(read, (int fd, char __user *u_buf, size_t size))
+{
+	struct evl_file *sfilp = evl_get_file(fd);
+	struct file *filp;
+	ssize_t ret;
+
+	if (sfilp == NULL)
+		return -EBADF;
+
+	filp = sfilp->filp;
+	if (!(filp->f_mode & FMODE_READ)) {
+		ret = -EBADF;
+		goto out;
+	}
+
+	if (filp->f_op->oob_read == NULL) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	ret = filp->f_op->oob_read(filp, u_buf, size);
+out:
+	evl_put_file(sfilp);
+
+	return ret;
+}
+
+static EVENLESS_SYSCALL(write, (int fd, const char __user *u_buf, size_t size))
+{
+	struct evl_file *sfilp = evl_get_file(fd);
+	struct file *filp;
+	ssize_t ret;
+
+	if (sfilp == NULL)
+		return -EBADF;
+
+	filp = sfilp->filp;
+	if (!(filp->f_mode & FMODE_WRITE)) {
+		ret = -EBADF;
+		goto out;
+	}
+
+	if (filp->f_op->oob_write == NULL) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	ret = filp->f_op->oob_write(filp, u_buf, size);
+out:
+	evl_put_file(sfilp);
+
+	return ret;
+}
+
+static EVENLESS_SYSCALL(ioctl, (int fd, unsigned int request,
+				unsigned long arg))
+{
+	struct evl_file *sfilp = evl_get_file(fd);
+	struct file *filp;
+	long ret;
+
+	if (sfilp == NULL)
+		return -EBADF;
+
+	filp = sfilp->filp;
+	if (filp->f_op->oob_ioctl) {
+		ret = filp->f_op->oob_ioctl(filp, request, arg);
+		if (ret == -ENOIOCTLCMD)
+			ret = -ENOTTY;
+	} else
+		ret = -ENOTTY;
+
+	evl_put_file(sfilp);
+
+	return ret;
+}
+
+static int EvEnLeSs_ni(void)
+{
+	return -ENOSYS;
+}
+
+#define __syshand__(__name)	((evl_syshand)(EvEnLeSs_ ## __name))
+
+#define __EVENLESS_CALL_ENTRIES		\
+	__EVENLESS_CALL_ENTRY(read)	\
+	__EVENLESS_CALL_ENTRY(write)	\
+	__EVENLESS_CALL_ENTRY(ioctl)
+
+#define __EVENLESS_NI	__syshand__(ni)
+
+#define __EVENLESS_CALL_NI		\
+	[0 ... __NR_EVENLESS_SYSCALLS-1] = __EVENLESS_NI,
+
+#define __EVENLESS_CALL_ENTRY(__name)	\
+	[sys_evenless_ ## __name] = __syshand__(__name),
+
+static const evl_syshand evl_syscalls[] = {
+	__EVENLESS_CALL_NI
+	__EVENLESS_CALL_ENTRIES
+};
diff --git a/kernel/evenless/thread.c b/kernel/evenless/thread.c
new file mode 100644
index 000000000000..53f00f922071
--- /dev/null
+++ b/kernel/evenless/thread.c
@@ -0,0 +1,2643 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2006, 2016 Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>
+ * Copyright (C) 2001, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <stdarg.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/wait.h>
+#include <linux/slab.h>
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/irq_work.h>
+#include <linux/sched/signal.h>
+#include <linux/sched/types.h>
+#include <linux/sched/task.h>
+#include <linux/jiffies.h>
+#include <linux/cred.h>
+#include <linux/err.h>
+#include <linux/ptrace.h>
+#include <linux/math64.h>
+#include <evenless/sched.h>
+#include <evenless/timer.h>
+#include <evenless/synch.h>
+#include <evenless/clock.h>
+#include <evenless/stat.h>
+#include <evenless/assert.h>
+#include <evenless/lock.h>
+#include <evenless/thread.h>
+#include <evenless/sched.h>
+#include <evenless/memory.h>
+#include <evenless/file.h>
+#include <evenless/factory.h>
+#include <evenless/monitor.h>
+#include <asm/evenless/syscall.h>
+#include <trace/events/evenless.h>
+
+static DECLARE_WAIT_QUEUE_HEAD(join_all);
+
+static void timeout_handler(struct evl_timer *timer) /* hard irqs off */
+{
+	struct evl_thread *thread = container_of(timer, struct evl_thread, rtimer);
+
+	xnlock_get(&nklock);
+	thread->info |= T_TIMEO;
+	evl_resume_thread(thread, T_DELAY);
+	xnlock_put(&nklock);
+}
+
+static void periodic_handler(struct evl_timer *timer) /* hard irqs off */
+{
+	struct evl_thread *thread =
+		container_of(timer, struct evl_thread, ptimer);
+
+	xnlock_get(&nklock);
+	/*
+	 * Prevent unwanted round-robin, and do not wake up threads
+	 * blocked on a resource.
+	 */
+	if ((thread->state & (T_DELAY|T_PEND)) == T_DELAY)
+		evl_resume_thread(thread, T_DELAY);
+
+	evl_set_timer_rq(&thread->ptimer, evl_thread_rq(thread));
+
+	xnlock_put(&nklock);
+}
+
+static inline void enlist_new_thread(struct evl_thread *thread)
+{				/* nklock held, irqs off */
+	list_add_tail(&thread->next, &evl_thread_list);
+	evl_nrthreads++;
+}
+
+static inline void set_oob_threadinfo(struct evl_thread *thread)
+{
+	struct oob_thread_state *p;
+
+	p = dovetail_current_state();
+	p->thread = thread;
+}
+
+static void pin_to_initial_cpu(struct evl_thread *thread)
+{
+	struct task_struct *p = current;
+	unsigned long flags;
+	struct evl_rq *rq;
+	int cpu;
+
+	/*
+	 * @thread is the EVL extension of the current in-band
+	 * task. If the current CPU is part of the affinity mask of
+	 * this thread, pin the latter on this CPU. Otherwise pin it
+	 * to the first CPU of that mask.
+	 */
+	cpu = task_cpu(p);
+	if (!cpumask_test_cpu(cpu, &thread->affinity))
+		cpu = cpumask_first(&thread->affinity);
+
+	set_cpus_allowed_ptr(p, cpumask_of(cpu));
+	/*
+	 * @thread is still unstarted EVL-wise, we are in the process
+	 * of mapping the current in-band task to it. Therefore
+	 * evl_migrate_thread() can be called for pinning it on a
+	 * real-time CPU.
+	 */
+	xnlock_get_irqsave(&nklock, flags);
+	rq = evl_cpu_rq(cpu);
+	evl_migrate_thread(thread, rq);
+	xnlock_put_irqrestore(&nklock, flags);
+}
+
+int evl_init_thread(struct evl_thread *thread,
+		    const struct evl_init_thread_attr *iattr,
+		    struct evl_rq *rq,
+		    const char *fmt, ...)
+{
+	int flags = iattr->flags & ~T_SUSP, ret, gravity;
+	struct cpumask affinity;
+	va_list args;
+
+	if (!(flags & T_ROOT))
+		flags |= T_DORMANT;
+
+	/*
+	 * If no rq was given, pick an initial CPU for the new thread
+	 * which is part of its affinity mask, and therefore also part
+	 * of the supported CPUs. This CPU may change in
+	 * pin_to_initial_cpu().
+	 */
+	if (rq == NULL) {
+		cpumask_and(&affinity, &iattr->affinity, &evl_cpu_affinity);
+		if (cpumask_empty(&affinity))
+			return -EINVAL;
+		rq = evl_cpu_rq(cpumask_first(&affinity));
+	}
+
+	va_start(args, fmt);
+	thread->name = kvasprintf(GFP_KERNEL, fmt, args);
+	va_end(args);
+	if (thread->name == NULL)
+		return -ENOMEM;
+
+	/*
+	 * We mirror the global user debug state into the per-thread
+	 * state, to speed up branch taking in libevenless wherever this
+	 * needs to be tested.
+	 */
+	if (IS_ENABLED(CONFIG_EVENLESS_DEBUG_MONITOR_SLEEP))
+		flags |= T_DEBUG;
+
+	cpumask_and(&thread->affinity, &iattr->affinity, &evl_cpu_affinity);
+	thread->rq = rq;
+	thread->state = flags;
+	thread->info = 0;
+	thread->local_info = 0;
+	thread->wprio = EVL_IDLE_PRIO;
+	thread->cprio = EVL_IDLE_PRIO;
+	thread->bprio = EVL_IDLE_PRIO;
+	thread->lock_count = 0;
+	thread->rrperiod = EVL_INFINITE;
+	thread->wchan = NULL;
+	thread->wwake = NULL;
+	thread->wait_data = NULL;
+	thread->res_count = 0;
+	thread->u_window = NULL;
+	memset(&thread->poll_context, 0, sizeof(thread->poll_context));
+	memset(&thread->stat, 0, sizeof(thread->stat));
+	memset(&thread->altsched, 0, sizeof(thread->altsched));
+
+	INIT_LIST_HEAD(&thread->next);
+	INIT_LIST_HEAD(&thread->boosters);
+	init_completion(&thread->exited);
+
+	gravity = flags & T_USER ? EVL_TIMER_UGRAVITY : EVL_TIMER_KGRAVITY;
+	evl_init_timer(&thread->rtimer, &evl_mono_clock, timeout_handler,
+		       rq, gravity);
+	evl_set_timer_name(&thread->rtimer, thread->name);
+	evl_set_timer_priority(&thread->rtimer, EVL_TIMER_HIPRIO);
+	evl_init_timer(&thread->ptimer, &evl_mono_clock, periodic_handler,
+		       rq, gravity);
+	evl_set_timer_name(&thread->ptimer, thread->name);
+	evl_set_timer_priority(&thread->ptimer, EVL_TIMER_HIPRIO);
+
+	thread->base_class = NULL; /* evl_set_thread_policy() sets it. */
+	ret = evl_init_rq_thread(thread);
+	if (ret)
+		goto err_out;
+
+	ret = evl_set_thread_policy(thread, iattr->sched_class,
+				    &iattr->sched_param);
+	if (ret)
+		goto err_out;
+
+	trace_evl_init_thread(thread, iattr, ret);
+
+	return 0;
+
+err_out:
+	evl_destroy_timer(&thread->rtimer);
+	evl_destroy_timer(&thread->ptimer);
+	trace_evl_init_thread(thread, iattr, ret);
+	kfree(thread->name);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_init_thread);
+
+/* Undoes evl_init_thread(), and only that. */
+static void uninit_thread(struct evl_thread *thread)
+{
+	unsigned long flags;
+
+	evl_destroy_timer(&thread->rtimer);
+	evl_destroy_timer(&thread->ptimer);
+
+	xnlock_get_irqsave(&nklock, flags);
+	evl_forget_thread(thread);
+	xnlock_put_irqrestore(&nklock, flags);
+
+	kfree(thread->name);
+}
+
+static inline void release_all_ownerships(struct evl_thread *curr)
+{
+	struct evl_syn *synch, *tmp;
+
+	/*
+	 * Release all the ownerships obtained by a thread on
+	 * synchronization objects. This routine must be entered
+	 * interrupts off.
+	 */
+	for_each_evl_booster_safe(synch, tmp, curr)
+		evl_release_syn(synch, curr);
+}
+
+static void do_cleanup_current(struct evl_thread *curr)
+{
+	unsigned long flags;
+
+	evl_unindex_element(&curr->element);
+
+	if (curr->state & T_USER) {
+		evl_free_chunk(&evl_shared_heap, curr->u_window);
+		curr->u_window = NULL;
+		if (curr->poll_context.table)
+			evl_free(curr->poll_context.table);
+	}
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	list_del(&curr->next);
+	evl_nrthreads--;
+
+	if (curr->state & T_READY) {
+		EVL_WARN_ON(CORE, (curr->state & EVL_THREAD_BLOCK_BITS));
+		evl_dequeue_thread(curr);
+		curr->state &= ~T_READY;
+	}
+
+	if (curr->state & T_PEND)
+		evl_forget_syn_waiter(curr);
+
+	curr->state |= T_ZOMBIE;
+	/*
+	 * NOTE: we must be running over the root thread, or @curr
+	 * is dormant, which means that we don't risk sched->curr to
+	 * disappear due to voluntary rescheduling while holding the
+	 * nklock, despite @curr bears the zombie bit.
+	 */
+	release_all_ownerships(curr);
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	uninit_thread(curr);
+}
+
+static void cleanup_current_thread(void)
+{
+	struct oob_thread_state *p = dovetail_current_state();
+	struct evl_thread *curr = evl_current_thread();
+
+	/*
+	 * We are called for exiting kernel and user threads over the
+	 * in-band context.
+	 */
+	trace_evl_thread_unmap(curr);
+	dovetail_stop_altsched();
+	do_cleanup_current(curr);
+
+	/* Wake up the joiner if any (we can't have more than one). */
+	complete(&curr->exited);
+
+	/* Notify our exit to evl_killall() if need be. */
+	if (waitqueue_active(&join_all))
+		wake_up(&join_all);
+
+	p->thread = NULL;	/* evl_current_thread() <- NULL */
+}
+
+static void put_current_thread(void)
+{
+	struct evl_thread *curr = evl_current_thread();
+
+	cleanup_current_thread();
+	evl_put_element(&curr->element);
+}
+
+static void wakeup_kthread_parent(struct irq_work *irq_work)
+{
+	struct evl_kthread *kthread;
+	kthread = container_of(irq_work, struct evl_kthread, irq_work);
+	complete(&kthread->done);
+}
+
+static int map_kthread_self(struct evl_kthread *kthread)
+{
+	struct evl_thread *curr = &kthread->thread;
+	unsigned long flags;
+
+	pin_to_initial_cpu(curr);
+
+	dovetail_init_altsched(&curr->altsched);
+	evl_stop_thread(curr, T_INBAND);
+	set_oob_threadinfo(curr);
+	dovetail_start_altsched();
+	evl_resume_thread(curr, T_DORMANT);
+
+	trace_evl_thread_map(curr);
+
+	/*
+	 * Upon -weird- error from evl_switch_oob() for an emerging
+	 * kernel thread, we still finalize the registration but the
+	 * caller should self-cancel eventually.
+	 */
+	kthread->status = evl_switch_oob();
+
+	/*
+	 * In the normal case, __evl_run_kthread() can't start us
+	 * before we stopped in evl_stop_thread() because
+	 * irq_work_queue() schedules the in-band wakeup request on
+	 * the current CPU: since we are currently running OOB on that
+	 * CPU, there is no way __evl_run_kthread() could resume
+	 * before we suspend. If we fail switching to OOB context,
+	 * kthread->status tells __evl_run_kthread() not to start but
+	 * cancel us instead.
+	 */
+	init_irq_work(&kthread->irq_work, wakeup_kthread_parent);
+	irq_work_queue(&kthread->irq_work);
+
+	xnlock_get_irqsave(&nklock, flags);
+	enlist_new_thread(curr);
+	xnlock_put_irqrestore(&nklock, flags);
+	evl_stop_thread(curr, T_DORMANT);
+
+	return kthread->status;
+}
+
+static int kthread_trampoline(void *arg)
+{
+	struct evl_kthread *kthread = arg;
+	struct evl_thread *curr = &kthread->thread;
+	struct sched_param param;
+	int policy, prio, ret;
+
+	/*
+	 * It only makes sense to create EVL kthreads with the
+	 * SCHED_FIFO, SCHED_NORMAL or SCHED_WEAK policies. So
+	 * anything that is not from EVL's RT class is assumed to
+	 * belong to in-band SCHED_NORMAL.
+	 */
+	if (curr->sched_class != &evl_sched_rt) {
+		policy = SCHED_NORMAL;
+		prio = 0;
+	} else {
+		policy = SCHED_FIFO;
+		prio = curr->cprio;
+		/* Normalize priority linux-wise. */
+		if (prio >= MAX_RT_PRIO)
+			prio = MAX_RT_PRIO - 1;
+	}
+
+	param.sched_priority = prio;
+	sched_setscheduler(current, policy, &param);
+
+	ret = map_kthread_self(kthread);
+	if (!ret) {
+		trace_evl_kthread_entry(curr);
+		kthread->threadfn(kthread);
+	}
+
+	/* Handles nitty-gritty details like in-band switch. */
+	evl_cancel_thread(curr);
+
+	return 0;
+}
+
+int __evl_run_kthread(struct evl_kthread *kthread)
+{
+	struct evl_thread *thread = &kthread->thread;
+	struct task_struct *p;
+	int ret;
+
+	ret = evl_init_element(&thread->element, &evl_thread_factory);
+	if (ret)
+		goto fail_element;
+
+	ret = evl_create_element_device(&thread->element,
+					&evl_thread_factory,
+					thread->name);
+	if (ret)
+		goto fail_device;
+
+	p = kthread_run(kthread_trampoline, kthread, "%s", thread->name);
+	if (IS_ERR(p)) {
+		ret = PTR_ERR(p);
+		goto fail_spawn;
+	}
+
+	evl_index_element(&thread->element);
+	wait_for_completion(&kthread->done);
+	if (kthread->status)
+		return kthread->status;
+
+	evl_start_thread(thread);
+
+	return 0;
+
+fail_spawn:
+	evl_remove_element_device(&thread->element);
+fail_device:
+	evl_destroy_element(&thread->element);
+fail_element:
+	uninit_thread(thread);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(__evl_run_kthread);
+
+void evl_start_thread(struct evl_thread *thread)
+{
+	unsigned long flags;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	/*
+	 * A user-space thread starts immediately EVL-wise since we
+	 * already have an underlying in-band context for it, so we
+	 * can enlist it now.
+	 *
+	 * NOTE: starting an already started thread moves it to the
+	 * head of the runqueue if running, nop otherwise.
+	 */
+	if ((thread->state & (T_DORMANT|T_USER)) == (T_DORMANT|T_USER))
+		enlist_new_thread(thread);
+
+	trace_evl_thread_start(thread);
+	evl_resume_thread(thread, T_DORMANT);
+	evl_schedule();
+
+	xnlock_put_irqrestore(&nklock, flags);
+}
+EXPORT_SYMBOL_GPL(evl_start_thread);
+
+void evl_suspend_thread(struct evl_thread *thread, int mask,
+			ktime_t timeout, enum evl_tmode timeout_mode,
+			struct evl_clock *clock,
+			struct evl_syn *wchan)
+{
+	unsigned long oldstate;
+	struct evl_rq *rq;
+	unsigned long flags;
+
+	/*
+	 * Two things we can't do: suspend the root thread, ask for a
+	 * conjunctive wait on multiple syns.
+	 */
+	if (EVL_WARN_ON(CORE, (thread->state & T_ROOT) ||
+			(wchan && thread->wchan)))
+		return;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	trace_evl_suspend_thread(thread, mask, timeout,
+				 timeout_mode, clock, wchan);
+	rq = thread->rq;
+	oldstate = thread->state;
+
+	/*
+	 * If attempting to suspend a runnable thread which is pending
+	 * a forced switch to in-band context (T_KICKED), just raise
+	 * the T_BREAK status and return immediately, except if we are
+	 * precisely doing such switch by applying T_INBAND.
+	 *
+	 * In the latter case, we also make sure to clear T_KICKED,
+	 * since we won't go through prepare_for_signal() once
+	 * running in in-band context.
+	 */
+	if (likely((oldstate & EVL_THREAD_BLOCK_BITS) == 0)) {
+		if (likely((mask & T_INBAND) == 0)) {
+			if (thread->info & T_KICKED)
+				goto abort;
+		}
+		if (thread == rq->curr)
+			thread->info &= ~(T_RMID|T_TIMEO|T_BREAK|
+					  T_WAKEN|T_ROBBED|T_KICKED);
+	}
+
+	/*
+	 * Don't start the timer for a thread delayed
+	 * indefinitely. Zero relative delay currently means infinite
+	 * wait, all other combinations mean timed wait.
+	 */
+	if (timeout_mode != EVL_REL || !timeout_infinite(timeout)) {
+		evl_prepare_timer_wait(&thread->rtimer, clock,
+				       evl_thread_rq(thread));
+		if (timeout_mode == EVL_REL)
+			timeout = evl_abs_timeout(&thread->rtimer, timeout);
+		evl_start_timer(&thread->rtimer, timeout, EVL_INFINITE);
+		thread->state |= T_DELAY;
+	}
+
+	if (oldstate & T_READY) {
+		evl_dequeue_thread(thread);
+		thread->state &= ~T_READY;
+	}
+
+	thread->state |= mask;
+
+	/*
+	 * We must make sure that we don't clear the wait channel if a
+	 * thread is first blocked (wchan != NULL) then forcibly
+	 * suspended (wchan == NULL), since these are conjunctive
+	 * conditions.
+	 */
+	if (wchan)
+		thread->wchan = wchan;
+
+	/*
+	 * If the current thread is switching to in-band context, we
+	 * must have been called from evl_switch_inband(), in which
+	 * case we introduce an opportunity for interrupt delivery
+	 * right before switching context, which shortens the
+	 * uninterruptible code path.
+	 *
+	 * CAVEAT: dovetail_leave_head() must run _before_ the in-band
+	 * kernel is allowed to take interrupts again from the root
+	 * stage, so that try_to_wake_up() does not block the wake up
+	 * request for the switching thread, testing
+	 * task_is_off_stage().
+	 */
+	if (likely(thread == rq->curr)) {
+		evl_set_resched(rq);
+		if (unlikely(mask & T_INBAND)) {
+			dovetail_leave_oob();
+			xnlock_clear_irqon(&nklock);
+			oob_irq_disable();
+			__evl_schedule(rq);
+			oob_irq_enable();
+			dovetail_resume_inband();
+			return;
+		}
+		/*
+		 * If the thread is running on another CPU,
+		 * evl_schedule will trigger the IPI as required.
+		 */
+		__evl_schedule(rq);
+		goto out;
+	}
+
+	/*
+	 * Ok, this one is an interesting corner case, which requires
+	 * a bit of background first. Here, we handle the case of
+	 * suspending an in-band user thread which is _not_ current.
+	 *
+	 * The net effect is that we are attempting to stop the thread
+	 * for the EVL core, whilst it is actually running some code
+	 * under the control of the Linux scheduler.
+	 *
+	 *  To make this possible, we force the target task to switch
+	 * back to the OOB context by sending it a
+	 * SIGSHADOW_ACTION_HOME request.
+	 *
+	 * By forcing this switch, we make sure that EVL controls,
+	 * hence properly stops, the target thread according to the
+	 * requested suspension condition. Otherwise, the thread
+	 * currently running in-band would just keep doing so, thus
+	 * breaking the most common assumptions regarding suspended
+	 * threads.
+	 *
+	 * We only care for threads that are not current, and for
+	 * T_SUSP, T_DELAY, T_DORMANT and T_HALT conditions, because:
+	 *
+	 * - There is no point in dealing with in-band threads, since
+	 * any OOB request causes the caller to switch to OOB context
+	 * before it is handled.
+	 *
+	 * - among all blocking bits (EVL_THREAD_BLOCK_BITS), only
+	 * T_SUSP, T_DELAY and T_HALT may be applied by the current
+	 * thread to a non-current thread. T_PEND is always added by
+	 * the caller to its own state, T_INBAND has special semantics
+	 * escaping this issue.
+	 *
+	 * We don't signal threads which are already in a dormant
+	 * state, since they are suspended by definition.
+	 */
+	if (((oldstate & (EVL_THREAD_BLOCK_BITS|T_USER)) == (T_INBAND|T_USER)) &&
+	    (mask & (T_DELAY | T_SUSP | T_HALT)) != 0)
+		evl_signal_thread(thread, SIGSHADOW,
+				  SIGSHADOW_ACTION_HOME);
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+	return;
+
+abort:
+	if (wchan) {
+		thread->wchan = wchan;
+		evl_forget_syn_waiter(thread);
+	}
+	thread->info &= ~(T_RMID|T_TIMEO);
+	thread->info |= T_BREAK;
+	xnlock_put_irqrestore(&nklock, flags);
+}
+EXPORT_SYMBOL_GPL(evl_suspend_thread);
+
+struct lostage_wakeup {
+	struct task_struct *task;
+	struct irq_work work;
+};
+
+static void lostage_task_wakeup(struct irq_work *work)
+{
+	struct lostage_wakeup *rq;
+
+	rq = container_of(work, struct lostage_wakeup, work);
+	trace_evl_inband_wakeup(rq->task);
+	wake_up_process(rq->task);
+	evl_free_irq_work(rq);
+}
+
+static void post_wakeup(struct task_struct *p)
+{
+	struct lostage_wakeup *rq;
+
+	rq = evl_alloc_irq_work(sizeof(*rq));
+	init_irq_work(&rq->work, lostage_task_wakeup);
+	rq->task = p;
+	trace_evl_inband_request("wakeup", p);
+	irq_work_queue(&rq->work);
+}
+
+void evl_switch_inband(int cause)
+{
+	struct evl_thread *curr = evl_current_thread();
+	struct task_struct *p = current;
+	struct kernel_siginfo si;
+	int cpu __maybe_unused;
+
+	oob_context_only();
+
+	trace_evl_switching_inband(cause);
+
+	/*
+	 * Enqueue the request to move the running thread from the oob
+	 * stage to the in-band stage.  This will cause the in-band
+	 * task to resume using the same register file.
+	 *
+	 * If you intend to change the following interrupt-free
+	 * sequence, /first/ make sure to check the special handling
+	 * of T_INBAND in evl_suspend_thread() when switching out the
+	 * current thread, not to break basic assumptions we make
+	 * there.
+	 *
+	 * We disable interrupts during the stage transition, but
+	 * evl_suspend_thread() has an interrupts-on section built in.
+	 */
+	oob_irq_disable();
+	post_wakeup(p);
+
+	/*
+	 * This is the only location where we may assert T_INBAND for a
+	 * thread.
+	 */
+	evl_stop_thread(curr, T_INBAND);
+
+	/*
+	 * Basic sanity check after an expected transition to in-band
+	 * context.
+	 */
+	EVL_WARN(CORE, !running_inband(),
+		 "evl_switch_inband() failed for thread %s[%d]",
+		 curr->name, evl_get_inband_pid(curr));
+
+	/* Account for switch to in-band context. */
+	evl_inc_counter(&curr->stat.isw);
+
+	trace_evl_switched_inband(curr);
+
+	/*
+	 * When switching to in-band context, we check for propagating
+	 * the current EVL schedparams that might have been set for
+	 * current while running in OOB context.
+	 *
+	 * CAUTION: This obviously won't update the schedparams cached
+	 * by the glibc for the caller in user-space, but this is the
+	 * deal: we don't switch threads which issue
+	 * EVL_THRIOC_SET_SCHEDPARAM to in-band mode, but then only
+	 * the kernel side will be aware of the change, and glibc
+	 * might cache obsolete information.
+	 */
+	evl_propagate_schedparam_change(curr);
+
+#ifdef CONFIG_SMP
+	if (curr->local_info & T_MOVED) {
+		curr->local_info &= ~T_MOVED;
+		cpu = evl_rq_cpu(curr->rq);
+		set_cpus_allowed_ptr(p, cpumask_of(cpu));
+	}
+#endif
+
+	if ((curr->state & T_USER) && cause != SIGDEBUG_UNDEFINED) {
+		if (curr->state & T_WARN) {
+			/* Help debugging spurious mode switches. */
+			memset(&si, 0, sizeof(si));
+			si.si_signo = SIGDEBUG;
+			si.si_code = SI_QUEUE;
+			si.si_int = cause | sigdebug_marker;
+			send_sig_info(SIGDEBUG, &si, p);
+		}
+		evl_detect_boost_drop(curr);
+	}
+
+	/* @current is now running inband. */
+	evl_sync_uwindow(curr);
+}
+EXPORT_SYMBOL_GPL(evl_switch_inband);
+
+int evl_switch_oob(void)
+{
+	struct task_struct *p = current;
+	struct evl_thread *curr;
+	struct evl_rq *rq;
+	int ret;
+
+	inband_context_only();
+
+	curr = evl_current_thread();
+	if (curr == NULL)
+		return -EPERM;
+
+	if (signal_pending(p))
+		return -ERESTARTSYS;
+
+	trace_evl_switching_oob(curr);
+
+	evl_clear_sync_uwindow(curr, T_INBAND);
+
+	ret = dovetail_leave_inband();
+	if (ret) {
+		evl_test_cancel();
+		evl_set_sync_uwindow(curr, T_INBAND);
+		return ret;
+	}
+
+	/*
+	 * current is now running on the head interrupt stage. Hard
+	 * irqs must be off, otherwise something is really wrong in
+	 * the Dovetail layer.
+	 */
+	if (EVL_WARN_ON_ONCE(CORE, !hard_irqs_disabled()))
+		hard_irqs_disabled();
+
+	rq = this_evl_rq();
+
+	xnlock_clear_irqon(&nklock);
+	evl_test_cancel();
+
+	trace_evl_switched_oob(curr);
+
+	/*
+	 * Recheck pending signals once again. As we block task
+	 * wakeups during the stage transition and handle_sigwake_event()
+	 * ignores signals until T_INBAND is cleared, any signal in
+	 * between is just silently queued up to here.
+	 */
+	if (signal_pending(p)) {
+		evl_switch_inband(!(curr->state & T_SSTEP) ?
+				  SIGDEBUG_MIGRATE_SIGNAL:
+				  SIGDEBUG_UNDEFINED);
+		return -ERESTARTSYS;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(evl_switch_oob);
+
+void evl_stop_thread(struct evl_thread *thread, int mask)
+{
+	evl_suspend_thread(thread, mask, EVL_INFINITE,
+			   EVL_REL, NULL, NULL);
+}
+EXPORT_SYMBOL_GPL(evl_stop_thread);
+
+void evl_set_kthread_priority(struct evl_kthread *kthread, int priority)
+{
+	union evl_sched_param param = { .rt = { .prio = priority } };
+	evl_set_thread_schedparam(&kthread->thread, &evl_sched_rt, &param);
+	evl_schedule();
+}
+EXPORT_SYMBOL_GPL(evl_set_kthread_priority);
+
+int evl_unblock_kthread(struct evl_kthread *kthread)
+{
+	int ret = evl_unblock_thread(&kthread->thread);
+	evl_schedule();
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_unblock_kthread);
+
+ktime_t evl_get_thread_timeout(struct evl_thread *thread)
+{
+	struct evl_timer *timer;
+	ktime_t timeout, now;
+
+	if (!(thread->state & T_DELAY))
+		return 0LL;
+
+	if (evl_timer_is_running(&thread->rtimer))
+		timer = &thread->rtimer;
+	else if (evl_timer_is_running(&thread->ptimer))
+		timer = &thread->ptimer;
+	else
+		return 0;
+
+	now = evl_ktime_monotonic();
+	timeout = evl_get_timer_date(timer);
+	if (timeout <= now)
+		return ktime_set(0, 1);
+
+	return ktime_sub(timeout, now);
+}
+EXPORT_SYMBOL_GPL(evl_get_thread_timeout);
+
+ktime_t evl_get_thread_period(struct evl_thread *thread)
+{
+	ktime_t period = 0;
+	/*
+	 * The current thread period might be:
+	 * - the value of the timer interval for periodic threads (ns/ticks)
+	 * - or, the value of the alloted round-robin quantum (ticks)
+	 * - or zero, meaning "no periodic activity".
+	 */
+	if (evl_timer_is_running(&thread->ptimer))
+		period = thread->ptimer.interval;
+	else if (thread->state & T_RRB)
+		period = thread->rrperiod;
+
+	return period;
+}
+EXPORT_SYMBOL_GPL(evl_get_thread_period);
+
+ktime_t evl_delay_thread(ktime_t timeout, enum evl_tmode timeout_mode,
+			 struct evl_clock *clock)
+{
+	struct evl_thread *curr = evl_current_thread();
+	unsigned long flags;
+	ktime_t rem = 0;
+
+	evl_suspend_thread(curr, T_DELAY, timeout,
+			   timeout_mode, clock, NULL);
+
+	if (curr->info & T_BREAK) {
+		xnlock_get_irqsave(&nklock, flags);
+		rem = __evl_get_stopped_timer_delta(&curr->rtimer);
+		xnlock_put_irqrestore(&nklock, flags);
+	}
+
+	return rem;
+}
+EXPORT_SYMBOL_GPL(evl_delay_thread);
+
+void evl_resume_thread(struct evl_thread *thread, int mask)
+{
+	unsigned long oldstate;
+	struct evl_rq *rq;
+	unsigned long flags;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	trace_evl_resume_thread(thread, mask);
+
+	rq = thread->rq;
+	oldstate = thread->state;
+
+	if ((oldstate & EVL_THREAD_BLOCK_BITS) == 0) {
+		if (oldstate & T_READY)
+			evl_dequeue_thread(thread);
+		goto enqueue;
+	}
+
+	/* Clear the specified block bit(s) */
+	thread->state &= ~mask;
+
+	/*
+	 * If T_DELAY was set in the clear mask, evl_unblock_thread()
+	 * was called for the thread, or a timeout has elapsed. In the
+	 * latter case, stopping the timer is a no-op.
+	 */
+	if (mask & T_DELAY)
+		evl_stop_timer(&thread->rtimer);
+
+	if (!(thread->state & EVL_THREAD_BLOCK_BITS))
+		goto clear_wchan;
+
+	if (mask & T_DELAY) {
+		mask = thread->state & T_PEND;
+		if (mask == 0)
+			goto unlock_and_exit;
+		if (thread->wchan)
+			evl_forget_syn_waiter(thread);
+		goto recheck_state;
+	}
+
+	if (thread->state & T_DELAY) {
+		if (mask & T_PEND) {
+			/*
+			 * A resource became available to the thread.
+			 * Cancel the watchdog timer.
+			 */
+			evl_stop_timer(&thread->rtimer);
+			thread->state &= ~T_DELAY;
+		}
+		goto recheck_state;
+	}
+
+	/*
+	 * The thread is still suspended, but is no more pending on a
+	 * resource.
+	 */
+	if ((mask & T_PEND) != 0 && thread->wchan)
+		evl_forget_syn_waiter(thread);
+
+	goto unlock_and_exit;
+
+recheck_state:
+	if (thread->state & EVL_THREAD_BLOCK_BITS)
+		goto unlock_and_exit;
+
+clear_wchan:
+	/*
+	 * If the thread was actually suspended, clear the wait
+	 * channel. This allows requests like
+	 * evl_suspend_thread(thread, T_DELAY,...) to skip the
+	 * following code when the suspended thread is woken up while
+	 * undergoing a simple delay.
+	 */
+	if ((mask & ~T_DELAY) != 0 && thread->wchan != NULL)
+		evl_forget_syn_waiter(thread);
+
+	if (unlikely((oldstate & mask) & T_HALT)) {
+		evl_requeue_thread(thread);
+		goto ready;
+	}
+enqueue:
+	evl_enqueue_thread(thread);
+ready:
+	thread->state |= T_READY;
+	evl_set_resched(rq);
+unlock_and_exit:
+	xnlock_put_irqrestore(&nklock, flags);
+}
+EXPORT_SYMBOL_GPL(evl_resume_thread);
+
+int evl_unblock_thread(struct evl_thread *thread)
+{
+	unsigned long flags;
+	int ret = 1;
+
+	/*
+	 * Attempt to abort an undergoing wait for the given thread.
+	 * If this state is due to an alarm that has been armed to
+	 * limit the sleeping thread's waiting time while it pends for
+	 * a resource, the corresponding T_PEND state will be cleared
+	 * by evl_resume_thread() in the same move. Otherwise, this call
+	 * may abort an undergoing infinite wait for a resource (if
+	 * any).
+	 */
+	xnlock_get_irqsave(&nklock, flags);
+
+	trace_evl_unblock_thread(thread);
+
+	if (thread->state & T_DELAY)
+		evl_resume_thread(thread, T_DELAY);
+	else if (thread->state & T_PEND)
+		evl_resume_thread(thread, T_PEND);
+	else
+		ret = 0;
+
+	/*
+	 * We should not clear a previous break state if this service
+	 * is called more than once before the target thread actually
+	 * resumes, so we only set the bit here and never clear
+	 * it. However, we must not raise the T_BREAK bit if the
+	 * target thread was already awake at the time of this call,
+	 * so that downstream code does not get confused by some
+	 * "successful but interrupted syscall" condition. IOW, a
+	 * break state raised here must always trigger an error code
+	 * downstream, and an already successful syscall cannot be
+	 * marked as interrupted.
+	 */
+	if (ret)
+		thread->info |= T_BREAK;
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_unblock_thread);
+
+int evl_sleep_until(ktime_t timeout)
+{
+	ktime_t rem;
+
+	if (!EVL_ASSERT(CORE, !evl_cannot_block()))
+		return -EPERM;
+
+	rem = evl_delay_thread(timeout, EVL_ABS, &evl_mono_clock);
+
+	return rem ? -EINTR : 0;
+}
+EXPORT_SYMBOL_GPL(evl_sleep_until);
+
+int evl_sleep(ktime_t delay)
+{
+	ktime_t end = ktime_add(evl_read_clock(&evl_mono_clock), delay);
+	return evl_sleep_until(end);
+}
+EXPORT_SYMBOL_GPL(evl_sleep);
+
+int evl_set_thread_period(struct evl_clock *clock,
+			  ktime_t idate, ktime_t period)
+{
+	struct evl_thread *curr = evl_current_thread();
+	unsigned long flags;
+	int ret = 0;
+
+	if (curr == NULL)
+		return -EPERM;
+
+	if (clock == NULL || period == EVL_INFINITE) {
+		evl_stop_timer(&curr->ptimer);
+		return 0;
+	}
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	/*
+	 * LART: detect periods which are shorter than the target
+	 * clock gravity for kernel thread timers. This can't work,
+	 * caller must have messed up arguments.
+	 */
+	if (period < evl_get_clock_gravity(clock, kernel)) {
+		ret = -EINVAL;
+		goto unlock_and_exit;
+	}
+
+	evl_prepare_timer_wait(&curr->ptimer, clock,
+			       evl_thread_rq(curr));
+
+	if (timeout_infinite(idate))
+		idate = evl_abs_timeout(&curr->ptimer, period);
+
+	evl_start_timer(&curr->ptimer, idate, period);
+
+unlock_and_exit:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_set_thread_period);
+
+int evl_wait_thread_period(unsigned long *overruns_r)
+{
+	unsigned long overruns = 0, flags;
+	struct evl_thread *curr;
+	struct evl_clock *clock;
+	ktime_t now;
+	int ret = 0;
+
+	if (!EVL_ASSERT(CORE, !evl_cannot_block()))
+		return -EPERM;
+
+	curr = evl_current_thread();
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (unlikely(!evl_timer_is_running(&curr->ptimer))) {
+		ret = -EWOULDBLOCK;
+		goto out;
+	}
+
+	trace_evl_thread_wait_period(curr);
+
+	clock = curr->ptimer.clock;
+	now = evl_read_clock(clock);
+	if (likely(now < evl_get_timer_next_date(&curr->ptimer))) {
+		evl_stop_thread(curr, T_DELAY);
+		if (unlikely(curr->info & T_BREAK)) {
+			ret = -EINTR;
+			goto out;
+		}
+	}
+
+	overruns = evl_get_timer_overruns(&curr->ptimer);
+	if (overruns) {
+		ret = -ETIMEDOUT;
+		trace_evl_thread_missed_period(curr);
+	}
+
+	if (likely(overruns_r != NULL))
+		*overruns_r = overruns;
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_wait_thread_period);
+
+void evl_cancel_thread(struct evl_thread *thread)
+{
+	unsigned long flags;
+
+	if (EVL_WARN_ON(CORE, thread->state & T_ROOT))
+		return;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (thread->info & T_CANCELD)
+		goto check_self_cancel;
+
+	trace_evl_thread_cancel(thread);
+
+	thread->info |= T_CANCELD;
+
+	/*
+	 * If @thread is not started yet, fake a start request,
+	 * raising the kicked condition bit to make sure it will reach
+	 * evl_test_cancel() on its wakeup path.
+	 *
+	 * NOTE: if T_DORMANT and !T_INBAND, then some non-mapped
+	 * emerging thread is self-cancelling due to an early error in
+	 * the prep work.
+	 */
+	if (thread->state & T_DORMANT) {
+		if (!(thread->state & T_INBAND))
+			goto check_self_cancel;
+		thread->info |= T_KICKED;
+		evl_resume_thread(thread, T_DORMANT);
+		goto out;
+	}
+
+check_self_cancel:
+	if (evl_current_thread() == thread) {
+		xnlock_put_irqrestore(&nklock, flags);
+		evl_test_cancel();
+		/*
+		 * May return if on behalf of an IRQ handler which has
+		 * preempted @thread.
+		 */
+		return;
+	}
+
+	/*
+	 * Force the non-current thread to exit:
+	 *
+	 * - unblock a user thread, switch it to weak scheduling,
+	 * then send it SIGTERM.
+	 *
+	 * - just unblock a kernel thread, it is expected to reach a
+	 * cancellation point soon after
+	 * (i.e. evl_test_cancel()).
+	 */
+	if (thread->state & T_USER) {
+		__evl_demote_thread(thread);
+		evl_signal_thread(thread, SIGTERM, 0);
+	} else
+		__evl_kick_thread(thread);
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	evl_schedule();
+}
+EXPORT_SYMBOL_GPL(evl_cancel_thread);
+
+int evl_detach_self(void)
+{
+	if (evl_current_thread() == NULL)
+		return -EPERM;
+
+	put_current_thread();
+
+	return 0;
+}
+
+struct wait_grace_struct {
+	struct completion done;
+	struct rcu_head rcu;
+};
+
+static void grace_elapsed(struct rcu_head *head)
+{
+	struct wait_grace_struct *wgs;
+
+	wgs = container_of(head, struct wait_grace_struct, rcu);
+	complete(&wgs->done);
+}
+
+static void wait_for_rcu_grace_period(struct pid *pid)
+{
+	struct wait_grace_struct wait = {
+		.done = COMPLETION_INITIALIZER_ONSTACK(wait.done),
+	};
+	struct task_struct *p;
+
+	init_rcu_head_on_stack(&wait.rcu);
+
+	for (;;) {
+		call_rcu(&wait.rcu, grace_elapsed);
+		wait_for_completion(&wait.done);
+		if (pid == NULL)
+			break;
+		rcu_read_lock(); /* pid_task() is RCU-protected. */
+		p = pid_task(pid, PIDTYPE_PID);
+		rcu_read_unlock();
+		if (p == NULL)
+			break;
+		reinit_completion(&wait.done);
+	}
+}
+
+int evl_join_thread(struct evl_thread *thread, bool uninterruptible)
+{
+	struct evl_thread *curr = evl_current_thread();
+	bool switched = false;
+	unsigned long flags;
+	struct pid *pid;
+	int ret = 0;
+	pid_t tpid;
+
+	if (EVL_WARN_ON(CORE, thread->state & T_ROOT))
+		return -EINVAL;
+
+	if (thread == curr)
+		return -EDEADLK;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	/*
+	 * We allow multiple callers to join @thread, this is purely a
+	 * synchronization mechanism with no resource collection.
+	 */
+
+	if (thread->info & T_DORMANT)
+		goto out;
+
+	trace_evl_thread_join(thread);
+
+	tpid = evl_get_inband_pid(thread);
+
+	if (curr && !(curr->state & T_INBAND)) {
+		xnlock_put_irqrestore(&nklock, flags);
+		evl_switch_inband(SIGDEBUG_UNDEFINED);
+		switched = true;
+	} else
+		xnlock_put_irqrestore(&nklock, flags);
+
+	/*
+	 * Since in theory, we might be sleeping there for a long
+	 * time, we get a reference on the pid struct holding our
+	 * target, then we check for its existence upon wake up.
+	 */
+	pid = find_get_pid(tpid);
+	if (pid == NULL)
+		goto done;
+
+	/*
+	 * We have a tricky issue to deal with, which involves code
+	 * relying on the assumption that a destroyed thread will have
+	 * scheduled away from do_exit() before evl_join_thread()
+	 * returns. A typical example is illustrated by the following
+	 * sequence, with a EVL kthread implemented in a dynamically
+	 * loaded module:
+	 *
+	 * CPU0:  evl_cancel_kthread(kthread)
+	 *           evl_cancel_thread(kthread)
+	 *           evl_join_thread(kthread)
+	 *        ...<back to user>..
+	 *        rmmod(module)
+	 *
+	 * CPU1:  in kthread()
+	 *        ...
+	 *        ...
+	 *          __evl_test_cancel()
+	 *             do_exit()
+         *                schedule()
+	 *
+	 * In such a sequence, the code on CPU0 would expect the EVL
+	 * kthread to have scheduled away upon return from
+	 * evl_cancel_kthread(), so that unmapping the cancelled
+	 * kthread code and data memory when unloading the module is
+	 * always safe.
+	 *
+	 * To address this, the joiner first waits for the joinee to
+	 * signal completion from the EVL thread cleanup handler
+	 * (cleanup_current_thread), then waits for a full RCU grace
+	 * period to have elapsed. Since the completion signal is sent
+	 * on behalf of do_exit(), we may assume that the joinee has
+	 * scheduled away before the RCU grace period ends.
+	 */
+	if (uninterruptible)
+		wait_for_completion(&thread->exited);
+	else {
+		ret = wait_for_completion_interruptible(&thread->exited);
+		if (ret < 0) {
+			put_pid(pid);
+			return -EINTR;
+		}
+	}
+
+	/* Make sure the joinee has scheduled away ultimately. */
+	wait_for_rcu_grace_period(pid);
+
+	put_pid(pid);
+done:
+	ret = 0;
+	if (switched)
+		ret = evl_switch_oob();
+
+	return ret;
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_join_thread);
+
+#ifdef CONFIG_SMP
+
+void evl_migrate_thread(struct evl_thread *thread, struct evl_rq *rq)
+{				/* nklocked, IRQs off */
+	if (thread->rq == rq)
+		return;
+
+	trace_evl_thread_migrate(thread, evl_rq_cpu(rq));
+	/*
+	 * Timer migration is postponed until the next timeout happens
+	 * for the periodic and rrb timers. The resource timer will be
+	 * moved to the right CPU next time it is armed in
+	 * evl_suspend_thread().
+	 */
+	evl_migrate_rq(thread, rq);
+
+	evl_reset_account(&thread->stat.lastperiod);
+}
+
+#endif	/* CONFIG_SMP */
+
+int evl_set_thread_schedparam(struct evl_thread *thread,
+			      struct evl_sched_class *sched_class,
+			      const union evl_sched_param *sched_param)
+{
+	unsigned long flags;
+	int ret;
+
+	xnlock_get_irqsave(&nklock, flags);
+	ret = __evl_set_thread_schedparam(thread, sched_class, sched_param);
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_set_thread_schedparam);
+
+int __evl_set_thread_schedparam(struct evl_thread *thread,
+				struct evl_sched_class *sched_class,
+				const union evl_sched_param *sched_param)
+{
+	int old_wprio, new_wprio, ret;
+
+	old_wprio = thread->wprio;
+
+	ret = evl_set_thread_policy(thread, sched_class, sched_param);
+	if (ret)
+		return ret;
+
+	new_wprio = thread->wprio;
+
+	/*
+	 * If the thread is waiting on a synchronization object,
+	 * update its position in the corresponding wait queue, unless
+	 * the (weighted) priority has not changed (to prevent
+	 * spurious round-robin effects).
+	 */
+	if (old_wprio != new_wprio && thread->wchan &&
+	    (thread->wchan->status & EVL_SYN_PRIO))
+		evl_requeue_syn_waiter(thread);
+	/*
+	 * We should not move the thread at the end of its priority
+	 * group, if any of these conditions is true:
+	 *
+	 * - thread is not runnable;
+	 * - thread bears the ready bit which means that evl_set_thread_policy()
+	 * already reordered the run queue;
+	 * - thread currently holds the scheduler lock, so we don't want
+	 * any round-robin effect to take place;
+	 * - a priority boost is undergoing for this thread.
+	 */
+	if (!(thread->state & (EVL_THREAD_BLOCK_BITS|T_READY|T_BOOST)) &&
+	    thread->lock_count == 0)
+		evl_putback_thread(thread);
+
+	thread->info |= T_SCHEDP;
+	/* Ask the target thread to call back if in-band. */
+	if (thread->state & T_INBAND)
+		evl_signal_thread(thread, SIGSHADOW,
+				  SIGSHADOW_ACTION_HOME);
+
+	return ret;
+}
+
+void __evl_test_cancel(struct evl_thread *curr)
+{
+	/*
+	 * Just in case evl_test_cancel() is called from an IRQ
+	 * handler, in which case we may not take the exit path.
+	 *
+	 * NOTE: curr->rq is stable from our POV and can't change
+	 * under our feet.
+	 */
+	if (curr->rq->lflags & RQ_IRQ)
+		return;
+
+	if (!(curr->state & T_INBAND))
+		evl_switch_inband(SIGDEBUG_UNDEFINED);
+
+	do_exit(0);
+	/* ... won't return ... */
+	EVL_WARN_ON(EVENLESS, 1);
+}
+EXPORT_SYMBOL_GPL(__evl_test_cancel);
+
+void __evl_propagate_schedparam_change(struct evl_thread *curr)
+{
+	int kpolicy = SCHED_FIFO, kprio = curr->bprio, ret;
+	struct task_struct *p = current;
+	struct sched_param param;
+	unsigned long flags;
+
+	/*
+	 * Test-set race for T_SCHEDP is ok, the propagation is meant
+	 * to be done asap but not guaranteed to be carried out
+	 * immediately, and the request will remain pending until it
+	 * is eventually handled. We just have to protect against a
+	 * set-clear race.
+	 */
+	xnlock_get_irqsave(&nklock, flags);
+	curr->info &= ~T_SCHEDP;
+	xnlock_put_irqrestore(&nklock, flags);
+
+	/*
+	 * Map our policies/priorities to the regular kernel's
+	 * (approximated).
+	 */
+	if ((curr->state & T_WEAK) && kprio == 0)
+		kpolicy = SCHED_NORMAL;
+	else if (kprio >= MAX_USER_RT_PRIO)
+		kprio = MAX_USER_RT_PRIO - 1;
+
+	if (p->policy != kpolicy || (kprio > 0 && p->rt_priority != kprio)) {
+		param.sched_priority = kprio;
+		ret = sched_setscheduler_nocheck(p, kpolicy, &param);
+		EVL_WARN_ON(CORE, ret != 0);
+	}
+}
+
+struct lostage_signal {
+	struct task_struct *task;
+	int signo, sigval;
+	struct irq_work work;
+};
+
+static inline void do_kthread_signal(struct task_struct *p,
+				     struct evl_thread *thread,
+				     struct lostage_signal *rq)
+{
+	printk(EVL_WARNING
+	       "kthread %s received unhandled signal %d (action=0x%x)\n",
+	       thread->name, rq->signo, rq->sigval);
+}
+
+static void lostage_task_signal(struct irq_work *work)
+{
+	struct evl_thread *thread;
+	struct lostage_signal *rq;
+	struct kernel_siginfo si;
+	struct task_struct *p;
+	int signo;
+
+	rq = container_of(work, struct lostage_signal, work);
+	p = rq->task;
+	thread = evl_thread_from_task(p);
+	if (thread && !(thread->state & T_USER))
+		do_kthread_signal(p, thread, rq);
+	else {
+		signo = rq->signo;
+		trace_evl_inband_signal(p, signo);
+		if (signo == SIGSHADOW || signo == SIGDEBUG) {
+			memset(&si, '\0', sizeof(si));
+			si.si_signo = signo;
+			si.si_code = SI_QUEUE;
+			si.si_int = rq->sigval;
+			send_sig_info(signo, &si, p);
+		} else
+			send_sig(signo, p, 1);
+	}
+
+	evl_free_irq_work(rq);
+}
+
+static int force_wakeup(struct evl_thread *thread) /* nklock locked, irqs off */
+{
+	int ret = 0;
+
+	if (thread->info & T_KICKED)
+		return 1;
+
+	if (evl_unblock_thread(thread)) {
+		thread->info |= T_KICKED;
+		ret = 1;
+	}
+
+	/*
+	 * CAUTION: we must NOT raise T_BREAK when clearing a forcible
+	 * block state, such as T_SUSP, T_HALT. The caller of
+	 * evl_suspend_thread() we unblock shall proceed as for a
+	 * normal return, until it traverses a cancellation point if
+	 * T_CANCELD was raised earlier, or calls
+	 * evl_suspend_thread() which will detect T_KICKED and act
+	 * accordingly.
+	 *
+	 * Rationale: callers of evl_suspend_thread() may assume
+	 * that receiving T_BREAK means that the process that
+	 * motivated the blocking did not go to completion. E.g. the
+	 * wait context was NOT updated before evl_sleep_on_syn()
+	 * returned, leaving no useful data there.  Therefore, in case
+	 * only T_SUSP remains set for the thread on entry to
+	 * force_wakeup(), after T_PEND was lifted earlier when the
+	 * wait went to successful completion (i.e. no timeout), then
+	 * we want the kicked thread to know that it did receive the
+	 * requested resource, not finding T_BREAK in its state word.
+	 *
+	 * Callers of evl_suspend_thread() may inquire for T_KICKED
+	 * to detect forcible unblocks from T_SUSP, T_HALT, if they
+	 * should act upon this case specifically.
+	 */
+	if (thread->state & (T_SUSP|T_HALT)) {
+		evl_resume_thread(thread, T_SUSP|T_HALT);
+		thread->info |= T_KICKED;
+	}
+
+	/*
+	 * Tricky cases:
+	 *
+	 * - a thread which was ready on entry wasn't actually
+	 * running, but nevertheless waits for the CPU in OOB context,
+	 * so we have to make sure that it will be notified of the
+	 * pending break condition as soon as it enters
+	 * evl_suspend_thread() from a blocking EVL syscall.
+	 *
+	 * - a ready/readied thread on exit may be prevented from
+	 * running by the scheduling policy module it belongs
+	 * to. Typically, policies enforcing a runtime budget do not
+	 * block threads with no budget, but rather keep them out of
+	 * their run queue, so that ->sched_pick() won't elect
+	 * them. We tell the policy handler about the fact that we do
+	 * want such thread to run until it switches to in-band
+	 * context, whatever this entails internally for the
+	 * implementation.
+	 */
+	if (thread->state & T_READY)
+		evl_force_thread(thread);
+
+	return ret;
+}
+
+void __evl_kick_thread(struct evl_thread *thread) /* nklock locked, irqs off */
+{
+	struct task_struct *p = thread->altsched.task;
+
+	if (thread->state & T_INBAND) /* nop? */
+		return;
+
+	/*
+	 * First, try to kick the thread out of any blocking syscall
+	 * EVL-wise. If that succeeds, then the thread will switch to
+	 * in-band context on its return path to user-space.
+	 */
+	if (force_wakeup(thread))
+		return;
+
+	/*
+	 * If that did not work out because the thread was not blocked
+	 * (i.e. T_PEND/T_DELAY) in a syscall, then force a mayday
+	 * trap. Note that we don't want to send that thread any linux
+	 * signal, we only want to force it to switch to in-band
+	 * context asap.
+	 */
+	thread->info |= T_KICKED;
+
+	/*
+	 * We may send mayday signals to userland threads only.
+	 * However, no need to run a mayday trap if the current thread
+	 * kicks itself out of OOB context: it will switch to in-band
+	 * context on its way back to userland via the current syscall
+	 * epilogue. Otherwise, we want that thread to enter the
+	 * mayday trap asap.
+	 */
+	if (thread != this_evl_rq_thread() &&
+	    (thread->state & T_USER))
+		dovetail_send_mayday(p);
+}
+
+void evl_kick_thread(struct evl_thread *thread)
+{
+	unsigned long flags;
+
+	xnlock_get_irqsave(&nklock, flags);
+	__evl_kick_thread(thread);
+	xnlock_put_irqrestore(&nklock, flags);
+}
+EXPORT_SYMBOL_GPL(evl_kick_thread);
+
+void __evl_demote_thread(struct evl_thread *thread) /* nklock locked, irqs off */
+{
+	struct evl_sched_class *sched_class;
+	union evl_sched_param param;
+
+	/*
+	 * First we kick the thread out of oob context, and have it
+	 * resume execution immediately on the in-band stage.
+	 */
+	__evl_kick_thread(thread);
+
+	/*
+	 * Then we demote it, turning that thread into a non real-time
+	 * EVL thread, which still has access to EVL resources, but
+	 * won't compete for real-time scheduling anymore. In effect,
+	 * moving the thread to a weak scheduling class/priority will
+	 * prevent it from sticking back to OOB context.
+	 */
+	param.weak.prio = 0;
+	sched_class = &evl_sched_weak;
+	__evl_set_thread_schedparam(thread, sched_class, &param);
+}
+
+void evl_demote_thread(struct evl_thread *thread)
+{
+	unsigned long flags;
+
+	xnlock_get_irqsave(&nklock, flags);
+	__evl_demote_thread(thread);
+	xnlock_put_irqrestore(&nklock, flags);
+}
+EXPORT_SYMBOL_GPL(evl_demote_thread);
+
+void evl_signal_thread(struct evl_thread *thread, int sig, int arg)
+{
+	struct lostage_signal *rq;
+
+	rq = evl_alloc_irq_work(sizeof(*rq));
+	init_irq_work(&rq->work, lostage_task_signal);
+	rq->task = thread->altsched.task;
+	rq->signo = sig;
+	rq->sigval = sig == SIGDEBUG ? arg | sigdebug_marker : arg;
+	trace_evl_inband_request("signal", rq->task);
+	irq_work_queue(&rq->work);
+}
+EXPORT_SYMBOL_GPL(evl_signal_thread);
+
+#ifdef CONFIG_MMU
+
+static inline int commit_process_memory(void)
+{
+	struct task_struct *p = current;
+
+	if (!(p->mm->def_flags & VM_LOCKED))
+		return -EINVAL;
+
+	return force_commit_memory();
+}
+
+#else /* !CONFIG_MMU */
+
+static inline int commit_process_memory(void)
+{
+	return 0;
+}
+
+#endif /* !CONFIG_MMU */
+
+/* nklock locked, irqs off */
+void evl_call_mayday(struct evl_thread *thread, int reason)
+{
+	struct task_struct *p = thread->altsched.task;
+
+	/* Mayday traps are available to userland threads only. */
+	if (EVL_WARN_ON(CORE, !(thread->state & T_USER)))
+		return;
+
+	thread->info |= T_KICKED;
+	evl_signal_thread(thread, SIGDEBUG, reason);
+	dovetail_send_mayday(p);
+}
+EXPORT_SYMBOL_GPL(evl_call_mayday);
+
+int evl_killall(int mask)
+{
+	int nrkilled = 0, nrthreads, count;
+	struct evl_thread *t;
+	unsigned long flags;
+	long ret;
+
+	inband_context_only();
+
+	if (evl_current_thread())
+		return -EPERM;
+
+	/*
+	 * We may hold the core lock across calls to evl_cancel_thread()
+	 * provided that we won't self-cancel.
+	 */
+	xnlock_get_irqsave(&nklock, flags);
+
+	nrthreads = evl_nrthreads;
+
+	for_each_evl_thread(t) {
+		if ((t->state & T_ROOT) || (t->state & mask) != mask)
+			continue;
+
+		if (EVL_DEBUG(CORE))
+			printk(EVL_INFO "terminating %s[%d]\n",
+			       t->name, evl_get_inband_pid(t));
+		nrkilled++;
+		evl_cancel_thread(t);
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	count = nrthreads - nrkilled;
+	if (EVL_DEBUG(CORE))
+		printk(EVL_INFO "waiting for %d threads to exit\n",
+		       nrkilled);
+
+	ret = wait_event_interruptible(join_all,
+				       evl_nrthreads == count);
+
+	/* Wait for a full RCU grace period to expire. */
+	wait_for_rcu_grace_period(NULL);
+
+	if (EVL_DEBUG(CORE))
+		printk(EVL_INFO "joined %d threads\n",
+		       count + nrkilled - evl_nrthreads);
+
+	return ret < 0 ? -EINTR : 0;
+}
+EXPORT_SYMBOL_GPL(evl_killall);
+
+pid_t evl_get_inband_pid(struct evl_thread *thread)
+{
+	if (thread->state & (T_ROOT|T_DORMANT|T_ZOMBIE))
+		return 0;
+
+	if (thread->altsched.task == NULL)
+		return -1;	/* weird */
+
+	return task_pid_nr(thread->altsched.task);
+}
+
+void arch_inband_task_init(struct task_struct *tsk)
+{
+	struct oob_thread_state *p = dovetail_task_state(tsk);
+
+	evl_init_thread_state(p);
+}
+
+void handle_oob_trap(unsigned int trapnr, struct pt_regs *regs)
+{
+	struct evl_thread *curr;
+
+	oob_context_only();
+
+	curr = evl_current_thread();
+	trace_evl_thread_fault(trapnr, regs);
+
+#if defined(CONFIG_EVENLESS_DEBUG_CORE) || defined(CONFIG_EVENLESS_DEBUG_USER)
+	if (xnarch_fault_notify(trapnr))
+		printk(EVL_WARNING
+		       "%s switching in-band [pid=%d, excpt=%#x, %spc=%#lx]\n",
+		       curr->name,
+		       evl_get_inband_pid(curr),
+		       trapnr,
+		       user_mode(regs) ? "" : "kernel_",
+		       instruction_pointer(regs));
+#endif
+	if (xnarch_fault_pf_p(trapnr))
+		/*
+		 * The page fault counter is not SMP-safe, but it's a
+		 * simple indicator that something went wrong wrt
+		 * memory locking anyway.
+		 */
+		evl_inc_counter(&curr->stat.pf);
+
+	/*
+	 * We received a trap on the oob stage, switch to in-band
+	 * before handling the exception.
+	 */
+	evl_switch_inband(xnarch_fault_notify(trapnr) ?
+			  SIGDEBUG_MIGRATE_FAULT :
+			  SIGDEBUG_UNDEFINED);
+}
+
+void handle_oob_mayday(struct pt_regs *regs)
+{
+	struct evl_thread *curr = evl_current_thread();
+
+	if (EVL_WARN_ON(CORE, !(curr->state & T_USER)))
+		return;
+
+	/*
+	 * It might happen that a thread gets a mayday trap right
+	 * after it switched to in-band mode while returning from a
+	 * syscall. Filter this case out.
+	 */
+	if (!(curr->state & T_INBAND))
+		evl_switch_inband(SIGDEBUG_UNDEFINED);
+}
+
+#ifdef CONFIG_SMP
+
+static void handle_migration_event(struct dovetail_migration_data *d)
+{
+	struct task_struct *p = d->task;
+	struct evl_thread *thread;
+
+	thread = evl_thread_from_task(p);
+	if (thread == NULL)
+		return;
+
+	/*
+	 * Detect an EVL thread sleeping in OOB context which is
+	 * required to migrate to another CPU by the in-band kernel.
+	 *
+	 * We may NOT fix up thread->sched immediately using the
+	 * migration call, because the latter always has to take place
+	 * on behalf of the target thread itself while running
+	 * in-band. Therefore, that thread needs to switch to in-band
+	 * context first, then move back to OOB, so that affinity_ok()
+	 * does the fixup work.
+	 *
+	 * We force this by sending a SIGSHADOW signal to the migrated
+	 * thread, asking it to switch back to OOB context from the
+	 * handler, at which point the interrupted syscall may be
+	 * restarted.
+	 */
+	if (thread->state & (EVL_THREAD_BLOCK_BITS & ~T_INBAND))
+		evl_signal_thread(thread, SIGSHADOW,
+				  SIGSHADOW_ACTION_HOME);
+}
+
+static inline bool affinity_ok(struct task_struct *p) /* nklocked, IRQs off */
+{
+	struct evl_thread *thread = evl_thread_from_task(p);
+	struct evl_rq *rq;
+	int cpu = task_cpu(p);
+
+	/*
+	 * To maintain consistency between both the EVL and in-band
+	 * schedulers, reflecting a thread migration to another CPU
+	 * into EVL's scheduler state must happen from in-band context
+	 * only, on behalf of the migrated thread itself once it runs
+	 * on the target CPU.
+	 *
+	 * This means that the EVL scheduler state regarding the CPU
+	 * information lags behind the in-band scheduler state until
+	 * the migrated thread switches back to OOB context
+	 * (i.e. task_cpu(p) !=
+	 * evl_rq_cpu(evl_thread_from_task(p)->rq)).  This is ok since
+	 * EVL will not schedule such thread until then.
+	 *
+	 * affinity_ok() detects when a EVL thread switching back to
+	 * OOB context did move to another CPU earlier while running
+	 * in-band. If so, do the fixups to reflect the change.
+	 */
+	if (!is_threading_cpu(cpu)) {
+		printk(EVL_WARNING "thread %s[%d] switched to non-rt CPU%d, aborted.\n",
+		       thread->name, evl_get_inband_pid(thread), cpu);
+		/*
+		 * Can't call evl_cancel_thread() from a CPU migration
+		 * point, that would break. Since we are on the wakeup
+		 * path to OOB context, just raise T_CANCELD to catch
+		 * it in evl_switch_oob().
+		 */
+		thread->info |= T_CANCELD;
+		return false;
+	}
+
+	rq = evl_cpu_rq(cpu);
+	if (rq == thread->rq)
+		return true;
+
+	/*
+	 * The current thread moved to a supported real-time CPU,
+	 * which is not part of its original affinity mask
+	 * though. Assume user wants to extend this mask.
+	 */
+	if (!cpumask_test_cpu(cpu, &thread->affinity))
+		cpumask_set_cpu(cpu, &thread->affinity);
+
+	evl_migrate_thread(thread, rq);
+
+	return true;
+}
+
+#else /* !CONFIG_SMP */
+
+static void handle_migration_event(struct dovetail_migration_data *d)
+{
+}
+
+static inline bool affinity_ok(struct task_struct *p)
+{
+	return true;
+}
+
+#endif /* CONFIG_SMP */
+
+void resume_oob_task(struct task_struct *p) /* hw IRQs off */
+{
+	struct evl_thread *thread = evl_thread_from_task(p);
+
+	/*
+	 * We fire the handler before the thread is migrated, so that
+	 * thread->rq does not change between paired invocations of
+	 * relax_thread/harden handlers.
+	 */
+	xnlock_get(&nklock);
+	if (affinity_ok(p))
+		evl_resume_thread(thread, T_INBAND);
+	xnlock_put(&nklock);
+
+	evl_schedule();
+}
+
+static void handle_schedule_event(struct task_struct *next_task)
+{
+	struct task_struct *prev_task;
+	struct evl_thread *next;
+	unsigned long flags;
+	sigset_t pending;
+
+	evl_notify_inband_yield();
+
+	prev_task = current;
+	next = evl_thread_from_task(next_task);
+	if (next == NULL)
+		return;
+
+	/*
+	 * Check whether we need to unlock the timers, each time a
+	 * Linux task resumes from a stopped state, excluding tasks
+	 * resuming shortly for entering a stopped state asap due to
+	 * ptracing. To identify the latter, we need to check for
+	 * SIGSTOP and SIGINT in order to encompass both the NPTL and
+	 * LinuxThreads behaviours.
+	 */
+	if (next->state & T_SSTEP) {
+		if (signal_pending(next_task)) {
+			/*
+			 * Do not grab the sighand lock here: it's
+			 * useless, and we already own the runqueue
+			 * lock, so this would expose us to deadlock
+			 * situations on SMP.
+			 */
+			sigorsets(&pending,
+				  &next_task->pending.signal,
+				  &next_task->signal->shared_pending.signal);
+			if (sigismember(&pending, SIGSTOP) ||
+			    sigismember(&pending, SIGINT))
+				goto check;
+		}
+		xnlock_get_irqsave(&nklock, flags);
+		next->state &= ~T_SSTEP;
+		xnlock_put_irqrestore(&nklock, flags);
+		next->local_info |= T_HICCUP;
+	}
+
+check:
+	/*
+	 * Do basic sanity checks on the incoming thread state.
+	 * NOTE: we allow ptraced threads to run shortly in order to
+	 * properly recover from a stopped state.
+	 */
+	if (!EVL_WARN(CORE, !(next->state & T_INBAND),
+		      "Ouch: out-of-band thread %s[%d] running on the in-band stage"
+		      "(status=0x%x, sig=%d, prev=%s[%d])",
+		      next->name, task_pid_nr(next_task),
+		      next->state,
+		      signal_pending(next_task),
+		      prev_task->comm, task_pid_nr(prev_task)))
+		EVL_WARN(CORE,
+			 !(next_task->ptrace & PT_PTRACED) &&
+			 !(next->state & T_DORMANT)
+			 && (next->state & T_PEND),
+			 "Ouch: blocked EVL thread %s[%d] rescheduled in-band"
+			 "(status=0x%x, sig=%d, prev=%s[%d])",
+			 next->name, task_pid_nr(next_task),
+			 next->state,
+			 signal_pending(next_task), prev_task->comm,
+			 task_pid_nr(prev_task));
+}
+
+static void handle_sigwake_event(struct task_struct *p)
+{
+	struct evl_thread *thread;
+	unsigned long flags;
+	sigset_t pending;
+
+	thread = evl_thread_from_task(p);
+	if (thread == NULL)
+		return;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	/*
+	 * CAUTION: __TASK_TRACED is not set in p->state yet. This
+	 * state bit will be set right after we return, when the task
+	 * is woken up.
+	 */
+	if ((p->ptrace & PT_PTRACED) && !(thread->state & T_SSTEP)) {
+		/* We already own the siglock. */
+		sigorsets(&pending,
+			  &p->pending.signal,
+			  &p->signal->shared_pending.signal);
+
+		if (sigismember(&pending, SIGTRAP) ||
+		    sigismember(&pending, SIGSTOP)
+		    || sigismember(&pending, SIGINT))
+			thread->state &= ~T_SSTEP;
+	}
+
+	if (thread->state & T_INBAND) {
+		xnlock_put_irqrestore(&nklock, flags);
+		return;
+	}
+
+	/*
+	 * A thread running on the oob stage may not be picked by the
+	 * in-band scheduler as it bears the _TLF_OFFSTAGE flag. We
+	 * need to force that thread to switch to in-band context,
+	 * which will clear that flag.
+	 */
+	__evl_kick_thread(thread);
+
+	evl_schedule();
+
+	xnlock_put_irqrestore(&nklock, flags);
+}
+
+static void handle_cleanup_event(struct mm_struct *mm)
+{
+	struct evl_thread *curr = evl_current_thread();
+
+	/*
+	 * Detect an EVL thread running exec(), i.e. still attached to
+	 * the current Linux task (PF_EXITING is cleared for a task
+	 * which did not explicitly run do_exit()). In this case, we
+	 * emulate a task exit, since the EVL binding shall not
+	 * survive the exec() syscall.
+	 *
+	 * NOTE: We are called for every userland task exiting from
+	 * in-band context. We are NOT called for exiting kernel
+	 * threads since they have no mm proper. We may get there
+	 * after cleanup_current_thread() already ran though, so check
+	 * @curr.
+	 */
+	if (curr && !(current->flags & PF_EXITING))
+		put_current_thread();
+}
+
+void handle_inband_event(enum inband_event_type event, void *data)
+{
+	switch (event) {
+	case INBAND_TASK_SCHEDULE:
+		handle_schedule_event(data);
+		break;
+	case INBAND_TASK_SIGNAL:
+		handle_sigwake_event(data);
+		break;
+	case INBAND_TASK_EXIT:
+		put_current_thread();
+		break;
+	case INBAND_TASK_MIGRATION:
+		handle_migration_event(data);
+		break;
+	case INBAND_PROCESS_CLEANUP:
+		handle_cleanup_event(data);
+		break;
+	}
+}
+
+static int set_time_slice(struct evl_thread *thread, ktime_t quantum) /* nklock held, irqs off */
+{
+	struct evl_rq *rq;
+
+	rq = thread->rq;
+	thread->rrperiod = quantum;
+
+	if (!timeout_infinite(quantum)) {
+		if (quantum <= evl_get_clock_gravity(&evl_mono_clock, user))
+			return -EINVAL;
+
+		if (thread->base_class->sched_tick == NULL)
+			return -EINVAL;
+
+		thread->state |= T_RRB;
+		if (rq->curr == thread)
+			evl_start_timer(&rq->rrbtimer,
+					evl_abs_timeout(&rq->rrbtimer, quantum),
+					EVL_INFINITE);
+	} else {
+		thread->state &= ~T_RRB;
+		if (rq->curr == thread)
+			evl_stop_timer(&rq->rrbtimer);
+	}
+
+	return 0;
+}
+
+static int set_sched_attrs(struct evl_thread *thread,
+			   const struct evl_sched_attrs *attrs)
+{
+	struct evl_sched_class *sched_class;
+	union evl_sched_param param;
+	unsigned long flags;
+	int ret = -EINVAL;
+	ktime_t tslice;
+
+	trace_evl_thread_setsched(thread, attrs);
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	tslice = thread->rrperiod;
+	sched_class = evl_find_sched_class(&param, attrs, &tslice);
+	if (sched_class == NULL)
+		goto out;
+
+	ret = set_time_slice(thread, tslice);
+	if (ret)
+		goto out;
+
+	ret = __evl_set_thread_schedparam(thread, sched_class, &param);
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	evl_schedule();
+
+	return ret;
+}
+
+static int get_sched_attrs(struct evl_thread *thread,
+			   struct evl_sched_attrs *attrs)
+{
+	struct evl_sched_class *base_class;
+	unsigned long flags;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	base_class = thread->base_class;
+	attrs->sched_policy = base_class->policy;
+	attrs->sched_priority = thread->bprio;
+	if (attrs->sched_priority == 0) /* SCHED_FIFO/SCHED_WEAK */
+		attrs->sched_policy = SCHED_NORMAL;
+
+	if (base_class == &evl_sched_rt) {
+		if (thread->state & T_RRB) {
+			attrs->sched_rr_quantum =
+				ktime_to_timespec(thread->rrperiod);
+			attrs->sched_policy = SCHED_RR;
+		}
+		goto out;
+	}
+
+	if (base_class == &evl_sched_weak) {
+		if (attrs->sched_policy != SCHED_WEAK)
+			attrs->sched_priority = -attrs->sched_priority;
+		goto out;
+	}
+
+#ifdef CONFIG_EVENLESS_SCHED_QUOTA
+	if (base_class == &evl_sched_quota) {
+		attrs->sched_quota_group = thread->quota->tgid;
+		goto out;
+	}
+#endif
+
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	trace_evl_thread_getsched(thread, attrs);
+
+	return 0;
+}
+
+static int update_state_bits(struct evl_thread *thread,
+			     __u32 mask, bool set)
+{
+	struct evl_thread *curr = evl_current_thread();
+	unsigned long flags;
+
+	if (curr != thread)
+		return -EPERM;
+
+	if (mask & ~T_WARN)
+		return -EINVAL;
+
+	trace_evl_thread_update_mode(mask, set);
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (set)
+		curr->state |= mask;
+	else
+		curr->state &= ~mask;
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return 0;
+}
+
+static long thread_common_ioctl(struct evl_thread *thread,
+				unsigned int cmd, unsigned long arg)
+{
+	struct evl_sched_attrs attrs;
+	long ret;
+
+	switch (cmd) {
+	case EVL_THRIOC_SET_SCHEDPARAM:
+		ret = raw_copy_from_user(&attrs,
+					 (struct evl_sched_attrs *)arg, sizeof(attrs));
+		if (ret)
+			return -EFAULT;
+		ret = set_sched_attrs(thread, &attrs);
+		break;
+	case EVL_THRIOC_GET_SCHEDPARAM:
+		ret = get_sched_attrs(thread, &attrs);
+		if (ret)
+			return ret;
+		ret = raw_copy_to_user((struct evl_sched_attrs *)arg,
+				       &attrs, sizeof(attrs));
+		if (ret)
+			return -EFAULT;
+		break;
+	default:
+		ret = -ENOTTY;
+	}
+
+	return ret;
+}
+
+static long thread_oob_ioctl(struct file *filp, unsigned int cmd,
+			     unsigned long arg)
+{
+	struct evl_thread *thread = element_of(filp, struct evl_thread);
+	__u32 monfd, mask;
+	long ret;
+
+	if (thread->state & T_ZOMBIE)
+		return -ESTALE;
+
+	switch (cmd) {
+	case EVL_THRIOC_SIGNAL:
+		ret = raw_get_user(monfd, (__u32 *)arg);
+		if (ret)
+			return -EFAULT;
+		ret = evl_signal_monitor_targeted(thread, monfd);
+		break;
+	case EVL_THRIOC_SET_MODE:
+	case EVL_THRIOC_CLEAR_MODE:
+		ret = raw_get_user(mask, (__u32 *)arg);
+		if (ret)
+			return -EFAULT;
+		ret = update_state_bits(thread, mask,
+					cmd == EVL_THRIOC_SET_MODE);
+		break;
+	default:
+		ret = thread_common_ioctl(thread, cmd, arg);
+	}
+
+	return ret;
+}
+
+static long thread_ioctl(struct file *filp, unsigned int cmd,
+			 unsigned long arg)
+{
+	struct evl_thread *thread = element_of(filp, struct evl_thread);
+	long ret;
+
+	if (thread->state & T_ZOMBIE)
+		return -ESTALE;
+
+	switch (cmd) {
+	case EVL_THRIOC_JOIN:
+		ret = evl_join_thread(thread, false);
+		break;
+	default:
+		ret = thread_common_ioctl(thread, cmd, arg);
+	}
+
+	return ret;
+}
+
+static const struct file_operations thread_fops = {
+	.open		= evl_open_element,
+	.release	= evl_close_element,
+	.unlocked_ioctl	= thread_ioctl,
+	.oob_ioctl	= thread_oob_ioctl,
+};
+
+static int map_uthread_self(struct evl_thread *thread)
+{
+	struct evl_user_window *u_window;
+	int ret;
+
+	ret = commit_process_memory();
+	if (ret)
+		return ret;
+
+	u_window = evl_zalloc_chunk(&evl_shared_heap, sizeof(*u_window));
+	if (u_window == NULL)
+		return -ENOMEM;
+
+	thread->u_window = u_window;
+	pin_to_initial_cpu(thread);
+	trace_evl_thread_map(thread);
+
+	dovetail_init_altsched(&thread->altsched);
+	evl_stop_thread(thread, T_INBAND);
+	set_oob_threadinfo(thread);
+
+	/*
+	 * CAUTION: we enable dovetailing only when *thread is
+	 * consistent, so that we won't trigger false positive in
+	 * debug code from handle_schedule_event() and friends.
+	 */
+	dovetail_start_altsched();
+	evl_start_thread(thread);
+	evl_sync_uwindow(thread);
+
+	return 0;
+}
+
+/*
+ * Deconstruct a thread we just failed to map over a userland task.
+ * Since the former must be dormant, it can't be part of any runqueue.
+ */
+static void discard_unmapped_uthread(struct evl_thread *thread)
+{
+	unsigned long flags;
+
+	evl_destroy_timer(&thread->rtimer);
+	evl_destroy_timer(&thread->ptimer);
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (!list_empty(&thread->next)) {
+		list_del(&thread->next);
+		evl_nrthreads--;
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	if (thread->u_window)
+		evl_free_chunk(&evl_shared_heap, thread->u_window);
+
+	kfree(thread);
+}
+
+static struct evl_element *
+thread_factory_build(struct evl_factory *fac, const char *name,
+		     void __user *u_attrs, u32 *state_offp)
+{
+	struct task_struct *tsk = current;
+	struct evl_init_thread_attr iattr;
+	struct evl_thread *curr;
+	int ret;
+
+	if (evl_current_thread())
+		return ERR_PTR(-EBUSY);
+
+	curr = kzalloc(sizeof(*curr), GFP_KERNEL);
+	if (curr == NULL)
+		return ERR_PTR(-ENOMEM);
+
+	ret = evl_init_element(&curr->element, &evl_thread_factory);
+	if (ret) {
+		kfree(curr);
+		return ERR_PTR(ret);
+	}
+
+	iattr.flags = T_USER;
+	iattr.affinity = CPU_MASK_ALL;
+	iattr.sched_class = &evl_sched_weak;
+	iattr.sched_param.weak.prio = 0;
+	ret = evl_init_thread(curr, &iattr, NULL, "%s", name);
+	if (ret) {
+		evl_destroy_element(&curr->element);
+		kfree(curr);
+		return ERR_PTR(ret);
+	}
+
+	ret = map_uthread_self(curr);
+	if (ret) {
+		evl_destroy_element(&curr->element);
+		discard_unmapped_uthread(curr);
+		return ERR_PTR(ret);
+	}
+
+	*state_offp = evl_shared_offset(curr->u_window);
+	evl_index_element(&curr->element);
+
+	/*
+	 * Unlike most elements, a thread may exist in absence of any
+	 * file reference, so we get a reference on the emerging
+	 * thread here to block automatic disposal on last file
+	 * release. put_current_thread() drops this reference when the
+	 * thread exits, or voluntarily detaches by sending the
+	 * EVL_CTLIOC_DETACH_SELF control request.
+	 */
+	evl_get_element(&curr->element);
+
+	strncpy(tsk->comm, name, sizeof(tsk->comm));
+	tsk->comm[sizeof(tsk->comm) - 1] = '\0';
+
+	return &curr->element;
+}
+
+static void thread_factory_dispose(struct evl_element *e)
+{
+	struct evl_thread *thread;
+
+	thread = container_of(e, struct evl_thread, element);
+
+	/*
+	 * Two ways to get there: if open_factory_node() fails
+	 * creating a device for @thread which is current, or when the
+	 * last file reference to @thread is dropped after it has
+	 * exited. We detect the first case by checking the zombie
+	 * state.
+	 */
+	if (!(thread->state & T_ZOMBIE)) {
+		if (EVL_WARN_ON(CORE, evl_current_thread() != thread))
+			return;
+		cleanup_current_thread();
+	}
+
+	evl_destroy_element(&thread->element);
+
+	if (thread->state & T_USER)
+		kfree_rcu(thread, element.rcu);
+}
+
+static ssize_t state_show(struct device *dev,
+			  struct device_attribute *attr,
+			  char *buf)
+{
+	struct evl_thread *thread;
+	ssize_t ret;
+
+	thread = evl_get_element_by_dev(dev, struct evl_thread);
+	ret = snprintf(buf, PAGE_SIZE, "%#x\n", thread->state);
+	evl_put_element(&thread->element);
+
+	return ret;
+}
+static DEVICE_ATTR_RO(state);
+
+static ssize_t sched_show(struct device *dev,
+			  struct device_attribute *attr,
+			  char *buf)
+{
+	struct evl_sched_class *sched_class;
+	struct evl_thread *thread;
+	unsigned long flags;
+	ssize_t ret, _ret;
+
+	thread = evl_get_element_by_dev(dev, struct evl_thread);
+
+	sched_class = thread->sched_class;
+
+	ret = snprintf(buf, PAGE_SIZE, "%d %d %d %s ",
+		       evl_rq_cpu(thread->rq),
+		       thread->bprio,
+		       thread->cprio,
+		       sched_class->name);
+
+	if (sched_class->sched_show) {
+		xnlock_get_irqsave(&nklock, flags);
+		_ret = sched_class->sched_show(thread, buf + ret,
+					       PAGE_SIZE - ret);
+		xnlock_put_irqrestore(&nklock, flags);
+		if (_ret > 0) {
+			ret += _ret;
+			goto out;
+		}
+	}
+
+	/* overwrites trailing whitespace */
+	buf[ret - 1] = '\n';
+out:
+	evl_put_element(&thread->element);
+
+	return ret;
+}
+static DEVICE_ATTR_RO(sched);
+
+static ssize_t stats_show(struct device *dev,
+			  struct device_attribute *attr,
+			  char *buf)
+{
+	ktime_t period, exectime, account;
+	struct evl_thread *thread;
+	unsigned long flags;
+	struct evl_rq *rq;
+	ssize_t ret;
+	int usage;
+
+	thread = evl_get_element_by_dev(dev, struct evl_thread);
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	rq = evl_thread_rq(thread);
+
+	period = rq->last_account_switch - thread->stat.lastperiod.start;
+	if (period == 0 && thread == rq->curr) {
+		exectime = ktime_set(0, 1);
+		account = ktime_set(0, 1);
+	} else {
+		exectime = thread->stat.account.total -
+			thread->stat.lastperiod.total;
+		account = period;
+	}
+
+	thread->stat.lastperiod.total = thread->stat.account.total;
+	thread->stat.lastperiod.start = rq->last_account_switch;
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	if (account) {
+		while (account > 0xffffffffUL) {
+			exectime >>= 16;
+			account >>= 16;
+		}
+
+		exectime = ns_to_ktime(ktime_to_ns(exectime) * 1000LL);
+		exectime = ktime_add_ns(exectime, ktime_to_ns(account) >> 1);
+		usage = ktime_divns(exectime, account);
+	} else
+		usage = 0;
+
+	ret = snprintf(buf, PAGE_SIZE, "%lu %lu %lu %Lu %d\n",
+		       thread->stat.isw.counter,
+		       thread->stat.csw.counter,
+		       thread->stat.sc.counter,
+		       ktime_to_ns(thread->stat.account.total),
+		       usage);
+
+	evl_put_element(&thread->element);
+
+	return ret;
+}
+static DEVICE_ATTR_RO(stats);
+
+static ssize_t timeout_show(struct device *dev,
+			    struct device_attribute *attr,
+			    char *buf)
+{
+	struct evl_thread *thread;
+	ssize_t ret;
+
+	thread = evl_get_element_by_dev(dev, struct evl_thread);
+	ret = snprintf(buf, PAGE_SIZE, "%Lu\n",
+		       ktime_to_ns(evl_get_thread_timeout(thread)));
+	evl_put_element(&thread->element);
+
+	return ret;
+}
+static DEVICE_ATTR_RO(timeout);
+
+static ssize_t pid_show(struct device *dev,
+			struct device_attribute *attr,
+			char *buf)
+{
+	struct evl_thread *thread;
+	ssize_t ret;
+
+	thread = evl_get_element_by_dev(dev, struct evl_thread);
+	ret = snprintf(buf, PAGE_SIZE, "%d\n", evl_get_inband_pid(thread));
+	evl_put_element(&thread->element);
+
+	return ret;
+}
+static DEVICE_ATTR_RO(pid);
+
+static struct attribute *thread_attrs[] = {
+	&dev_attr_state.attr,
+	&dev_attr_sched.attr,
+	&dev_attr_timeout.attr,
+	&dev_attr_stats.attr,
+	&dev_attr_pid.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(thread);
+
+struct evl_factory evl_thread_factory = {
+	.name	=	"thread",
+	.fops	=	&thread_fops,
+	.build	=	thread_factory_build,
+	.dispose =	thread_factory_dispose,
+	.nrdev	=	CONFIG_EVENLESS_NR_THREADS,
+	.attrs	=	thread_groups,
+	.flags	=	EVL_FACTORY_CLONE,
+};
diff --git a/kernel/evenless/tick.c b/kernel/evenless/tick.c
new file mode 100644
index 000000000000..d79b7864ac88
--- /dev/null
+++ b/kernel/evenless/tick.c
@@ -0,0 +1,323 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2016, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/percpu.h>
+#include <linux/cpumask.h>
+#include <linux/clockchips.h>
+#include <linux/tick.h>
+#include <linux/irqdomain.h>
+#include <linux/ktime.h>
+#include <linux/kernel.h>
+#include <linux/timekeeping.h>
+#include <linux/irq_pipeline.h>
+#include <linux/slab.h>
+#include <evenless/sched.h>
+#include <evenless/timer.h>
+#include <evenless/clock.h>
+#include <evenless/tick.h>
+#include <evenless/control.h>
+#include <trace/events/evenless.h>
+
+/*
+ * This is our high-precision clock tick device, which operates the
+ * best rated clock event device taken over from the kernel. A head
+ * stage handler forwards tick events to our clock management core.
+ */
+struct core_tick_device {
+	struct clock_event_device *real_device;
+};
+
+static DEFINE_PER_CPU(struct core_tick_device, clock_cpu_device);
+
+static int proxy_set_next_ktime(ktime_t expires,
+				struct clock_event_device *proxy_ced)
+{
+	struct evl_rq *rq;
+	unsigned long flags;
+	ktime_t delta;
+
+	/*
+	 * Negative delta have been observed. evl_start_timer()
+	 * will trigger an immediate shot in such an event.
+	 */
+	delta = ktime_sub(expires, ktime_get());
+
+	flags = hard_local_irq_save(); /* Prevent CPU migration. */
+	rq = this_evl_rq();
+	evl_start_timer(&rq->htimer,
+			evl_abs_timeout(&rq->htimer, delta),
+			EVL_INFINITE);
+	hard_local_irq_restore(flags);
+
+	return 0;
+}
+
+static int proxy_set_oneshot_stopped(struct clock_event_device *ced)
+{
+	struct core_tick_device *ctd = this_cpu_ptr(&clock_cpu_device);
+	unsigned long flags;
+	struct evl_rq *rq;
+
+	/*
+	 * In-band wants to disable the clock hardware on entering a
+	 * tickless state, so we have to stop our in-band tick
+	 * emulation. Propagate the request for shutting down the
+	 * hardware to the real device only if we have no outstanding
+	 * OOB timers. CAUTION: the in-band timer is counted when
+	 * assessing the RQ_IDLE condition, so we need to stop it
+	 * prior to testing the latter.
+	 */
+	flags = hard_local_irq_save();
+
+	rq = this_evl_rq();
+	evl_stop_timer(&rq->htimer);
+	rq->lflags |= RQ_TSTOPPED;
+
+	if (rq->lflags & RQ_IDLE)
+		ctd->real_device->set_state_oneshot_stopped(ctd->real_device);
+
+	hard_local_irq_restore(flags);
+
+	return 0;
+}
+
+static void proxy_device_register(struct clock_event_device *proxy_ced,
+				  struct clock_event_device *real_ced)
+{
+	struct core_tick_device *ctd = this_cpu_ptr(&clock_cpu_device);
+
+	ctd->real_device = real_ced;
+	proxy_ced->features |= CLOCK_EVT_FEAT_KTIME;
+	proxy_ced->set_next_ktime = proxy_set_next_ktime;
+	proxy_ced->set_next_event = NULL;
+	if (real_ced->set_state_oneshot_stopped)
+		proxy_ced->set_state_oneshot_stopped =
+			proxy_set_oneshot_stopped;
+	proxy_ced->rating = real_ced->rating + 1;
+	proxy_ced->min_delta_ns = 1;
+	proxy_ced->max_delta_ns = KTIME_MAX;
+	proxy_ced->min_delta_ticks = 1;
+	proxy_ced->max_delta_ticks = ULONG_MAX;
+	clockevents_register_device(proxy_ced);
+}
+
+static void proxy_device_unregister(struct clock_event_device *proxy_ced,
+				    struct clock_event_device *real_ced)
+{
+	struct core_tick_device *ctd = this_cpu_ptr(&clock_cpu_device);
+
+	ctd->real_device = NULL;
+}
+
+/*
+ * This is our high-precision clock tick handler. We only have two
+ * possible callers, each of them may only run over a CPU which is a
+ * member of the real-time set:
+ *
+ * - our TIMER_OOB_IPI handler, such IPI is directed to members of our
+ * real-time CPU set exclusively.
+ *
+ * - our clock_event_handler() routine. The IRQ pipeline
+ * guarantees that such handler always runs over a CPU which is a
+ * member of the CPU set passed to enable_clock_devices() (i.e. our
+ * real-time CPU set).
+ *
+ * hard IRQs are off.
+ */
+static void clock_event_handler(struct clock_event_device *dummy)
+{
+	struct evl_rq *this_rq = this_evl_rq();
+
+	if (EVL_WARN_ON_ONCE(CORE, !is_evl_cpu(evl_rq_cpu(this_rq))))
+		return;
+
+	evl_announce_tick(&evl_mono_clock);
+
+	/*
+	 * If a real-time thread was preempted by this clock
+	 * interrupt, any transition to the root thread will cause a
+	 * in-band tick to be propagated by evl_schedule() from
+	 * irq_finish_head(), so we only need to propagate the in-band
+	 * tick in case the root thread was preempted.
+	 */
+	if ((this_rq->lflags & RQ_TPROXY) && (this_rq->curr->state & T_ROOT))
+		evl_notify_proxy_tick(this_rq);
+}
+
+void evl_notify_proxy_tick(struct evl_rq *this_rq) /* hard IRQs off. */
+{
+	/*
+	 * A proxy clock event device is active on this CPU, make it
+	 * tick asap when the in-band code resumes; this will honour a
+	 * previous set_next_ktime() request received from the kernel
+	 * we have carried out using our core timing services.
+	 */
+	this_rq->lflags &= ~RQ_TPROXY;
+	tick_notify_proxy();
+}
+
+#ifdef CONFIG_SMP
+
+static irqreturn_t clock_ipi_handler(int irq, void *dev_id)
+{
+	clock_event_handler(NULL);
+
+	return IRQ_HANDLED;
+}
+
+#endif
+
+static struct proxy_tick_ops proxy_ops = {
+	.register_device = proxy_device_register,
+	.unregister_device = proxy_device_unregister,
+	.handle_event = clock_event_handler,
+};
+
+int evl_enable_tick(void)
+{
+	int ret;
+
+#ifdef CONFIG_SMP
+	ret = __request_percpu_irq(TIMER_OOB_IPI,
+				   clock_ipi_handler,
+				   IRQF_OOB, "Evenless timer IPI",
+				   &evl_machine_cpudata);
+	if (ret)
+		return ret;
+#endif
+
+	/*
+	 * CAUTION:
+	 *
+	 * - EVL timers may be started only _after_ the proxy clock
+	 * device has been set up for the target CPU.
+	 *
+	 * - do not hold any lock across calls to evl_enable_tick().
+	 *
+	 * - tick_install_proxy() guarantees that the real clock
+	 * device supports oneshot mode, or fails.
+	 */
+	ret = tick_install_proxy(&proxy_ops, &evl_oob_cpus);
+	if (ret) {
+#ifdef CONFIG_SMP
+		free_percpu_irq(TIMER_OOB_IPI,
+				&evl_machine_cpudata);
+#endif
+		return ret;
+	}
+
+	return 0;
+}
+
+void evl_disable_tick(void)
+{
+	tick_uninstall_proxy(&proxy_ops, &evl_oob_cpus);
+#ifdef CONFIG_SMP
+	free_percpu_irq(TIMER_OOB_IPI, &evl_machine_cpudata);
+#endif
+	/*
+	 * When the kernel is swapping clock event devices on behalf
+	 * of enable_clock_devices(), it may end up calling
+	 * program_timer() via the synthetic device's
+	 * ->set_next_event() handler for resuming the in-band timer.
+	 * Therefore, no timer should remain queued before
+	 * enable_clock_devices() is called, or unpleasant hangs may
+	 * happen if the in-band timer is not at front of the
+	 * queue. You have been warned.
+	 */
+	evl_stop_timers(&evl_mono_clock);
+}
+
+/* per-cpu timer queue locked. */
+void evl_program_proxy_tick(struct evl_clock *clock)
+{
+	struct core_tick_device *ctd = raw_cpu_ptr(&clock_cpu_device);
+	struct clock_event_device *real_ced = ctd->real_device;
+	struct evl_rq *this_rq = this_evl_rq();
+	struct evl_timerbase *tmb;
+	struct evl_timer *timer;
+	struct evl_tnode *tn;
+	int64_t delta;
+	u64 cycles;
+	ktime_t t;
+	int ret;
+
+	/*
+	 * Do not reprogram locally when inside the tick handler -
+	 * will be done on exit anyway. Also exit if there is no
+	 * pending timer.
+	 */
+	if (this_rq->lflags & RQ_TIMER)
+		return;
+
+	tmb = evl_this_cpu_timers(clock);
+	tn = evl_get_tqueue_head(&tmb->q);
+	if (tn == NULL) {
+		this_rq->lflags |= RQ_IDLE;
+		return;
+	}
+
+	/*
+	 * Try to defer the next in-band tick, so that it does not
+	 * preempt an OOB activity uselessly, in two cases:
+	 *
+	 * 1) a rescheduling is pending for the current CPU. We may
+	 * assume that an EVL thread is about to resume, so we want to
+	 * move the in-band tick out of the way until in-band activity
+	 * resumes, unless there is no other outstanding timers.
+	 *
+	 * 2) the current EVL thread is running OOB, in which case we
+	 * may defer the in-band tick until the in-band activity
+	 * resumes.
+	 *
+	 * The in-band tick deferral is cleared whenever EVL is about
+	 * to yield control to the in-band code (see
+	 * ___evl_schedule()), or a timer with an earlier timeout date
+	 * is scheduled, whichever comes first.
+	 */
+	this_rq->lflags &= ~(RQ_TDEFER|RQ_IDLE|RQ_TSTOPPED);
+	timer = container_of(tn, struct evl_timer, node);
+	if (timer == &this_rq->htimer) {
+		if (evl_need_resched(this_rq) ||
+		    !(this_rq->curr->state & T_ROOT)) {
+			tn = evl_get_tqueue_next(&tmb->q, tn);
+			if (tn) {
+				this_rq->lflags |= RQ_TDEFER;
+				timer = container_of(tn, struct evl_timer, node);
+			}
+		}
+	}
+
+	t = evl_tdate(timer);
+	delta = ktime_to_ns(ktime_sub(t, evl_read_clock(clock)));
+
+	if (real_ced->features & CLOCK_EVT_FEAT_KTIME) {
+		real_ced->set_next_ktime(t, real_ced);
+		trace_evl_timer_shot(delta);
+	} else {
+		if (delta <= 0)
+			delta = real_ced->min_delta_ns;
+		else {
+			delta = min(delta, (int64_t)real_ced->max_delta_ns);
+			delta = max(delta, (int64_t)real_ced->min_delta_ns);
+		}
+		cycles = ((u64)delta * real_ced->mult) >> real_ced->shift;
+		ret = real_ced->set_next_event(cycles, real_ced);
+		trace_evl_timer_shot(delta);
+		if (ret) {
+			real_ced->set_next_event(real_ced->min_delta_ticks, real_ced);
+			trace_evl_timer_shot(real_ced->min_delta_ns);
+		}
+	}
+}
+
+#ifdef CONFIG_SMP
+void evl_send_timer_ipi(struct evl_clock *clock, struct evl_rq *rq)
+{
+	irq_pipeline_send_remote(TIMER_OOB_IPI,
+				 cpumask_of(evl_rq_cpu(rq)));
+}
+#endif
diff --git a/kernel/evenless/timer.c b/kernel/evenless/timer.c
new file mode 100644
index 000000000000..11727d465f2e
--- /dev/null
+++ b/kernel/evenless/timer.c
@@ -0,0 +1,471 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2004 Gilles Chanteperdrix  <gilles.chanteperdrix@xenomai.org>
+ * Copyright (C) 2001, 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/sched.h>
+#include <linux/cred.h>
+#include <linux/err.h>
+#include <evenless/sched.h>
+#include <evenless/thread.h>
+#include <evenless/timer.h>
+#include <evenless/clock.h>
+#include <evenless/tick.h>
+#include <asm/div64.h>
+#include <trace/events/evenless.h>
+
+static struct evl_timerbase *
+lock_timer_base(struct evl_timer *timer, unsigned long *flags)
+{
+	struct evl_timerbase *base;
+
+	for (;;) {
+		base = timer->base;
+		raw_spin_lock_irqsave(&base->lock, *flags);
+		/*
+		 * Careful about a bolting of the same timer happening
+		 * concurrently from a different CPU.
+		 */
+		if (base == timer->base)
+			break;
+		raw_spin_unlock_irqrestore(&base->lock, *flags);
+	}
+
+	return base;
+}
+
+static inline void unlock_timer_base(struct evl_timerbase *base,
+				     unsigned long flags)
+{
+	raw_spin_unlock_irqrestore(&base->lock, flags);
+}
+
+/* timer base locked. */
+static bool timer_at_front(struct evl_timer *timer)
+{
+	struct evl_rq *rq = evl_get_timer_rq(timer);
+	struct evl_tqueue *tq;
+	struct evl_tnode *tn;
+
+	tq = &timer->base->q;
+	tn = evl_get_tqueue_head(tq);
+	if (tn == &timer->node)
+		return true;
+
+	if (rq->lflags & RQ_TDEFER) {
+		tn = evl_get_tqueue_next(tq, tn);
+		if (tn == &timer->node)
+			return true;
+	}
+
+	return false;
+}
+
+/* timer base locked. */
+static void program_timer(struct evl_timer *timer,
+			  struct evl_tqueue *tq)
+{
+	struct evl_rq *rq;
+
+	evl_enqueue_timer(timer, tq);
+
+	rq = evl_get_timer_rq(timer);
+	if (!(rq->lflags & RQ_TSTOPPED) && !timer_at_front(timer))
+		return;
+
+	if (rq != this_evl_rq())
+		evl_program_remote_tick(timer->clock, rq);
+	else
+		evl_program_local_tick(timer->clock);
+}
+
+void evl_start_timer(struct evl_timer *timer,
+		     ktime_t value, ktime_t interval)
+{
+	struct evl_timerbase *base;
+	struct evl_tqueue *tq;
+	ktime_t date, gravity;
+	unsigned long flags;
+
+	trace_evl_timer_start(timer, value, interval);
+
+	base = lock_timer_base(timer, &flags);
+	tq = &base->q;
+
+	if ((timer->status & EVL_TIMER_DEQUEUED) == 0)
+		evl_dequeue_timer(timer, tq);
+
+	timer->status &= ~(EVL_TIMER_FIRED | EVL_TIMER_PERIODIC);
+
+	date = ktime_sub(value, timer->clock->offset);
+
+	/*
+	 * To cope with the basic system latency, we apply a clock
+	 * gravity value, which is the amount of time expressed in
+	 * nanoseconds by which we should anticipate the shot for the
+	 * timer. The gravity value varies with the type of context
+	 * the timer wakes up, i.e. irq handler, kernel or user
+	 * thread.
+	 */
+	gravity = evl_get_timer_gravity(timer);
+	evl_tdate(timer) = ktime_sub(date, gravity);
+
+	timer->interval = EVL_INFINITE;
+	if (!timeout_infinite(interval)) {
+		timer->interval = interval;
+		timer->start_date = value;
+		timer->pexpect_ticks = 0;
+		timer->periodic_ticks = 0;
+		timer->status |= EVL_TIMER_PERIODIC;
+	}
+
+	timer->status |= EVL_TIMER_RUNNING;
+	program_timer(timer, tq);
+
+	unlock_timer_base(base, flags);
+}
+EXPORT_SYMBOL_GPL(evl_start_timer);
+
+/* timer base locked. */
+bool evl_timer_deactivate(struct evl_timer *timer)
+{
+	struct evl_tqueue *tq = &timer->base->q;
+	bool heading = true;
+
+	if (!(timer->status & EVL_TIMER_DEQUEUED)) {
+		heading = timer_at_front(timer);
+		evl_dequeue_timer(timer, tq);
+	}
+
+	timer->status &= ~(EVL_TIMER_FIRED|EVL_TIMER_RUNNING);
+
+	return heading;
+}
+
+void __evl_stop_timer(struct evl_timer *timer)
+{
+	struct evl_timerbase *base;
+	unsigned long flags;
+	bool heading;
+
+	trace_evl_timer_stop(timer);
+
+	base = lock_timer_base(timer, &flags);
+
+	/*
+	 * If we removed the heading timer, reprogram the next shot if
+	 * any. If the timer was running on another CPU, let it tick.
+	 */
+	if (evl_timer_is_running(timer)) {
+		heading = evl_timer_deactivate(timer);
+		if (heading && evl_timer_on_rq(timer, this_evl_rq()))
+			evl_program_local_tick(timer->clock);
+	}
+
+	unlock_timer_base(base, flags);
+}
+EXPORT_SYMBOL_GPL(__evl_stop_timer);
+
+ktime_t evl_get_timer_date(struct evl_timer *timer)
+{
+	struct evl_timerbase *base;
+	unsigned long flags;
+	ktime_t expiry;
+
+	base = lock_timer_base(timer, &flags);
+
+	if (!evl_timer_is_running(timer))
+		expiry = EVL_INFINITE;
+	else
+		expiry = evl_get_timer_expiry(timer);
+
+	unlock_timer_base(base, flags);
+
+	return expiry;
+}
+EXPORT_SYMBOL_GPL(evl_get_timer_date);
+
+ktime_t __evl_get_timer_delta(struct evl_timer *timer)
+{
+	struct evl_timerbase *base;
+	ktime_t expiry, now;
+	unsigned long flags;
+
+	base = lock_timer_base(timer, &flags);
+	expiry = evl_get_timer_expiry(timer);
+	unlock_timer_base(base, flags);
+	now = evl_read_clock(timer->clock);
+	if (expiry <= now)
+		return ktime_set(0, 1);  /* Will elapse shortly. */
+
+	return ktime_sub(expiry, now);
+}
+EXPORT_SYMBOL_GPL(__evl_get_timer_delta);
+
+#ifdef CONFIG_SMP
+
+static inline int get_clock_cpu(struct evl_clock *clock, int cpu)
+{
+	/*
+	 * Check a CPU number against the possible set of CPUs
+	 * receiving events from the underlying clock device. If the
+	 * suggested CPU does not receive events from this device,
+	 * return the first one which does instead.
+	 *
+	 * A global clock device with no particular IRQ affinity may
+	 * tick on any CPU, but timers should always be queued on
+	 * CPU0.
+	 *
+	 * NOTE: we have scheduler slots initialized for all online
+	 * CPUs, we can program and receive clock ticks on any of
+	 * them. So there is no point in restricting the valid CPU set
+	 * to cobalt_cpu_affinity, which specifically refers to the
+	 * set of CPUs which may run real-time threads. Although
+	 * receiving a clock tick for waking up a thread living on a
+	 * remote CPU is not optimal since this involves IPI-signaled
+	 * rescheds, this is still a valid case.
+	 */
+	if (cpumask_empty(&clock->affinity))
+		return 0;
+
+	if (cpumask_test_cpu(cpu, &clock->affinity))
+		return cpu;
+
+	return cpumask_first(&clock->affinity);
+}
+
+/**
+ * __evl_set_timer_rq - change the CPU affinity of a timer
+ * @timer:      timer to modify
+ * @rq:         runqueue to assign the timer to
+ */
+void __evl_set_timer_rq(struct evl_timer *timer,
+			struct evl_clock *clock,
+			struct evl_rq *rq)
+{
+	int cpu;
+
+	atomic_only();
+
+	/*
+	 * Figure out which CPU is best suited for managing this
+	 * timer, preferably picking xnsched_cpu(sched) if the ticking
+	 * device moving the timer clock beats on that CPU. Otherwise,
+	 * pick the first CPU from the clock affinity mask if set. If
+	 * not, the timer is backed by a global device with no
+	 * particular IRQ affinity, so it should always be queued to
+	 * CPU0.
+	 */
+	cpu = 0;
+	if (!cpumask_empty(&clock->master->affinity))
+		cpu = get_clock_cpu(clock->master, evl_rq_cpu(rq));
+
+	evl_bolt_timer(timer, clock, evl_cpu_rq(cpu));
+}
+EXPORT_SYMBOL_GPL(__evl_set_timer_rq);
+
+#endif /* CONFIG_SMP */
+
+void __evl_init_timer(struct evl_timer *timer,
+		      struct evl_clock *clock,
+		      void (*handler)(struct evl_timer *timer),
+		      struct evl_rq *rq,
+		      int opflags)
+{
+	int cpu __maybe_unused;
+
+	timer->clock = clock;
+	evl_tdate(timer) = EVL_INFINITE;
+	evl_set_timer_priority(timer, EVL_TIMER_STDPRIO);
+	timer->status = (EVL_TIMER_DEQUEUED|(opflags & EVL_TIMER_INIT_MASK));
+	timer->handler = handler;
+	timer->interval = EVL_INFINITE;
+	/*
+	 * Set the timer affinity, preferably to rq if given, CPU0
+	 * otherwise.
+	 */
+	if (!rq)
+		rq = evl_cpu_rq(0);
+#ifdef CONFIG_SMP
+	cpu = 0;
+	if (!cpumask_empty(&clock->master->affinity))
+		cpu = get_clock_cpu(clock->master, evl_rq_cpu(rq));
+	timer->rq = evl_cpu_rq(cpu);
+#endif
+	timer->base = evl_percpu_timers(clock, evl_rq_cpu(rq));
+	timer->clock = clock;
+
+#ifdef CONFIG_EVENLESS_STATS
+	timer->name = "anon";
+	evl_reset_timer_stats(timer);
+#endif /* CONFIG_EVENLESS_STATS */
+}
+EXPORT_SYMBOL_GPL(__evl_init_timer);
+
+void evl_set_timer_gravity(struct evl_timer *timer, int gravity)
+{
+	struct evl_timerbase *base;
+	unsigned long flags;
+
+	base = lock_timer_base(timer, &flags);
+	timer->status &= ~EVL_TIMER_GRAVITY_MASK;
+	timer->status |= gravity;
+	unlock_timer_base(base, flags);
+
+}
+EXPORT_SYMBOL_GPL(evl_set_timer_gravity);
+
+void evl_destroy_timer(struct evl_timer *timer)
+{
+	evl_stop_timer(timer);
+	timer->status |= EVL_TIMER_KILLED;
+#ifdef CONFIG_SMP
+	timer->rq = NULL;
+#endif
+	timer->base = NULL;
+}
+EXPORT_SYMBOL_GPL(evl_destroy_timer);
+
+/*
+ * evl_bolt_timer - change the reference clock and/or the CPU
+ *                     affinity of a timer
+ * @timer:      timer to modify
+ * @clock:      reference clock
+ * @rq:         runqueue to assign the timer to
+ */
+void evl_bolt_timer(struct evl_timer *timer,
+		    struct evl_clock *clock, struct evl_rq *rq)
+{	/* nklocked, IRQs off */
+	struct evl_timerbase *old_base, *new_base;
+	struct evl_clock *master = clock->master;
+	unsigned long flags;
+	int cpu;
+
+	trace_evl_timer_bolt(timer, clock, evl_rq_cpu(rq));
+
+	old_base = lock_timer_base(timer, &flags);
+
+	if (evl_timer_on_rq(timer, rq) && clock == timer->clock) {
+		unlock_timer_base(old_base, flags);
+		return;
+	}
+
+	/*
+	 * This assertion triggers when the timer is migrated to a CPU
+	 * for which we do not expect any clock events/IRQs from the
+	 * associated clock device. If so, the timer would never fire
+	 * since clock ticks would never happen on that CPU.
+	 */
+	cpu = evl_rq_cpu(rq);
+	EVL_WARN_ON_SMP(EVENLESS,
+			!cpumask_empty(&master->affinity) &&
+			!cpumask_test_cpu(cpu, &master->affinity));
+
+	new_base = evl_percpu_timers(master, cpu);
+
+	if (timer->status & EVL_TIMER_RUNNING) {
+		__evl_stop_timer(timer);
+		raw_spin_lock(&new_base->lock);
+#ifdef CONFIG_SMP
+		timer->rq = rq;
+#endif
+		timer->base = new_base;
+		evl_enqueue_timer(timer, &new_base->q);
+		if (timer_at_front(timer))
+			evl_program_remote_tick(clock, rq);
+		raw_spin_unlock(&new_base->lock);
+	} else {
+#ifdef CONFIG_SMP
+		timer->rq = rq;
+#endif
+		timer->base = new_base;
+	}
+
+	timer->clock = clock;
+
+	raw_spin_unlock_irqrestore(&old_base->lock, flags);
+}
+EXPORT_SYMBOL_GPL(evl_bolt_timer);
+
+unsigned long evl_get_timer_overruns(struct evl_timer *timer)
+{
+	unsigned long overruns = 0, flags;
+	struct evl_timerbase *base;
+	struct evl_thread *thread;
+	struct evl_tqueue *tq;
+	ktime_t now, delta;
+
+	now = evl_read_clock(timer->clock);
+	base = lock_timer_base(timer, &flags);
+
+	delta = ktime_sub(now, evl_get_timer_next_date(timer));
+	if (likely(delta < timer->interval))
+		goto done;
+
+	overruns = ktime_divns(delta, ktime_to_ns(timer->interval));
+	timer->pexpect_ticks += overruns;
+	if (!evl_timer_is_running(timer))
+		goto done;
+
+	EVL_WARN_ON_ONCE(CORE, (timer->status &
+				(EVL_TIMER_DEQUEUED|EVL_TIMER_PERIODIC))
+			 != EVL_TIMER_PERIODIC);
+	tq = &base->q;
+	evl_dequeue_timer(timer, tq);
+	while (evl_tdate(timer) < now) {
+		timer->periodic_ticks++;
+		evl_update_timer_date(timer);
+	}
+
+	program_timer(timer, tq);
+done:
+	timer->pexpect_ticks++;
+
+	unlock_timer_base(base, flags);
+
+	/*
+	 * Hide overruns due to the most recent ptracing session from
+	 * the caller.
+	 */
+	thread = evl_current_thread();
+	if (thread->local_info & T_HICCUP)
+		return 0;
+
+	return overruns;
+}
+EXPORT_SYMBOL_GPL(evl_get_timer_overruns);
+
+/* same or earlier date. */
+static inline bool date_is_earlier(struct evl_tnode *left,
+				   struct evl_tnode *right)
+{
+	return left->date < right->date
+		|| (left->date == right->date && left->prio > right->prio);
+}
+
+void evl_insert_tnode(struct evl_tqueue *tq, struct evl_tnode *node)
+{
+	struct rb_node **new = &tq->root.rb_node, *parent = NULL;
+
+	if (!tq->head)
+		tq->head = node;
+	else if (date_is_earlier(node, tq->head)) {
+		parent = &tq->head->rb;
+		new = &parent->rb_left;
+		tq->head = node;
+	} else while (*new) {
+			struct evl_tnode *i = container_of(*new, struct evl_tnode, rb);
+
+			parent = *new;
+			if (date_is_earlier(node, i))
+				new = &((*new)->rb_left);
+			else
+				new = &((*new)->rb_right);
+		}
+
+	rb_link_node(&node->rb, parent, new);
+	rb_insert_color(&node->rb, &tq->root);
+}
diff --git a/kernel/evenless/timerfd.c b/kernel/evenless/timerfd.c
new file mode 100644
index 000000000000..262748cdf0c7
--- /dev/null
+++ b/kernel/evenless/timerfd.c
@@ -0,0 +1,260 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Derived from Xenomai Cobalt (http://git.xenomai.org/xenomai-3.git/)
+ * Copyright (C) 2013 Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <evenless/thread.h>
+#include <evenless/clock.h>
+#include <evenless/timer.h>
+#include <evenless/lock.h>
+#include <evenless/poller.h>
+#include <evenless/factory.h>
+#include <asm/evenless/syscall.h>
+#include <uapi/evenless/timerfd.h>
+#include <trace/events/evenless.h>
+
+struct evl_timerfd {
+	struct evl_timer timer;
+	struct evl_syn readers;
+	bool ticked;
+	struct evl_poll_head poll_head;
+	struct evl_element element;
+};
+
+static void get_timer_value(struct evl_timer *__restrict__ timer,
+			    struct itimerspec *__restrict__ value)
+{
+	value->it_interval = ktime_to_timespec(timer->interval);
+
+	if (!evl_timer_is_running(timer)) {
+		value->it_value.tv_sec = 0;
+		value->it_value.tv_nsec = 0;
+	} else
+		value->it_value =
+			ktime_to_timespec(evl_get_timer_delta(timer));
+}
+
+static int set_timer_value(struct evl_timer *__restrict__ timer,
+			   const struct itimerspec *__restrict__ value)
+{
+	ktime_t start, period;
+
+	if (value->it_value.tv_nsec == 0 && value->it_value.tv_sec == 0) {
+		evl_stop_timer(timer);
+		return 0;
+	}
+
+	if ((unsigned long)value->it_value.tv_nsec >= ONE_BILLION ||
+	    ((unsigned long)value->it_interval.tv_nsec >= ONE_BILLION &&
+	     (value->it_value.tv_sec != 0 || value->it_value.tv_nsec != 0)))
+		return -EINVAL;
+
+	period = timespec_to_ktime(value->it_interval);
+	start = timespec_to_ktime(value->it_value);
+	evl_start_timer(timer, start, period);
+
+	return 0;
+}
+
+static int set_timerfd(struct evl_timerfd *timerfd,
+		       struct evl_timerfd_setreq *sreq)
+{
+	unsigned long flags;
+
+	get_timer_value(&timerfd->timer, &sreq->ovalue);
+	xnlock_get_irqsave(&nklock, flags);
+	evl_set_timer_rq(&timerfd->timer, evl_current_thread_rq());
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return set_timer_value(&timerfd->timer, &sreq->value);
+}
+
+static long timerfd_oob_ioctl(struct file *filp,
+			      unsigned int cmd, unsigned long arg)
+{
+	struct evl_timerfd *timerfd = element_of(filp, struct evl_timerfd);
+	struct evl_timerfd_setreq sreq, __user *u_sreq;
+	struct evl_timerfd_getreq greq, __user *u_greq;
+	long ret = 0;
+
+	switch (cmd) {
+	case EVL_TFDIOC_SET:
+		u_sreq = (typeof(u_sreq))arg;
+		ret = raw_copy_from_user(&sreq, u_sreq, sizeof(sreq));
+		if (ret)
+			return -EFAULT;
+		ret = set_timerfd(timerfd, &sreq);
+		if (ret)
+			return ret;
+		if (raw_copy_to_user(&u_sreq->ovalue, &sreq.ovalue,
+				     sizeof(sreq.ovalue)))
+			return -EFAULT;
+		break;
+	case EVL_TFDIOC_GET:
+		get_timer_value(&timerfd->timer, &greq.value);
+		u_greq = (typeof(u_greq))arg;
+		if (raw_copy_to_user(&u_greq->value, &greq.value,
+				     sizeof(greq.value)))
+			return -EFAULT;
+		break;
+	default:
+		ret = -ENOTTY;
+	}
+
+	return ret;
+}
+
+static void timerfd_handler(struct evl_timer *timer) /* hard IRQs off */
+{
+	struct evl_timerfd *timerfd;
+
+	timerfd = container_of(timer, struct evl_timerfd, timer);
+
+	timerfd->ticked = true;
+	evl_signal_poll_events(&timerfd->poll_head, POLLIN);
+	evl_flush_syn(&timerfd->readers, 0);
+}
+
+static ssize_t timerfd_oob_read(struct file *filp,
+				char __user *u_buf, size_t count)
+{
+	struct evl_timerfd *timerfd = element_of(filp, struct evl_timerfd);
+	__u64 __user *u_ticks = (__u64 __user *)u_buf, ticks = 0;
+	unsigned long flags;
+	int ret;
+
+	if (count < sizeof(ticks))
+		return -EINVAL;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (timerfd->ticked) {
+		ret = 0;
+		goto out;
+	}
+
+	if (filp->f_flags & O_NONBLOCK) {
+		ret = -EAGAIN;
+		goto fail;
+	}
+
+	do
+		ret = evl_sleep_on_syn(&timerfd->readers,
+				       EVL_INFINITE, EVL_REL);
+	while (ret == 0 && !timerfd->ticked);
+
+	if (ret & T_BREAK) {
+		ret = -EINTR;
+		goto fail;
+	}
+out:
+	if (ret == 0) {
+		ticks = 1;
+		if (evl_timer_is_periodic(&timerfd->timer))
+			ticks += evl_get_timer_overruns(&timerfd->timer);
+
+		timerfd->ticked = false;
+		evl_clear_poll_events(&timerfd->poll_head, POLLIN);
+	}
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	if (ret == 0 && raw_put_user(ticks, u_ticks))
+		ret = -EFAULT;
+
+	return ret ?: sizeof(ticks);
+fail:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+
+static __poll_t timerfd_oob_poll(struct file *filp,
+				 struct oob_poll_wait *wait)
+{
+	struct evl_timerfd *timerfd = element_of(filp, struct evl_timerfd);
+
+	evl_poll_watch(&timerfd->poll_head, wait);
+
+	return timerfd->ticked ? POLLIN|POLLRDNORM : 0;
+}
+
+static const struct file_operations timerfd_fops = {
+	.open		= evl_open_element,
+	.release	= evl_close_element,
+	.oob_ioctl	= timerfd_oob_ioctl,
+	.oob_read	= timerfd_oob_read,
+	.oob_poll	= timerfd_oob_poll,
+};
+
+static struct evl_element *
+timerfd_factory_build(struct evl_factory *fac, const char *name,
+		      void __user *u_attrs, u32 *state_offp)
+{
+	struct evl_timerfd_attrs attrs;
+	struct evl_timerfd *timerfd;
+	struct evl_clock *clock;
+	int ret;
+
+	ret = copy_from_user(&attrs, u_attrs, sizeof(attrs));
+	if (ret)
+		return ERR_PTR(-EFAULT);
+
+	clock = evl_get_clock_by_fd(attrs.clockfd);
+	if (clock == NULL)
+		return ERR_PTR(-EINVAL);
+
+	timerfd = kzalloc(sizeof(*timerfd), GFP_KERNEL);
+	if (timerfd == NULL) {
+		ret = -ENOMEM;
+		goto fail_alloc;
+	}
+
+	ret = evl_init_element(&timerfd->element,
+			       &evl_timerfd_factory);
+	if (ret)
+		goto fail_element;
+
+	evl_init_timer(&timerfd->timer, clock, timerfd_handler,
+		       NULL, EVL_TIMER_UGRAVITY);
+	evl_init_syn(&timerfd->readers, EVL_SYN_PRIO, clock, NULL);
+	evl_init_poll_head(&timerfd->poll_head);
+
+	return &timerfd->element;
+
+fail_element:
+	kfree(timerfd);
+fail_alloc:
+	evl_put_clock(clock);
+
+	return ERR_PTR(ret);
+}
+
+static void timerfd_factory_dispose(struct evl_element *e)
+{
+	struct evl_timerfd *timerfd;
+
+	timerfd = container_of(e, struct evl_timerfd, element);
+
+	evl_destroy_timer(&timerfd->timer);
+	evl_put_clock(timerfd->readers.clock);
+	evl_destroy_syn(&timerfd->readers);
+	evl_destroy_element(&timerfd->element);
+
+	kfree_rcu(timerfd, element.rcu);
+}
+
+struct evl_factory evl_timerfd_factory = {
+	.name	=	"timerfd",
+	.fops	=	&timerfd_fops,
+	.build =	timerfd_factory_build,
+	.dispose =	timerfd_factory_dispose,
+	.nrdev	=	CONFIG_EVENLESS_NR_TIMERFDS,
+	.flags	=	EVL_FACTORY_CLONE,
+};
diff --git a/kernel/evenless/trace.c b/kernel/evenless/trace.c
new file mode 100644
index 000000000000..8d798a74d4a8
--- /dev/null
+++ b/kernel/evenless/trace.c
@@ -0,0 +1,93 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/types.h>
+#include <linux/uaccess.h>
+#include <evenless/factory.h>
+#include <uapi/evenless/trace.h>
+#include <trace/events/evenless.h>
+#include <trace/trace.h>
+
+static long trace_common_ioctl(struct file *filp, unsigned int cmd,
+			       unsigned long arg)
+{
+	long ret = 0;
+
+	switch (cmd) {
+	case EVL_TRCIOC_TRACE_SNAPSHOT:
+#ifdef CONFIG_TRACER_SNAPSHOT
+		tracing_snapshot();
+#endif
+		break;
+	default:
+		ret = -ENOTTY;
+	}
+
+	return ret;
+}
+
+static long trace_oob_ioctl(struct file *filp, unsigned int cmd,
+			    unsigned long arg)
+{
+	return trace_common_ioctl(filp, cmd, arg);
+}
+
+static notrace ssize_t
+trace_oob_write(struct file *filp,
+		const char __user *u_buf, size_t count)
+{
+	char buf[128];
+	int ret;
+
+	if (!IS_ENABLED(CONFIG_FTRACE))
+		return -EINVAL;
+
+	if (count >= sizeof(buf))
+		count = sizeof(buf) - 1;
+
+	/* May be called from in-band context too. */
+	ret = raw_copy_from_user(buf, u_buf, count);
+	if (ret)
+		return -EFAULT;
+
+	buf[count] = '\0';
+
+	/*
+	 * trace_printk() is slow and triggers a scary and noisy
+	 * warning at boot. Prefer a common tracepoint for issuing the
+	 * message to the log.
+	 */
+	trace_evl_trace(buf);
+
+	return count;
+}
+
+static long trace_ioctl(struct file *filp, unsigned int cmd,
+			unsigned long arg)
+{
+	return trace_common_ioctl(filp, cmd, arg);
+}
+
+static notrace
+ssize_t trace_write(struct file *filp,
+		    const char __user *u_buf, size_t count,
+		    loff_t *ppos)
+{
+	return trace_oob_write(filp, u_buf, count);
+}
+
+static const struct file_operations trace_fops = {
+	.unlocked_ioctl	=	trace_ioctl,
+	.write		=	trace_write,
+	.oob_ioctl	=	trace_oob_ioctl,
+	.oob_write	=	trace_oob_write,
+};
+
+struct evl_factory evl_trace_factory = {
+	.name	=	"trace",
+	.fops	=	&trace_fops,
+	.flags	=	EVL_FACTORY_SINGLE,
+};
diff --git a/kernel/evenless/xbuf.c b/kernel/evenless/xbuf.c
new file mode 100644
index 000000000000..daf34018bbf0
--- /dev/null
+++ b/kernel/evenless/xbuf.c
@@ -0,0 +1,774 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>
+ */
+
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <linux/poll.h>
+#include <linux/irq_work.h>
+#include <linux/wait.h>
+#include <linux/log2.h>
+#include <linux/atomic.h>
+#include <evenless/synch.h>
+#include <evenless/thread.h>
+#include <evenless/clock.h>
+#include <evenless/xbuf.h>
+#include <evenless/memory.h>
+#include <evenless/lock.h>
+#include <evenless/factory.h>
+#include <evenless/sched.h>
+#include <evenless/poller.h>
+#include <evenless/wait.h>
+#include <uapi/evenless/xbuf.h>
+
+struct xbuf_ring {
+	void *bufmem;
+	size_t bufsz;
+	size_t fillsz;
+	unsigned int rdoff;
+	unsigned int rdrsvd;
+	unsigned int wroff;
+	unsigned int wrrsvd;
+	int (*wait_input)(struct xbuf_ring *ring, size_t len);
+	void (*signal_input)(struct xbuf_ring *ring);
+	int (*wait_output)(struct xbuf_ring *ring, size_t len);
+	void (*unblock_output)(struct xbuf_ring *ring);
+	bool (*in_output_contention)(struct xbuf_ring *ring);
+	void (*signal_pollable)(struct xbuf_ring *ring, int events);
+	void (*clear_pollable)(struct xbuf_ring *ring, int events);
+};
+
+struct xbuf_inbound {		/* oob_write->read */
+	struct wait_queue_head i_event;
+	struct evl_wait_flag o_event;
+	struct irq_work irq_work;
+	struct xbuf_ring ring;
+};
+
+struct xbuf_outbound {		/* write->oob_read */
+	struct evl_wait_flag i_event;
+	struct wait_queue_head o_event;
+	struct irq_work irq_work;
+	struct xbuf_ring ring;
+};
+
+struct evl_xbuf {
+	struct evl_element element;
+	struct xbuf_inbound ibnd;
+	struct xbuf_outbound obnd;
+	struct evl_poll_head poll_head;
+};
+
+struct xbuf_wait_data {
+	size_t len;
+};
+
+struct xbuf_rdesc {
+	char *buf;
+	char *buf_ptr;
+	size_t count;
+	int (*xfer)(struct xbuf_rdesc *dst, char *src, size_t len);
+};
+
+static int write_to_user(struct xbuf_rdesc *dst, char *src, size_t len)
+{
+	return raw_copy_to_user(dst->buf_ptr, src, len);
+}
+
+static int write_to_kernel(struct xbuf_rdesc *dst, char *src, size_t len)
+{
+	memcpy(dst->buf_ptr, src, len);
+
+	return 0;
+}
+
+struct xbuf_wdesc {
+	const char *buf;
+	const char *buf_ptr;
+	size_t count;
+	int (*xfer)(char *dst, struct xbuf_wdesc *src, size_t len);
+};
+
+static int read_from_user(char *dst, struct xbuf_wdesc *src, size_t len)
+{
+	return raw_copy_from_user(dst, src->buf_ptr, len);
+}
+
+static int read_from_kernel(char *dst, struct xbuf_wdesc *src, size_t len)
+{
+	memcpy(dst, src->buf_ptr, len);
+
+	return 0;
+}
+
+static ssize_t do_xbuf_read(struct xbuf_ring *ring,
+			    struct xbuf_rdesc *rd, int f_flags)
+{
+	ssize_t len, ret, rbytes, n;
+	unsigned int rdoff, avail;
+	unsigned long flags;
+	int xret;
+
+	len = rd->count;
+	if (len == 0)
+		return 0;
+
+	if (ring->bufsz == 0)
+		return -ENOBUFS;
+retry:
+	rd->buf_ptr = rd->buf;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	for (;;) {
+		/*
+		 * We should be able to read a complete message of the
+		 * requested length if O_NONBLOCK is clear. If set and
+		 * some bytes are available, return them. Otherwise,
+		 * send -EAGAIN. The actual count of bytes available
+		 * for reading excludes the data which might be in
+		 * flight to userland as we drop the lock during copy.
+		 */
+		avail = ring->fillsz - ring->rdrsvd;
+		if (avail < len) {
+			if (!(f_flags & O_NONBLOCK))
+				goto wait;
+			if (avail == 0) {
+				ret = -EAGAIN;
+				break;
+			}
+			len = avail;
+		}
+
+		/* Reserve a read slot into the circular buffer. */
+		rdoff = ring->rdoff;
+		ring->rdoff = (rdoff + len) % ring->bufsz;
+		ring->rdrsvd += len;
+		rbytes = ret = len;
+
+		do {
+			if (rdoff + rbytes > ring->bufsz)
+				n = ring->bufsz - rdoff;
+			else
+				n = rbytes;
+
+			/*
+			 * Drop the lock before copying data to
+			 * user. The read slot is consumed in any
+			 * case: the non-copied portion of the message
+			 * is lost on bad write.
+			 */
+			xnlock_put_irqrestore(&nklock, flags);
+			xret = rd->xfer(rd, ring->bufmem + rdoff, n);
+			xnlock_get_irqsave(&nklock, flags);
+			if (xret) {
+				ret = -EFAULT;
+				break;
+			}
+
+			rd->buf_ptr += n;
+			rbytes -= n;
+			rdoff = (rdoff + n) % ring->bufsz;
+		} while (rbytes > 0);
+
+		if (ring->fillsz == ring->bufsz)
+			/* -> writable */
+			ring->signal_pollable(ring, POLLOUT|POLLWRNORM);
+
+		ring->rdrsvd -= len;
+		ring->fillsz -= len;
+
+		if (ring->fillsz == 0)
+			/* -> non-readable */
+			ring->clear_pollable(ring, POLLIN|POLLRDNORM);
+
+		/*
+		 * Wake up the thread heading the output wait queue if
+		 * we freed enough room to post its message.
+		 */
+		ring->unblock_output(ring);
+		evl_schedule();
+		goto out;
+	wait:
+		if (len > ring->bufsz)
+			return -EINVAL;
+		/*
+		 * Check whether writers are already waiting for
+		 * sending data, while we are about to wait for
+		 * receiving some. In such a case, we have a
+		 * pathological use of the buffer. We must allow for a
+		 * short read to prevent a deadlock.
+		 */
+		if (ring->fillsz > 0 && ring->in_output_contention(ring)) {
+			len = ring->fillsz;
+			goto retry;
+		}
+
+		xnlock_put_irqrestore(&nklock, flags);
+
+		ret = ring->wait_input(ring, len);
+		if (unlikely(ret))
+			return ret;
+
+		xnlock_get_irqsave(&nklock, flags);
+	}
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+
+static ssize_t do_xbuf_write(struct xbuf_ring *ring,
+			     struct xbuf_wdesc *wd, int f_flags)
+{
+	ssize_t len, ret, wbytes, n;
+	unsigned int wroff, avail;
+	unsigned long flags;
+	int xret;
+
+	len = wd->count;
+	if (len == 0)
+		return 0;
+
+	if (ring->bufsz == 0)
+		return -ENOBUFS;
+
+	wd->buf_ptr = wd->buf;
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	for (;;) {
+		/*
+		 * No short or scattered writes: we should write the
+		 * entire message atomically or block.
+		 */
+		avail = ring->fillsz + ring->wrrsvd;
+		if (avail + len > ring->bufsz)
+			goto wait;
+
+		/* Reserve a write slot into the circular buffer. */
+		wroff = ring->wroff;
+		ring->wroff = (wroff + len) % ring->bufsz;
+		ring->wrrsvd += len;
+		wbytes = ret = len;
+
+		do {
+			if (wroff + wbytes > ring->bufsz)
+				n = ring->bufsz - wroff;
+			else
+				n = wbytes;
+
+			/*
+			 * We have to drop the lock while reading in
+			 * data, but we can't rollback on bad read
+			 * from user because some other thread might
+			 * have populated the memory ahead of our
+			 * write slot already: bluntly clear the
+			 * unavailable bytes on copy error.
+			 */
+			xnlock_put_irqrestore(&nklock, flags);
+			xret = wd->xfer(ring->bufmem + wroff, wd, n);
+			xnlock_get_irqsave(&nklock, flags);
+			if (xret) {
+				memset(ring->bufmem + wroff + n - xret, 0, xret);
+				ret = -EFAULT;
+				break;
+			}
+
+			wd->buf_ptr += n;
+			wbytes -= n;
+			wroff = (wroff + n) % ring->bufsz;
+		} while (wbytes > 0);
+
+		ring->fillsz += len;
+		ring->wrrsvd -= len;
+
+		if (ring->fillsz == len)
+			/* -> readable */
+			ring->signal_pollable(ring, POLLIN|POLLRDNORM);
+
+		if (ring->fillsz == ring->bufsz)
+			/* non-writable */
+			ring->clear_pollable(ring, POLLOUT|POLLWRNORM);
+
+		ring->signal_input(ring);
+		evl_schedule();
+		goto out;
+	wait:
+		if (f_flags & O_NONBLOCK) {
+			ret = -EAGAIN;
+			break;
+		}
+
+		xnlock_put_irqrestore(&nklock, flags);
+
+		ret = ring->wait_output(ring, len);
+		if (unlikely(ret))
+			return ret;
+
+		xnlock_get_irqsave(&nklock, flags);
+	}
+out:
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ret;
+}
+
+static int inbound_wait_input(struct xbuf_ring *ring, size_t len)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, ibnd.ring);
+
+	return wait_event_interruptible(xbuf->ibnd.i_event,
+					ring->fillsz >= len);
+}
+
+static void resume_inband_reader(struct irq_work *work)
+{
+	struct evl_xbuf *xbuf = container_of(work, struct evl_xbuf, ibnd.irq_work);
+
+	wake_up(&xbuf->ibnd.i_event);
+}
+
+static void inbound_signal_input(struct xbuf_ring *ring)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, ibnd.ring);
+
+	irq_work_queue(&xbuf->ibnd.irq_work);
+}
+
+static int inbound_wait_output(struct xbuf_ring *ring, size_t len)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, ibnd.ring);
+	struct evl_thread *curr = evl_current_thread();
+	struct xbuf_wait_data wait;
+
+	wait.len = len;
+	curr->wait_data = &wait;
+
+	return evl_wait_flag(&xbuf->ibnd.o_event);
+}
+
+static void inbound_unblock_output(struct xbuf_ring *ring)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, ibnd.ring);
+	struct evl_thread *waiter;
+	struct xbuf_wait_data *wc;
+
+	waiter = evl_wait_flag_head(&xbuf->ibnd.o_event);
+	if (waiter == NULL)
+		return;
+
+	wc = waiter->wait_data;
+	if (wc->len + ring->fillsz <= ring->bufsz)
+		evl_raise_flag(&xbuf->ibnd.o_event); /* Implicit resched. */
+}
+
+static bool inbound_output_contention(struct xbuf_ring *ring)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, ibnd.ring);
+
+	return !!evl_wait_flag_head(&xbuf->ibnd.o_event);
+}
+
+static void inbound_signal_pollable(struct xbuf_ring *ring, int events)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, ibnd.ring);
+
+	if (events & POLLOUT)
+		evl_signal_poll_events(&xbuf->poll_head, events);
+}
+
+static void inbound_clear_pollable(struct xbuf_ring *ring, int events)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, ibnd.ring);
+
+	if (events & POLLOUT)
+		evl_clear_poll_events(&xbuf->poll_head, events);
+}
+
+static ssize_t xbuf_read(struct file *filp, char __user *u_buf,
+			 size_t count, loff_t *ppos)
+{
+	struct evl_xbuf *xbuf = element_of(filp, struct evl_xbuf);
+	struct xbuf_rdesc rd = {
+		.buf = u_buf,
+		.count = count,
+		.xfer = write_to_user,
+	};
+
+	return do_xbuf_read(&xbuf->ibnd.ring, &rd, filp->f_flags);
+}
+
+static ssize_t xbuf_write(struct file *filp, const char __user *u_buf,
+			  size_t count, loff_t *ppos)
+{
+	struct evl_xbuf *xbuf = element_of(filp, struct evl_xbuf);
+	struct xbuf_wdesc wd = {
+		.buf = u_buf,
+		.count = count,
+		.xfer = read_from_user,
+	};
+
+	return do_xbuf_write(&xbuf->obnd.ring, &wd, filp->f_flags);
+}
+
+static long xbuf_ioctl(struct file *filp,
+		       unsigned int cmd, unsigned long arg)
+{
+	return -ENOTTY;
+}
+
+static __poll_t xbuf_poll(struct file *filp, poll_table *wait)
+{
+	struct evl_xbuf *xbuf = element_of(filp, struct evl_xbuf);
+	unsigned long flags;
+	__poll_t ready = 0;
+
+	poll_wait(filp, &xbuf->ibnd.i_event, wait);
+	poll_wait(filp, &xbuf->obnd.o_event, wait);
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (xbuf->ibnd.ring.fillsz > 0)
+		ready |= POLLIN|POLLRDNORM;
+
+	if (xbuf->obnd.ring.fillsz < xbuf->obnd.ring.bufsz)
+		ready |= POLLOUT|POLLWRNORM;
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ready;
+}
+
+static long xbuf_oob_ioctl(struct file *filp,
+			   unsigned int cmd, unsigned long arg)
+{
+	return -ENOTTY;
+}
+
+static int outbound_wait_input(struct xbuf_ring *ring, size_t len)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, obnd.ring);
+	struct evl_thread *curr = evl_current_thread();
+	struct xbuf_wait_data wait;
+
+	wait.len = len;
+	curr->wait_data = &wait;
+
+	return evl_wait_flag(&xbuf->obnd.i_event);
+}
+
+static void outbound_signal_input(struct xbuf_ring *ring)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, obnd.ring);
+	struct evl_thread *waiter;
+	struct xbuf_wait_data *wc;
+
+	waiter = evl_wait_flag_head(&xbuf->obnd.i_event);
+	if (waiter == NULL)
+		return;
+
+	wc = waiter->wait_data;
+	if (wc->len <= ring->fillsz)
+		evl_raise_flag(&xbuf->obnd.i_event); /* Implicit resched. */
+}
+
+static int outbound_wait_output(struct xbuf_ring *ring, size_t len)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, obnd.ring);
+
+	return wait_event_interruptible(xbuf->obnd.o_event,
+				  ring->fillsz + len <= ring->bufsz);
+}
+
+static void resume_inband_writer(struct irq_work *work)
+{
+	struct evl_xbuf *xbuf = container_of(work, struct evl_xbuf, obnd.irq_work);
+
+	wake_up(&xbuf->obnd.o_event);
+}
+
+static void outbound_unblock_output(struct xbuf_ring *ring)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, obnd.ring);
+
+	irq_work_queue(&xbuf->obnd.irq_work);
+}
+
+static bool outbound_output_contention(struct xbuf_ring *ring)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, obnd.ring);
+
+	return wq_has_sleeper(&xbuf->obnd.o_event);
+}
+
+static void outbound_signal_pollable(struct xbuf_ring *ring, int events)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, obnd.ring);
+
+	if (events & POLLIN)
+		evl_signal_poll_events(&xbuf->poll_head, events);
+}
+
+static void outbound_clear_pollable(struct xbuf_ring *ring, int events)
+{
+	struct evl_xbuf *xbuf = container_of(ring, struct evl_xbuf, obnd.ring);
+
+	if (events & POLLIN)
+		evl_clear_poll_events(&xbuf->poll_head, events);
+}
+
+static ssize_t xbuf_oob_read(struct file *filp,
+			     char __user *u_buf, size_t count)
+{
+	struct evl_xbuf *xbuf = element_of(filp, struct evl_xbuf);
+	struct xbuf_rdesc rd = {
+		.buf = u_buf,
+		.count = count,
+		.xfer = write_to_user,
+	};
+
+	return do_xbuf_read(&xbuf->obnd.ring, &rd, filp->f_flags);
+}
+
+static ssize_t xbuf_oob_write(struct file *filp,
+			      const char __user *u_buf, size_t count)
+{
+	struct evl_xbuf *xbuf = element_of(filp, struct evl_xbuf);
+	struct xbuf_wdesc wd = {
+		.buf = u_buf,
+		.count = count,
+		.xfer = read_from_user,
+	};
+
+	return do_xbuf_write(&xbuf->ibnd.ring, &wd, filp->f_flags);
+}
+
+static __poll_t xbuf_oob_poll(struct file *filp,
+			      struct oob_poll_wait *wait)
+{
+	struct evl_xbuf *xbuf = element_of(filp, struct evl_xbuf);
+	unsigned long flags;
+	__poll_t ready = 0;
+
+	evl_poll_watch(&xbuf->poll_head, wait);
+
+	xnlock_get_irqsave(&nklock, flags);
+
+	if (xbuf->obnd.ring.fillsz > 0)
+		ready |= POLLIN|POLLRDNORM;
+
+	if (xbuf->ibnd.ring.fillsz < xbuf->ibnd.ring.bufsz)
+		ready |= POLLOUT|POLLWRNORM;
+
+	xnlock_put_irqrestore(&nklock, flags);
+
+	return ready;
+}
+
+static const struct file_operations xbuf_fops = {
+	.open		= evl_open_element,
+	.release	= evl_close_element,
+	.unlocked_ioctl	= xbuf_ioctl,
+	.read		= xbuf_read,
+	.write		= xbuf_write,
+	.poll		= xbuf_poll,
+	.oob_ioctl	= xbuf_oob_ioctl,
+	.oob_read	= xbuf_oob_read,
+	.oob_write	= xbuf_oob_write,
+	.oob_poll	= xbuf_oob_poll,
+};
+
+struct evl_xbuf *evl_get_xbuf(int efd, struct evl_file **sfilpp)
+{
+	struct evl_file *sfilp = evl_get_file(efd);
+
+	if (sfilp && sfilp->filp->f_op == &xbuf_fops) {
+		*sfilpp = sfilp;
+		return element_of(sfilp->filp, struct evl_xbuf);
+	}
+
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(evl_get_xbuf);
+
+void evl_put_xbuf(struct evl_file *sfilp)
+{
+	evl_put_file(sfilp);
+}
+EXPORT_SYMBOL_GPL(evl_put_xbuf);
+
+ssize_t evl_read_xbuf(struct evl_xbuf *xbuf, void *buf,
+		      size_t count, int f_flags)
+{
+	struct xbuf_rdesc rd = {
+		.buf = buf,
+		.count = count,
+		.xfer = write_to_kernel,
+	};
+
+	if (!(f_flags & O_NONBLOCK) && evl_cannot_block())
+		return -EPERM;
+
+	return do_xbuf_read(&xbuf->obnd.ring, &rd, f_flags);
+}
+EXPORT_SYMBOL_GPL(evl_read_xbuf);
+
+ssize_t evl_write_xbuf(struct evl_xbuf *xbuf, const void *buf,
+		       size_t count, int f_flags)
+{
+	struct xbuf_wdesc wd = {
+		.buf = buf,
+		.count = count,
+		.xfer = read_from_kernel,
+	};
+
+	if (!(f_flags & O_NONBLOCK) && evl_cannot_block())
+		return -EPERM;
+
+	return do_xbuf_write(&xbuf->ibnd.ring, &wd, f_flags);
+}
+EXPORT_SYMBOL_GPL(evl_write_xbuf);
+
+static struct evl_element *
+xbuf_factory_build(struct evl_factory *fac, const char *name,
+		   void __user *u_attrs, u32 *state_offp)
+{
+	void *i_bufmem = NULL, *o_bufmem = NULL;
+	struct evl_xbuf_attrs attrs;
+	struct evl_xbuf *xbuf;
+	int ret;
+
+	ret = copy_from_user(&attrs, u_attrs, sizeof(attrs));
+	if (ret)
+		return ERR_PTR(-EFAULT);
+
+	/* LART */
+	if ((attrs.i_bufsz == 0 && attrs.o_bufsz == 0) ||
+	    order_base_2(attrs.i_bufsz) > 30 ||
+	    order_base_2(attrs.o_bufsz) > 30)
+		return ERR_PTR(-EINVAL);
+
+	xbuf = kzalloc(sizeof(*xbuf), GFP_KERNEL);
+	if (xbuf == NULL)
+		return ERR_PTR(-ENOMEM);
+
+	if (attrs.i_bufsz > 0) {
+		i_bufmem = kzalloc(attrs.i_bufsz, GFP_KERNEL);
+		if (i_bufmem == NULL) {
+			ret = -ENOMEM;
+			goto fail_ibufmem;
+		}
+	}
+
+	if (attrs.o_bufsz > 0) {
+		o_bufmem = kzalloc(attrs.o_bufsz, GFP_KERNEL);
+		if (o_bufmem == NULL) {
+			ret = -ENOMEM;
+			goto fail_obufmem;
+		}
+	}
+
+	ret = evl_init_element(&xbuf->element, &evl_xbuf_factory);
+	if (ret)
+		goto fail_element;
+
+	/* Inbound traffic: oob_write() -> read(). */
+	init_waitqueue_head(&xbuf->ibnd.i_event);
+	evl_init_flag(&xbuf->ibnd.o_event);
+	init_irq_work(&xbuf->ibnd.irq_work, resume_inband_reader);
+	xbuf->ibnd.ring.bufmem = i_bufmem;
+	xbuf->ibnd.ring.bufsz = attrs.i_bufsz;
+	xbuf->ibnd.ring.wait_input = inbound_wait_input;
+	xbuf->ibnd.ring.signal_input = inbound_signal_input;
+	xbuf->ibnd.ring.wait_output = inbound_wait_output;
+	xbuf->ibnd.ring.unblock_output = inbound_unblock_output;
+	xbuf->ibnd.ring.in_output_contention = inbound_output_contention;
+	xbuf->ibnd.ring.signal_pollable = inbound_signal_pollable;
+	xbuf->ibnd.ring.clear_pollable = inbound_clear_pollable;
+
+	/* Outbound traffic: write() -> oob_read(). */
+	evl_init_flag(&xbuf->obnd.i_event);
+	init_waitqueue_head(&xbuf->obnd.o_event);
+	init_irq_work(&xbuf->obnd.irq_work, resume_inband_writer);
+	xbuf->obnd.ring.bufmem = o_bufmem;
+	xbuf->obnd.ring.bufsz = attrs.o_bufsz;
+	xbuf->obnd.ring.wait_input = outbound_wait_input;
+	xbuf->obnd.ring.signal_input = outbound_signal_input;
+	xbuf->obnd.ring.wait_output = outbound_wait_output;
+	xbuf->obnd.ring.unblock_output = outbound_unblock_output;
+	xbuf->obnd.ring.in_output_contention = outbound_output_contention;
+	xbuf->obnd.ring.signal_pollable = outbound_signal_pollable;
+	xbuf->obnd.ring.clear_pollable = outbound_clear_pollable;
+
+	evl_init_poll_head(&xbuf->poll_head);
+
+	return &xbuf->element;
+
+fail_element:
+	if (o_bufmem)
+		kfree(o_bufmem);
+fail_obufmem:
+	if (i_bufmem)
+		kfree(i_bufmem);
+fail_ibufmem:
+	kfree(xbuf);
+
+	return ERR_PTR(ret);
+}
+
+static void xbuf_factory_dispose(struct evl_element *e)
+{
+	struct evl_xbuf *xbuf;
+
+	xbuf = container_of(e, struct evl_xbuf, element);
+
+	evl_destroy_flag(&xbuf->obnd.i_event);
+	evl_destroy_flag(&xbuf->ibnd.o_event);
+	evl_destroy_element(&xbuf->element);
+	if (xbuf->ibnd.ring.bufmem)
+		kfree(xbuf->ibnd.ring.bufmem);
+	if (xbuf->obnd.ring.bufmem)
+		kfree(xbuf->obnd.ring.bufmem);
+	kfree_rcu(xbuf, element.rcu);
+}
+
+static ssize_t rings_show(struct device *dev,
+			  struct device_attribute *attr,
+			  char *buf)
+{
+	struct evl_xbuf *xbuf;
+	ssize_t ret;
+
+	xbuf = evl_get_element_by_dev(dev, struct evl_xbuf);
+
+	ret = snprintf(buf, PAGE_SIZE, "%zu %zu %zu %zu\n",
+		       xbuf->ibnd.ring.fillsz,
+		       xbuf->ibnd.ring.bufsz,
+		       xbuf->obnd.ring.fillsz,
+		       xbuf->obnd.ring.bufsz);
+
+	evl_put_element(&xbuf->element);
+
+	return ret;
+}
+static DEVICE_ATTR_RO(rings);
+
+static struct attribute *xbuf_attrs[] = {
+	&dev_attr_rings.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(xbuf);
+
+struct evl_factory evl_xbuf_factory = {
+	.name	=	"xbuf",
+	.fops	=	&xbuf_fops,
+	.build =	xbuf_factory_build,
+	.dispose =	xbuf_factory_dispose,
+	.nrdev	=	CONFIG_EVENLESS_NR_XBUFS,
+	.attrs	=	xbuf_groups,
+	.flags	=	EVL_FACTORY_CLONE,
+};
-- 
2.16.4

