From 8db2386b7a4c359f95f65a0032f1229eb73e5f09 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Thu, 31 Jan 2019 18:44:29 +0100
Subject: [PATCH] evl/lock: drop legacy debug

---
 include/evenless/assert.h    |   9 ++--
 include/evenless/lock.h      | 125 +++++++------------------------------------
 include/evenless/sched.h     |   4 +-
 kernel/evenless/Kconfig      |  13 -----
 kernel/evenless/lock.c       |  80 +++------------------------
 kernel/evenless/sched/core.c |   4 +-
 6 files changed, 33 insertions(+), 202 deletions(-)

diff --git a/include/evenless/assert.h b/include/evenless/assert.h
index af06494b898..7dd97051453 100644
--- a/include/evenless/assert.h
+++ b/include/evenless/assert.h
@@ -34,10 +34,9 @@
 
 #define oob_context_only()	EVL_WARN_ON_ONCE(CONTEXT, running_inband())
 #define inband_context_only()	EVL_WARN_ON_ONCE(CONTEXT, !running_inband())
-#ifdef CONFIG_EVENLESS_DEBUG_LOCKING
-#define atomic_only()		EVL_WARN_ON_ONCE(CONTEXT, !(xnlock_is_owner(&nklock) && hard_irqs_disabled()))
-#else
-#define atomic_only()		EVL_WARN_ON_ONCE(CONTEXT, !hard_irqs_disabled())
-#endif
+
+/* TEMP: needed until we have gotten rid of the infamous nklock. */
+#define atomic_only()	WARN_ON_ONCE(!(xnlock_is_owner(&nklock) && hard_irqs_disabled()))
+#define no_ugly_lock()	WARN_ON_ONCE(xnlock_is_owner(&nklock))
 
 #endif /* !_EVENLESS_ASSERT_H */
diff --git a/include/evenless/lock.h b/include/evenless/lock.h
index 28825c887e2..a5ee5642552 100644
--- a/include/evenless/lock.h
+++ b/include/evenless/lock.h
@@ -24,57 +24,6 @@
 #define splexit(x)  oob_irq_restore(x)
 #endif /* !CONFIG_SMP */
 
-#ifdef CONFIG_EVENLESS_DEBUG_LOCKING
-
-struct xnlock {
-	unsigned owner;
-	arch_spinlock_t alock;
-	const char *file;
-	const char *function;
-	unsigned int line;
-	int cpu;
-	ktime_t spin_time;
-	ktime_t lock_date;
-};
-
-struct xnlockinfo {
-	ktime_t spin_time;
-	ktime_t lock_time;
-	const char *file;
-	const char *function;
-	unsigned int line;
-};
-
-#define XNARCH_LOCK_UNLOCKED (struct xnlock) {	\
-	~0,					\
-	__ARCH_SPIN_LOCK_UNLOCKED,		\
-	NULL,					\
-	NULL,					\
-	0,					\
-	-1,					\
-	0LL,					\
-	0LL,					\
-}
-
-#define T_LOCK_DBG_CONTEXT		, __FILE__, __LINE__, __FUNCTION__
-#define T_LOCK_DBG_CONTEXT_ARGS					\
-	, const char *file, int line, const char *function
-#define T_LOCK_DBG_PASS_CONTEXT		, file, line, function
-
-void xnlock_dbg_prepare_acquire(ktime_t *start);
-void xnlock_dbg_prepare_spin(unsigned int *spin_limit);
-void xnlock_dbg_acquired(struct xnlock *lock, int cpu,
-			 ktime_t *start,
-			 const char *file, int line,
-			 const char *function);
-int xnlock_dbg_release(struct xnlock *lock,
-			 const char *file, int line,
-			 const char *function);
-
-DECLARE_PER_CPU(struct xnlockinfo, xnlock_stats);
-
-#else /* !CONFIG_EVENLESS_DEBUG_LOCKING */
-
 struct xnlock {
 	unsigned owner;
 	arch_spinlock_t alock;
@@ -86,41 +35,14 @@ struct xnlock {
 		__ARCH_SPIN_LOCK_UNLOCKED,	\
 	}
 
-#define T_LOCK_DBG_CONTEXT
-#define T_LOCK_DBG_CONTEXT_ARGS
-#define T_LOCK_DBG_PASS_CONTEXT
-
-static inline
-void xnlock_dbg_prepare_acquire(ktime_t *start)
-{
-}
-
-static inline
-void xnlock_dbg_prepare_spin(unsigned int *spin_limit)
-{
-}
+#if defined(CONFIG_SMP)
 
-static inline void
-xnlock_dbg_acquired(struct xnlock *lock, int cpu,
-		    ktime_t *start)
-{
-}
-
-static inline int xnlock_dbg_release(struct xnlock *lock)
-{
-	return 0;
-}
-
-#endif /* !CONFIG_EVENLESS_DEBUG_LOCKING */
-
-#if defined(CONFIG_SMP) || defined(CONFIG_EVENLESS_DEBUG_LOCKING)
-
-#define xnlock_get(lock)		__xnlock_get(lock  T_LOCK_DBG_CONTEXT)
-#define xnlock_put(lock)		__xnlock_put(lock  T_LOCK_DBG_CONTEXT)
+#define xnlock_get(lock)		__xnlock_get(lock)
+#define xnlock_put(lock)		__xnlock_put(lock)
 #define xnlock_get_irqsave(lock,x) \
-	((x) = __xnlock_get_irqsave(lock  T_LOCK_DBG_CONTEXT))
+	((x) = __xnlock_get_irqsave(lock))
 #define xnlock_put_irqrestore(lock,x) \
-	__xnlock_put_irqrestore(lock,x  T_LOCK_DBG_CONTEXT)
+	__xnlock_put_irqrestore(lock,x)
 #define xnlock_clear_irqoff(lock)	xnlock_put_irqrestore(lock, 1)
 #define xnlock_clear_irqon(lock)	xnlock_put_irqrestore(lock, 0)
 
@@ -134,55 +56,46 @@ static inline void xnlock_init (struct xnlock *lock)
 #define DEFINE_XNLOCK(lock)		struct xnlock lock = XNARCH_LOCK_UNLOCKED
 #define DEFINE_PRIVATE_XNLOCK(lock)	static DEFINE_XNLOCK(lock)
 
-static inline int ____xnlock_get(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+static inline int ____xnlock_get(struct xnlock *lock)
 {
 	int cpu = raw_smp_processor_id();
-	ktime_t start;
 
 	if (lock->owner == cpu)
 		return 2;
 
-	xnlock_dbg_prepare_acquire(&start);
-
 	arch_spin_lock(&lock->alock);
 	lock->owner = cpu;
 
-	xnlock_dbg_acquired(lock, cpu, &start /*, */ T_LOCK_DBG_PASS_CONTEXT);
-
 	return 0;
 }
 
-static inline void ____xnlock_put(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+static inline void ____xnlock_put(struct xnlock *lock)
 {
-	if (xnlock_dbg_release(lock /*, */ T_LOCK_DBG_PASS_CONTEXT))
-		return;
-
 	lock->owner = ~0U;
 	arch_spin_unlock(&lock->alock);
 }
 
-int ___xnlock_get(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS);
+int ___xnlock_get(struct xnlock *lock);
 
-void ___xnlock_put(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS);
+void ___xnlock_put(struct xnlock *lock);
 
 static inline unsigned long
-__xnlock_get_irqsave(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+__xnlock_get_irqsave(struct xnlock *lock)
 {
 	unsigned long flags;
 
 	splhigh(flags);
 
-	flags |= ___xnlock_get(lock /*, */ T_LOCK_DBG_PASS_CONTEXT);
+	flags |= ___xnlock_get(lock);
 
 	return flags;
 }
 
-static inline void __xnlock_put_irqrestore(struct xnlock *lock, unsigned long flags
-					   /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+static inline void __xnlock_put_irqrestore(struct xnlock *lock, unsigned long flags)
 {
 	/* Only release the lock if we didn't take it recursively. */
 	if (!(flags & 2))
-		___xnlock_put(lock /*, */ T_LOCK_DBG_PASS_CONTEXT);
+		___xnlock_put(lock);
 
 	splexit(flags & 1);
 }
@@ -192,17 +105,17 @@ static inline int xnlock_is_owner(struct xnlock *lock)
 	return lock->owner == raw_smp_processor_id();
 }
 
-static inline int __xnlock_get(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+static inline int __xnlock_get(struct xnlock *lock)
 {
-	return ___xnlock_get(lock /* , */ T_LOCK_DBG_PASS_CONTEXT);
+	return ___xnlock_get(lock);
 }
 
-static inline void __xnlock_put(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+static inline void __xnlock_put(struct xnlock *lock)
 {
-	___xnlock_put(lock /*, */ T_LOCK_DBG_PASS_CONTEXT);
+	___xnlock_put(lock);
 }
 
-#else /* !(CONFIG_SMP || CONFIG_EVENLESS_DEBUG_LOCKING) */
+#else /* !CONFIG_SMP */
 
 #define xnlock_init(lock)		do { } while(0)
 #define xnlock_get(lock)		do { } while(0)
@@ -218,7 +131,7 @@ static inline void __xnlock_put(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_AR
 #define DEFINE_XNLOCK(lock)
 #define DEFINE_PRIVATE_XNLOCK(lock)
 
-#endif /* !(CONFIG_SMP || CONFIG_EVENLESS_DEBUG_LOCKING) */
+#endif /* !CONFIG_SMP */
 
 DECLARE_EXTERN_XNLOCK(nklock);
 
diff --git a/include/evenless/sched.h b/include/evenless/sched.h
index 1a05a6af8a8..2241652e863 100644
--- a/include/evenless/sched.h
+++ b/include/evenless/sched.h
@@ -309,7 +309,7 @@ static inline void __evl_enable_preempt(void)
 		evl_schedule();
 }
 
-#ifdef CONFIG_EVENLESS_DEBUG_LOCKING
+#ifdef CONFIG_EVENLESS_DEBUG_CORE
 
 void evl_disable_preempt(void);
 void evl_enable_preempt(void);
@@ -326,7 +326,7 @@ static inline void evl_enable_preempt(void)
 	__evl_enable_preempt();
 }
 
-#endif /* !CONFIG_EVENLESS_DEBUG_LOCKING */
+#endif
 
 static inline bool evl_in_irq(void)
 {
diff --git a/kernel/evenless/Kconfig b/kernel/evenless/Kconfig
index 149e0e485b3..edf24514296 100644
--- a/kernel/evenless/Kconfig
+++ b/kernel/evenless/Kconfig
@@ -231,19 +231,6 @@ config EVENLESS_DEBUG_MEMORY
 	  kernel. This option may induce significant overhead with large
 	  heaps.
 
-config EVENLESS_DEBUG_LOCKING
-	bool "Spinlock debugging support"
-	default n
-	help
-	  This option activates runtime assertions, and measurements
-	  of spinlocks spinning time and duration in the Evenless
-	  kernel. It helps finding latency spots due to interrupt
-	  masked sections. Statistics about the longest masked section
-	  can be found in /proc/evenless/debug/lock.
-
-	  This option may induce a measurable overhead on low end
-	  machines.
-
 config EVENLESS_DEBUG_USER
 	bool "User consistency checks"
 	help
diff --git a/kernel/evenless/lock.c b/kernel/evenless/lock.c
index 56e9dd0fa16..2150252c65e 100644
--- a/kernel/evenless/lock.c
+++ b/kernel/evenless/lock.c
@@ -16,87 +16,19 @@
 #include <evenless/clock.h>
 
 DEFINE_XNLOCK(nklock);
-#if defined(CONFIG_SMP) || defined(CONFIG_EVENLESS_DEBUG_LOCKING)
+#if defined(CONFIG_SMP)
 EXPORT_SYMBOL_GPL(nklock);
 
-int ___xnlock_get(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+int ___xnlock_get(struct xnlock *lock)
 {
-	return ____xnlock_get(lock /* , */ T_LOCK_DBG_PASS_CONTEXT);
+	return ____xnlock_get(lock);
 }
 EXPORT_SYMBOL_GPL(___xnlock_get);
 
-void ___xnlock_put(struct xnlock *lock /*, */ T_LOCK_DBG_CONTEXT_ARGS)
+void ___xnlock_put(struct xnlock *lock)
 {
-	____xnlock_put(lock /* , */ T_LOCK_DBG_PASS_CONTEXT);
+	____xnlock_put(lock);
 }
 EXPORT_SYMBOL_GPL(___xnlock_put);
-#endif /* CONFIG_SMP || CONFIG_EVENLESS_DEBUG_LOCKING */
 
-#ifdef CONFIG_EVENLESS_DEBUG_LOCKING
-
-DEFINE_PER_CPU(struct xnlockinfo, xnlock_stats);
-EXPORT_PER_CPU_SYMBOL_GPL(xnlock_stats);
-
-void xnlock_dbg_prepare_acquire(ktime_t *start)
-{
-	*start = evl_read_clock(&evl_mono_clock);
-}
-EXPORT_SYMBOL_GPL(xnlock_dbg_prepare_acquire);
-
-void xnlock_dbg_acquired(struct xnlock *lock, int cpu, ktime_t *start,
-			const char *file, int line, const char *function)
-{
-	lock->lock_date = *start;
-	lock->spin_time = ktime_sub(evl_read_clock(&evl_mono_clock), *start);
-	lock->file = file;
-	lock->function = function;
-	lock->line = line;
-	lock->cpu = cpu;
-}
-EXPORT_SYMBOL_GPL(xnlock_dbg_acquired);
-
-int xnlock_dbg_release(struct xnlock *lock,
-		const char *file, int line, const char *function)
-{
-	struct xnlockinfo *stats;
-	ktime_t lock_time;
-	int cpu;
-
-	lock_time = ktime_sub(evl_read_clock(&evl_mono_clock), lock->lock_date);
-	cpu = raw_smp_processor_id();
-	stats = &per_cpu(xnlock_stats, cpu);
-
-	if (lock->file == NULL) {
-		lock->file = "??";
-		lock->line = 0;
-		lock->function = "invalid";
-	}
-
-	if (unlikely(lock->owner != cpu)) {
-		printk(EVL_ERR "lock %p already unlocked on CPU #%d\n"
-			"          last owner = %s:%u (%s(), CPU #%d)\n",
-			lock, cpu, lock->file, lock->line, lock->function,
-			lock->cpu);
-		show_stack(NULL,NULL);
-		return 1;
-	}
-
-	/* File that we released it. */
-	lock->cpu = -lock->cpu;
-	lock->file = file;
-	lock->line = line;
-	lock->function = function;
-
-	if (lock_time > stats->lock_time) {
-		stats->lock_time = lock_time;
-		stats->spin_time = lock->spin_time;
-		stats->file = lock->file;
-		stats->function = lock->function;
-		stats->line = lock->line;
-	}
-
-	return 0;
-}
-EXPORT_SYMBOL_GPL(xnlock_dbg_release);
-
-#endif
+#endif /* CONFIG_SMP */
diff --git a/kernel/evenless/sched/core.c b/kernel/evenless/sched/core.c
index c5a999a8a6b..1bd162d0d33 100644
--- a/kernel/evenless/sched/core.c
+++ b/kernel/evenless/sched/core.c
@@ -271,7 +271,7 @@ struct evl_thread *evl_pick_thread(struct evl_rq *rq)
 	return NULL; /* Never executed because of the idle class. */
 }
 
-#ifdef CONFIG_EVENLESS_DEBUG_LOCKING
+#ifdef CONFIG_EVENLESS_DEBUG_CORE
 
 void evl_disable_preempt(void)
 {
@@ -287,7 +287,7 @@ void evl_enable_preempt(void)
 }
 EXPORT_SYMBOL(evl_enable_preempt);
 
-#endif /* CONFIG_EVENLESS_DEBUG_LOCKING */
+#endif /* CONFIG_EVENLESS_DEBUG_CORE */
 
 /* nklock locked, interrupts off. */
 void evl_putback_thread(struct evl_thread *thread)
-- 
2.16.4

