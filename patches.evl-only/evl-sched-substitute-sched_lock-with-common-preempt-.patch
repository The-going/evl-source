From 5a76c4ef7997e3de62f0c71e652e3fdd076b7c35 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Sun, 27 Jan 2019 17:45:06 +0100
Subject: [PATCH] evl/sched: substitute sched_lock with common preempt count
 protection

The legacy scheduler lock implementation inherited from Xenomai is
terminally racy and slow, fix this with a stack-based counter like the
in-band kernel does.

At this chance, drop the requirement to apply a round-robin effect to
runnable threads on scheduling parameters change: this is weird POSIX
stuff which implementation was racy too, and makes no sense rt-wise
anyway.
---
 include/asm-generic/evenless/thread_info.h |  2 ++
 include/evenless/sched.h                   | 47 +++++++++++++++++-------------
 include/evenless/thread.h                  |  2 --
 kernel/evenless/sched/core.c               | 37 ++++-------------------
 kernel/evenless/sched/rt.c                 |  2 +-
 kernel/evenless/thread.c                   | 15 ----------
 6 files changed, 34 insertions(+), 71 deletions(-)

diff --git a/include/asm-generic/evenless/thread_info.h b/include/asm-generic/evenless/thread_info.h
index 233d737a300..441e721fef6 100644
--- a/include/asm-generic/evenless/thread_info.h
+++ b/include/asm-generic/evenless/thread_info.h
@@ -6,12 +6,14 @@ struct evl_thread;
 
 struct oob_thread_state {
 	struct evl_thread *thread;
+	int preempt_count;
 };
 
 static inline
 void evl_init_thread_state(struct oob_thread_state *p)
 {
 	p->thread = NULL;
+	p->preempt_count = 0;
 }
 
 #endif /* !_ASM_GENERIC_EVENLESS_THREAD_INFO_H */
diff --git a/include/evenless/sched.h b/include/evenless/sched.h
index 5c2e19b5f31..78fada0ba29 100644
--- a/include/evenless/sched.h
+++ b/include/evenless/sched.h
@@ -292,19 +292,20 @@ static inline bool __evl_schedule(struct evl_rq *this_rq)
 	return (bool)run_oob_call((int (*)(void *))___evl_schedule, this_rq);
 }
 
-static inline bool evl_schedule(void)
+static inline int evl_preempt_count(void)
 {
-	struct evl_rq *this_rq = this_evl_rq();
+	return dovetail_current_state()->preempt_count;
+}
 
-	/*
-	 * Block rescheduling if either the current thread holds the
-	 * scheduler lock, or an interrupt context is active.
-	 */
-	smp_rmb();
-	if (unlikely(this_rq->curr->lock_count > 0))
-		return false;
+static inline void __evl_disable_preempt(void)
+{
+	dovetail_current_state()->preempt_count++;
+}
 
-	return __evl_schedule(this_rq);
+static inline void __evl_enable_preempt(void)
+{
+	if (--dovetail_current_state()->preempt_count == 0)
+		__evl_schedule(this_evl_rq());
 }
 
 #ifdef CONFIG_EVENLESS_DEBUG_LOCKING
@@ -316,24 +317,28 @@ void evl_enable_preempt(void);
 
 static inline void evl_disable_preempt(void)
 {
-	struct evl_rq *rq = this_evl_rq();
-	struct evl_thread *curr = rq->curr;
-
-	if (!(rq->lflags & RQ_IRQ))
-		curr->lock_count++;
+	__evl_disable_preempt();
 }
 
 static inline void evl_enable_preempt(void)
 {
-	struct evl_rq *rq = this_evl_rq();
-	struct evl_thread *curr = rq->curr;
-
-	if (!(rq->lflags & RQ_IRQ) && --curr->lock_count == 0)
-		evl_schedule();
+	__evl_enable_preempt();
 }
 
 #endif /* !CONFIG_EVENLESS_DEBUG_LOCKING */
 
+static inline bool evl_schedule(void)
+{
+	/*
+	 * Block rescheduling if either the current thread holds the
+	 * scheduler lock.
+	 */
+	if (unlikely(evl_preempt_count() > 0))
+		return false;
+
+	return __evl_schedule(this_evl_rq());
+}
+
 static inline bool evl_in_irq(void)
 {
 	return !!(this_evl_rq()->lflags & RQ_IRQ);
@@ -414,7 +419,7 @@ static inline void evl_sched_tick(struct evl_rq *rq)
 	if (sched_class == curr->base_class &&
 	    sched_class->sched_tick &&
 	    (curr->state & (EVL_THREAD_BLOCK_BITS|T_RRB)) == T_RRB &&
-	    curr->lock_count == 0)
+	    evl_preempt_count() == 0)
 		sched_class->sched_tick(rq);
 }
 
diff --git a/include/evenless/thread.h b/include/evenless/thread.h
index 25da9090ded..3e6207a218a 100644
--- a/include/evenless/thread.h
+++ b/include/evenless/thread.h
@@ -71,8 +71,6 @@ struct evl_thread {
 	 */
 	int wprio;
 
-	int lock_count;	/* Scheduler lock count. */
-
 	struct list_head rq_next;	/* evl_rq->policy.runqueue */
 	struct list_head syn_next;	/* evl_syn->wait_list */
 	struct list_head next;	/* evl_thread_list */
diff --git a/kernel/evenless/sched/core.c b/kernel/evenless/sched/core.c
index 73f71c0bd4d..a81783df277 100644
--- a/kernel/evenless/sched/core.c
+++ b/kernel/evenless/sched/core.c
@@ -244,7 +244,7 @@ struct evl_thread *evl_pick_thread(struct evl_rq *rq)
 		 * considered for the root thread which may never
 		 * defer scheduling.
 		 */
-		if (curr->lock_count > 0 && !(curr->state & T_ROOT)) {
+		if (evl_preempt_count() > 0 && !(curr->state & T_ROOT)) {
 			evl_set_self_resched(rq);
 			return curr;
 		}
@@ -278,42 +278,15 @@ struct evl_thread *evl_pick_thread(struct evl_rq *rq)
 
 void evl_disable_preempt(void)
 {
-	struct evl_rq *rq = this_evl_rq();
-	struct evl_thread *curr = rq->curr;
-
-	/* If RQ_IRQ is set, the scheduler is already locked. */
-
-	if (rq->lflags & RQ_IRQ)
-		return;
-
-	/*
-	 * The fast evl_current_thread() accessor carries the
-	 * relevant lock nesting count only if current runs in OOB
-	 * context. Otherwise, if the caller is running in-band,
-	 * we fall back to the root thread on the current rq, which
-	 * must be done with IRQs off to prevent CPU migration.
-	 * Either way, we don't need to grab the super lock.
-	 */
-	EVL_WARN_ON_ONCE(CORE, (curr->state & T_ROOT) &&
-			 !hard_irqs_disabled());
-
-	curr->lock_count++;
+	oob_context_only();
+	__evl_disable_preempt();
 }
 EXPORT_SYMBOL(evl_disable_preempt);
 
 void evl_enable_preempt(void)
 {
-	struct evl_rq *rq = this_evl_rq();
-	struct evl_thread *curr = rq->curr;
-
-	if (rq->lflags & RQ_IRQ)
-		return;
-
-	if (!EVL_ASSERT(CORE, curr->lock_count > 0))
-		return;
-
-	if (--curr->lock_count == 0)
-		evl_schedule();
+	oob_context_only();
+	__evl_enable_preempt();
 }
 EXPORT_SYMBOL(evl_enable_preempt);
 
diff --git a/kernel/evenless/sched/rt.c b/kernel/evenless/sched/rt.c
index be2307dc000..922501805d3 100644
--- a/kernel/evenless/sched/rt.c
+++ b/kernel/evenless/sched/rt.c
@@ -59,7 +59,7 @@ static void evl_rt_rotate(struct evl_rq *rq,
 	 */
 	if (thread != curr ||
 	    (!(curr->state & EVL_THREAD_BLOCK_BITS) &&
-	     curr->lock_count == 0))
+	     evl_preempt_count() == 0))
 		evl_putback_thread(thread);
 }
 
diff --git a/kernel/evenless/thread.c b/kernel/evenless/thread.c
index 14b8300943f..7d5913d46f0 100644
--- a/kernel/evenless/thread.c
+++ b/kernel/evenless/thread.c
@@ -161,7 +161,6 @@ int evl_init_thread(struct evl_thread *thread,
 	thread->wprio = EVL_IDLE_PRIO;
 	thread->cprio = EVL_IDLE_PRIO;
 	thread->bprio = EVL_IDLE_PRIO;
-	thread->lock_count = 0;
 	thread->rrperiod = EVL_INFINITE;
 	thread->wchan = NULL;
 	thread->wwake = NULL;
@@ -1337,20 +1336,6 @@ int __evl_set_thread_schedparam(struct evl_thread *thread,
 	if (old_wprio != new_wprio && thread->wchan &&
 	    (thread->wchan->status & EVL_SYN_PRIO))
 		evl_requeue_syn_waiter(thread);
-	/*
-	 * We should not move the thread at the end of its priority
-	 * group, if any of these conditions is true:
-	 *
-	 * - thread is not runnable;
-	 * - thread bears the ready bit which means that evl_set_thread_policy()
-	 * already reordered the run queue;
-	 * - thread currently holds the scheduler lock, so we don't want
-	 * any round-robin effect to take place;
-	 * - a priority boost is undergoing for this thread.
-	 */
-	if (!(thread->state & (EVL_THREAD_BLOCK_BITS|T_READY|T_BOOST)) &&
-	    thread->lock_count == 0)
-		evl_putback_thread(thread);
 
 	thread->info |= T_SCHEDP;
 	/* Ask the target thread to call back if in-band. */
-- 
2.16.4

