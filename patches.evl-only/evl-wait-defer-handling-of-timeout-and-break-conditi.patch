From da16e753edcbaa45441e8d224464c05ce6452e75 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Thu, 24 Oct 2019 18:18:03 +0200
Subject: [PATCH] evl/wait: defer handling of timeout and break conditions

Handling an aborted wait due to a timeout/break condition directly
from the context signaling the wakeup requires the waitqueue lock to
be held around the call to evl_wakeup_thread(), which leads to a
catch-22 situation: we have to ensure that the wait channel
(thread->wchan) is still valid for locking the waitqueue prior to
calling evl_wakeup_thread() as the abort_wait() handler requires, but
that would also require to serialize with evl_wakeup_thread() running
on (a) different core(s) targeting the same thread to make sure
thread->wchan does not vanish.

For instance, we might have a timeout triggered on the current CPU
while the condition we were waiting for is simulateously satisfied on
a remote one. This would end up with an ABBA locking issue for
enforcing this, between the waitqueue lock abort_wait() requires, and
the one protecting the runqueue state evl_wakeup_thread() takes.

To address this, defer such handling until the waiter resumes,
removing itself from the waitqueue upon detecting an aborted wait on
return from evl_schedule(), which lifts the requirement for the
signaling context to do so. Conversely, callers of evl_wake_up() must
now unlink the waiter explicitly instead of relying on the
abort_wait() callback fired by evl_wakeup_thread(). Careful
assumptions and checks in evl_wait_schedule() ensure that we cannot
lose or duplicate wakeup events.

As a result, drop the annoying abort_wait() callback which caused the
catch-22 situation by design.

This is crucial prep work for enabling per-waitqueue locking, getting
rid of the ugly lock in this layer.
---
 include/evl/mutex.h  |   4 --
 include/evl/thread.h |   3 --
 include/evl/wait.h   |  37 +++---------------
 kernel/evl/monitor.c |  12 +-----
 kernel/evl/mutex.c   | 104 +++++++++++++++++++++++++--------------------------
 kernel/evl/thread.c  |  18 +--------
 kernel/evl/wait.c    |  81 ++++++++++++++++++++++++++++++++++-----
 7 files changed, 130 insertions(+), 129 deletions(-)

diff --git a/include/evl/mutex.h b/include/evl/mutex.h
index 0a001a98835f..c1e4baa36ea4 100644
--- a/include/evl/mutex.h
+++ b/include/evl/mutex.h
@@ -72,9 +72,6 @@ void evl_commit_mutex_ceiling(struct evl_mutex *mutex);
 
 void evl_detect_boost_drop(struct evl_thread *owner);
 
-void evl_abort_mutex_wait(struct evl_thread *thread,
-			struct evl_wait_channel *wchan);
-
 void evl_reorder_mutex_wait(struct evl_thread *thread);
 
 void evl_drop_tracking_mutexes(struct evl_thread *thread);
@@ -94,7 +91,6 @@ struct evl_kmutex {
 			.clock = &evl_mono_clock,			\
 			.lock = __EVL_SPIN_LOCK_INITIALIZER((__name).lock), \
 			.wchan = {					\
-				.abort_wait = evl_abort_mutex_wait,	\
 				.reorder_wait = evl_reorder_mutex_wait,	\
 				.wait_list = LIST_HEAD_INIT((__name).mutex.wchan.wait_list), \
 			},						\
diff --git a/include/evl/thread.h b/include/evl/thread.h
index 9fa17c8c87b8..226b32f6f960 100644
--- a/include/evl/thread.h
+++ b/include/evl/thread.h
@@ -28,7 +28,6 @@
 
 #define EVL_THREAD_BLOCK_BITS   (T_SUSP|T_PEND|T_DELAY|T_WAIT|T_DORMANT|T_INBAND|T_HALT)
 #define EVL_THREAD_INFO_MASK	(T_RMID|T_TIMEO|T_BREAK|T_WAKEN|T_ROBBED|T_KICKED|T_BCAST)
-#define EVL_THREAD_WAKE_MASK	(T_RMID|T_TIMEO|T_BREAK|T_BCAST)
 
 struct evl_thread;
 struct evl_rq;
@@ -43,8 +42,6 @@ struct evl_init_thread_attr {
 };
 
 struct evl_wait_channel {
-	void (*abort_wait)(struct evl_thread *thread,
-			struct evl_wait_channel *wchan);
 	void (*reorder_wait)(struct evl_thread *thread);
 	struct list_head wait_list;
 };
diff --git a/include/evl/wait.h b/include/evl/wait.h
index 8408d559a644..4d6bef86e8a2 100644
--- a/include/evl/wait.h
+++ b/include/evl/wait.h
@@ -34,7 +34,6 @@ struct evl_wait_queue {
 		.clock = &evl_mono_clock,				\
 		.lock = __EVL_SPIN_LOCK_INITIALIZER((__name).lock),	\
 		.wchan = {						\
-			.abort_wait = evl_abort_wait,			\
 			.reorder_wait = evl_reorder_wait,		\
 			.wait_list = LIST_HEAD_INIT((__name).wchan.wait_list), \
 		},							\
@@ -51,25 +50,9 @@ struct evl_wait_queue {
 	list_for_each_entry_safe(__pos, __tmp,				\
 				&(__wq)->wchan.wait_list, wait_next)
 
-#define evl_wait_schedule()						\
-({									\
-	int __ret = 0, __info;						\
-									\
-	no_ugly_lock();							\
-	evl_schedule();							\
-	__info = evl_current()->info;					\
-	if (__info & T_RMID)						\
-		__ret = -EIDRM;						\
-	else if (__info & T_TIMEO)					\
-		__ret = -ETIMEDOUT;					\
-	else if (__info & T_BREAK)					\
-		__ret = -EINTR;						\
-	__ret;								\
-})
-
 #define evl_wait_event_timeout(__wq, __timeout, __timeout_mode, __cond)	\
 ({									\
-	int __ret = 0, __info = 0;					\
+	int __ret = 0, __bcast;						\
 	unsigned long __flags;						\
 									\
 	no_ugly_lock();							\
@@ -82,20 +65,13 @@ struct evl_wait_queue {
 				evl_add_wait_queue(__wq, __timeout,	\
 						__timeout_mode);	\
 				xnlock_put_irqrestore(&nklock, __flags); \
-				evl_schedule();				\
+				__ret = evl_wait_schedule();		\
+				__bcast = evl_current()->info & T_BCAST; \
 				xnlock_get_irqsave(&nklock, __flags);	\
-				__info = evl_current()->info;		\
-				__info &= EVL_THREAD_WAKE_MASK;		\
-			} while (!__info && !(__cond));			\
+			} while (!__ret && !__bcast && !(__cond));	\
 		}							\
 	}								\
 	xnlock_put_irqrestore(&nklock, __flags);			\
-	if (__info & T_BREAK)						\
-		__ret = -EINTR;						\
-	else if (__info & T_TIMEO)					\
-		__ret = -ETIMEDOUT;					\
-	else if (__info & T_RMID)					\
-		__ret = -EIDRM;						\
 	__ret;								\
 })
 
@@ -106,6 +82,8 @@ void evl_add_wait_queue(struct evl_wait_queue *wq,
 			ktime_t timeout,
 			enum evl_tmode timeout_mode);
 
+int evl_wait_schedule(void);
+
 static inline bool evl_wait_active(struct evl_wait_queue *wq)
 {
 	requires_ugly_lock();
@@ -141,9 +119,6 @@ struct evl_thread *evl_wake_up_head(struct evl_wait_queue *wq)
 	return evl_wake_up(wq, NULL);
 }
 
-void evl_abort_wait(struct evl_thread *thread,
-		struct evl_wait_channel *wchan);
-
 void evl_reorder_wait(struct evl_thread *thread);
 
 #endif /* !_EVL_WAIT_H_ */
diff --git a/kernel/evl/monitor.c b/kernel/evl/monitor.c
index ab8c9a3115bf..8dce3de0cae9 100644
--- a/kernel/evl/monitor.c
+++ b/kernel/evl/monitor.c
@@ -185,7 +185,6 @@ static int __enter_monitor(struct evl_monitor *gate,
 {
 	ktime_t timeout = EVL_INFINITE;
 	enum evl_tmode tmode;
-	int info;
 
 	no_ugly_lock();
 
@@ -196,17 +195,8 @@ static int __enter_monitor(struct evl_monitor *gate,
 	}
 
 	tmode = timeout ? EVL_ABS : EVL_REL;
-	info = evl_lock_mutex_timeout(&gate->lock, timeout, tmode);
-	if (info & T_BREAK)
-		return -EINTR;
 
-	if (info & T_RMID)
-		return -EIDRM;
-
-	if (info & T_TIMEO)
-		return -ETIMEDOUT;
-
-	return 0;
+	return evl_lock_mutex_timeout(&gate->lock, timeout, tmode);
 }
 
 static int enter_monitor(struct evl_monitor *gate,
diff --git a/kernel/evl/mutex.c b/kernel/evl/mutex.c
index bb8787bc8ba3..707f32bbd353 100644
--- a/kernel/evl/mutex.c
+++ b/kernel/evl/mutex.c
@@ -12,6 +12,7 @@
 #include <evl/thread.h>
 #include <evl/mutex.h>
 #include <evl/monitor.h>
+#include <evl/wait.h>
 #include <uapi/evl/signal.h>
 #include <trace/events/evl.h>
 
@@ -374,7 +375,6 @@ static void init_mutex(struct evl_mutex *mutex,
 	mutex->wprio = -1;
 	mutex->ceiling_ref = ceiling_ref;
 	mutex->clock = clock;
-	mutex->wchan.abort_wait = evl_abort_mutex_wait;
 	mutex->wchan.reorder_wait = evl_reorder_mutex_wait;
 	INIT_LIST_HEAD(&mutex->wchan.wait_list);
 	evl_spin_lock_init(&mutex->lock);
@@ -409,8 +409,10 @@ void evl_flush_mutex(struct evl_mutex *mutex, int reason)
 		EVL_WARN_ON(CORE, mutex->flags & EVL_MUTEX_CLAIMED);
 	else {
 		list_for_each_entry_safe(waiter, tmp,
-					&mutex->wchan.wait_list, wait_next)
+					&mutex->wchan.wait_list, wait_next) {
+			list_del_init(&waiter->wait_next);
 			evl_wakeup_thread(waiter, T_PEND, reason);
+		}
 
 		if (mutex->flags & EVL_MUTEX_CLAIMED)
 			clear_pi_boost(mutex, mutex->owner);
@@ -457,6 +459,46 @@ int evl_trylock_mutex(struct evl_mutex *mutex)
 }
 EXPORT_SYMBOL_GPL(evl_trylock_mutex);
 
+/* nklock held, irqs off */
+static void finish_wait(struct evl_mutex *mutex)
+{
+	struct evl_thread *owner, *target;
+
+	/*
+	 * Do all the necessary housekeeping chores to stop current
+	 * from waiting on a mutex. Doing so may require to update a
+	 * PI chain.
+	 */
+	requires_ugly_lock();
+
+	/*
+	 * Only a waiter leaving a PI chain triggers an update.
+	 * NOTE: PP mutexes never bear the CLAIMED bit.
+	 */
+	if (!(mutex->flags & EVL_MUTEX_CLAIMED))
+		return;
+
+	owner = mutex->owner;
+
+	if (list_empty(&mutex->wchan.wait_list)) {
+		/* No more waiters: clear the PI boost. */
+		clear_pi_boost(mutex, owner);
+		return;
+	}
+
+	/*
+	 * Reorder the booster queue of the current owner after we
+	 * left the wait list, then set its priority to the new
+	 * required minimum required to prevent priority inversion.
+	 */
+	target = list_first_entry(&mutex->wchan.wait_list,
+				struct evl_thread, wait_next);
+	mutex->wprio = target->wprio;
+	list_del(&mutex->next_booster);	/* owner->boosters */
+	list_add_priff(mutex, &owner->boosters, wprio, next_booster);
+	adjust_boost(owner, target);
+}
+
 int evl_lock_mutex_timeout(struct evl_mutex *mutex, ktime_t timeout,
 			enum evl_tmode timeout_mode)
 {
@@ -520,7 +562,7 @@ int evl_lock_mutex_timeout(struct evl_mutex *mutex, ktime_t timeout,
 	if (owner == NULL) {
 		untrack_owner(mutex);
 		xnlock_put_irqrestore(&nklock, flags);
-		return T_RMID;
+		return -EIDRM;
 	}
 
 	/*
@@ -588,9 +630,9 @@ int evl_lock_mutex_timeout(struct evl_mutex *mutex, ktime_t timeout,
 
 	evl_sleep_on(timeout, timeout_mode, mutex->clock, &mutex->wchan);
 	xnlock_put_irqrestore(&nklock, flags);
-	evl_schedule();
+	ret = evl_wait_schedule();
 	xnlock_get_irqsave(&nklock, flags);
-	ret = curr->info & (T_RMID|T_TIMEO|T_BREAK);
+	finish_wait(mutex);
 	curr->wwake = NULL;
 	curr->info &= ~T_WAKEN;
 
@@ -610,7 +652,7 @@ int evl_lock_mutex_timeout(struct evl_mutex *mutex, ktime_t timeout,
 			xnlock_put_irqrestore(&nklock, flags);
 			goto redo;
 		}
-		ret = T_TIMEO;
+		ret = -ETIMEDOUT;
 		goto out;
 	}
 grab:
@@ -656,14 +698,12 @@ static void transfer_ownership(struct evl_mutex *mutex,
 	 * We clear the wait channel early on - instead of waiting for
 	 * evl_wakeup_thread() to do so - because we want to hide
 	 * n_owner from the PI/PP adjustment which takes place over
-	 * set_current_owner_locked(). Because of that, we also have
-	 * to unlink the thread from the wait list manually since the
-	 * abort_wait() handler won't be called. NOTE: we do want
+	 * set_current_owner_locked(). NOTE: we do want
 	 * set_current_owner_locked() to run before
 	 * evl_wakeup_thread() is called.
 	 */
 	n_owner->wchan = NULL;
-	list_del(&n_owner->wait_next);
+	list_del_init(&n_owner->wait_next);
 	n_owner->wwake = &mutex->wchan;
 	set_current_owner_locked(mutex, n_owner);
 	evl_wakeup_thread(n_owner, T_PEND, T_WAKEN);
@@ -778,50 +818,6 @@ wchan_to_mutex(struct evl_wait_channel *wchan)
 	return container_of(wchan, struct evl_mutex, wchan);
 }
 
-/* nklock held, irqs off */
-void evl_abort_mutex_wait(struct evl_thread *thread,
-			struct evl_wait_channel *wchan)
-{
-	struct evl_mutex *mutex = wchan_to_mutex(wchan);
-	struct evl_thread *owner, *target;
-
-	requires_ugly_lock();
-
-	/*
-	 * Do all the necessary housekeeping chores to stop a thread
-	 * from waiting on a mutex. Doing so may require to update a
-	 * PI chain.
-	 */
-	list_del(&thread->wait_next); /* mutex->wchan.wait_list */
-
-	/*
-	 * Only a waiter leaving a PI chain triggers an update.
-	 * NOTE: PP mutexes never bear the CLAIMED bit.
-	 */
-	if (!(mutex->flags & EVL_MUTEX_CLAIMED))
-		return;
-
-	owner = mutex->owner;
-
-	if (list_empty(&mutex->wchan.wait_list)) {
-		/* No more waiters: clear the PI boost. */
-		clear_pi_boost(mutex, owner);
-		return;
-	}
-
-	/*
-	 * Reorder the booster queue of the current owner after we
-	 * left the wait list, then set its priority to the new
-	 * required minimum required to prevent priority inversion.
-	 */
-	target = list_first_entry(&mutex->wchan.wait_list,
-				struct evl_thread, wait_next);
-	mutex->wprio = target->wprio;
-	list_del(&mutex->next_booster);	/* owner->boosters */
-	list_add_priff(mutex, &owner->boosters, wprio, next_booster);
-	adjust_boost(owner, target);
-}
-
 /* nklock held, irqs off */
 void evl_reorder_mutex_wait(struct evl_thread *thread)
 {
diff --git a/kernel/evl/thread.c b/kernel/evl/thread.c
index be460a5f1121..4c683cfcdfbc 100644
--- a/kernel/evl/thread.c
+++ b/kernel/evl/thread.c
@@ -459,16 +459,6 @@ int __evl_run_kthread(struct evl_kthread *kthread)
 }
 EXPORT_SYMBOL_GPL(__evl_run_kthread);
 
-static inline void abort_wait(struct evl_thread *thread)
-{
-	struct evl_wait_channel *wchan = thread->wchan;
-
-	if (wchan) {
-		thread->wchan = NULL;
-		wchan->abort_wait(thread, wchan);
-	}
-}
-
 void evl_sleep_on(ktime_t timeout, enum evl_tmode timeout_mode,
 		struct evl_clock *clock,
 		struct evl_wait_channel *wchan)
@@ -492,10 +482,6 @@ void evl_sleep_on(ktime_t timeout, enum evl_tmode timeout_mode,
 	 */
 	if (likely(!(oldstate & EVL_THREAD_BLOCK_BITS))) {
 		if (curr->info & T_KICKED) {
-			if (wchan) {
-				curr->wchan = wchan;
-				abort_wait(curr);
-			}
 			curr->info &= ~(T_RMID|T_TIMEO);
 			curr->info |= T_BREAK;
 			goto out;
@@ -566,8 +552,8 @@ void evl_wakeup_thread(struct evl_thread *thread, int mask, int info)
 			evl_stop_timer(&thread->rtimer);
 
 		if (mask & T_PEND & oldstate)
-			abort_wait(thread);
-
+			thread->wchan = NULL;
+	
 		thread->info |= info;
 
 		if (!(thread->state & EVL_THREAD_BLOCK_BITS)) {
diff --git a/kernel/evl/wait.c b/kernel/evl/wait.c
index 03762ead8df5..870fccf4bad1 100644
--- a/kernel/evl/wait.c
+++ b/kernel/evl/wait.c
@@ -19,7 +19,6 @@ void evl_init_wait(struct evl_wait_queue *wq,
 	wq->flags = flags;
 	wq->clock = clock;
 	evl_spin_lock_init(&wq->lock);
-	wq->wchan.abort_wait = evl_abort_wait;
 	wq->wchan.reorder_wait = evl_reorder_wait;
 	INIT_LIST_HEAD(&wq->wchan.wait_list);
 }
@@ -70,6 +69,7 @@ struct evl_thread *evl_wake_up(struct evl_wait_queue *wq,
 		if (waiter == NULL)
 			waiter = list_first_entry(&wq->wchan.wait_list,
 						struct evl_thread, wait_next);
+		list_del_init(&waiter->wait_next);
 		evl_wakeup_thread(waiter, T_PEND, 0);
 	}
 
@@ -86,8 +86,10 @@ void evl_flush_wait_locked(struct evl_wait_queue *wq, int reason)
 
 	trace_evl_flush_wait(wq);
 
-	list_for_each_entry_safe(waiter, tmp, &wq->wchan.wait_list, wait_next)
+	list_for_each_entry_safe(waiter, tmp, &wq->wchan.wait_list, wait_next) {
+		list_del_init(&waiter->wait_next);
 		evl_wakeup_thread(waiter, T_PEND, reason);
+	}
 }
 EXPORT_SYMBOL_GPL(evl_flush_wait_locked);
 
@@ -108,14 +110,6 @@ wchan_to_wait_queue(struct evl_wait_channel *wchan)
 	return container_of(wchan, struct evl_wait_queue, wchan);
 }
 
-/* nklock held, irqs off */
-void evl_abort_wait(struct evl_thread *thread,
-		struct evl_wait_channel *wchan)
-{
-	requires_ugly_lock();
-	list_del(&thread->wait_next);
-}
-
 /* nklock held, irqs off */
 void evl_reorder_wait(struct evl_thread *thread)
 {
@@ -129,3 +123,70 @@ void evl_reorder_wait(struct evl_thread *thread)
 	}
 }
 EXPORT_SYMBOL_GPL(evl_reorder_wait);
+
+int evl_wait_schedule(void)
+{
+	struct evl_thread *curr = evl_current();
+	unsigned long flags;
+	int ret = 0, info;
+
+	no_ugly_lock();
+
+	evl_schedule();
+
+	/*
+	 * Upon return from schedule, we may or may not have been
+	 * unlinked from the wait channel, depending on whether we
+	 * actually resumed as a result of receiving a wakeup signal
+	 * from evl_wake_up() or evl_flush_wait(). The following logic
+	 * applies in order, depending on the information flags:
+	 *
+	 * - if T_RMID is set, evl_flush_wait() removed us from the
+	 * waitqueue before the wait channel got destroyed, and
+	 * therefore cannot be referred to anymore since it may be
+	 * stale: -EIDRM is returned.
+	 *
+	 * - if neither T_TIMEO or T_BREAK are set, we got a wakeup
+	 * and success is returned (zero). In addition, the caller may
+	 * need to check for T_BCAST if the signal is not paired with
+	 * a condition but works as a pulse instead.
+	 *
+	 * - otherwise, if any of T_TIMEO or T_BREAK is set:
+	 *
+	 *   + if we are still linked to the waitqueue, the wait was
+	 * aborted prior to receiving any wakeup so we translate the
+	 * information bit to the corresponding error status,
+	 * i.e. -ETIMEDOUT or -EINTR respectively.
+	 *
+	 *  + in the rare case where we have been unlinked and we also
+	 * got any of T_TIMEO|T_BREAK, then both the wakeup signal and
+	 * some abort condition have occurred simultaneously on
+	 * different cores, in which case we ignore the latter. In the
+	 * particular case of T_BREAK caused by
+	 * handle_sigwake_event(), T_KICKED will be detected on the
+	 * return path from the OOB syscall, yielding -ERESTARTSYS as
+	 * expected.
+	 */
+	info = evl_current()->info;
+	if (info & T_RMID)
+		return -EIDRM;
+
+	if (info & (T_TIMEO|T_BREAK)) {
+		xnlock_get_irqsave(&nklock, flags);
+		if (!list_empty(&curr->wait_next)) {
+			list_del_init(&curr->wait_next);
+			if (info & T_TIMEO)
+				ret = -ETIMEDOUT;
+			else if (info & T_BREAK)
+				ret = -EINTR;
+		}
+		xnlock_put_irqrestore(&nklock, flags);
+	} else if (IS_ENABLED(CONFIG_EVL_DEBUG_CORE)) {
+		xnlock_get_irqsave(&nklock, flags);
+		EVL_WARN_ON_ONCE(CORE, !list_empty(&curr->wait_next));
+		xnlock_put_irqrestore(&nklock, flags);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(evl_wait_schedule);
-- 
2.16.4

