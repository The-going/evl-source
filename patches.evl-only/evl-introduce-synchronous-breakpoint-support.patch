From 6b36e833d1a57568d31914889ca4773e2bbcc6e7 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Tue, 28 Jan 2020 11:10:57 +0100
Subject: [PATCH] evl: introduce synchronous breakpoint support

Synchronous breakpoints make sure to keep a ptrace-stepped thread
synchronized with its siblings from the same process running in the
background, as follows:

- as soon as a ptracer (e.g. gdb) regains control over a thread which
  just hit a breakpoint or received SIGINT, sibling threads from the
  same process which run out-of-band are immediately frozen.

- all sibling threads which have been frozen are set to wait on a
  common barrier before they can be released. Such release happens
  once all of them have joined the barrier in out-of-band context,
  after the (single-)stepped thread resumed.

Signed-off-by: Philippe Gerum <rpm@xenomai.org>
---
 arch/arm/include/dovetail/mm_info.h   |   7 +
 arch/arm64/include/dovetail/mm_info.h |   7 +
 arch/x86/include/dovetail/mm_info.h   |   7 +
 include/asm-generic/evl/mm_info.h     |  27 ++
 include/evl/lock.h                    |   2 +-
 include/evl/sched.h                   |   3 +
 include/evl/thread.h                  |  37 ++-
 include/trace/events/evl.h            |  20 +-
 include/uapi/evl/control.h            |   2 +-
 include/uapi/evl/signal.h             |   1 -
 include/uapi/evl/thread.h             |  80 +++---
 kernel/evl/control.c                  |  30 +++
 kernel/evl/factory.c                  |   7 +-
 kernel/evl/sched/core.c               | 158 +++++++++---
 kernel/evl/syscall.c                  |  30 ++-
 kernel/evl/thread.c                   | 458 ++++++++++++++++++++++++----------
 kernel/evl/timer.c                    |   2 +-
 17 files changed, 656 insertions(+), 222 deletions(-)
 create mode 100644 arch/arm/include/dovetail/mm_info.h
 create mode 100644 arch/arm64/include/dovetail/mm_info.h
 create mode 100644 arch/x86/include/dovetail/mm_info.h
 create mode 100644 include/asm-generic/evl/mm_info.h

diff --git a/arch/arm/include/dovetail/mm_info.h b/arch/arm/include/dovetail/mm_info.h
new file mode 100644
index 000000000000..13087687d61a
--- /dev/null
+++ b/arch/arm/include/dovetail/mm_info.h
@@ -0,0 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _EVL_DOVETAIL_MM_INFO_H
+#define _EVL_DOVETAIL_MM_INFO_H
+
+#include <asm-generic/evl/mm_info.h>
+
+#endif /* !_EVL_DOVETAIL_MM_INFO_H */
diff --git a/arch/arm64/include/dovetail/mm_info.h b/arch/arm64/include/dovetail/mm_info.h
new file mode 100644
index 000000000000..13087687d61a
--- /dev/null
+++ b/arch/arm64/include/dovetail/mm_info.h
@@ -0,0 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _EVL_DOVETAIL_MM_INFO_H
+#define _EVL_DOVETAIL_MM_INFO_H
+
+#include <asm-generic/evl/mm_info.h>
+
+#endif /* !_EVL_DOVETAIL_MM_INFO_H */
diff --git a/arch/x86/include/dovetail/mm_info.h b/arch/x86/include/dovetail/mm_info.h
new file mode 100644
index 000000000000..13087687d61a
--- /dev/null
+++ b/arch/x86/include/dovetail/mm_info.h
@@ -0,0 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _EVL_DOVETAIL_MM_INFO_H
+#define _EVL_DOVETAIL_MM_INFO_H
+
+#include <asm-generic/evl/mm_info.h>
+
+#endif /* !_EVL_DOVETAIL_MM_INFO_H */
diff --git a/include/asm-generic/evl/mm_info.h b/include/asm-generic/evl/mm_info.h
new file mode 100644
index 000000000000..53b739740cde
--- /dev/null
+++ b/include/asm-generic/evl/mm_info.h
@@ -0,0 +1,27 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_GENERIC_EVL_MM_INFO_H
+#define _ASM_GENERIC_EVL_MM_INFO_H
+
+#ifdef CONFIG_EVL
+
+#include <linux/list.h>
+
+#define EVL_MM_PTSYNC_BIT  0
+#define EVL_MM_ACTIVE_BIT  30
+#define EVL_MM_INIT_BIT    31
+
+struct evl_wait_queue;
+
+struct oob_mm_state {
+	unsigned long flags;	/* Guaranteed zero initially. */
+	struct list_head ptrace_sync;
+	struct evl_wait_queue *ptsync_barrier;
+};
+
+#else
+
+struct oob_mm_state { };
+
+#endif	/* !CONFIG_EVL */
+
+#endif /* !_ASM_GENERIC_EVL_MM_INFO_H */
diff --git a/include/evl/lock.h b/include/evl/lock.h
index 177ce71acbf8..13623d5c6037 100644
--- a/include/evl/lock.h
+++ b/include/evl/lock.h
@@ -6,7 +6,7 @@
 #ifndef _EVL_LOCK_H
 #define _EVL_LOCK_H
 
-#include <linux/irq_pipeline.h>
+#include <linux/spinlock.h>
 
 /*
  * The spinlock API used in the EVL core, which preserves Dovetail's
diff --git a/include/evl/sched.h b/include/evl/sched.h
index 4b1cd974c4d4..1ab40711965d 100644
--- a/include/evl/sched.h
+++ b/include/evl/sched.h
@@ -10,6 +10,7 @@
 
 #include <linux/percpu.h>
 #include <linux/list.h>
+#include <linux/irq_pipeline.h>
 #include <evl/lock.h>
 #include <evl/thread.h>
 #include <evl/sched/queue.h>
@@ -281,6 +282,8 @@ void evl_migrate_thread(struct evl_thread *thread,
 
 #endif /* !CONFIG_SMP */
 
+void evl_start_ptsync(struct evl_thread *stopper);
+
 #define for_each_evl_cpu(cpu)		\
 	for_each_online_cpu(cpu)	\
 		if (is_evl_cpu(cpu))
diff --git a/include/evl/thread.h b/include/evl/thread.h
index 04dff620f920..2f76247a7d71 100644
--- a/include/evl/thread.h
+++ b/include/evl/thread.h
@@ -27,9 +27,19 @@
 #include <uapi/evl/sched.h>
 #include <asm/evl/thread.h>
 
-#define EVL_THREAD_BLOCK_BITS   (T_SUSP|T_PEND|T_DELAY|T_WAIT|T_DORMANT|T_INBAND|T_HALT)
+/* All bits which may cause an EVL thread to block in oob context. */
+#define EVL_THREAD_BLOCK_BITS   (T_SUSP|T_PEND|T_DELAY|T_WAIT|T_DORMANT|T_INBAND|T_HALT|T_PTSYNC)
+/* Information bits an EVL thread may receive from a blocking op. */
 #define EVL_THREAD_INFO_MASK	(T_RMID|T_TIMEO|T_BREAK|T_WAKEN|T_ROBBED|T_KICKED|T_BCAST)
 
+/*
+ * These are special internal values of SIGDEBUG causes which are
+ * never sent to user-space, but specifically handled by
+ * evl_switch_inband().
+ */
+#define SIGDEBUG_NONE   0
+#define SIGDEBUG_TRAP  -1
+
 struct evl_thread;
 struct evl_rq;
 struct evl_sched_class;
@@ -54,7 +64,7 @@ struct evl_thread {
 	evl_spinlock_t lock;
 
 	/*
-	 * Shared data, covered by ->lock.
+	 * Shared thread-specific data, covered by ->lock.
 	 */
 	struct evl_rq *rq;
 	struct evl_sched_class *base_class;
@@ -84,11 +94,18 @@ struct evl_thread {
 
 	struct evl_timer rtimer;  /* Resource timer */
 	struct evl_timer ptimer;  /* Periodic timer */
-	ktime_t rrperiod;  /* Round-robin period (ns) */
+	ktime_t rrperiod;	  /* Round-robin period (ns) */
 
 	/*
-	 * Shared data, covered by both thread->lock AND
-	 * thread->rq->lock.
+	 * Shared scheduler-specific data covered by both thread->lock
+	 * AND thread->rq->lock. For such data, the first lock
+	 * protects against the thread moving to a different rq, it
+	 * may be omitted if the target cannot be subject to such
+	 * migration (i.e. @thread == evl_this_rq()->curr, which
+	 * implies that we are out-of-band and thus cannot trigger
+	 * evl_migrate_thread()). The second one serializes with the
+	 * scheduler core and must ALWAYS be taken for accessing this
+	 * data.
 	 */
 	__u32 state;
 	__u32 info;
@@ -105,7 +122,8 @@ struct evl_thread {
 	struct list_head next;		/* in evl_thread_list */
 
 	/*
-	 * Thread-local data the owner may modify locklessly.
+	 * Thread-local data only the owner may modify, therefore it
+	 * may do so locklessly.
 	 */
 	struct dovetail_altsched_context altsched;
 	__u32 local_info;
@@ -136,6 +154,8 @@ struct evl_thread {
 	struct completion exited;
 	kernel_cap_t raised_cap;
 	struct list_head kill_next;
+	struct oob_mm_state *oob_mm; /* Mostly RO. */
+	struct list_head ptsync_next; /* covered by oob_mm->lock. */
 	char *name;
 };
 
@@ -270,7 +290,8 @@ void evl_get_thread_state(struct evl_thread *thread,
 
 int evl_detach_self(void);
 
-void evl_kick_thread(struct evl_thread *thread);
+void evl_kick_thread(struct evl_thread *thread,
+		int info);
 
 void evl_demote_thread(struct evl_thread *thread);
 
@@ -343,4 +364,6 @@ void evl_set_kthread_priority(struct evl_kthread *thread,
 
 pid_t evl_get_inband_pid(struct evl_thread *thread);
 
+int activate_oob_mm_state(struct oob_mm_state *p);
+
 #endif /* !_EVL_THREAD_H */
diff --git a/include/trace/events/evl.h b/include/trace/events/evl.h
index 7d654ea8f374..c502e0b0af4e 100644
--- a/include/trace/events/evl.h
+++ b/include/trace/events/evl.h
@@ -487,7 +487,7 @@ DEFINE_EVENT(curr_thread_event, evl_watchdog_signal,
 	TP_ARGS(curr)
 );
 
-DEFINE_EVENT(curr_thread_event, evl_switching_oob,
+DEFINE_EVENT(curr_thread_event, evl_switch_oob,
 	TP_PROTO(struct evl_thread *curr),
 	TP_ARGS(curr)
 );
@@ -499,12 +499,18 @@ DEFINE_EVENT(curr_thread_event, evl_switched_oob,
 
 #define evl_print_switch_cause(cause)					\
 	__print_symbolic(cause,						\
-			 { SIGDEBUG_NONE,		"undefined" },	\
-			 { SIGDEBUG_MIGRATE_SIGNAL,	"signal" },	\
-			 { SIGDEBUG_MIGRATE_SYSCALL,	"syscall" },	\
-			 { SIGDEBUG_MIGRATE_FAULT,	"fault" })
-
-TRACE_EVENT(evl_switching_inband,
+			{ SIGDEBUG_TRAP,		"breakpoint trap" }, \
+			{ SIGDEBUG_NONE,		"undefined" },	\
+			{ SIGDEBUG_MIGRATE_SIGNAL,	"signal" },	\
+			{ SIGDEBUG_MIGRATE_SYSCALL,	"syscall" },	\
+			{ SIGDEBUG_MIGRATE_FAULT,	"fault" },	\
+			{ SIGDEBUG_MIGRATE_PRIOINV,     "priority inversion" }, \
+			{ SIGDEBUG_WATCHDOG,		"watchdog" },	\
+			{ SIGDEBUG_MUTEX_IMBALANCE,     "mutex imbalance" }, \
+			{ SIGDEBUG_MUTEX_SLEEP,         "mutex sleep" }, \
+			{ SIGDEBUG_STAGE_LOCKED,        "stage exclusion" } )
+
+TRACE_EVENT(evl_switch_inband,
 	TP_PROTO(int cause),
 	TP_ARGS(cause),
 
diff --git a/include/uapi/evl/control.h b/include/uapi/evl/control.h
index 71630cbfe0b8..d9b3bf27bd5e 100644
--- a/include/uapi/evl/control.h
+++ b/include/uapi/evl/control.h
@@ -10,7 +10,7 @@
 #include <linux/types.h>
 #include <uapi/evl/sched.h>
 
-#define EVL_ABI_LEVEL  15
+#define EVL_ABI_LEVEL  16
 
 #define EVL_CONTROL_DEV  "/dev/evl/control"
 
diff --git a/include/uapi/evl/signal.h b/include/uapi/evl/signal.h
index cbdf68b7b90a..95f5185d1d53 100644
--- a/include/uapi/evl/signal.h
+++ b/include/uapi/evl/signal.h
@@ -25,7 +25,6 @@
 	((sigdebug_code(si) & 0xffff0000) == sigdebug_marker)
 
 /* Possible values of sigdebug_cause() */
-#define SIGDEBUG_NONE			0
 #define SIGDEBUG_MIGRATE_SIGNAL		1
 #define SIGDEBUG_MIGRATE_SYSCALL	2
 #define SIGDEBUG_MIGRATE_FAULT		3
diff --git a/include/uapi/evl/thread.h b/include/uapi/evl/thread.h
index 0f11a51218bb..cf353a6dfb94 100644
--- a/include/uapi/evl/thread.h
+++ b/include/uapi/evl/thread.h
@@ -15,45 +15,49 @@
 
 /* State flags (shared) */
 
-#define T_SUSP    0x00000001 /*< Suspended */
-#define T_PEND    0x00000002 /*< Blocked on a wait_queue/mutex */
-#define T_DELAY   0x00000004 /*< Delayed/timed */
-#define T_WAIT    0x00000008 /*< Periodic wait */
-#define T_READY   0x00000010 /*< Ready to run (in rq) */
-#define T_DORMANT 0x00000020 /*< Not started yet */
-#define T_ZOMBIE  0x00000040 /*< Dead, waiting for disposal */
-#define T_INBAND  0x00000080 /*< Running in-band */
-#define T_HALT    0x00000100 /*< Halted */
-#define T_BOOST   0x00000200 /*< PI/PP boost undergoing */
-#define T_SSTEP   0x00000400 /*< Single-stepped by debugger */
-#define T_RRB     0x00000800 /*< Undergoes round-robin scheduling */
-#define T_ROOT    0x00001000 /*< Root thread (in-band kernel placeholder) */
-#define T_WEAK    0x00002000 /*< Weak scheduling (non real-time) */
-#define T_USER    0x00004000 /*< Userland thread */
-#define T_WOSS    0x00008000 /*< Warn on stage switch (SIGDEBUG) */
-#define T_WOLI    0x00010000 /*< Warn on locking inconsistency (SIGDEBUG)  */
-#define T_WOSX    0x00020000 /*< Warn on stage exclusion (SIGDEBUG)  */
+#define T_SUSP    0x00000001 /* Suspended */
+#define T_PEND    0x00000002 /* Blocked on a wait_queue/mutex */
+#define T_DELAY   0x00000004 /* Delayed/timed */
+#define T_WAIT    0x00000008 /* Periodic wait */
+#define T_READY   0x00000010 /* Ready to run (in rq) */
+#define T_DORMANT 0x00000020 /* Not started yet */
+#define T_ZOMBIE  0x00000040 /* Dead, waiting for disposal */
+#define T_INBAND  0x00000080 /* Running in-band */
+#define T_HALT    0x00000100 /* Halted */
+#define T_BOOST   0x00000200 /* PI/PP boost undergoing */
+#define T_PTSYNC  0x00000400 /* Synchronizing on ptrace event */
+#define T_RRB     0x00000800 /* Undergoes round-robin scheduling */
+#define T_ROOT    0x00001000 /* Root thread (in-band kernel placeholder) */
+#define T_WEAK    0x00002000 /* Weak scheduling (in-band) */
+#define T_USER    0x00004000 /* Userland thread */
+#define T_WOSS    0x00008000 /* Warn on stage switch (SIGDEBUG) */
+#define T_WOLI    0x00010000 /* Warn on locking inconsistency (SIGDEBUG) */
+#define T_WOSX    0x00020000 /* Warn on stage exclusion (SIGDEBUG) */
+#define T_PTRACE  0x00040000 /* Stopped on ptrace event */
 
 /* Information flags (shared) */
 
-#define T_TIMEO   0x00000001 /*< Woken up due to a timeout condition */
-#define T_RMID    0x00000002 /*< Pending on a removed resource */
-#define T_BREAK   0x00000004 /*< Forcibly awaken from a wait state */
-#define T_KICKED  0x00000008 /*< Forced out of OOB context */
-#define T_WAKEN   0x00000010 /*< Thread waken up upon resource availability */
-#define T_ROBBED  0x00000020 /*< Robbed from resource ownership */
-#define T_CANCELD 0x00000040 /*< Cancellation request is pending */
-#define T_PIALERT 0x00000080 /*< Priority inversion alert (SIGDEBUG sent) */
-#define T_SCHEDP  0x00000100 /*< schedparam propagation is pending */
-#define T_BCAST   0x00000200 /*< Woken up upon resource broadcast */
-#define T_SIGNAL  0x00000400 /*< Event monitor signaled */
-#define T_SXALERT 0x00000800 /*< Stage exclusion alert (SIGDEBUG sent) */
+#define T_TIMEO   0x00000001 /* Woken up due to a timeout condition */
+#define T_RMID    0x00000002 /* Pending on a removed resource */
+#define T_BREAK   0x00000004 /* Forcibly awaken from a wait state */
+#define T_KICKED  0x00000008 /* Forced out of OOB context */
+#define T_WAKEN   0x00000010 /* Thread waken up upon resource availability */
+#define T_ROBBED  0x00000020 /* Robbed from resource ownership */
+#define T_CANCELD 0x00000040 /* Cancellation request is pending */
+#define T_PIALERT 0x00000080 /* Priority inversion alert (SIGDEBUG sent) */
+#define T_SCHEDP  0x00000100 /* Schedparam propagation is pending */
+#define T_BCAST   0x00000200 /* Woken up upon resource broadcast */
+#define T_SIGNAL  0x00000400 /* Event monitor signaled */
+#define T_SXALERT 0x00000800 /* Stage exclusion alert (SIGDEBUG sent) */
+#define T_PTSIG   0x00001000 /* Ptrace signal is pending */
+#define T_PTSTOP  0x00002000 /* Ptrace stop is ongoing */
+#define T_PTJOIN  0x00004000 /* Ptracee should join ptsync barrier */
 
 /* Local information flags (private to current thread) */
 
-#define T_SYSRST  0x00000001 /*< Thread awaiting syscall restart after signal */
-#define T_HICCUP  0x00000002 /*< Just left from ptracing - timings wrecked */
-#define T_INFAULT 0x00000004 /*< In fault handling */
+#define T_SYSRST  0x00000001 /* Thread awaiting syscall restart after signal */
+#define T_IGNOVR  0x00000002 /* Overrun detection temporarily disabled */
+#define T_INFAULT 0x00000004 /* In fault handling */
 
 /*
  * Must follow strictly the declaration order of the state flags
@@ -67,14 +71,14 @@
  * 'U' -> Dormant
  * 'Z' -> Zombie
  * 'X' -> Running in-band
- * 'H' -> Held in emergency
+ * 'H' -> Halted
  * 'b' -> Priority boost undergoing
- * 'T' -> Ptraced and stopped
+ * '#' -> Ptrace sync ongoing
  * 'r' -> Undergoes round-robin
- * 'g' -> Warned on stage switch (SIGDEBUG)
- * 'G' -> Warned on locking inconsistency (SIGDEBUG)
+ * 't' -> Warned on stage switch (T_WOSS -> SIGDEBUG)
+ * 'T' -> Stopped on ptrace event
  */
-#define EVL_THREAD_STATE_LABELS  "SWDpRUZXHbTr...12"
+#define EVL_THREAD_STATE_LABELS  "SWDpRUZXHb#r...t..T"
 
 struct evl_user_window {
 	__u32 state;
diff --git a/kernel/evl/control.c b/kernel/evl/control.c
index 0c0642716a6d..0f607d90bf02 100644
--- a/kernel/evl/control.c
+++ b/kernel/evl/control.c
@@ -10,6 +10,7 @@
 #include <evl/memory.h>
 #include <evl/thread.h>
 #include <evl/factory.h>
+#include <evl/flag.h>
 #include <evl/tick.h>
 #include <evl/sched.h>
 #include <evl/control.h>
@@ -251,6 +252,34 @@ static long control_common_ioctl(struct file *filp, unsigned int cmd,
 	return ret;
 }
 
+static int control_open(struct inode *inode, struct file *filp)
+{
+	struct oob_mm_state *oob_mm = dovetail_mm_state();
+	int ret = 0;
+
+	/*
+	 * Opening the control device is a strong hint that we are
+	 * about to host EVL threads in the current process, so this
+	 * makes sense to allocate the resources we'll need to
+	 * maintain them here. The in-band kernel has no way to figure
+	 * out when initializing the oob context for a new mm might be
+	 * relevant, so this has to be done on demand based on some
+	 * information only EVL has. This is the reason why there is
+	 * no initialization call for the oob_mm state defined in the
+	 * Dovetail interface, the in-band kernel would not know when
+	 * to call it.
+	 */
+
+	if (!oob_mm)	/* Userland only. */
+		return -EPERM;
+
+	/* The control device might be opened multiple times. */
+	if (!test_and_set_bit(EVL_MM_INIT_BIT, &oob_mm->flags))
+		ret = activate_oob_mm_state(oob_mm);
+
+	return ret;
+}
+
 static long control_oob_ioctl(struct file *filp, unsigned int cmd,
 			unsigned long arg)
 {
@@ -291,6 +320,7 @@ static int control_mmap(struct file *filp, struct vm_area_struct *vma)
 }
 
 static const struct file_operations control_fops = {
+	.open		=	control_open,
 	.oob_ioctl	=	control_oob_ioctl,
 	.unlocked_ioctl	=	control_ioctl,
 	.mmap		=	control_mmap,
diff --git a/kernel/evl/factory.c b/kernel/evl/factory.c
index 632cfb3370ac..1d8f1c7cf041 100644
--- a/kernel/evl/factory.c
+++ b/kernel/evl/factory.c
@@ -19,6 +19,7 @@
 #include <linux/uaccess.h>
 #include <linux/hashtable.h>
 #include <linux/stringhash.h>
+#include <linux/dovetail.h>
 #include <evl/assert.h>
 #include <evl/file.h>
 #include <evl/control.h>
@@ -83,11 +84,12 @@ void evl_destroy_element(struct evl_element *e)
 void evl_get_element(struct evl_element *e)
 {
 	unsigned long flags;
+	int old_refs;
 
 	raw_spin_lock_irqsave(&e->ref_lock, flags);
-	EVL_WARN_ON(CORE, e->refs == 0);
-	e->refs++;
+	old_refs = e->refs++;
 	raw_spin_unlock_irqrestore(&e->ref_lock, flags);
+	EVL_WARN_ON(CORE, old_refs == 0);
 }
 
 int evl_open_element(struct inode *inode, struct file *filp)
@@ -268,6 +270,7 @@ static struct device *create_device(dev_t rdev, struct evl_factory *fac,
 	dev->groups = fac->attrs;
 	dev->release = release_device;
 	dev_set_drvdata(dev, drvdata);
+
 	ret = dev_set_name(dev, "%s", name);
 	if (ret)
 		goto fail;
diff --git a/kernel/evl/sched/core.c b/kernel/evl/sched/core.c
index 28fbba0aea70..80987f0bceac 100644
--- a/kernel/evl/sched/core.c
+++ b/kernel/evl/sched/core.c
@@ -23,6 +23,7 @@
 #include <evl/tick.h>
 #include <evl/monitor.h>
 #include <evl/mutex.h>
+#include <evl/flag.h>
 #include <uapi/evl/signal.h>
 #include <trace/events/evl.h>
 
@@ -728,8 +729,7 @@ static inline void set_next_running(struct evl_rq *rq,
 		evl_stop_timer(&rq->rrbtimer);
 }
 
-/* rq->curr->lock + rq->lock held, irqs off. */
-static struct evl_thread *pick_next_thread(struct evl_rq *rq)
+static struct evl_thread *__pick_next_thread(struct evl_rq *rq)
 {
 	struct evl_sched_class *sched_class;
 	struct evl_thread *curr = rq->curr;
@@ -763,15 +763,51 @@ static struct evl_thread *pick_next_thread(struct evl_rq *rq)
 	 */
 	for_each_evl_sched_class(sched_class) {
 		next = sched_class->sched_pick(rq);
-		if (likely(next)) {
-			set_next_running(rq, next);
+		if (likely(next))
 			return next;
-		}
 	}
 
 	return NULL; /* NOT REACHED (idle class). */
 }
 
+/* rq->curr->lock + rq->lock held, irqs off. */
+static struct evl_thread *pick_next_thread(struct evl_rq *rq)
+{
+	struct oob_mm_state *oob_mm;
+	struct evl_thread *next;
+
+	for (;;) {
+		next = __pick_next_thread(rq);
+		oob_mm = next->oob_mm;
+		if (unlikely(!oob_mm)) /* Includes the root thread. */
+			break;
+		/*
+		 * Obey any pending request for a ptsync freeze.
+		 * Either we freeze @next before a sigwake event lifts
+		 * T_PTSYNC, setting T_PTSTOP, or after in which case
+		 * we already have T_PTSTOP set so we don't have to
+		 * raise T_PTSYNC. The basic assumption is that we
+		 * should get SIGSTOP/SIGTRAP for any thread involved.
+		 */
+		if (likely(!test_bit(EVL_MM_PTSYNC_BIT, &oob_mm->flags)))
+			break;	/* Fast and most likely path. */
+		if (next->info & (T_PTSTOP|T_PTSIG|T_KICKED))
+			break;
+		/*
+		 * NOTE: We hold next->rq->lock by construction, so
+		 * changing next->state is ok despite that we don't
+		 * hold next->lock. This properly serializes with
+		 * evl_kick_thread() which might raise T_PTSTOP.
+		 */
+		next->state |= T_PTSYNC;
+		next->state &= ~T_READY;
+	}
+
+	set_next_running(rq, next);
+
+	return next;
+}
+
 static inline void prepare_rq_switch(struct evl_rq *this_rq,
 				struct evl_thread *next)
 {
@@ -924,10 +960,44 @@ void __evl_schedule(void) /* oob or oob stalled (CPU migration-safe) */
 }
 EXPORT_SYMBOL_GPL(__evl_schedule);
 
+/* this_rq->lock held, oob stage stalled. */
+static void start_ptsync_locked(struct evl_thread *stopper,
+				struct evl_rq *this_rq)
+{
+	struct oob_mm_state *oob_mm = stopper->oob_mm;
+
+	if (!test_and_set_bit(EVL_MM_PTSYNC_BIT, &oob_mm->flags)) {
+#ifdef CONFIG_SMP
+		cpumask_copy(&this_rq->resched_cpus, &evl_oob_cpus);
+		cpumask_clear_cpu(raw_smp_processor_id(), &this_rq->resched_cpus);
+#endif
+		evl_set_self_resched(this_rq);
+	}
+}
+
+void evl_start_ptsync(struct evl_thread *stopper)
+{
+	struct evl_rq *this_rq;
+	unsigned long flags;
+
+	if (EVL_WARN_ON(CORE, !(stopper->state & T_USER)))
+		return;
+
+	flags = oob_irq_save();
+	this_rq = this_evl_rq();
+	evl_spin_lock(&this_rq->lock);
+	start_ptsync_locked(stopper, this_rq);
+	evl_spin_unlock_irqrestore(&this_rq->lock, flags);
+}
+
 void resume_oob_task(struct task_struct *p) /* inband, oob stage stalled */
 {
 	struct evl_thread *thread = evl_thread_from_task(p);
 
+	/*
+	 * If T_PTSTOP is set, pick_next_thread() is not allowed to
+	 * freeze @thread while in flight to the out-of-band stage.
+	 */
 	if (check_cpu_affinity(p))
 		evl_release_thread(thread, T_INBAND, 0);
 
@@ -936,20 +1006,20 @@ void resume_oob_task(struct task_struct *p) /* inband, oob stage stalled */
 
 int evl_switch_oob(void)
 {
+	struct evl_thread *curr = evl_current();
 	struct task_struct *p = current;
-	struct evl_thread *curr;
+	unsigned long flags;
 	int ret;
 
 	inband_context_only();
 
-	curr = evl_current();
 	if (curr == NULL)
 		return -EPERM;
 
 	if (signal_pending(p))
 		return -ERESTARTSYS;
 
-	trace_evl_switching_oob(curr);
+	trace_evl_switch_oob(curr);
 
 	evl_clear_sync_uwindow(curr, T_INBAND);
 
@@ -968,21 +1038,23 @@ int evl_switch_oob(void)
 	 */
 	oob_context_only();
 	finish_rq_switch_from_inband();
+
 	evl_test_cancel();
 
 	trace_evl_switched_oob(curr);
 
 	/*
-	 * Recheck pending signals once again. As we block task
-	 * wakeups during the stage transition and handle_sigwake_event()
-	 * ignores signals until T_INBAND is cleared, any signal in
-	 * between is just silently queued up to here.
+	 * Since handle_sigwake_event()->evl_kick_thread() won't set
+	 * T_KICKED unless T_INBAND is cleared, a signal received
+	 * during the stage transition process might have gone
+	 * unnoticed. Recheck for signals here and raise T_KICKED if
+	 * some are pending, so that we switch back in-band asap for
+	 * handling them.
 	 */
 	if (signal_pending(p)) {
-		evl_switch_inband(!(curr->state & T_SSTEP) ?
-				SIGDEBUG_MIGRATE_SIGNAL:
-				SIGDEBUG_NONE);
-		return -ERESTARTSYS;
+		evl_spin_lock_irqsave(&curr->rq->lock, flags);
+		curr->info |= T_KICKED;
+		evl_spin_unlock_irqrestore(&curr->rq->lock, flags);
 	}
 
 	return 0;
@@ -994,39 +1066,65 @@ void evl_switch_inband(int cause)
 	struct evl_thread *curr = evl_current();
 	struct task_struct *p = current;
 	struct kernel_siginfo si;
-	struct evl_rq *rq;
+	struct evl_rq *this_rq;
+	bool notify;
 
 	oob_context_only();
 
-	trace_evl_switching_inband(cause);
+	trace_evl_switch_inband(cause);
 
 	/*
 	 * This is the only location where we may assert T_INBAND for
 	 * a thread. Basic assumption: switching to the inband stage
 	 * only applies to the current thread running out-of-band on
-	 * this CPU.
-	 *
-	 * CAVEAT: dovetail_leave_oob() must run _before_ the in-band
-	 * kernel is allowed to take interrupts again, so that
-	 * try_to_wake_up() does not block the wake up request for the
-	 * switching thread as a result of testing task_is_off_stage().
+	 * this CPU. See caveat about dovetail_leave_oob() below.
 	 */
 	oob_irq_disable();
 	irq_work_queue(&curr->inband_work);
+
 	evl_spin_lock(&curr->lock);
-	rq = curr->rq;
-	evl_spin_lock(&rq->lock);
+	this_rq = curr->rq;
+	evl_spin_lock(&this_rq->lock);
+
 	if (curr->state & T_READY) {
 		evl_dequeue_thread(curr);
 		curr->state &= ~T_READY;
 	}
-	curr->info &= ~EVL_THREAD_INFO_MASK;
+
 	curr->state |= T_INBAND;
 	curr->local_info &= ~T_SYSRST;
-	evl_set_resched(rq);
-	evl_spin_unlock(&rq->lock);
+	notify = curr->state & T_USER && cause > SIGDEBUG_NONE;
+
+	/*
+	 * If we are initiating the ptsync sequence on breakpoint or
+	 * SIGSTOP/SIGINT is pending, do not send SIGDEBUG since
+	 * switching in-band is ok.
+	 */
+	if (cause == SIGDEBUG_TRAP) {
+		curr->info |= T_PTSTOP;
+		curr->info &= ~T_PTJOIN;
+		start_ptsync_locked(curr, this_rq);
+	} else if (curr->info & T_PTSIG) {
+		curr->info &= ~T_PTSIG;
+		notify = false;
+	}
+
+	curr->info &= ~EVL_THREAD_INFO_MASK;
+
+	evl_set_resched(this_rq);
+
+	evl_spin_unlock(&this_rq->lock);
 	evl_spin_unlock(&curr->lock);
+
+	/*
+	 * CAVEAT: dovetail_leave_oob() must run _before_ the in-band
+	 * kernel is allowed to take interrupts again, so that
+	 * try_to_wake_up() does not block the wake up request for the
+	 * switching thread as a result of testing
+	 * task_is_off_stage().
+	 */
 	dovetail_leave_oob();
+
 	__evl_schedule();
 	/*
 	 * this_rq()->lock was released when the root thread resumed
@@ -1062,7 +1160,7 @@ void evl_switch_inband(int cause)
 	 */
 	evl_propagate_schedparam_change(curr);
 
-	if ((curr->state & T_USER) && cause != SIGDEBUG_NONE) {
+	if (notify) {
 		/*
 		 * Help debugging spurious stage switches by sending
 		 * SIGDEBUG. We are running inband on the context of
diff --git a/kernel/evl/syscall.c b/kernel/evl/syscall.c
index 8448a6984b54..4bfc461726dc 100644
--- a/kernel/evl/syscall.c
+++ b/kernel/evl/syscall.c
@@ -57,7 +57,6 @@ static void prepare_for_signal(struct task_struct *p,
 			struct evl_thread *curr,
 			struct pt_regs *regs)
 {
-	int cause = SIGDEBUG_NONE;
 	unsigned long flags;
 
 	/*
@@ -69,15 +68,21 @@ static void prepare_for_signal(struct task_struct *p,
 
 	/*
 	 * @curr == this_evl_rq()->curr over oob so no need to grab
-	 * @curr->lock.
+	 * @curr->lock (i.e. @curr cannot go away under out feet).
 	 */
 	evl_spin_lock_irqsave(&curr->rq->lock, flags);
 
+	/*
+	 * We are called from out-of-band mode only to act upon a
+	 * pending signal receipt. We may observe signal_pending(p)
+	 * which implies that T_KICKED was set too
+	 * (handle_sigwake_event()), or T_KICKED alone which means
+	 * that we have been unblocked from a wait for some other
+	 * reason.
+	 */
 	if (curr->info & T_KICKED) {
 		if (signal_pending(p)) {
 			set_oob_error(regs, -ERESTARTSYS);
-			if (!(curr->state & T_SSTEP))
-				cause = SIGDEBUG_MIGRATE_SIGNAL;
 			curr->info &= ~T_BREAK;
 		}
 		curr->info &= ~T_KICKED;
@@ -87,7 +92,7 @@ static void prepare_for_signal(struct task_struct *p,
 
 	evl_test_cancel();
 
-	evl_switch_inband(cause);
+	evl_switch_inband(SIGDEBUG_MIGRATE_SIGNAL);
 }
 
 /*
@@ -241,7 +246,14 @@ static int do_inband_syscall(struct irq_stage *stage, struct pt_regs *regs)
 	trace_evl_inband_sysentry(nr);
 
 	ret = evl_switch_oob();
-	if (ret) {
+	/*
+	 * -ERESTARTSYS might be received if switching oob was blocked
+	 * by a pending signal, otherwise -EINTR might be received
+	 * upon signal detection after the transition to oob context,
+	 * in which case the common logic applies (i.e. based on
+	 * T_KICKED and/or signal_pending()).
+	 */
+	if (ret == -ERESTARTSYS) {
 		set_oob_error(regs, ret);
 		goto done;
 	}
@@ -250,14 +262,16 @@ static int do_inband_syscall(struct irq_stage *stage, struct pt_regs *regs)
 
 	if (!evl_is_inband()) {
 		p = current;
-		if (signal_pending(p))
+		if (signal_pending(p) || (curr->info & T_KICKED))
 			prepare_for_signal(p, curr, regs);
 		else if ((curr->state & T_WEAK) &&
 			!atomic_read(&curr->inband_disable_count))
 			evl_switch_inband(SIGDEBUG_NONE);
 	}
 done:
-	curr->local_info &= ~T_HICCUP;
+	if (curr->local_info & T_IGNOVR)
+		curr->local_info &= ~T_IGNOVR;
+
 	evl_inc_counter(&curr->stat.sc);
 	evl_sync_uwindow(curr);
 
diff --git a/kernel/evl/thread.c b/kernel/evl/thread.c
index 3c1bbe187e46..4faa44f747eb 100644
--- a/kernel/evl/thread.c
+++ b/kernel/evl/thread.c
@@ -38,6 +38,7 @@
 #include <evl/monitor.h>
 #include <evl/mutex.h>
 #include <evl/poll.h>
+#include <evl/flag.h>
 #include <asm/evl/syscall.h>
 #include <trace/events/evl.h>
 
@@ -51,6 +52,8 @@ static DECLARE_WAIT_QUEUE_HEAD(join_all);
 
 static void inband_task_wakeup(struct irq_work *work);
 
+static void skip_ptsync(struct evl_thread *thread);
+
 static void timeout_handler(struct evl_timer *timer) /* oob stage stalled */
 {
 	struct evl_thread *thread = container_of(timer, struct evl_thread, rtimer);
@@ -96,6 +99,11 @@ static inline void set_oob_threadinfo(struct evl_thread *thread)
 	p->thread = thread;
 }
 
+static inline void set_oob_mminfo(struct evl_thread *thread)
+{
+	thread->oob_mm = dovetail_mm_state();
+}
+
 static inline void add_u_cap(struct evl_thread *thread,
 			struct cred *newcap,
 			int cap)
@@ -211,6 +219,8 @@ int evl_init_thread(struct evl_thread *thread,
 	raw_spin_lock_init(&thread->tracking_lock);
 	evl_spin_lock_init(&thread->lock);
 	init_completion(&thread->exited);
+	INIT_LIST_HEAD(&thread->ptsync_next);
+	thread->oob_mm = NULL;
 
 	gravity = flags & T_USER ? EVL_TIMER_UGRAVITY : EVL_TIMER_KGRAVITY;
 	evl_init_timer_on_rq(&thread->rtimer, &evl_mono_clock, timeout_handler,
@@ -325,6 +335,9 @@ static void put_current_thread(void)
 {
 	struct evl_thread *curr = evl_current();
 
+	if (curr->state & T_USER)
+		skip_ptsync(curr);
+
 	cleanup_current_thread();
 	evl_put_element(&curr->element);
 }
@@ -648,7 +661,7 @@ static void evl_release_thread_locked(struct evl_thread *thread,
 	assert_evl_lock(&thread->lock);
 	assert_evl_lock(&thread->rq->lock);
 
-	if (EVL_WARN_ON(CORE, mask & ~(T_SUSP|T_HALT|T_INBAND|T_DORMANT)))
+	if (EVL_WARN_ON(CORE, mask & ~(T_SUSP|T_HALT|T_INBAND|T_DORMANT|T_PTSYNC)))
 		return;
 
 	trace_evl_release_thread(thread, mask, info);
@@ -661,7 +674,7 @@ static void evl_release_thread_locked(struct evl_thread *thread,
 		if (thread->state & EVL_THREAD_BLOCK_BITS)
 			return;
 
-		if (unlikely((oldstate & mask) & T_HALT)) {
+		if (unlikely((oldstate & mask) & (T_HALT|T_PTSYNC))) {
 			/* Requeue at head of priority group. */
 			evl_requeue_thread(thread);
 			goto ready;
@@ -924,7 +937,7 @@ void evl_cancel_thread(struct evl_thread *thread)
 		evl_demote_thread(thread);
 		evl_signal_thread(thread, SIGTERM, 0);
 	} else
-		evl_kick_thread(thread);
+		evl_kick_thread(thread, 0);
 out:
 	evl_schedule();
 }
@@ -1123,7 +1136,7 @@ void evl_unblock_thread(struct evl_thread *thread, int reason)
 }
 EXPORT_SYMBOL_GPL(evl_unblock_thread);
 
-void evl_kick_thread(struct evl_thread *thread)
+void evl_kick_thread(struct evl_thread *thread, int info)
 {
 	struct task_struct *p = thread->altsched.task;
 	unsigned long flags;
@@ -1131,12 +1144,32 @@ void evl_kick_thread(struct evl_thread *thread)
 
 	rq = evl_get_thread_rq(thread, flags);
 
-	if ((thread->info & T_KICKED) || (thread->state & T_INBAND))
+	if (thread->state & T_INBAND)
+		goto out;
+
+	/*
+	 * We might get T_PTSIG on top of T_KICKED, never filter out
+	 * the former.
+	 */
+	if (!(info & T_PTSIG) && thread->info & T_KICKED)
 		goto out;
 
 	/* See comment in evl_unblock_thread(). */
 	evl_wakeup_thread_locked(thread, T_DELAY|T_PEND|T_WAIT,
 				T_KICKED|T_BREAK);
+
+	/*
+	 * If @thread receives multiple ptrace-stop requests, ensure
+	 * that disabling T_PTJOIN has precedence over enabling for
+	 * the whole set.
+	 */
+	if (thread->info & T_PTSTOP) {
+		if (thread->info & T_PTJOIN)
+			thread->info &= ~T_PTJOIN;
+		else
+			info &= ~T_PTJOIN;
+	}
+
 	/*
 	 * CAUTION: we must NOT raise T_BREAK when clearing a forcible
 	 * block state, such as T_SUSP, T_HALT. The caller of
@@ -1157,8 +1190,24 @@ void evl_kick_thread(struct evl_thread *thread)
 	 * Callers of evl_sleep_on() may inquire for T_KICKED locally
 	 * to detect forcible unblocks from T_SUSP, T_HALT, if they
 	 * should act upon this case specifically.
+	 *
+	 * If @thread was frozen by an ongoing ptrace sync sequence
+	 * (T_PTSYNC), release it so that it can reach the next
+	 * in-band switch point (either from the EVL syscall return
+	 * path, or from the mayday trap).
+	 */
+	evl_release_thread_locked(thread, T_SUSP|T_HALT|T_PTSYNC, T_KICKED);
+
+	/*
+	 * We may send mayday signals to userland threads only.
+	 * However, no need to run a mayday trap if the current thread
+	 * kicks itself out of OOB context: it will switch to in-band
+	 * context on its way back to userland via the current syscall
+	 * epilogue. Otherwise, we want that thread to enter the
+	 * mayday trap asap.
 	 */
-	evl_release_thread_locked(thread, T_SUSP|T_HALT, T_KICKED);
+	if ((thread->state & T_USER) && thread != this_evl_rq_thread())
+		dovetail_send_mayday(p);
 
 	/*
 	 * Tricky cases:
@@ -1171,7 +1220,7 @@ void evl_kick_thread(struct evl_thread *thread)
 	 *
 	 * - a ready/readied thread on exit may be prevented from
 	 * running by the scheduling policy module it belongs
-	 * to. Typically, policies enforcing a runtime budget do not
+ 	 * to. Typically, policies enforcing a runtime budget do not
 	 * block threads with no budget, but rather keep them out of
 	 * their run queue, so that ->sched_pick() won't elect
 	 * them. We tell the policy handler about the fact that we do
@@ -1188,16 +1237,8 @@ void evl_kick_thread(struct evl_thread *thread)
 	else
 		thread->info |= T_KICKED;
 
-	/*
-	 * We may send mayday signals to userland threads only.
-	 * However, no need to run a mayday trap if the current thread
-	 * kicks itself out of OOB context: it will switch to in-band
-	 * context on its way back to userland via the current syscall
-	 * epilogue. Otherwise, we want that thread to enter the
-	 * mayday trap asap.
-	 */
-	if ((thread->state & T_USER) && thread != this_evl_rq_thread())
-		dovetail_send_mayday(p);
+	if (info)
+		thread->info |= info;
 out:
 	evl_put_thread_rq(thread, rq, flags);
 }
@@ -1226,7 +1267,7 @@ void evl_demote_thread(struct evl_thread *thread)
 	evl_put_thread_rq(thread, rq, flags);
 
 	/* Then unblock it from any wait state. */
-	evl_kick_thread(thread);
+	evl_kick_thread(thread, 0);
 }
 EXPORT_SYMBOL_GPL(evl_demote_thread);
 
@@ -1397,6 +1438,40 @@ pid_t evl_get_inband_pid(struct evl_thread *thread)
 	return task_pid_nr(thread->altsched.task);
 }
 
+int activate_oob_mm_state(struct oob_mm_state *p)
+{
+	/*
+	 * A bit silly but we need a dynamic allocation for the EVL
+	 * wait queue only to work around some inclusion hell when
+	 * defining EVL's version of struct oob_mm_state.
+	 */
+	p->ptsync_barrier = kmalloc(sizeof(*p->ptsync_barrier), GFP_KERNEL);
+	if (p->ptsync_barrier == NULL)
+		return -ENOMEM;
+
+	evl_init_wait(p->ptsync_barrier, &evl_mono_clock, EVL_WAIT_PRIO);
+	INIT_LIST_HEAD(&p->ptrace_sync);
+	smp_mb__before_atomic();
+	set_bit(EVL_MM_ACTIVE_BIT, &p->flags);
+
+	return 0;
+}
+
+static void flush_oob_mm_state(struct oob_mm_state *p)
+{
+	/*
+	 * We are called for every mm dropped. Since every oob state
+	 * is zeroed before use by the in-band kernel, processes with
+	 * no active out-of-band state will escape this cleanup work
+	 * on test_and_clear_bit().
+	 */
+	if (test_and_clear_bit(EVL_MM_ACTIVE_BIT, &p->flags)) {
+		EVL_WARN_ON(CORE, !list_empty(&p->ptrace_sync));
+		evl_destroy_wait(p->ptsync_barrier);
+		kfree(p->ptsync_barrier);
+	}
+}
+
 void arch_inband_task_init(struct task_struct *tsk)
 {
 	struct oob_thread_state *p = dovetail_task_state(tsk);
@@ -1430,14 +1505,14 @@ void handle_oob_trap(unsigned int trapnr, struct pt_regs *regs)
 	struct evl_thread *curr;
 	bool is_bp = false;
 
-	oob_context_only();
-
 	curr = evl_current();
 	if (curr->local_info & T_INFAULT) {
 		note_trap(curr, trapnr, regs, "recursive fault");
 		return;
 	}
 
+	oob_context_only();
+
 	curr->local_info |= T_INFAULT;
 
 	trace_evl_thread_fault(trapnr, regs);
@@ -1450,10 +1525,9 @@ void handle_oob_trap(unsigned int trapnr, struct pt_regs *regs)
 
 	/*
 	 * We received a trap on the oob stage, switch to in-band
-	 * before handling the exception. Don't emit SIGDEBUG if the
-	 * fault was caused by a debugger breakpoint.
+	 * before handling the exception.
 	 */
-	evl_switch_inband(is_bp ? SIGDEBUG_NONE : SIGDEBUG_MIGRATE_FAULT);
+	evl_switch_inband(is_bp ? SIGDEBUG_TRAP : SIGDEBUG_MIGRATE_FAULT);
 
 	curr->local_info &= ~T_INFAULT;
 }
@@ -1474,10 +1548,9 @@ void handle_oob_mayday(struct pt_regs *regs)
 		evl_switch_inband(SIGDEBUG_NONE);
 }
 
-#ifdef CONFIG_SMP
-
 static void handle_migration_event(struct dovetail_migration_data *d)
 {
+#ifdef CONFIG_SMP
 	struct task_struct *p = d->task;
 	struct evl_thread *thread;
 
@@ -1503,127 +1576,241 @@ static void handle_migration_event(struct dovetail_migration_data *d)
 	 */
 	if (thread->state & (EVL_THREAD_BLOCK_BITS & ~T_INBAND))
 		evl_signal_thread(thread, SIGEVL, SIGEVL_ACTION_HOME);
+#endif
 }
 
-#else /* !CONFIG_SMP */
+static void handle_sigwake_event(struct task_struct *p)
+{
+	struct evl_thread *thread;
+	sigset_t sigpending;
+	bool ptsync = false;
+	int info = 0;
 
-static void handle_migration_event(struct dovetail_migration_data *d)
+	thread = evl_thread_from_task(p);
+	if (thread == NULL)
+		return;
+
+	if (thread->state & T_USER && p->ptrace & PT_PTRACED) {
+		/* We already own p->sighand->siglock. */
+		sigorsets(&sigpending,
+			&p->pending.signal,
+			&p->signal->shared_pending.signal);
+
+		if (sigismember(&sigpending, SIGINT) ||
+			sigismember(&sigpending, SIGTRAP)) {
+			info = T_PTSIG|T_PTSTOP;
+			ptsync = true;
+		}
+		/*
+		 * CAUTION: we want T_JOIN to appear whenever SIGSTOP
+		 * is present, regardless of other signals which might
+		 * be pending.
+		 */
+		if (sigismember(&sigpending, SIGSTOP))
+			info |= T_PTSIG|T_PTSTOP|T_PTJOIN;
+	}
+
+	/*
+	 * A thread running on the oob stage may not be picked by the
+	 * in-band scheduler as it bears the _TLF_OFFSTAGE flag. We
+	 * need to force that thread to switch to in-band context,
+	 * which will clear that flag. If we got there due to a ptrace
+	 * signal, then setting T_PTSTOP ensures that @thread will be
+	 * released from T_PTSYNC and will not receive any WOSS alert
+	 * next time it switches in-band.
+	 */
+	evl_kick_thread(thread, info);
+
+	/*
+	 * Start a ptrace sync sequence if @thread is the initial stop
+	 * target and runs oob. It is important to do this asap, so
+	 * that sibling threads from the same process which also run
+	 * oob cannot delay the in-band ptrace chores on this CPU,
+	 * moving too far away from the stop point.
+	 */
+	if (ptsync)
+		evl_start_ptsync(thread);
+
+	evl_schedule();
+}
+
+/* curr locked, curr->rq locked. */
+static void join_ptsync(struct evl_thread *curr)
 {
+	struct oob_mm_state *oob_mm = curr->oob_mm;
+
+	evl_spin_lock(&oob_mm->ptsync_barrier->lock);
+
+	/* In non-stop mode, no ptsync sequence is started. */
+	if (test_bit(EVL_MM_PTSYNC_BIT, &oob_mm->flags) &&
+		list_empty(&curr->ptsync_next))
+		list_add_tail(&curr->ptsync_next, &oob_mm->ptrace_sync);
+
+	evl_spin_unlock(&oob_mm->ptsync_barrier->lock);
+}
+
+static int leave_ptsync(struct evl_thread *leaver)
+{
+	struct oob_mm_state *oob_mm = leaver->oob_mm;
+	unsigned long flags;
+	int ret = 0;
+
+	evl_spin_lock_irqsave(&oob_mm->ptsync_barrier->lock, flags);
+
+	if (!test_bit(EVL_MM_PTSYNC_BIT, &oob_mm->flags))
+		goto out;
+
+	ret = -1;
+	if (!list_empty(&leaver->ptsync_next))
+		list_del_init(&leaver->ptsync_next);
+
+	if (list_empty(&oob_mm->ptrace_sync)) {
+		clear_bit(EVL_MM_PTSYNC_BIT, &oob_mm->flags);
+		ret = 1;
+	}
+out:
+	evl_spin_unlock_irqrestore(&oob_mm->ptsync_barrier->lock, flags);
+
+	return ret;
 }
 
-#endif /* CONFIG_SMP */
+static void skip_ptsync(struct evl_thread *thread)
+{
+	struct oob_mm_state *oob_mm = thread->oob_mm;
+
+	if (test_bit(EVL_MM_ACTIVE_BIT, &oob_mm->flags) &&
+		leave_ptsync(thread) > 0) {
+		evl_flush_wait(oob_mm->ptsync_barrier, 0);
+		evl_schedule();
+	}
+}
 
-static void handle_schedule_event(struct task_struct *next_task)
+static void handle_ptstop_event(void)
 {
-	struct task_struct *prev_task;
-	struct evl_thread *next;
+	struct evl_thread *curr = evl_current();
 	unsigned long flags;
-	sigset_t pending;
+	struct evl_rq *rq;
 
-	prev_task = current;
-	next = evl_thread_from_task(next_task);
-	if (next == NULL)
-		return;
+	/*
+	 * T_PTRACE denotes a stopped state as defined by ptrace()
+	 * which means blocked in ptrace_stop(). Our T_PTSTOP bit has
+	 * a broader scope which starts from the in-band request to
+	 * stop (handle_sigwake_event()), then ends after the tracee
+	 * switched back to oob context via RETUSER handler.
+	 */
+	rq = evl_get_thread_rq(curr, flags);
+
+	curr->state |= T_PTRACE;
 
 	/*
-	 * Check whether we need to unlock the timers, each time a
-	 * Linux task resumes from a stopped state, excluding tasks
-	 * resuming shortly for entering a stopped state asap due to
-	 * ptracing. To identify the latter, we need to check for
-	 * SIGSTOP and SIGINT in order to encompass both the NPTL and
-	 * LinuxThreads behaviours.
+	 * If we were running out-of-band when SIGSTOP reached us, we
+	 * have to join the ptsync queue.
 	 */
-	evl_spin_lock_irqsave(&next->lock, flags);
-	if (next->state & T_SSTEP) {
-		if (signal_pending(next_task)) {
-			/*
-			 * Do not grab the sighand lock here: it's
-			 * useless, and we already own the runqueue
-			 * lock, so this would expose us to deadlock
-			 * situations on SMP.
-			 */
-			sigorsets(&pending,
-				&next_task->pending.signal,
-				&next_task->signal->shared_pending.signal);
-			if (sigismember(&pending, SIGSTOP) ||
-				sigismember(&pending, SIGINT))
-				goto check;
-		}
-		evl_spin_lock(&next->rq->lock);
-		next->state &= ~T_SSTEP;
-		evl_spin_unlock(&next->rq->lock);
-		next->local_info |= T_HICCUP;
+	if (curr->info & T_PTJOIN) {
+		join_ptsync(curr);
+		curr->info &= ~T_PTJOIN;
 	}
 
-check:
-	evl_spin_unlock_irqrestore(&next->lock, flags);
+	evl_put_thread_rq(curr, rq, flags);
+}
+
+static void handle_ptstep_event(struct task_struct *task)
+{
+	struct evl_thread *tracee = evl_thread_from_task(task);
 
 	/*
-	 * Do basic sanity checks on the incoming thread state.
-	 * NOTE: we allow ptraced threads to run shortly in order to
-	 * properly recover from a stopped state.
+	 * The ptracer might have switched focus, (single-)stepping a
+	 * thread which did not hit the latest breakpoint
+	 * (i.e. bearing T_PTJOIN). For this reason, we do need to
+	 * listen to PTSTEP events to remove that thread from the
+	 * ptsync queue.
 	 */
-	if (!EVL_WARN(CORE, !(next->state & T_INBAND),
-			"Ouch: out-of-band thread %s[%d] running on the in-band stage"
-			"(status=0x%x, sig=%d, prev=%s[%d])",
-			next->name, task_pid_nr(next_task),
-			next->state,
-			signal_pending(next_task),
-			prev_task->comm, task_pid_nr(prev_task)))
-		EVL_WARN(CORE,
-			!(next_task->ptrace & PT_PTRACED) &&
-			!(next->state & T_DORMANT)
-			&& (next->state & T_PEND),
-			"Ouch: blocked EVL thread %s[%d] rescheduled in-band"
-			"(status=0x%x, sig=%d, prev=%s[%d])",
-			next->name, task_pid_nr(next_task),
-			next->state,
-			signal_pending(next_task), prev_task->comm,
-			task_pid_nr(prev_task));
+	skip_ptsync(tracee);
 }
 
-static void handle_sigwake_event(struct task_struct *p)
+static void handle_ptcont_event(void)
 {
-	struct evl_thread *thread;
-	unsigned long flags;
-	sigset_t pending;
+	struct evl_thread *curr = evl_current();
 
-	thread = evl_thread_from_task(p);
-	if (thread == NULL)
-		return;
+	if (curr->state & T_PTRACE) {
+		/*
+		 * Since we stopped executing due to ptracing, any
+		 * ongoing periodic timeline is now lost: disable
+		 * overrun detection for the next round.
+		 */
+		curr->local_info |= T_IGNOVR;
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
+		/*
+		 * Request to receive INBAND_TASK_RETUSER on the
+		 * return path to user mode so that we can switch back
+		 * to out-of-band mode for synchronizing on the ptsync
+		 * barrier.
+		 */
+		dovetail_request_ucall(current);
+	}
+}
+
+/* oob stage, hard irqs on. */
+static int ptrace_sync(void)
+{
+	struct evl_thread *curr = evl_current();
+	struct oob_mm_state *oob_mm = curr->oob_mm;
+	struct evl_rq *this_rq = curr->rq;
+	unsigned long flags;
+	bool sigpending;
+	int ret;
 
 	/*
-	 * CAUTION: __TASK_TRACED is not set in p->state yet. This
-	 * state bit will be set right after we return, when the task
-	 * is woken up.
+	 * The last thread resuming from a ptsync to switch back to
+	 * out-of-band mode has to release the others which have been
+	 * waiting for this event on the ptrace sync barrier.
 	 */
-	if ((p->ptrace & PT_PTRACED) && !(thread->state & T_SSTEP)) {
-		/* We already own the siglock. */
-		sigorsets(&pending,
-			&p->pending.signal,
-			&p->signal->shared_pending.signal);
+	sigpending = signal_pending(current);
+	ret = leave_ptsync(curr);
+	if (ret > 0) {
+		evl_flush_wait(oob_mm->ptsync_barrier, 0);
+		ret = 0;
+	} else if (ret < 0)
+		ret = sigpending ? -ERESTARTSYS :
+			evl_wait_event(oob_mm->ptsync_barrier,
+				list_empty(&oob_mm->ptrace_sync));
+
+	evl_spin_lock_irqsave(&this_rq->lock, flags);
 
-		if (sigismember(&pending, SIGTRAP) ||
-			sigismember(&pending, SIGSTOP)
-			|| sigismember(&pending, SIGINT))
-			evl_spin_lock(&thread->rq->lock);
-			thread->state |= T_SSTEP;
-			evl_spin_unlock(&thread->rq->lock);
+	/*
+	 * If we got interrupted while waiting on the ptsync barrier,
+	 * make sure pick_next_thread() will let us slip through again
+	 * by keeping T_PTSTOP set.
+	 */
+	if (!ret && !(curr->info & T_PTSIG)) {
+		curr->info &= ~T_PTSTOP;
+		curr->state &= ~T_PTRACE;
 	}
 
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_spin_unlock_irqrestore(&this_rq->lock, flags);
 
-	/*
-	 * A thread running on the oob stage may not be picked by the
-	 * in-band scheduler as it bears the _TLF_OFFSTAGE flag. We
-	 * need to force that thread to switch to in-band context,
-	 * which will clear that flag.
-	 */
-	evl_kick_thread(thread);
+	return ret ? -ERESTARTSYS : 0;
+}
 
-	evl_schedule();
+static void handle_retuser_event(void)
+{
+	struct evl_thread *curr = evl_current();
+	int ret;
+
+	ret = evl_switch_oob();
+	if (ret) {
+		/* Ask for retry until we succeed. */
+		dovetail_request_ucall(current);
+		return;
+	}
+
+	if (curr->state & T_PTRACE) {
+		ret = ptrace_sync();
+		if (ret)
+			dovetail_request_ucall(current);
+
+		evl_schedule();
+	}
 }
 
 static void handle_cleanup_event(struct mm_struct *mm)
@@ -1631,28 +1818,30 @@ static void handle_cleanup_event(struct mm_struct *mm)
 	struct evl_thread *curr = evl_current();
 
 	/*
+	 * This event is fired whenever a user task is dropping its
+	 * mm.
+	 *
 	 * Detect an EVL thread running exec(), i.e. still attached to
-	 * the current Linux task (PF_EXITING is cleared for a task
-	 * which did not explicitly run do_exit()). In this case, we
+	 * the current in-band task but not bearing PF_EXITING, which
+	 * indicates that it did not call do_exit(). In this case, we
 	 * emulate a task exit, since the EVL binding shall not
-	 * survive the exec() syscall.
+	 * survive the exec() syscall.  We may get there after
+	 * cleanup_current_thread() already ran, so check @curr for
+	 * NULL.
 	 *
-	 * NOTE: We are called for every userland task exiting from
-	 * in-band context. We are NOT called for exiting kernel
-	 * threads since they have no mm proper. We may get there
-	 * after cleanup_current_thread() already ran though, so check
-	 * @curr.
+	 * Otherwise, release the oob state for the dropped mm if
+	 * any. We may or may not have one, EVL_MM_ACTIVE_BIT tells us
+	 * so.
 	 */
 	if (curr && !(current->flags & PF_EXITING))
 		put_current_thread();
+	else
+		flush_oob_mm_state(&mm->oob_state);
 }
 
 void handle_inband_event(enum inband_event_type event, void *data)
 {
 	switch (event) {
-	case INBAND_TASK_SCHEDULE:
-		handle_schedule_event(data);
-		break;
 	case INBAND_TASK_SIGNAL:
 		handle_sigwake_event(data);
 		break;
@@ -1662,6 +1851,18 @@ void handle_inband_event(enum inband_event_type event, void *data)
 	case INBAND_TASK_MIGRATION:
 		handle_migration_event(data);
 		break;
+	case INBAND_TASK_RETUSER:
+		handle_retuser_event();
+		break;
+	case INBAND_TASK_PTSTOP:
+		handle_ptstop_event();
+		break;
+	case INBAND_TASK_PTCONT:
+		handle_ptcont_event();
+		break;
+	case INBAND_TASK_PTSTEP:
+		handle_ptstep_event(data);
+		break;
 	case INBAND_PROCESS_CLEANUP:
 		handle_cleanup_event(data);
 		break;
@@ -1960,6 +2161,7 @@ static int map_uthread_self(struct evl_thread *thread)
 
 	dovetail_init_altsched(&thread->altsched);
 	set_oob_threadinfo(thread);
+	set_oob_mminfo(thread);
 
 	/*
 	 * CAUTION: we enable dovetailing only when *thread is
@@ -2008,6 +2210,10 @@ thread_factory_build(struct evl_factory *fac, const char *name,
 	if (evl_current())
 		return ERR_PTR(-EBUSY);
 
+	/* @current must open the control device first. */
+	if (!test_bit(EVL_MM_ACTIVE_BIT, &dovetail_mm_state()->flags))
+		return ERR_PTR(-EPERM);
+
 	curr = kzalloc(sizeof(*curr), GFP_KERNEL);
 	if (curr == NULL)
 		return ERR_PTR(-ENOMEM);
diff --git a/kernel/evl/timer.c b/kernel/evl/timer.c
index 640dccb67e30..2353bec4a1c0 100644
--- a/kernel/evl/timer.c
+++ b/kernel/evl/timer.c
@@ -449,7 +449,7 @@ unsigned long evl_get_timer_overruns(struct evl_timer *timer)
 	 * the caller.
 	 */
 	thread = evl_current();
-	if (thread->local_info & T_HICCUP)
+	if (thread->local_info & T_IGNOVR)
 		return 0;
 
 	return overruns;
-- 
2.16.4

