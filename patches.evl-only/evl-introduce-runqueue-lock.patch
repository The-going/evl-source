From b89a3b024c2d06732c47ca1196baeb63cf207f30 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Thu, 31 Oct 2019 18:31:28 +0100
Subject: [PATCH] evl: introduce runqueue lock

This is the last step to get rid of the ugly lock, which was still
required for serializing accesses to the runqueue information.
---
 include/evl/assert.h       |  10 +-
 include/evl/irq.h          |  12 +-
 include/evl/sched.h        | 179 +++++++++------
 include/evl/thread.h       |  25 +-
 include/evl/tick.h         |   2 +-
 include/evl/timer.h        |  17 --
 include/trace/events/evl.h |  28 ++-
 kernel/evl/clock.c         |  37 ++-
 kernel/evl/monitor.c       |  14 +-
 kernel/evl/mutex.c         |  33 +--
 kernel/evl/sched/core.c    | 552 ++++++++++++++++++++++++++++++++++++---------
 kernel/evl/sched/quota.c   |  34 +--
 kernel/evl/sched/tp.c      |  31 +--
 kernel/evl/syscall.c       |   8 +-
 kernel/evl/thread.c        | 455 ++++++-------------------------------
 kernel/evl/tick.c          |  12 +-
 kernel/evl/timer.c         |  14 +-
 17 files changed, 766 insertions(+), 697 deletions(-)

diff --git a/include/evl/assert.h b/include/evl/assert.h
index ad5faf8667f9..2fff3c558112 100644
--- a/include/evl/assert.h
+++ b/include/evl/assert.h
@@ -36,10 +36,18 @@
 #ifdef CONFIG_SMP
 #define assert_hard_lock(__lock) EVL_WARN_ON_ONCE(CORE, \
 				!(raw_spin_is_locked(__lock) && hard_irqs_disabled()))
+#define assert_evl_lock(__lock) EVL_WARN_ON_ONCE(CORE, \
+				!(raw_spin_is_locked(&(__lock)->_lock) && oob_irqs_disabled()))
 #else
 #define assert_hard_lock(__lock) EVL_WARN_ON_ONCE(CORE, !hard_irqs_disabled())
+#define assert_evl_lock(__lock)  EVL_WARN_ON_ONCE(CORE, !oob_irqs_disabled())
 #endif
-#define assert_evl_lock(__lock) assert_hard_lock(&(__lock)->_lock)
+
+#define assert_thread_pinned(__thread)			\
+	do {						\
+		assert_evl_lock(&(__thread)->lock);	\
+		assert_evl_lock(&(__thread)->rq->lock);	\
+	} while (0)
 
 /* TEMP: needed until we have gotten rid of the infamous nklock. */
 #ifdef CONFIG_SMP
diff --git a/include/evl/irq.h b/include/evl/irq.h
index dd5cd274f66b..8720c8969846 100644
--- a/include/evl/irq.h
+++ b/include/evl/irq.h
@@ -9,20 +9,20 @@
 
 #include <evl/sched.h>
 
-/* hard IRQs off. */
+/* hard irqs off. */
 static inline void evl_enter_irq(void)
 {
 	struct evl_rq *rq = this_evl_rq();
 
-	rq->lflags |= RQ_IRQ;
+	rq->local_flags |= RQ_IRQ;
 }
 
-/* hard IRQs off. */
+/* hard irqs off. */
 static inline void evl_exit_irq(void)
 {
-	struct evl_rq *rq = this_evl_rq();
+	struct evl_rq *this_rq = this_evl_rq();
 
-	rq->lflags &= ~RQ_IRQ;
+	this_rq->local_flags &= ~RQ_IRQ;
 
 	/*
 	 * We are only interested in RQ_SCHED previously set by an OOB
@@ -33,7 +33,7 @@ static inline void evl_exit_irq(void)
 	 * CAUTION: Switching stages as a result of rescheduling may
 	 * re-enable irqs, shut them off before returning if so.
 	 */
-	if (rq->status & RQ_SCHED) {
+	if (evl_need_resched(this_rq)) {
 		evl_schedule();
 		if (!hard_irqs_disabled())
 			hard_local_irq_disable();
diff --git a/include/evl/sched.h b/include/evl/sched.h
index 884c8433c4c7..4b1cd974c4d4 100644
--- a/include/evl/sched.h
+++ b/include/evl/sched.h
@@ -19,19 +19,18 @@
 #include <evl/assert.h>
 #include <evl/init.h>
 
-/** Shared scheduler status bits **/
-
 /*
- * A rescheduling call is pending.
+ * Shared rq flags bits.
+ *
+ * A rescheduling operation is pending. May also be present in the
+ * private flags.
  */
 #define RQ_SCHED	0x10000000
 
 /**
- * Private scheduler flags (combined in test operations with shared
- * bits, must not conflict with them).
- */
-
-/*
+ * Private rq flags (combined in test operation with shared bits by
+ * evl_schedule(), care for any conflict).
+ *
  * Currently running in tick handler context.
  */
 #define RQ_TIMER	0x00010000
@@ -45,14 +44,13 @@
  */
 #define RQ_IRQ		0x00004000
 /*
- * Proxy tick is deferred, because we have more urgent real-time
- * duties to carry out first.
+ * Proxy tick is deferred, because we have more urgent out-of-band
+ * work to carry out first.
  */
 #define RQ_TDEFER	0x00002000
 /*
  * Idle state: there is no outstanding timer. We check this flag to
- * know whether we may allow the regular kernel to enter the CPU idle
- * state.
+ * know whether we may allow inband to enter the CPU idle state.
  */
 #define RQ_IDLE		0x00001000
 /*
@@ -65,13 +63,13 @@ struct evl_sched_fifo {
 };
 
 struct evl_rq {
-	unsigned long status;	/* Shared flags */
-	unsigned long lflags;	/* Private flags (lockless) */
+	evl_spinlock_t lock;
+
+	/*
+	 * Shared data, covered by ->lock.
+	 */
+	unsigned long flags;
 	struct evl_thread *curr;
-#ifdef CONFIG_SMP
-	int cpu;
-	struct cpumask resched;	/* CPUs pending resched */
-#endif
 	struct evl_sched_fifo fifo;
 	struct evl_sched_weak weak;
 #ifdef CONFIG_EVL_SCHED_QUOTA
@@ -80,18 +78,28 @@ struct evl_rq {
 #ifdef CONFIG_EVL_SCHED_TP
 	struct evl_sched_tp tp;
 #endif
-	struct evl_timer inband_timer;
-	struct evl_timer rrbtimer; /* Round-robin */
 	struct evl_thread root_thread;
-	char *proxy_timer_name;
-	char *rrb_timer_name;
-#ifdef CONFIG_EVL_WATCHDOG
-	struct evl_timer wdtimer;
-#endif
 #ifdef CONFIG_EVL_RUNSTATS
 	ktime_t last_account_switch;
 	struct evl_account *current_account;
 #endif
+
+	/*
+	 * runqueue-local data the owner may modify locklessly.
+	 */
+	unsigned long local_flags;
+#ifdef CONFIG_SMP
+	int cpu;
+	struct cpumask resched_cpus;
+#endif
+	struct evl_timer inband_timer;
+	struct evl_timer rrbtimer;
+#ifdef CONFIG_EVL_WATCHDOG
+	struct evl_timer wdtimer;
+#endif
+	/* Misc stuff. */
+	char *proxy_timer_name;
+	char *rrb_timer_name;
 };
 
 DECLARE_PER_CPU(struct evl_rq, evl_runqueues);
@@ -187,17 +195,17 @@ static inline struct evl_thread *this_evl_rq_thread(void)
 /* Test resched flag of given rq. */
 static inline int evl_need_resched(struct evl_rq *rq)
 {
-	return rq->status & RQ_SCHED;
+	return rq->flags & RQ_SCHED;
 }
 
-/* Set self resched flag for the current scheduler. */
+/* Set resched flag for the current rq. */
 static inline void evl_set_self_resched(struct evl_rq *rq)
 {
-	requires_ugly_lock();
-	rq->status |= RQ_SCHED;
+	assert_evl_lock(&rq->lock);
+	rq->flags |= RQ_SCHED;
 }
 
-/* Set resched flag for the given scheduler. */
+/* Set resched flag for the given rq. */
 #ifdef CONFIG_SMP
 
 static inline bool is_evl_cpu(int cpu)
@@ -214,12 +222,25 @@ static inline void evl_set_resched(struct evl_rq *rq)
 {
 	struct evl_rq *this_rq = this_evl_rq();
 
-	if (this_rq == rq)
-		this_rq->status |= RQ_SCHED;
-	else if (!evl_need_resched(rq)) {
-		cpumask_set_cpu(evl_rq_cpu(rq), &this_rq->resched);
-		rq->status |= RQ_SCHED;
-		this_rq->status |= RQ_SCHED;
+	assert_evl_lock(&rq->lock); /* Implies oob is stalled. */
+
+	if (this_rq == rq) {
+		this_rq->flags |= RQ_SCHED;
+	} else if (!evl_need_resched(rq)) {
+		rq->flags |= RQ_SCHED;
+		/*
+		 * The following updates change CPU-local data and oob
+		 * is stalled on the current CPU, so this is safe
+		 * despite that we don't hold this_rq->lock.
+		 *
+		 * NOTE: raising RQ_SCHED in the local_flags too
+		 * ensures that the current CPU will pass through
+		 * evl_schedule() to __evl_schedule() at the next
+		 * opportunity for sending the resched IPIs (see
+		 * test_resched()).
+		 */
+		this_rq->local_flags |= RQ_SCHED;
+		cpumask_set_cpu(evl_rq_cpu(rq), &this_rq->resched_cpus);
 	}
 }
 
@@ -228,6 +249,9 @@ static inline bool is_threading_cpu(int cpu)
 	return !!cpumask_test_cpu(cpu, &evl_cpu_affinity);
 }
 
+void evl_migrate_thread(struct evl_thread *thread,
+			struct evl_rq *dst_rq);
+
 #else /* !CONFIG_SMP */
 
 static inline bool is_evl_cpu(int cpu)
@@ -250,6 +274,11 @@ static inline bool is_threading_cpu(int cpu)
 	return true;
 }
 
+static inline
+void evl_migrate_thread(struct evl_thread *thread,
+			struct evl_rq *dst_rq)
+{ }
+
 #endif /* !CONFIG_SMP */
 
 #define for_each_evl_cpu(cpu)		\
@@ -263,11 +292,11 @@ static inline void evl_schedule(void)
 	struct evl_rq *this_rq = this_evl_rq();
 
 	/*
-	 * If we race here reading the scheduler state locklessly
-	 * because of a CPU migration, we must be running over the
-	 * in-band stage, in which case the call to __evl_schedule()
-	 * will be escalated to the oob stage where migration cannot
-	 * happen, ensuring safe access to the runqueue state.
+	 * If we race here reading the rq state locklessly because of
+	 * a CPU migration, we must be running over the in-band stage,
+	 * in which case the call to __evl_schedule() will be
+	 * escalated to the oob stage where migration cannot happen,
+	 * ensuring safe access to the runqueue state.
 	 *
 	 * Remote RQ_SCHED requests are paired with out-of-band IPIs
 	 * running on the oob stage by definition, so we can't miss
@@ -276,7 +305,7 @@ static inline void evl_schedule(void)
 	 * Finally, RQ_IRQ is always tested from the CPU which handled
 	 * an out-of-band interrupt, there is no coherence issue.
 	 */
-	if (((this_rq->status|this_rq->lflags) & (RQ_IRQ|RQ_SCHED)) != RQ_SCHED)
+	if (((this_rq->flags|this_rq->local_flags) & (RQ_IRQ|RQ_SCHED)) != RQ_SCHED)
 		return;
 
 	if (likely(running_oob())) {
@@ -287,6 +316,10 @@ static inline void evl_schedule(void)
 	run_oob_call((int (*)(void *))__evl_schedule, NULL);
 }
 
+int evl_switch_oob(void);
+
+void evl_switch_inband(int cause);
+
 static inline int evl_preempt_count(void)
 {
 	return dovetail_current_state()->preempt_count;
@@ -300,7 +333,7 @@ static inline void __evl_disable_preempt(void)
 static inline void __evl_enable_preempt(void)
 {
 	if (--dovetail_current_state()->preempt_count == 0 &&
-		!hard_irqs_disabled())
+		!oob_irqs_disabled())
 		evl_schedule();
 }
 
@@ -325,7 +358,7 @@ static inline void evl_enable_preempt(void)
 
 static inline bool evl_in_irq(void)
 {
-	return !!(this_evl_rq()->lflags & RQ_IRQ);
+	return !!(this_evl_rq()->local_flags & RQ_IRQ);
 }
 
 static inline bool evl_is_inband(void)
@@ -338,6 +371,21 @@ static inline bool evl_cannot_block(void)
 	return evl_in_irq() || evl_is_inband();
 }
 
+#define evl_get_thread_rq(__thread, __flags)				\
+	({								\
+		struct evl_rq *__rq;					\
+		evl_spin_lock_irqsave(&(__thread)->lock, __flags);	\
+		__rq = (__thread)->rq;					\
+		evl_spin_lock(&__rq->lock);				\
+		__rq;							\
+	})
+
+#define evl_put_thread_rq(__thread, __rq, __flags)			\
+	do {								\
+		evl_spin_unlock(&(__rq)->lock);				\
+		evl_spin_unlock_irqrestore(&(__thread)->lock, __flags);	\
+	} while (0)
+
 bool evl_set_effective_thread_priority(struct evl_thread *thread,
 				       int prio);
 
@@ -360,9 +408,6 @@ void evl_track_thread_policy(struct evl_thread *thread,
 void evl_protect_thread_priority(struct evl_thread *thread,
 				 int prio);
 
-void evl_migrate_rq(struct evl_thread *thread,
-		    struct evl_rq *rq);
-
 static inline
 void evl_rotate_rq(struct evl_rq *rq,
 		   struct evl_sched_class *sched_class,
@@ -391,19 +436,18 @@ static inline int evl_init_rq_thread(struct evl_thread *thread)
 	return ret;
 }
 
-/* nklock held, irqs off */
+/* rq->lock held, irqs off */
 static inline void evl_sched_tick(struct evl_rq *rq)
 {
 	struct evl_thread *curr = rq->curr;
 	struct evl_sched_class *sched_class = curr->sched_class;
 
-	requires_ugly_lock();
+	assert_evl_lock(&rq->lock);
 
 	/*
 	 * A thread that undergoes round-robin scheduling only
 	 * consumes its time slice when it runs within its own
-	 * scheduling class, which excludes temporary PI boosts, and
-	 * does not hold the scheduler lock.
+	 * scheduling class, which excludes temporary PI boosts.
 	 */
 	if (sched_class == curr->base_class &&
 	    sched_class->sched_tick &&
@@ -419,8 +463,7 @@ int evl_check_schedparams(struct evl_sched_class *sched_class,
 {
 	int ret = 0;
 
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	if (sched_class->sched_chkparam)
 		ret = sched_class->sched_chkparam(thread, p);
@@ -435,8 +478,7 @@ int evl_declare_thread(struct evl_sched_class *sched_class,
 {
 	int ret;
 
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	if (sched_class->sched_declare) {
 		ret = sched_class->sched_declare(thread, p);
@@ -459,8 +501,7 @@ static __always_inline void evl_enqueue_thread(struct evl_thread *thread)
 {
 	struct evl_sched_class *sched_class = thread->sched_class;
 
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	/*
 	 * Enqueue for next pick: i.e. move to end of current priority
@@ -476,8 +517,7 @@ static __always_inline void evl_dequeue_thread(struct evl_thread *thread)
 {
 	struct evl_sched_class *sched_class = thread->sched_class;
 
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	/*
 	 * Pull from the runnable thread queue.
@@ -492,8 +532,7 @@ static __always_inline void evl_requeue_thread(struct evl_thread *thread)
 {
 	struct evl_sched_class *sched_class = thread->sched_class;
 
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	/*
 	 * Put back at same place: i.e. requeue to head of current
@@ -509,8 +548,7 @@ static inline
 bool evl_set_schedparam(struct evl_thread *thread,
 			const union evl_sched_param *p)
 {
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	return thread->base_class->sched_setparam(thread, p);
 }
@@ -518,8 +556,7 @@ bool evl_set_schedparam(struct evl_thread *thread,
 static inline void evl_get_schedparam(struct evl_thread *thread,
 				      union evl_sched_param *p)
 {
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	thread->sched_class->sched_getparam(thread, p);
 }
@@ -527,8 +564,7 @@ static inline void evl_get_schedparam(struct evl_thread *thread,
 static inline void evl_track_priority(struct evl_thread *thread,
 				      const union evl_sched_param *p)
 {
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	thread->sched_class->sched_trackprio(thread, p);
 	thread->wprio = evl_calc_weighted_prio(thread->sched_class, thread->cprio);
@@ -536,8 +572,7 @@ static inline void evl_track_priority(struct evl_thread *thread,
 
 static inline void evl_ceil_priority(struct evl_thread *thread, int prio)
 {
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	thread->sched_class->sched_ceilprio(thread, prio);
 	thread->wprio = evl_calc_weighted_prio(thread->sched_class, thread->cprio);
@@ -547,8 +582,7 @@ static inline void evl_forget_thread(struct evl_thread *thread)
 {
 	struct evl_sched_class *sched_class = thread->base_class;
 
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	--sched_class->nthreads;
 
@@ -560,8 +594,7 @@ static inline void evl_force_thread(struct evl_thread *thread)
 {
 	struct evl_sched_class *sched_class = thread->base_class;
 
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	thread->info |= T_KICKED;
 
diff --git a/include/evl/thread.h b/include/evl/thread.h
index c6dcbe321089..e639c84cf00d 100644
--- a/include/evl/thread.h
+++ b/include/evl/thread.h
@@ -86,7 +86,8 @@ struct evl_thread {
 	ktime_t rrperiod;  /* Round-robin period (ns) */
 
 	/*
-	 * Shared data, covered by both thread->lock AND nklock.
+	 * Shared data, covered by both thread->lock AND
+	 * thread->rq->lock.
 	 */
 	__u32 state;
 	__u32 info;
@@ -100,10 +101,12 @@ struct evl_thread {
 	struct list_head tp_link;	/* evl_rq->tp.threads */
 #endif
 	struct list_head rq_next;	/* evl_rq->policy.runqueue */
+	struct list_head next;		/* in evl_thread_list */
 
 	/*
-	 * Thread-local data the owner may modified locklessly.
+	 * Thread-local data the owner may modify locklessly.
 	 */
+	struct dovetail_altsched_context altsched;
 	__u32 local_info;
 	void *wait_data;
 	struct {
@@ -112,6 +115,7 @@ struct evl_thread {
 		int nr;
 	} poll_context;
 	atomic_t inband_disable_count;
+	struct irq_work inband_work;
 	struct {
 		struct evl_counter isw;	/* in-band switches */
 		struct evl_counter csw;	/* context switches */
@@ -120,23 +124,18 @@ struct evl_thread {
 		struct evl_account account; /* exec time accounting */
 		struct evl_account lastperiod;
 	} stat;
+	struct evl_user_window *u_window;
 
 	/* Misc stuff. */
 
 	struct list_head trackers; /* Mutexes tracking @thread */
 	hard_spinlock_t tracking_lock;
-
-	struct list_head next;	/* in evl_thread_list */
-	struct dovetail_altsched_context altsched;
 	struct evl_element element;
 	struct cpumask affinity;
-
-	char *name;
 	struct completion exited;
-	struct irq_work inband_work;
 	kernel_cap_t raised_cap;
-	struct evl_user_window *u_window;
 	struct list_head kill_next;
+	char *name;
 };
 
 struct evl_kthread {
@@ -183,6 +182,10 @@ void __evl_test_cancel(struct evl_thread *curr);
 
 void evl_discard_thread(struct evl_thread *thread);
 
+/*
+ * Might differ from this_evl_rq() if @current is running inband, and
+ * evl_migrate_thread() is pending until it switches back to oob.
+ */
 static inline struct evl_thread *evl_current(void)
 {
 	return dovetail_current_state()->thread;
@@ -260,10 +263,6 @@ int evl_join_thread(struct evl_thread *thread,
 void evl_get_thread_state(struct evl_thread *thread,
 			struct evl_thread_state *statebuf);
 
-int evl_switch_oob(void);
-
-void evl_switch_inband(int cause);
-
 int evl_detach_self(void);
 
 void evl_kick_thread(struct evl_thread *thread);
diff --git a/include/evl/tick.h b/include/evl/tick.h
index 79bb5a852c79..581bb106b5d7 100644
--- a/include/evl/tick.h
+++ b/include/evl/tick.h
@@ -45,7 +45,7 @@ static inline void evl_notify_proxy_tick(struct evl_rq *this_rq)
 	 * previous set_next_ktime() request received from the kernel
 	 * we have carried out using our core timing services.
 	 */
-	this_rq->lflags &= ~RQ_TPROXY;
+	this_rq->local_flags &= ~RQ_TPROXY;
 	tick_notify_proxy();
 }
 
diff --git a/include/evl/timer.h b/include/evl/timer.h
index b25c6661368f..04dc8f67ce0d 100644
--- a/include/evl/timer.h
+++ b/include/evl/timer.h
@@ -319,15 +319,10 @@ static inline ktime_t evl_get_timer_expiry(struct evl_timer *timer)
 			evl_get_timer_gravity(timer));
 }
 
-/* no lock required. */
 ktime_t evl_get_timer_date(struct evl_timer *timer);
 
-/* no lock required. */
 ktime_t __evl_get_timer_delta(struct evl_timer *timer);
 
-ktime_t xntimer_get_interval(struct evl_timer *timer);
-
-/* no lock required. */
 static inline ktime_t evl_get_timer_delta(struct evl_timer *timer)
 {
 	if (!evl_timer_is_running(timer))
@@ -336,7 +331,6 @@ static inline ktime_t evl_get_timer_delta(struct evl_timer *timer)
 	return __evl_get_timer_delta(timer);
 }
 
-/* no lock required. */
 static inline
 ktime_t __evl_get_stopped_timer_delta(struct evl_timer *timer)
 {
@@ -382,13 +376,6 @@ void evl_move_timer(struct evl_timer *timer,
 
 #ifdef CONFIG_SMP
 
-static inline void evl_set_timer_rq(struct evl_timer *timer,
-				struct evl_rq *rq)
-{
-	if (rq != timer->rq)
-		evl_move_timer(timer, timer->clock, rq);
-}
-
 static inline void evl_prepare_timed_wait(struct evl_timer *timer,
 					struct evl_clock *clock,
 					struct evl_rq *rq)
@@ -406,10 +393,6 @@ static inline bool evl_timer_on_rq(struct evl_timer *timer,
 
 #else /* ! CONFIG_SMP */
 
-static inline void evl_set_timer_rq(struct evl_timer *timer,
-				struct evl_rq *rq)
-{ }
-
 static inline void evl_prepare_timed_wait(struct evl_timer *timer,
 					struct evl_clock *clock,
 					struct evl_rq *rq)
diff --git a/include/trace/events/evl.h b/include/trace/events/evl.h
index 96f9a86b71a0..7d654ea8f374 100644
--- a/include/trace/events/evl.h
+++ b/include/trace/events/evl.h
@@ -217,34 +217,32 @@ DECLARE_EVENT_CLASS(evl_clock_ident,
 	TP_printk("name=%s", __get_str(name))
 );
 
-TRACE_EVENT(evl_schedule,
+DECLARE_EVENT_CLASS(evl_schedule_event,
 	TP_PROTO(struct evl_rq *rq),
 	TP_ARGS(rq),
 
 	TP_STRUCT__entry(
-		__field(unsigned long, status)
+		__field(unsigned long, flags)
+		__field(unsigned long, local_flags)
 	),
 
 	TP_fast_assign(
-		__entry->status = rq->status;
+		__entry->flags = rq->flags;
+		__entry->local_flags = rq->local_flags;
 	),
 
-	TP_printk("status=%#lx", __entry->status)
+	TP_printk("flags=%#lx, local_flags=%#lx",
+		  __entry->flags, __entry->local_flags)
 );
 
-TRACE_EVENT(evl_schedule_remote,
+DEFINE_EVENT(evl_schedule_event, evl_schedule,
 	TP_PROTO(struct evl_rq *rq),
-	TP_ARGS(rq),
-
-	TP_STRUCT__entry(
-		__field(unsigned long, status)
-	),
-
-	TP_fast_assign(
-		__entry->status = rq->status;
-	),
+	TP_ARGS(rq)
+);
 
-	TP_printk("status=%#lx", __entry->status)
+DEFINE_EVENT(evl_schedule_event, evl_reschedule_ipi,
+	TP_PROTO(struct evl_rq *rq),
+	TP_ARGS(rq)
 );
 
 TRACE_EVENT(evl_switch_context,
diff --git a/kernel/evl/clock.c b/kernel/evl/clock.c
index cd6db5865c99..f96d878abb81 100644
--- a/kernel/evl/clock.c
+++ b/kernel/evl/clock.c
@@ -300,7 +300,7 @@ static void do_clock_tick(struct evl_clock *clock, struct evl_timerbase *tmb)
 	 * handler. This is a hint for the program_local_shot()
 	 * handler of the ticking clock.
 	 */
-	rq->lflags |= RQ_TIMER;
+	rq->local_flags |= RQ_TIMER;
 
 	now = evl_read_clock(clock);
 	while ((tn = evl_get_tqueue_head(tq)) != NULL) {
@@ -319,8 +319,8 @@ static void do_clock_tick(struct evl_clock *clock, struct evl_timerbase *tmb)
 		 * of the core tick interrupt.
 		 */
 		if (unlikely(timer == &rq->inband_timer)) {
-			rq->lflags |= RQ_TPROXY;
-			rq->lflags &= ~RQ_TDEFER;
+			rq->local_flags |= RQ_TPROXY;
+			rq->local_flags &= ~RQ_TDEFER;
 			continue;
 		}
 
@@ -339,7 +339,7 @@ static void do_clock_tick(struct evl_clock *clock, struct evl_timerbase *tmb)
 		}
 	}
 
-	rq->lflags &= ~RQ_TIMER;
+	rq->local_flags &= ~RQ_TIMER;
 
 	evl_program_local_tick(clock);
 
@@ -364,7 +364,7 @@ void evl_core_tick(struct clock_event_device *dummy) /* hard irqs off */
 	 * evl_exit_irq(), so we may have to propagate the in-band
 	 * tick immediately only if the in-band context was preempted.
 	 */
-	if ((this_rq->lflags & RQ_TPROXY) && (this_rq->curr->state & T_ROOT))
+	if ((this_rq->local_flags & RQ_TPROXY) && (this_rq->curr->state & T_ROOT))
 		evl_notify_proxy_tick(this_rq);
 }
 
@@ -618,16 +618,33 @@ struct evl_timerfd {
 	bool ticked;
 };
 
+#ifdef CONFIG_SMP
+
+/* Pin @timer to the current thread rq. */
+static void pin_timer(struct evl_timer *timer)
+{
+	unsigned long flags = oob_irq_save();
+	struct evl_rq *this_rq = evl_current_rq();
+
+	if (this_rq != timer->rq)
+		evl_move_timer(timer, timer->clock, this_rq);
+
+	oob_irq_restore(flags);
+}
+
+#else
+
+static inline void pin_timer(struct evl_timer *timer)
+{ }
+
+#endif
+
 static int set_timerfd(struct evl_timerfd *timerfd,
 		const struct itimerspec *__restrict__ value,
 		struct itimerspec *__restrict__ ovalue)
 {
-	unsigned long flags;
-
 	get_timer_value(&timerfd->timer, ovalue);
-	xnlock_get_irqsave(&nklock, flags);
-	evl_set_timer_rq(&timerfd->timer, evl_current_rq());
-	xnlock_put_irqrestore(&nklock, flags);
+	pin_timer(&timerfd->timer);
 
 	return set_timer_value(&timerfd->timer, value);
 }
diff --git a/kernel/evl/monitor.c b/kernel/evl/monitor.c
index 9ffa6de1cb04..d9108063a5c2 100644
--- a/kernel/evl/monitor.c
+++ b/kernel/evl/monitor.c
@@ -84,9 +84,9 @@ int evl_signal_monitor_targeted(struct evl_thread *target, int monfd)
 		event->state->flags |= (EVL_MONITOR_TARGETED|
 					EVL_MONITOR_SIGNALED);
 		evl_spin_lock(&target->lock);
-		xnlock_get(&nklock);
+		evl_spin_lock(&target->rq->lock);
 		target->info |= T_SIGNAL;
-		xnlock_put(&nklock);
+		evl_spin_unlock(&target->rq->lock);
 		evl_spin_unlock(&target->lock);
 		evl_spin_unlock_irqrestore(&event->wait_queue.lock, flags);
 	}
@@ -169,12 +169,6 @@ static void wakeup_waiters(struct evl_monitor *event)
 	 * detected. Otherwise, and in presence of a targeted wake up
 	 * request, only the target thread(s) are woken up. Otherwise,
 	 * the thread heading the wait queue is readied.
-	 *
-	 * CAUTION: we must keep the wake up ops rescheduling-free, so
-	 * that low priority threads cannot preempt us before high
-	 * priority ones have been readied. For the moment, we are
-	 * covered by disabling IRQs when grabbing the nklock: be
-	 * careful when killing the latter.
 	 */
 	if (evl_wait_active(&event->wait_queue)) {
 		if (bcast)
@@ -521,9 +515,9 @@ static int wait_monitor(struct file *filp,
 	evl_add_wait_queue(&event->wait_queue, timeout, tmode);
 
 	evl_spin_lock(&curr->lock);
-	xnlock_get(&nklock);
+	evl_spin_lock(&curr->rq->lock);
 	curr->info &= ~T_SIGNAL;
-	xnlock_put(&nklock);
+	evl_spin_unlock(&curr->rq->lock);
 	evl_spin_unlock(&curr->lock);
 	evl_spin_unlock(&event->wait_queue.lock);
 	__exit_monitor(gate, curr);
diff --git a/kernel/evl/mutex.c b/kernel/evl/mutex.c
index 39049f1d0047..64ed8f27fd66 100644
--- a/kernel/evl/mutex.c
+++ b/kernel/evl/mutex.c
@@ -63,7 +63,7 @@ static void raise_boost_flag(struct evl_thread *owner)
 	assert_evl_lock(&owner->lock);
 	no_ugly_lock();
 
-	xnlock_get(&nklock);
+	evl_spin_lock(&owner->rq->lock);
 
 	/* Backup the base priority at first boost only. */
 	if (!(owner->state & T_BOOST)) {
@@ -71,7 +71,7 @@ static void raise_boost_flag(struct evl_thread *owner)
 		owner->state |= T_BOOST;
 	}
 
-	xnlock_put(&nklock);
+	evl_spin_unlock(&owner->rq->lock);
 }
 
 /* owner->lock + contender->lock held, irqs off */
@@ -364,9 +364,9 @@ static void clear_boost(struct evl_mutex *mutex,
 
 	list_del(&mutex->next_booster);	/* owner->boosters */
 	if (list_empty(&owner->boosters)) {
-		xnlock_get(&nklock);
+		evl_spin_lock(&owner->rq->lock);
 		owner->state &= ~T_BOOST;
-		xnlock_put(&nklock);
+		evl_spin_unlock(&owner->rq->lock);
 		inherit_thread_priority(owner, owner, owner);
 	} else
 		adjust_boost(owner, NULL, mutex, owner);
@@ -386,22 +386,23 @@ static void detect_inband_owner(struct evl_mutex *mutex,
 	struct evl_thread *owner = mutex->owner;
 
 	no_ugly_lock();
+
 	/*
 	 * @curr == this_evl_rq()->curr so no need to grab
 	 * @curr->lock.
 	 */
-	xnlock_get(&nklock);
+	evl_spin_lock(&curr->rq->lock);
 
 	if (curr->info & T_PIALERT) {
 		curr->info &= ~T_PIALERT;
 	} else if (owner->state & T_INBAND) {
 		curr->info |= T_PIALERT;
-		xnlock_put(&nklock);
+		evl_spin_unlock(&curr->rq->lock);
 		evl_signal_thread(curr, SIGDEBUG, SIGDEBUG_MIGRATE_PRIOINV);
 		return;
 	}
 
-	xnlock_put(&nklock);
+	evl_spin_unlock(&curr->rq->lock);
 }
 
 /*
@@ -429,9 +430,9 @@ void evl_detect_boost_drop(struct evl_thread *owner)
 		evl_spin_lock(&mutex->lock);
 		for_each_evl_mutex_waiter(waiter, mutex) {
 			if (waiter->state & T_WOLI) {
-				xnlock_get(&nklock);
+				evl_spin_lock(&waiter->rq->lock);
 				waiter->info |= T_PIALERT;
-				xnlock_put(&nklock);
+				evl_spin_unlock(&waiter->rq->lock);
 				evl_signal_thread(waiter, SIGDEBUG,
 						SIGDEBUG_MIGRATE_PRIOINV);
 			}
@@ -749,9 +750,9 @@ int evl_lock_mutex_timeout(struct evl_mutex *mutex, ktime_t timeout,
 		if ((owner->info & T_WAKEN) && owner->wwake == &mutex->wchan) {
 			/* Ownership is still pending, steal the resource. */
 			set_current_owner_locked(mutex, curr);
-			xnlock_get(&nklock);
-			owner->info |= T_ROBBED; /* CAUTION: requires nklock */
-			xnlock_put(&nklock);
+			evl_spin_lock(&owner->rq->lock);
+			owner->info |= T_ROBBED;
+			evl_spin_unlock(&owner->rq->lock);
 			evl_spin_unlock(&owner->lock);
 			goto grab;
 		}
@@ -798,11 +799,11 @@ int evl_lock_mutex_timeout(struct evl_mutex *mutex, ktime_t timeout,
 	finish_mutex_wait(mutex);
 	evl_spin_lock(&curr->lock);
 	curr->wwake = NULL;
-	xnlock_get(&nklock);
+	evl_spin_lock(&curr->rq->lock);
 	curr->info &= ~T_WAKEN;
 
 	if (ret) {
-		xnlock_put(&nklock);
+		evl_spin_unlock(&curr->rq->lock);
 		goto out;
 	}
 
@@ -814,7 +815,7 @@ int evl_lock_mutex_timeout(struct evl_mutex *mutex, ktime_t timeout,
 		 * for the mutex, unless we know for sure it's too
 		 * late.
 		 */
-		xnlock_put(&nklock);
+		evl_spin_unlock(&curr->rq->lock);
 		if (timeout_mode != EVL_REL ||
 			timeout_infinite(timeout) ||
 			evl_get_stopped_timer_delta(&curr->rtimer) != 0) {
@@ -826,7 +827,7 @@ int evl_lock_mutex_timeout(struct evl_mutex *mutex, ktime_t timeout,
 		goto out;
 	}
 
-	xnlock_put(&nklock);
+	evl_spin_unlock(&curr->rq->lock);
 grab:
 	disable_inband_switch(curr);
 
diff --git a/kernel/evl/sched/core.c b/kernel/evl/sched/core.c
index d83a6443c8f9..01c5eda0e029 100644
--- a/kernel/evl/sched/core.c
+++ b/kernel/evl/sched/core.c
@@ -22,6 +22,7 @@
 #include <evl/clock.h>
 #include <evl/tick.h>
 #include <evl/monitor.h>
+#include <evl/mutex.h>
 #include <uapi/evl/signal.h>
 #include <trace/events/evl.h>
 
@@ -90,9 +91,9 @@ static void watchdog_handler(struct evl_timer *timer) /* oob stage stalled */
 
 	if (curr->state & T_USER) {
 		evl_spin_lock(&curr->lock);
-		xnlock_get(&nklock);
+		evl_spin_lock(&this_rq->lock);
 		curr->info |= T_KICKED;
-		xnlock_put(&nklock);
+		evl_spin_unlock(&this_rq->lock);
 		evl_spin_unlock(&curr->lock);
 		evl_signal_thread(curr, SIGDEBUG, SIGDEBUG_WATCHDOG);
 		dovetail_send_mayday(current);
@@ -110,9 +111,9 @@ static void watchdog_handler(struct evl_timer *timer) /* oob stage stalled */
 		 * exits next time it invokes evl_test_cancel().
 		 */
 		evl_spin_lock(&curr->lock);
-		xnlock_get(&nklock);
+		evl_spin_lock(&this_rq->lock);
 		curr->info |= (T_KICKED|T_CANCELD);
-		xnlock_put(&nklock);
+		evl_spin_unlock(&this_rq->lock);
 		evl_spin_unlock(&curr->lock);
 	}
 }
@@ -124,9 +125,9 @@ static void roundrobin_handler(struct evl_timer *timer) /* hard irqs off */
 	struct evl_rq *this_rq;
 
 	this_rq = container_of(timer, struct evl_rq, rrbtimer);
-	xnlock_get(&nklock);
+	evl_spin_lock(&this_rq->lock);
 	evl_sched_tick(this_rq);
-	xnlock_put(&nklock);
+	evl_spin_unlock(&this_rq->lock);
 }
 
 static void init_rq(struct evl_rq *rq, int cpu)
@@ -140,19 +141,21 @@ static void init_rq(struct evl_rq *rq, int cpu)
 	name_fmt = "ROOT/%u";
 	rq->proxy_timer_name = kasprintf(GFP_KERNEL, "[proxy-timer/%u]", cpu);
 	rq->rrb_timer_name = kasprintf(GFP_KERNEL, "[rrb-timer/%u]", cpu);
-	cpumask_clear(&rq->resched);
+	cpumask_clear(&rq->resched_cpus);
 #else
 	name_fmt = "ROOT";
 	rq->proxy_timer_name = kstrdup("[proxy-timer]", GFP_KERNEL);
 	rq->rrb_timer_name = kstrdup("[rrb-timer]", GFP_KERNEL);
 #endif
+	evl_spin_lock_init(&rq->lock);
+
 	for_each_evl_sched_class(sched_class) {
 		if (sched_class->sched_init)
 			sched_class->sched_init(rq);
 	}
 
-	rq->status = 0;
-	rq->lflags = RQ_IDLE;
+	rq->flags = 0;
+	rq->local_flags = RQ_IDLE;
 	rq->curr = &rq->root_thread;
 
 	/*
@@ -195,7 +198,7 @@ static void init_rq(struct evl_rq *rq, int cpu)
 	evl_nrthreads++;
 }
 
-static void destroy_rq(struct evl_rq *rq) /* nklock held, irqs off */
+static void destroy_rq(struct evl_rq *rq)
 {
 	evl_destroy_timer(&rq->inband_timer);
 	evl_destroy_timer(&rq->rrbtimer);
@@ -224,11 +227,166 @@ EXPORT_SYMBOL(evl_enable_preempt);
 
 #endif /* CONFIG_EVL_DEBUG_CORE */
 
-/* thread->lock + nklock locked, interrupts off. */
+#ifdef CONFIG_SMP
+
+static inline
+void evl_double_rq_lock(struct evl_rq *rq1, struct evl_rq *rq2)
+{
+	EVL_WARN_ON_ONCE(CORE, !oob_irqs_disabled());
+
+	/* Prevent ABBA deadlock, always lock rqs in address order. */
+
+	if (rq1 == rq2) {
+		evl_spin_lock(&rq1->lock);
+	} else if (rq1 < rq2) {
+		evl_spin_lock(&rq1->lock);
+		evl_spin_lock_nested(&rq2->lock, SINGLE_DEPTH_NESTING);
+	} else {
+		evl_spin_lock(&rq2->lock);
+		evl_spin_lock_nested(&rq1->lock, SINGLE_DEPTH_NESTING);
+	}
+}
+
+static inline
+void evl_double_rq_unlock(struct evl_rq *rq1, struct evl_rq *rq2)
+{
+	evl_spin_unlock(&rq1->lock);
+	if (rq1 != rq2)
+		evl_spin_unlock(&rq2->lock);
+}
+
+static void migrate_rq(struct evl_thread *thread, struct evl_rq *dst_rq)
+{
+	struct evl_sched_class *sched_class = thread->sched_class;
+	struct evl_rq *src_rq = thread->rq;
+
+	evl_double_rq_lock(src_rq, dst_rq);
+
+	if (thread->state & T_READY) {
+		evl_dequeue_thread(thread);
+		thread->state &= ~T_READY;
+	}
+
+	if (sched_class->sched_migrate)
+		sched_class->sched_migrate(thread, dst_rq);
+	/*
+	 * WARNING: the scheduling class may have just changed as a
+	 * result of calling the per-class migration hook.
+	 */
+	thread->rq = dst_rq;
+
+	if (!(thread->state & EVL_THREAD_BLOCK_BITS)) {
+		evl_requeue_thread(thread);
+		thread->state |= T_READY;
+		evl_set_resched(dst_rq);
+		evl_set_resched(src_rq);
+	}
+
+	evl_double_rq_unlock(src_rq, dst_rq);
+}
+
+/* thread->lock held, oob stalled. @thread must not be running oob. */
+void evl_migrate_thread(struct evl_thread *thread, struct evl_rq *dst_rq)
+{
+	assert_evl_lock(&thread->lock);
+
+	if (thread->rq == dst_rq)
+		return;
+
+	trace_evl_thread_migrate(thread, evl_rq_cpu(dst_rq));
+
+	/*
+	 * Timer migration is postponed until the next timeout happens
+	 * for the periodic and rrb timers. The resource/periodic
+	 * timer will be moved to the right CPU next time
+	 * evl_prepare_timed_wait() is called for it (via
+	 * evl_sleep_on()).
+	 */
+	migrate_rq(thread, dst_rq);
+
+	evl_reset_account(&thread->stat.lastperiod);
+}
+
+static bool check_cpu_affinity(struct task_struct *p) /* inband, oob stage stalled */
+{
+	struct evl_thread *thread = evl_thread_from_task(p);
+	int cpu = task_cpu(p);
+	struct evl_rq *rq = evl_cpu_rq(cpu);
+	bool ret = true;
+
+	evl_spin_lock(&thread->lock);
+
+	/*
+	 * To maintain consistency between both the EVL and in-band
+	 * schedulers, reflecting a thread migration to another CPU
+	 * into EVL's scheduler state must happen from in-band context
+	 * only, on behalf of the migrated thread itself once it runs
+	 * on the target CPU.
+	 *
+	 * This means that the EVL scheduler state regarding the CPU
+	 * information lags behind the in-band scheduler state until
+	 * the migrated thread switches back to OOB context
+	 * (i.e. task_cpu(p) !=
+	 * evl_rq_cpu(evl_thread_from_task(p)->rq)).  This is ok since
+	 * EVL will not schedule such thread until then.
+	 *
+	 * check_cpu_affinity() detects when a EVL thread switching back to
+	 * OOB context did move to another CPU earlier while running
+	 * in-band. If so, do the fixups to reflect the change.
+	 */
+	if (unlikely(!is_threading_cpu(cpu))) {
+		printk(EVL_WARNING "thread %s[%d] switched to non-rt CPU%d, aborted.\n",
+			thread->name, evl_get_inband_pid(thread), cpu);
+		/*
+		 * Can't call evl_cancel_thread() from a CPU migration
+		 * point, that would break. Since we are on the wakeup
+		 * path to OOB context, just raise T_CANCELD to catch
+		 * it in evl_switch_oob().
+		 */
+		evl_spin_lock(&thread->rq->lock);
+		thread->info |= T_CANCELD;
+		evl_spin_unlock(&thread->rq->lock);
+		ret = false;
+		goto out;
+	}
+
+	if (likely(rq == thread->rq))
+		goto out;
+
+	/*
+	 * If the current thread moved to a supported out-of-band CPU,
+	 * which is not part of its original affinity mask, assume
+	 * user wants to extend this mask.
+	 */
+	if (!cpumask_test_cpu(cpu, &thread->affinity))
+		cpumask_set_cpu(cpu, &thread->affinity);
+
+	evl_migrate_thread(thread, rq);
+out:
+	evl_spin_unlock(&thread->lock);
+
+	return ret;
+}
+
+#else
+
+#define evl_double_rq_lock(__rq1, __rq2)  \
+	EVL_WARN_ON_ONCE(CORE, !oob_irqs_disabled());
+
+#define evl_double_rq_unlock(__rq1, __rq2)  do { } while (0)
+
+static inline bool check_cpu_affinity(struct task_struct *p)
+{
+	return true;
+}
+
+#endif	/* CONFIG_SMP */
+
+/* thread->lock + thread->rq->lock held, irqs off. */
 void evl_putback_thread(struct evl_thread *thread)
 {
 	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_evl_lock(&thread->rq->lock);
 
 	if (thread->state & T_READY)
 		evl_dequeue_thread(thread);
@@ -239,7 +397,7 @@ void evl_putback_thread(struct evl_thread *thread)
 	evl_set_resched(thread->rq);
 }
 
-/* thread->lock + nklock held, interrupts off. */
+/* thread->lock + thread->rq->lock held, irqs off. */
 int evl_set_thread_policy_locked(struct evl_thread *thread,
 				struct evl_sched_class *sched_class,
 				const union evl_sched_param *p)
@@ -249,7 +407,7 @@ int evl_set_thread_policy_locked(struct evl_thread *thread,
 	int ret;
 
 	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_evl_lock(&thread->rq->lock);
 
 	/* Check parameters early on. */
 	ret = evl_check_schedparams(sched_class, thread, p);
@@ -333,24 +491,23 @@ int evl_set_thread_policy(struct evl_thread *thread,
 			const union evl_sched_param *p)
 {
 	unsigned long flags;
+	struct evl_rq *rq;
 	int ret;
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 	ret = evl_set_thread_policy_locked(thread, sched_class, p);
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 
 	return ret;
 }
 
-/* thread->lock + nklock held, interrupts off. */
+/* thread->lock + thread->rq->lock held, irqs off. */
 bool evl_set_effective_thread_priority(struct evl_thread *thread, int prio)
 {
 	int wprio = evl_calc_weighted_prio(thread->base_class, prio);
 
 	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_evl_lock(&thread->rq->lock);
 
 	thread->bprio = prio;
 	if (wprio == thread->wprio)
@@ -383,7 +540,8 @@ void evl_track_thread_policy(struct evl_thread *thread,
 	assert_evl_lock(&target->lock);
 	no_ugly_lock();
 
-	xnlock_get(&nklock);
+	evl_double_rq_lock(thread->rq, target->rq);
+
 	/*
 	 * Inherit (or reset) the effective scheduling class and
 	 * priority of a thread. Unlike evl_set_thread_policy(), this
@@ -403,7 +561,7 @@ void evl_track_thread_policy(struct evl_thread *thread,
 		/*
 		 * Per SuSv2, resetting the base scheduling parameters
 		 * should not move the thread to the tail of its
-		 * priority group. Go for POLA here.
+		 * priority group, which makes sense.
 		 */
 		if (thread->state & T_READY)
 			evl_requeue_thread(thread);
@@ -420,7 +578,7 @@ void evl_track_thread_policy(struct evl_thread *thread,
 
 	evl_set_resched(thread->rq);
 
-	xnlock_put(&nklock);
+	evl_double_rq_unlock(thread->rq, target->rq);
 }
 
 /* thread->lock, irqs off */
@@ -429,7 +587,7 @@ void evl_protect_thread_priority(struct evl_thread *thread, int prio)
 	assert_evl_lock(&thread->lock);
 	no_ugly_lock();
 
-	xnlock_get(&nklock);
+	evl_spin_lock(&thread->rq->lock);
 
 	/*
 	 * Apply a PP boost by changing the effective priority of a
@@ -455,39 +613,7 @@ void evl_protect_thread_priority(struct evl_thread *thread, int prio)
 
 	evl_set_resched(thread->rq);
 
-	xnlock_put(&nklock);
-}
-
-/*
- * thread->lock + nklock held, interrupts off. Thread may be blocked.
- */
-void evl_migrate_rq(struct evl_thread *thread, struct evl_rq *rq)
-{
-	struct evl_sched_class *sched_class = thread->sched_class;
-	struct evl_rq *last_rq = thread->rq;
-
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
-
-	if (thread->state & T_READY) {
-		evl_dequeue_thread(thread);
-		thread->state &= ~T_READY;
-	}
-
-	if (sched_class->sched_migrate)
-		sched_class->sched_migrate(thread, rq);
-	/*
-	 * WARNING: the scheduling class may have just changed as a
-	 * result of calling the per-class migration hook.
-	 */
-	thread->rq = rq;
-
-	if (!(thread->state & EVL_THREAD_BLOCK_BITS)) {
-		evl_requeue_thread(thread);
-		thread->state |= T_READY;
-		evl_set_resched(rq);
-		evl_set_resched(last_rq);
-	}
+	evl_spin_unlock(&thread->rq->lock);
 }
 
 void evl_init_schedq(struct evl_multilevel_queue *q)
@@ -565,22 +691,6 @@ struct evl_thread *evl_fifo_pick(struct evl_rq *rq)
 	return thread;
 }
 
-static inline int test_resched(struct evl_rq *rq)
-{
-	int resched = evl_need_resched(rq);
-#ifdef CONFIG_SMP
-	/* Send resched IPI to remote CPU(s). */
-	if (unlikely(!cpumask_empty(&rq->resched))) {
-		smp_mb();
-		irq_pipeline_send_remote(RESCHEDULE_OOB_IPI, &rq->resched);
-		cpumask_clear(&rq->resched);
-	}
-#endif
-	rq->status &= ~RQ_SCHED;
-
-	return resched;
-}
-
 static inline void enter_inband(struct evl_thread *root)
 {
 #ifdef CONFIG_EVL_WATCHDOG
@@ -598,35 +708,34 @@ static inline void leave_inband(struct evl_thread *root)
 #endif
 }
 
+/* oob stalled. */
 static irqreturn_t reschedule_interrupt(int irq, void *dev_id)
 {
-	/* hw interrupts are off. */
-	trace_evl_schedule_remote(this_evl_rq());
-	evl_schedule();
+	trace_evl_reschedule_ipi(this_evl_rq());
+
+	/* Will reschedule from evl_exit_irq(). */
 
 	return IRQ_HANDLED;
 }
 
-static inline void set_thread_running(struct evl_rq *rq,
-				struct evl_thread *thread)
+static inline void set_next_running(struct evl_rq *rq,
+				struct evl_thread *next)
 {
-	requires_ugly_lock();
-
-	thread->state &= ~T_READY;
-	if (thread->state & T_RRB)
+	next->state &= ~T_READY;
+	if (next->state & T_RRB)
 		evl_start_timer(&rq->rrbtimer,
-				evl_abs_timeout(&rq->rrbtimer, thread->rrperiod),
+				evl_abs_timeout(&rq->rrbtimer, next->rrperiod),
 				EVL_INFINITE);
 	else
 		evl_stop_timer(&rq->rrbtimer);
 }
 
-/* curr->lock + nklock held, irqs off. */
+/* rq->curr->lock + rq->lock held, irqs off. */
 static struct evl_thread *pick_next_thread(struct evl_rq *rq)
 {
 	struct evl_sched_class *sched_class;
 	struct evl_thread *curr = rq->curr;
-	struct evl_thread *thread;
+	struct evl_thread *next;
 
 	/*
 	 * We have to switch the current thread out if a blocking
@@ -650,18 +759,86 @@ static struct evl_thread *pick_next_thread(struct evl_rq *rq)
 	}
 
 	/*
-	 * Find the runnable thread having the highest priority among
-	 * all scheduling classes, scanned by decreasing priority.
+	 * Find the next runnable thread having the highest priority
+	 * amongst all scheduling classes, scanned by decreasing
+	 * priority.
 	 */
 	for_each_evl_sched_class(sched_class) {
-		thread = sched_class->sched_pick(rq);
-		if (likely(thread)) {
-			set_thread_running(rq, thread);
-			return thread;
+		next = sched_class->sched_pick(rq);
+		if (likely(next)) {
+			set_next_running(rq, next);
+			return next;
 		}
 	}
 
-	return NULL; /* Never executed because of the idle class. */
+	return NULL; /* NOT REACHED (idle class). */
+}
+
+static inline void prepare_rq_switch(struct evl_rq *this_rq,
+				struct evl_thread *next)
+{
+	if (irq_pipeline_debug_locking())
+		spin_release(&this_rq->lock._lock.rlock.dep_map,
+			_THIS_IP_);
+#ifdef CONFIG_DEBUG_SPINLOCK
+	this_rq->lock._lock.rlock.owner = next->altsched.task;
+#endif
+}
+
+static inline void finish_rq_switch(bool inband_tail, unsigned long flags)
+{
+	struct evl_rq *this_rq = this_evl_rq();
+
+	EVL_WARN_ON(CORE, this_rq->curr->state & EVL_THREAD_BLOCK_BITS);
+
+	/*
+	 * Check whether we are completing a transition to the inband
+	 * stage for the current task, i.e.:
+	 *
+	 * irq_work_queue() ->
+	 *        IRQ:wake_up_process() ->
+	 *                         schedule() ->
+	 *                               back from dovetail_context_switch()
+	 */
+	if (likely(!inband_tail)) {
+		if (irq_pipeline_debug_locking())
+			spin_acquire(&this_rq->lock._lock.rlock.dep_map,
+				0, 0, _THIS_IP_);
+		evl_spin_unlock_irqrestore(&this_rq->lock, flags);
+	}
+}
+
+static inline void finish_rq_switch_from_inband(void)
+{
+	struct evl_rq *this_rq = this_evl_rq();
+
+	assert_evl_lock(&this_rq->lock);
+
+	if (irq_pipeline_debug_locking())
+		spin_acquire(&this_rq->lock._lock.rlock.dep_map,
+			0, 0, _THIS_IP_);
+
+	evl_spin_unlock_irq(&this_rq->lock);
+}
+
+/* oob stalled. */
+static inline bool test_resched(struct evl_rq *this_rq)
+{
+	bool need_resched = evl_need_resched(this_rq);
+
+#ifdef CONFIG_SMP
+	/* Send resched IPI to remote CPU(s). */
+	if (unlikely(!cpumask_empty(&this_rq->resched_cpus))) {
+		irq_pipeline_send_remote(RESCHEDULE_OOB_IPI,
+					&this_rq->resched_cpus);
+		cpumask_clear(&this_rq->resched_cpus);
+		this_rq->local_flags &= ~RQ_SCHED;
+	}
+#endif
+	if (need_resched)
+		this_rq->flags &= ~RQ_SCHED;
+
+	return need_resched;
 }
 
 /*
@@ -697,11 +874,16 @@ void __evl_schedule(void) /* oob or oob stalled (CPU migration-safe) */
 	if (curr->state & T_USER)
 		evl_commit_monitor_ceiling();
 
+	/*
+	 * Only holding this_rq->lock is required for test_resched(),
+	 * but we grab curr->lock in advance in order to keep the
+	 * locking order safe from ABBA deadlocking.
+	 */
 	evl_spin_lock(&curr->lock);
-	xnlock_get(&nklock);
+	evl_spin_lock(&this_rq->lock);
 
 	if (unlikely(!test_resched(this_rq))) {
-		xnlock_put(&nklock);
+		evl_spin_unlock(&this_rq->lock);
 		evl_spin_unlock_irqrestore(&curr->lock, flags);
 		return;
 	}
@@ -709,12 +891,12 @@ void __evl_schedule(void) /* oob or oob stalled (CPU migration-safe) */
 	next = pick_next_thread(this_rq);
 	if (next == curr) {
 		if (unlikely(next->state & T_ROOT)) {
-			if (this_rq->lflags & RQ_TPROXY)
+			if (this_rq->local_flags & RQ_TPROXY)
 				evl_notify_proxy_tick(this_rq);
-			if (this_rq->lflags & RQ_TDEFER)
+			if (this_rq->local_flags & RQ_TDEFER)
 				evl_program_local_tick(&evl_mono_clock);
 		}
-		xnlock_put(&nklock);
+		evl_spin_unlock(&this_rq->lock);
 		evl_spin_unlock_irqrestore(&curr->lock, flags);
 		return;
 	}
@@ -728,9 +910,9 @@ void __evl_schedule(void) /* oob or oob stalled (CPU migration-safe) */
 		leave_inband(prev);
 		leaving_inband = true;
 	} else if (next->state & T_ROOT) {
-		if (this_rq->lflags & RQ_TPROXY)
+		if (this_rq->local_flags & RQ_TPROXY)
 			evl_notify_proxy_tick(this_rq);
-		if (this_rq->lflags & RQ_TDEFER)
+		if (this_rq->local_flags & RQ_TDEFER)
 			evl_program_local_tick(&evl_mono_clock);
 		enter_inband(next);
 	}
@@ -738,25 +920,175 @@ void __evl_schedule(void) /* oob or oob stalled (CPU migration-safe) */
 	evl_switch_account(this_rq, &next->stat.account);
 	evl_inc_counter(&next->stat.csw);
 	evl_spin_unlock(&prev->lock);
+
+	prepare_rq_switch(this_rq, next);
 	inband_tail = dovetail_context_switch(&prev->altsched,
 					&next->altsched, leaving_inband);
-	EVL_WARN_ON(CORE, this_evl_rq()->curr->state & EVL_THREAD_BLOCK_BITS);
+	finish_rq_switch(inband_tail, flags);
+}
+EXPORT_SYMBOL_GPL(__evl_schedule);
+
+void resume_oob_task(struct task_struct *p) /* inband, oob stage stalled */
+{
+	struct evl_thread *thread = evl_thread_from_task(p);
+
+	if (check_cpu_affinity(p))
+		evl_release_thread(thread, T_INBAND, 0);
+
+	evl_schedule();
+}
+
+int evl_switch_oob(void)
+{
+	struct task_struct *p = current;
+	struct evl_thread *curr;
+	int ret;
+
+	inband_context_only();
+
+	curr = evl_current();
+	if (curr == NULL)
+		return -EPERM;
+
+	if (signal_pending(p))
+		return -ERESTARTSYS;
+
+	trace_evl_switching_oob(curr);
+
+	evl_clear_sync_uwindow(curr, T_INBAND);
+
+	ret = dovetail_leave_inband();
+	if (ret) {
+		evl_test_cancel();
+		evl_set_sync_uwindow(curr, T_INBAND);
+		return ret;
+	}
 
 	/*
-	 * Check whether we are completing a transition to the inband
-	 * stage for the current task, i.e.:
+	 * The current task is now running on the out-of-band
+	 * execution stage, scheduled in by the latest call to
+	 * __evl_schedule() on this CPU: we must be holding the
+	 * runqueue lock and the oob stage must be stalled.
+	 */
+	oob_context_only();
+	finish_rq_switch_from_inband();
+	evl_test_cancel();
+
+	trace_evl_switched_oob(curr);
+
+	/*
+	 * Recheck pending signals once again. As we block task
+	 * wakeups during the stage transition and handle_sigwake_event()
+	 * ignores signals until T_INBAND is cleared, any signal in
+	 * between is just silently queued up to here.
+	 */
+	if (signal_pending(p)) {
+		evl_switch_inband(!(curr->state & T_SSTEP) ?
+				SIGDEBUG_MIGRATE_SIGNAL:
+				SIGDEBUG_NONE);
+		return -ERESTARTSYS;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(evl_switch_oob);
+
+void evl_switch_inband(int cause)
+{
+	struct evl_thread *curr = evl_current();
+	struct task_struct *p = current;
+	struct kernel_siginfo si;
+	struct evl_rq *rq;
+
+	oob_context_only();
+
+	trace_evl_switching_inband(cause);
+
+	/*
+	 * This is the only location where we may assert T_INBAND for
+	 * a thread. Basic assumption: switching to the inband stage
+	 * only applies to the current thread running out-of-band on
+	 * this CPU.
 	 *
-	 * irq_work_queue() ->
-	 *        IRQ:wake_up_process() ->
-	 *                         schedule() ->
-	 *                               back from dovetail_context_switch()
+	 * CAVEAT: dovetail_leave_oob() must run _before_ the in-band
+	 * kernel is allowed to take interrupts again, so that
+	 * try_to_wake_up() does not block the wake up request for the
+	 * switching thread as a result of testing task_is_off_stage().
 	 */
-	if (inband_tail)
-		return;
+	oob_irq_disable();
+	irq_work_queue(&curr->inband_work);
+	evl_spin_lock(&curr->lock);
+	rq = curr->rq;
+	evl_spin_lock(&rq->lock);
+	if (curr->state & T_READY) {
+		evl_dequeue_thread(curr);
+		curr->state &= ~T_READY;
+	}
+	curr->info &= ~EVL_THREAD_INFO_MASK;
+	curr->state |= T_INBAND;
+	curr->local_info &= ~T_SYSRST;
+	evl_set_resched(rq);
+	dovetail_leave_oob();
+	evl_spin_unlock(&rq->lock);
+	evl_spin_unlock(&curr->lock);
+	__evl_schedule();
+	/*
+	 * this_rq()->lock was released when the root thread resumed
+	 * from __evl_schedule() (i.e. inband_tail path).
+	 */
+	oob_irq_enable();
+	dovetail_resume_inband();
+
+	/*
+	 * Basic sanity check after an expected transition to in-band
+	 * context.
+	 */
+	EVL_WARN(CORE, !running_inband(),
+		"evl_switch_inband() failed for thread %s[%d]",
+		curr->name, evl_get_inband_pid(curr));
+
+	/* Account for switch to in-band context. */
+	evl_inc_counter(&curr->stat.isw);
+
+	trace_evl_switched_inband(curr);
+
+	/*
+	 * When switching to in-band context, we check for propagating
+	 * the current EVL schedparams that might have been set for
+	 * current while running in OOB context.
+	 *
+	 * CAUTION: This obviously won't update the schedparams cached
+	 * by the glibc for the caller in user-space, but this is the
+	 * deal: we don't switch threads which issue
+	 * EVL_THRIOC_SET_SCHEDPARAM to in-band mode, but then only
+	 * the kernel side will be aware of the change, and glibc
+	 * might cache obsolete information.
+	 */
+	evl_propagate_schedparam_change(curr);
 
-	xnlock_put_irqrestore(&nklock, flags);
+	if ((curr->state & T_USER) && cause != SIGDEBUG_NONE) {
+		/*
+		 * Help debugging spurious stage switches by sending
+		 * SIGDEBUG. We are running inband on the context of
+		 * the receiver, so we may bypass evl_signal_thread()
+		 * for this.
+		 */
+		if (curr->state & T_WOSS) {
+			memset(&si, 0, sizeof(si));
+			si.si_signo = SIGDEBUG;
+			si.si_code = SI_QUEUE;
+			si.si_int = cause | sigdebug_marker;
+			send_sig_info(SIGDEBUG, &si, p);
+		}
+		/* May check for locking inconsistency too. */
+		if (curr->state & T_WOLI)
+			evl_detect_boost_drop(curr);
+	}
+
+	/* @curr is now running inband. */
+	evl_sync_uwindow(curr);
 }
-EXPORT_SYMBOL_GPL(__evl_schedule);
+EXPORT_SYMBOL_GPL(evl_switch_inband);
 
 struct evl_sched_class *
 evl_find_sched_class(union evl_sched_param *param,
diff --git a/kernel/evl/sched/quota.c b/kernel/evl/sched/quota.c
index d184afd797a1..694986b3e567 100644
--- a/kernel/evl/sched/quota.c
+++ b/kernel/evl/sched/quota.c
@@ -154,7 +154,7 @@ static void quota_refill_handler(struct evl_timer *timer) /* oob stage stalled *
 	qs = container_of(timer, struct evl_sched_quota, refill_timer);
 	rq = container_of(qs, struct evl_rq, quota);
 
-	xnlock_get(&nklock);
+	evl_spin_lock(&rq->lock);
 
 	list_for_each_entry(tg, &qs->groups, next) {
 		/* Allot a new runtime budget for the group. */
@@ -179,7 +179,7 @@ static void quota_refill_handler(struct evl_timer *timer) /* oob stage stalled *
 
 	evl_set_self_resched(evl_get_timer_rq(timer));
 
-	xnlock_put(&nklock);
+	evl_spin_unlock(&rq->lock);
 }
 
 static void quota_limit_handler(struct evl_timer *timer) /* oob stage stalled */
@@ -192,9 +192,9 @@ static void quota_limit_handler(struct evl_timer *timer) /* oob stage stalled */
 	 * interrupt, so that the budget is re-evaluated for the
 	 * current group in evl_quota_pick().
 	 */
-	xnlock_get(&nklock);
+	evl_spin_lock(&rq->lock);
 	evl_set_self_resched(rq);
-	xnlock_put(&nklock);
+	evl_spin_unlock(&rq->lock);
 }
 
 static int quota_sum_all(struct evl_sched_quota *qs)
@@ -482,7 +482,7 @@ static int quota_create_group(struct evl_quota_group *tg,
 	int tgid, nr_groups = MAX_QUOTA_GROUPS;
 	struct evl_sched_quota *qs = &rq->quota;
 
-	requires_ugly_lock();
+	assert_evl_lock(&rq->lock);
 
 	tgid = find_first_zero_bit(group_map, nr_groups);
 	if (tgid >= nr_groups)
@@ -520,7 +520,7 @@ static int quota_destroy_group(struct evl_quota_group *tg,
 	struct evl_thread *thread, *tmp;
 	union evl_sched_param param;
 
-	requires_ugly_lock();
+	assert_evl_lock(&tg->rq->lock);
 
 	if (!list_empty(&tg->members)) {
 		if (!force)
@@ -554,7 +554,7 @@ static void quota_set_limit(struct evl_quota_group *tg,
 	ktime_t old_quota = tg->quota;
 	u64 n;
 
-	requires_ugly_lock();
+	assert_evl_lock(&tg->rq->lock);
 
 	if (quota_percent < 0 || quota_percent > 100) { /* Quota off. */
 		quota_percent = 100;
@@ -619,7 +619,7 @@ find_quota_group(struct evl_rq *rq, int tgid)
 {
 	struct evl_quota_group *tg;
 
-	requires_ugly_lock();
+	assert_evl_lock(&rq->lock);
 
 	if (list_empty(&rq->quota.groups))
 		return NULL;
@@ -653,10 +653,10 @@ static int quota_control(int cpu, union evl_sched_ctlparam *ctlp,
 			return -ENOMEM;
 		tg = &group->quota;
 		rq = evl_cpu_rq(cpu);
-		xnlock_get_irqsave(&nklock, flags);
+		evl_spin_lock_irqsave(&rq->lock, flags);
 		ret = quota_create_group(tg, rq, &quota_sum);
 		if (ret) {
-			xnlock_put_irqrestore(&nklock, flags);
+			evl_spin_unlock_irqrestore(&rq->lock, flags);
 			evl_free(group);
 			return ret;
 		}
@@ -665,7 +665,7 @@ static int quota_control(int cpu, union evl_sched_ctlparam *ctlp,
 	case evl_quota_remove:
 	case evl_quota_force_remove:
 		rq = evl_cpu_rq(cpu);
-		xnlock_get_irqsave(&nklock, flags);
+		evl_spin_lock_irqsave(&rq->lock, flags);
 		tg = find_quota_group(rq, pq->u.remove.tgid);
 		if (tg == NULL)
 			goto bad_tgid;
@@ -674,16 +674,16 @@ static int quota_control(int cpu, union evl_sched_ctlparam *ctlp,
 					pq->op == evl_quota_force_remove,
 					&quota_sum);
 		if (ret) {
-			xnlock_put_irqrestore(&nklock, flags);
+			evl_spin_unlock_irqrestore(&rq->lock, flags);
 			return ret;
 		}
 		list_del(&group->next);
-		xnlock_put_irqrestore(&nklock, flags);
+		evl_spin_unlock_irqrestore(&rq->lock, flags);
 		evl_free(group);
 		return 0;
 	case evl_quota_set:
 		rq = evl_cpu_rq(cpu);
-		xnlock_get_irqsave(&nklock, flags);
+		evl_spin_lock_irqsave(&rq->lock, flags);
 		tg = find_quota_group(rq, pq->u.set.tgid);
 		if (tg == NULL)
 			goto bad_tgid;
@@ -693,7 +693,7 @@ static int quota_control(int cpu, union evl_sched_ctlparam *ctlp,
 		break;
 	case evl_quota_get:
 		rq = evl_cpu_rq(cpu);
-		xnlock_get_irqsave(&nklock, flags);
+		evl_spin_lock_irqsave(&rq->lock, flags);
 		tg = find_quota_group(rq, pq->u.get.tgid);
 		if (tg == NULL)
 			goto bad_tgid;
@@ -706,14 +706,14 @@ static int quota_control(int cpu, union evl_sched_ctlparam *ctlp,
 	iq->tgid = tg->tgid;
 	iq->quota = tg->quota_percent;
 	iq->quota_peak = tg->quota_peak_percent;
-	xnlock_put_irqrestore(&nklock, flags);
+	evl_spin_unlock_irqrestore(&rq->lock, flags);
 	iq->quota_sum = quota_sum;
 
 	evl_schedule();
 
 	return 0;
 bad_tgid:
-	xnlock_put_irqrestore(&nklock, flags);
+	evl_spin_unlock_irqrestore(&rq->lock, flags);
 
 	return -EINVAL;
 }
diff --git a/kernel/evl/sched/tp.c b/kernel/evl/sched/tp.c
index b9a178e1f12a..e2338518d6be 100644
--- a/kernel/evl/sched/tp.c
+++ b/kernel/evl/sched/tp.c
@@ -16,7 +16,8 @@ static void tp_schedule_next(struct evl_sched_tp *tp)
 	ktime_t t, now;
 	int p_next;
 
-	requires_ugly_lock();
+	rq = container_of(tp, struct evl_rq, tp);
+	assert_evl_lock(&rq->lock);
 
 	for (;;) {
 		/*
@@ -54,15 +55,15 @@ static void tp_schedule_next(struct evl_sched_tp *tp)
 		evl_start_timer(&tp->tf_timer, t, EVL_INFINITE);
 	}
 
-	rq = container_of(tp, struct evl_rq, tp);
 	evl_set_resched(rq);
 }
 
 static void tp_tick_handler(struct evl_timer *timer)
 {
-	struct evl_sched_tp *tp = container_of(timer, struct evl_sched_tp, tf_timer);
+	struct evl_rq *rq = container_of(timer, struct evl_rq, tp.tf_timer);
+	struct evl_sched_tp *tp = &rq->tp;
 
-	xnlock_get(&nklock);
+	evl_spin_lock(&rq->lock);
 
 	/*
 	 * Advance beginning date of time frame by a full period if we
@@ -73,7 +74,7 @@ static void tp_tick_handler(struct evl_timer *timer)
 
 	tp_schedule_next(tp);
 
-	xnlock_put(&nklock);
+	evl_spin_unlock(&rq->lock);
 }
 
 static void tp_init(struct evl_rq *rq)
@@ -234,7 +235,7 @@ static void start_tp_schedule(struct evl_rq *rq)
 {
 	struct evl_sched_tp *tp = &rq->tp;
 
-	requires_ugly_lock();
+	assert_evl_lock(&rq->lock);
 
 	if (tp->gps == NULL)
 		return;
@@ -248,7 +249,7 @@ static void stop_tp_schedule(struct evl_rq *rq)
 {
 	struct evl_sched_tp *tp = &rq->tp;
 
-	requires_ugly_lock();
+	assert_evl_lock(&rq->lock);
 
 	if (tp->gps)
 		evl_stop_timer(&tp->tf_timer);
@@ -262,7 +263,7 @@ set_tp_schedule(struct evl_rq *rq, struct evl_tp_schedule *gps)
 	struct evl_tp_schedule *old_gps;
 	union evl_sched_param param;
 
-	requires_ugly_lock();
+	assert_evl_lock(&rq->lock);
 
 	if (EVL_WARN_ON(CORE, gps != NULL &&
 		(gps->pwin_nr <= 0 || gps->pwins[0].w_offset != 0)))
@@ -325,7 +326,7 @@ static int tp_control(int cpu, union evl_sched_ctlparam *ctlp,
 
 	rq = evl_cpu_rq(cpu);
 
-	xnlock_get_irqsave(&nklock, flags);
+	evl_spin_lock_irqsave(&rq->lock, flags);
 
 	switch (pt->op) {
 	case evl_install_tp:
@@ -337,11 +338,11 @@ static int tp_control(int cpu, union evl_sched_ctlparam *ctlp,
 		goto switch_schedule;
 	case evl_start_tp:
 		start_tp_schedule(rq);
-		xnlock_put_irqrestore(&nklock, flags);
+		evl_spin_unlock_irqrestore(&rq->lock, flags);
 		return 0;
 	case evl_stop_tp:
 		stop_tp_schedule(rq);
-		xnlock_put_irqrestore(&nklock, flags);
+		evl_spin_unlock_irqrestore(&rq->lock, flags);
 		return 0;
 	case evl_get_tp:
 		break;
@@ -350,7 +351,7 @@ static int tp_control(int cpu, union evl_sched_ctlparam *ctlp,
 	}
 
 	gps = get_tp_schedule(rq);
-	xnlock_put_irqrestore(&nklock, flags);
+	evl_spin_unlock_irqrestore(&rq->lock, flags);
 	if (gps == NULL)
 		return 0;
 
@@ -380,7 +381,7 @@ static int tp_control(int cpu, union evl_sched_ctlparam *ctlp,
 	return 0;
 
 install_schedule:
-	xnlock_put_irqrestore(&nklock, flags);
+	evl_spin_unlock_irqrestore(&rq->lock, flags);
 
 	gps = evl_alloc(sizeof(*gps) + pt->nr_windows * sizeof(*w));
 	if (gps == NULL)
@@ -414,9 +415,9 @@ static int tp_control(int cpu, union evl_sched_ctlparam *ctlp,
 	gps->pwin_nr = n;
 	gps->tf_duration = next_offset;
 switch_schedule:
-	xnlock_get_irqsave(&nklock, flags);
+	evl_spin_lock_irqsave(&rq->lock, flags);
 	ogps = set_tp_schedule(rq, gps);
-	xnlock_put_irqrestore(&nklock, flags);
+	evl_spin_unlock_irqrestore(&rq->lock, flags);
 
 	if (ogps)
 		put_tp_schedule(ogps);
diff --git a/kernel/evl/syscall.c b/kernel/evl/syscall.c
index 5473e2ca4b85..20630358d3bd 100644
--- a/kernel/evl/syscall.c
+++ b/kernel/evl/syscall.c
@@ -67,7 +67,11 @@ static void prepare_for_signal(struct task_struct *p,
 	 * 3 generic calls only).
 	 */
 
-	xnlock_get_irqsave(&nklock, flags);
+	/*
+	 * @curr == this_evl_rq()->curr over oob so no need to grab
+	 * @curr->lock.
+	 */
+	evl_spin_lock_irqsave(&curr->rq->lock, flags);
 
 	if (curr->info & T_KICKED) {
 		if (signal_pending(p)) {
@@ -79,7 +83,7 @@ static void prepare_for_signal(struct task_struct *p,
 		curr->info &= ~T_KICKED;
 	}
 
-	xnlock_put_irqrestore(&nklock, flags);
+	evl_spin_unlock_irqrestore(&curr->rq->lock, flags);
 
 	evl_test_cancel();
 
diff --git a/kernel/evl/thread.c b/kernel/evl/thread.c
index 2a6c4b72c7d6..b032fbb8efaa 100644
--- a/kernel/evl/thread.c
+++ b/kernel/evl/thread.c
@@ -116,38 +116,6 @@ static inline void drop_u_cap(struct evl_thread *thread,
 	}
 }
 
-#ifdef CONFIG_SMP
-
-/* thread->lock + nklock held, IRQs off */
-void evl_migrate_thread(struct evl_thread *thread, struct evl_rq *rq)
-{
-	requires_ugly_lock();
-	assert_evl_lock(&thread->lock);
-
-	if (thread->rq == rq)
-		return;
-
-	trace_evl_thread_migrate(thread, evl_rq_cpu(rq));
-	/*
-	 * Timer migration is postponed until the next timeout happens
-	 * for the periodic and rrb timers. The resource/periodic
-	 * timer will be moved to the right CPU next time
-	 * evl_prepare_timed_wait() is called for it (via
-	 * evl_sleep_on()).
-	 */
-	evl_migrate_rq(thread, rq);
-
-	evl_reset_account(&thread->stat.lastperiod);
-}
-
-#else
-
-static inline void evl_migrate_thread(struct evl_thread *thread,
-				struct evl_rq *rq)
-{ }
-
-#endif	/* CONFIG_SMP */
-
 static void pin_to_initial_cpu(struct evl_thread *thread)
 {
 	struct task_struct *p = current;
@@ -169,14 +137,12 @@ static void pin_to_initial_cpu(struct evl_thread *thread)
 	/*
 	 * @thread is still unstarted EVL-wise, we are in the process
 	 * of mapping the current in-band task to it. Therefore
-	 * evl_migrate_thread() can be called for pinning it on a
-	 * real-time CPU.
+	 * evl_migrate_thread() can be called for pinning it on an
+	 * out-of-band CPU.
 	 */
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
 	rq = evl_cpu_rq(cpu);
+	evl_spin_lock_irqsave(&thread->lock, flags);
 	evl_migrate_thread(thread, rq);
-	xnlock_put(&nklock);
 	evl_spin_unlock_irqrestore(&thread->lock, flags);
 }
 
@@ -284,17 +250,16 @@ EXPORT_SYMBOL_GPL(evl_init_thread);
 static void uninit_thread(struct evl_thread *thread)
 {
 	unsigned long flags;
+	struct evl_rq *rq;
 
 	no_ugly_lock();
 
 	evl_destroy_timer(&thread->rtimer);
 	evl_destroy_timer(&thread->ptimer);
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 	evl_forget_thread(thread);
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 
 	kfree(thread->name);
 }
@@ -303,6 +268,7 @@ static void do_cleanup_current(struct evl_thread *curr)
 {
 	struct cred *newcap;
 	unsigned long flags;
+	struct evl_rq *rq;
 
 	no_ugly_lock();
 
@@ -329,8 +295,7 @@ static void do_cleanup_current(struct evl_thread *curr)
 
 	dequeue_old_thread(curr);
 
-	evl_spin_lock_irqsave(&curr->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(curr, flags);
 
 	if (curr->state & T_READY) {
 		EVL_WARN_ON(CORE, (curr->state & EVL_THREAD_BLOCK_BITS));
@@ -340,9 +305,7 @@ static void do_cleanup_current(struct evl_thread *curr)
 
 	curr->state |= T_ZOMBIE;
 
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&curr->lock, flags);
-
+	evl_put_thread_rq(curr, rq, flags);
 	uninit_thread(curr);
 }
 
@@ -506,13 +469,10 @@ void evl_sleep_on(ktime_t timeout, enum evl_tmode timeout_mode,
 
 	oob_context_only();
 	no_ugly_lock();
-
-	evl_spin_lock_irqsave(&curr->lock, flags);
-	xnlock_get(&nklock);
-
 	trace_evl_sleep_on(timeout, timeout_mode, clock, wchan);
 
-	rq = curr->rq;
+	rq = evl_get_thread_rq(curr, flags);
+
 	oldstate = curr->state;
 
 	/*
@@ -563,28 +523,24 @@ void evl_sleep_on(ktime_t timeout, enum evl_tmode timeout_mode,
 
 	evl_set_resched(rq);
 out:
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&curr->lock, flags);
+	evl_put_thread_rq(curr, rq, flags);
 }
 
-/* thread->lock + nklock held, irqs off */
+/* thread->lock + thread->rq->lock held, irqs off */
 static void evl_wakeup_thread_locked(struct evl_thread *thread,
 				int mask, int info)
 {
+	struct evl_rq *rq = thread->rq;
 	unsigned long oldstate;
-	struct evl_rq *rq;
 
 	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_evl_lock(&thread->rq->lock);
 
 	if (EVL_WARN_ON(CORE, mask & ~(T_DELAY|T_PEND|T_WAIT)))
 		return;
 
-	xnlock_get(&nklock);
-
 	trace_evl_wakeup_thread(thread, mask, info);
 
-	rq = thread->rq;
 	oldstate = thread->state;
 	if (likely(oldstate & mask)) {
 		/* Clear T_DELAY along w/ T_PEND in state. */
@@ -609,19 +565,16 @@ static void evl_wakeup_thread_locked(struct evl_thread *thread,
 				evl_inc_counter(&thread->stat.rwa);
 		}
 	}
-
-	xnlock_put(&nklock);
 }
 
 void evl_wakeup_thread(struct evl_thread *thread, int mask, int info)
 {
 	unsigned long flags;
+	struct evl_rq *rq;
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 	evl_wakeup_thread_locked(thread, mask, info);
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 }
 
 void evl_hold_thread(struct evl_thread *thread, int mask)
@@ -634,12 +587,10 @@ void evl_hold_thread(struct evl_thread *thread, int mask)
 	if (EVL_WARN_ON(CORE, mask & ~(T_SUSP|T_HALT|T_DORMANT)))
 		return;
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
-
 	trace_evl_hold_thread(thread, mask);
 
-	rq = thread->rq;
+	rq = evl_get_thread_rq(thread, flags);
+
 	oldstate = thread->state;
 
 	/*
@@ -677,35 +628,31 @@ void evl_hold_thread(struct evl_thread *thread, int mask)
 	else if (((oldstate & (EVL_THREAD_BLOCK_BITS|T_USER)) == (T_INBAND|T_USER)))
 		evl_signal_thread(thread, SIGEVL, SIGEVL_ACTION_HOME);
  out:
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 }
 
-/* thread->lock + nklock held, irqs off */
+/* thread->lock + thread->rq->lock held, irqs off */
 static void evl_release_thread_locked(struct evl_thread *thread,
 				int mask, int info)
 {
+	struct evl_rq *rq = thread->rq;
 	unsigned long oldstate;
-	struct evl_rq *rq;
 
 	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_evl_lock(&thread->rq->lock);
 
 	if (EVL_WARN_ON(CORE, mask & ~(T_SUSP|T_HALT|T_INBAND|T_DORMANT)))
 		return;
 
-	xnlock_get(&nklock);
-
 	trace_evl_release_thread(thread, mask, info);
 
-	rq = thread->rq;
 	oldstate = thread->state;
 	if (oldstate & mask) {
 		thread->state &= ~mask;
 		thread->info |= info;
 
 		if (thread->state & EVL_THREAD_BLOCK_BITS)
-			goto out;
+			return;
 
 		if (unlikely((oldstate & mask) & T_HALT)) {
 			/* Requeue at head of priority group. */
@@ -723,19 +670,16 @@ static void evl_release_thread_locked(struct evl_thread *thread,
 	evl_set_resched(rq);
 	if (rq != this_evl_rq())
 		evl_inc_counter(&thread->stat.rwa);
-out:
-	xnlock_put(&nklock);
 }
 
 void evl_release_thread(struct evl_thread *thread, int mask, int info)
 {
 	unsigned long flags;
+	struct evl_rq *rq;
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 	evl_release_thread_locked(thread, mask, info);
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 }
 
 static void inband_task_wakeup(struct irq_work *work)
@@ -749,157 +693,6 @@ static void inband_task_wakeup(struct irq_work *work)
 	wake_up_process(p);
 }
 
-void evl_switch_inband(int cause)
-{
-	struct evl_thread *curr = evl_current();
-	struct task_struct *p = current;
-	struct kernel_siginfo si;
-	struct evl_rq *rq;
-
-	oob_context_only();
-
-	trace_evl_switching_inband(cause);
-
-	/*
-	 * This is the only location where we may assert T_INBAND for
-	 * a thread. Basic assumption: we are the current thread on
-	 * this CPU running in OOB context.
-	 *
-	 * CAVEAT: dovetail_leave_oob() must run _before_ the in-band
-	 * kernel is allowed to take interrupts again, so that
-	 * try_to_wake_up() does not block the wake up request for the
-	 * switching thread as a result of testing task_is_off_stage().
-	 */
-	oob_irq_disable();
-	evl_spin_lock(&curr->lock);
-	irq_work_queue(&curr->inband_work);
-	xnlock_get(&nklock);
-	if (curr->state & T_READY) {
-		evl_dequeue_thread(curr);
-		curr->state &= ~T_READY;
-	}
-	curr->info &= ~EVL_THREAD_INFO_MASK;
-	curr->state |= T_INBAND;
-	curr->local_info &= ~T_SYSRST;
-	rq = curr->rq;
-	evl_set_resched(rq);
-	dovetail_leave_oob();
-	xnlock_put(&nklock);
-	evl_spin_unlock(&curr->lock);
-	__evl_schedule();
-	oob_irq_enable();
-	dovetail_resume_inband();
-
-	/*
-	 * Basic sanity check after an expected transition to in-band
-	 * context.
-	 */
-	EVL_WARN(CORE, !running_inband(),
-		"evl_switch_inband() failed for thread %s[%d]",
-		curr->name, evl_get_inband_pid(curr));
-
-	/* Account for switch to in-band context. */
-	evl_inc_counter(&curr->stat.isw);
-
-	trace_evl_switched_inband(curr);
-
-	/*
-	 * When switching to in-band context, we check for propagating
-	 * the current EVL schedparams that might have been set for
-	 * current while running in OOB context.
-	 *
-	 * CAUTION: This obviously won't update the schedparams cached
-	 * by the glibc for the caller in user-space, but this is the
-	 * deal: we don't switch threads which issue
-	 * EVL_THRIOC_SET_SCHEDPARAM to in-band mode, but then only
-	 * the kernel side will be aware of the change, and glibc
-	 * might cache obsolete information.
-	 */
-	evl_propagate_schedparam_change(curr);
-
-	if ((curr->state & T_USER) && cause != SIGDEBUG_NONE) {
-		/*
-		 * Help debugging spurious stage switches by sending
-		 * SIGDEBUG. We are running inband on the context of
-		 * the receiver, so we may bypass evl_signal_thread()
-		 * for this.
-		 */
-		if (curr->state & T_WOSS) {
-			memset(&si, 0, sizeof(si));
-			si.si_signo = SIGDEBUG;
-			si.si_code = SI_QUEUE;
-			si.si_int = cause | sigdebug_marker;
-			send_sig_info(SIGDEBUG, &si, p);
-		}
-		/* May check for locking inconsistency too. */
-		if (curr->state & T_WOLI)
-			evl_detect_boost_drop(curr);
-	}
-
-	/* @curr is now running inband. */
-	evl_sync_uwindow(curr);
-}
-EXPORT_SYMBOL_GPL(evl_switch_inband);
-
-int evl_switch_oob(void)
-{
-	struct task_struct *p = current;
-	struct evl_thread *curr;
-	struct evl_rq *rq;
-	int ret;
-
-	inband_context_only();
-
-	curr = evl_current();
-	if (curr == NULL)
-		return -EPERM;
-
-	if (signal_pending(p))
-		return -ERESTARTSYS;
-
-	trace_evl_switching_oob(curr);
-
-	evl_clear_sync_uwindow(curr, T_INBAND);
-
-	ret = dovetail_leave_inband();
-	if (ret) {
-		evl_test_cancel();
-		evl_set_sync_uwindow(curr, T_INBAND);
-		return ret;
-	}
-
-	/*
-	 * The current task is now running on the out-of-band
-	 * execution stage. Hard irqs must be off, otherwise something
-	 * is really wrong in the Dovetail layer.
-	 */
-	if (EVL_WARN_ON_ONCE(CORE, !hard_irqs_disabled()))
-		hard_irqs_disabled();
-
-	rq = this_evl_rq();
-
-	xnlock_clear_irqon(&nklock);
-	evl_test_cancel();
-
-	trace_evl_switched_oob(curr);
-
-	/*
-	 * Recheck pending signals once again. As we block task
-	 * wakeups during the stage transition and handle_sigwake_event()
-	 * ignores signals until T_INBAND is cleared, any signal in
-	 * between is just silently queued up to here.
-	 */
-	if (signal_pending(p)) {
-		evl_switch_inband(!(curr->state & T_SSTEP) ?
-				SIGDEBUG_MIGRATE_SIGNAL:
-				SIGDEBUG_NONE);
-		return -ERESTARTSYS;
-	}
-
-	return 0;
-}
-EXPORT_SYMBOL_GPL(evl_switch_oob);
-
 void evl_set_kthread_priority(struct evl_kthread *kthread, int priority)
 {
 	union evl_sched_param param = { .fifo = { .prio = priority } };
@@ -955,17 +748,13 @@ ktime_t evl_delay_thread(ktime_t timeout, enum evl_tmode timeout_mode,
 			struct evl_clock *clock)
 {
 	struct evl_thread *curr = evl_current();
-	unsigned long flags;
 	ktime_t rem = 0;
 
 	evl_sleep_on(timeout, timeout_mode, clock, NULL);
 	evl_schedule();
 
-	if (curr->info & T_BREAK) {
-		xnlock_get_irqsave(&nklock, flags);
+	if (curr->info & T_BREAK)
 		rem = __evl_get_stopped_timer_delta(&curr->rtimer);
-		xnlock_put_irqrestore(&nklock, flags);
-	}
 
 	return rem;
 }
@@ -1069,18 +858,17 @@ EXPORT_SYMBOL_GPL(evl_wait_thread_period);
 void evl_cancel_thread(struct evl_thread *thread)
 {
 	unsigned long flags;
+	struct evl_rq *rq;
 
 	no_ugly_lock();
 
 	if (EVL_WARN_ON(CORE, thread->state & T_ROOT))
 		return;
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 
 	if (thread->state & T_ZOMBIE) {
-		xnlock_put(&nklock);
-		evl_spin_unlock_irqrestore(&thread->lock, flags);
+		evl_put_thread_rq(thread, rq, flags);
 		return;
 	}
 
@@ -1101,15 +889,13 @@ void evl_cancel_thread(struct evl_thread *thread)
 	 * the prep work.
 	 */
 	if ((thread->state & (T_DORMANT|T_INBAND)) == (T_DORMANT|T_INBAND)) {
-		xnlock_put(&nklock);
-		evl_spin_unlock_irqrestore(&thread->lock, flags);
-		evl_release_thread(thread, T_DORMANT, T_KICKED);
+		evl_release_thread_locked(thread, T_DORMANT, T_KICKED);
+		evl_put_thread_rq(thread, rq, flags);
 		goto out;
 	}
 
 check_self_cancel:
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 
 	if (evl_current() == thread) {
 		evl_test_cancel();
@@ -1154,6 +940,7 @@ int evl_join_thread(struct evl_thread *thread, bool uninterruptible)
 	struct evl_thread *curr = evl_current();
 	bool switched = false;
 	unsigned long flags;
+	struct evl_rq *rq;
 	int ret = 0;
 
 	if (EVL_WARN_ON(CORE, thread->state & T_ROOT))
@@ -1162,29 +949,25 @@ int evl_join_thread(struct evl_thread *thread, bool uninterruptible)
 	if (thread == curr)
 		return -EDEADLK;
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 
 	/*
 	 * We allow multiple callers to join @thread, this is purely a
 	 * synchronization mechanism with no resource collection.
 	 */
 	if (thread->info & T_DORMANT) {
-		xnlock_put(&nklock);
-		evl_spin_unlock_irqrestore(&thread->lock, flags);
+		evl_put_thread_rq(thread, rq, flags);
 		return 0;
 	}
 
 	trace_evl_thread_join(thread);
 
 	if (curr && !(curr->state & T_INBAND)) {
-		xnlock_put(&nklock);
-		evl_spin_unlock_irqrestore(&thread->lock, flags);
+		evl_put_thread_rq(thread, rq, flags);
 		evl_switch_inband(SIGDEBUG_NONE);
 		switched = true;
 	} else {
-		xnlock_put(&nklock);
-		evl_spin_unlock_irqrestore(&thread->lock, flags);
+		evl_put_thread_rq(thread, rq, flags);
 	}
 
 	/*
@@ -1213,15 +996,14 @@ int evl_set_thread_schedparam(struct evl_thread *thread,
 			const union evl_sched_param *sched_param)
 {
 	unsigned long flags;
+	struct evl_rq *rq;
 	int ret;
 
 	no_ugly_lock();
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 	ret = evl_set_thread_schedparam_locked(thread, sched_class, sched_param);
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 
 	return ret;
 }
@@ -1233,8 +1015,7 @@ int evl_set_thread_schedparam_locked(struct evl_thread *thread,
 {
 	int old_wprio, new_wprio, ret;
 
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	old_wprio = thread->wprio;
 
@@ -1272,7 +1053,7 @@ void __evl_test_cancel(struct evl_thread *curr)
 	 * NOTE: curr->rq is stable from our POV and can't change
 	 * under our feet.
 	 */
-	if (curr->rq->lflags & RQ_IRQ)
+	if (curr->rq->local_flags & RQ_IRQ)
 		return;
 
 	if (!(curr->state & T_INBAND))
@@ -1290,6 +1071,7 @@ void __evl_propagate_schedparam_change(struct evl_thread *curr)
 	struct task_struct *p = current;
 	struct sched_param param;
 	unsigned long flags;
+	struct evl_rq *rq;
 
 	no_ugly_lock();
 
@@ -1300,9 +1082,8 @@ void __evl_propagate_schedparam_change(struct evl_thread *curr)
 	 * is eventually handled. We just have to protect against a
 	 * set-clear race.
 	 */
-	evl_spin_lock_irqsave(&curr->lock, flags);
+	rq = evl_get_thread_rq(curr, flags);
 	kprio = curr->bprio;
-	xnlock_get(&nklock);
 	curr->info &= ~T_SCHEDP;
 
 	/*
@@ -1314,8 +1095,7 @@ void __evl_propagate_schedparam_change(struct evl_thread *curr)
 	else if (kprio > EVL_FIFO_MAX_PRIO)
 		kprio = EVL_FIFO_MAX_PRIO;
 
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&curr->lock, flags);
+	evl_put_thread_rq(curr, rq, flags);
 
 	if (p->policy != kpolicy || (kprio > 0 && p->rt_priority != kprio)) {
 		param.sched_priority = kprio;
@@ -1350,11 +1130,11 @@ void evl_kick_thread(struct evl_thread *thread)
 {
 	struct task_struct *p = thread->altsched.task;
 	unsigned long flags;
+	struct evl_rq *rq;
 
 	no_ugly_lock();
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 
 	if ((thread->info & T_KICKED) || (thread->state & T_INBAND))
 		goto out;
@@ -1424,8 +1204,7 @@ void evl_kick_thread(struct evl_thread *thread)
 	if ((thread->state & T_USER) && thread != this_evl_rq_thread())
 		dovetail_send_mayday(p);
 out:
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 }
 EXPORT_SYMBOL_GPL(evl_kick_thread);
 
@@ -1434,11 +1213,11 @@ void evl_demote_thread(struct evl_thread *thread)
 	struct evl_sched_class *sched_class;
 	union evl_sched_param param;
 	unsigned long flags;
+	struct evl_rq *rq;
 
 	no_ugly_lock();
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 
 	/*
 	 * First demote @thread to the weak class, which still has
@@ -1451,8 +1230,7 @@ void evl_demote_thread(struct evl_thread *thread)
 	sched_class = &evl_sched_weak;
 	evl_set_thread_schedparam_locked(thread, sched_class, &param);
 
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 
 	/* Then unblock it from any wait state. */
 	evl_kick_thread(thread);
@@ -1592,6 +1370,7 @@ int evl_update_mode(__u32 mask, bool set)
 {
 	struct evl_thread *curr = evl_current();
 	unsigned long flags;
+	struct evl_rq *rq;
 
 	if (curr == NULL)
 		return -EPERM;
@@ -1601,16 +1380,14 @@ int evl_update_mode(__u32 mask, bool set)
 
 	trace_evl_thread_update_mode(mask, set);
 
-	evl_spin_lock_irqsave(&curr->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(curr, flags);
 
 	if (set)
 		curr->state |= mask;
 	else
 		curr->state &= ~mask;
 
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&curr->lock, flags);
+	evl_put_thread_rq(curr, rq, flags);
 
 	return 0;
 }
@@ -1721,7 +1498,7 @@ static void handle_migration_event(struct dovetail_migration_data *d)
 	 * migration call, because the latter always has to take place
 	 * on behalf of the target thread itself while running
 	 * in-band. Therefore, that thread needs to switch to in-band
-	 * context first, then move back to OOB, so that affinity_ok()
+	 * context first, then move back to OOB, so that check_cpu_affinity()
 	 * does the fixup work.
 	 *
 	 * We force this by sending a SIGEVL signal to the migrated
@@ -1733,91 +1510,14 @@ static void handle_migration_event(struct dovetail_migration_data *d)
 		evl_signal_thread(thread, SIGEVL, SIGEVL_ACTION_HOME);
 }
 
-static bool affinity_ok(struct task_struct *p) /* oob stage stalled */
-{
-	struct evl_thread *thread = evl_thread_from_task(p);
-	int cpu = task_cpu(p);
-	struct evl_rq *rq;
-	bool ret = true;
-
-	evl_spin_lock(&thread->lock);
-	xnlock_get(&nklock);
-
-	/*
-	 * To maintain consistency between both the EVL and in-band
-	 * schedulers, reflecting a thread migration to another CPU
-	 * into EVL's scheduler state must happen from in-band context
-	 * only, on behalf of the migrated thread itself once it runs
-	 * on the target CPU.
-	 *
-	 * This means that the EVL scheduler state regarding the CPU
-	 * information lags behind the in-band scheduler state until
-	 * the migrated thread switches back to OOB context
-	 * (i.e. task_cpu(p) !=
-	 * evl_rq_cpu(evl_thread_from_task(p)->rq)).  This is ok since
-	 * EVL will not schedule such thread until then.
-	 *
-	 * affinity_ok() detects when a EVL thread switching back to
-	 * OOB context did move to another CPU earlier while running
-	 * in-band. If so, do the fixups to reflect the change.
-	 */
-	if (!is_threading_cpu(cpu)) {
-		printk(EVL_WARNING "thread %s[%d] switched to non-rt CPU%d, aborted.\n",
-			thread->name, evl_get_inband_pid(thread), cpu);
-		/*
-		 * Can't call evl_cancel_thread() from a CPU migration
-		 * point, that would break. Since we are on the wakeup
-		 * path to OOB context, just raise T_CANCELD to catch
-		 * it in evl_switch_oob().
-		 */
-		thread->info |= T_CANCELD;
-		ret = false;
-		goto out;
-	}
-
-	rq = evl_cpu_rq(cpu);
-	if (rq == thread->rq)
-		goto out;
-
-	/*
-	 * If the current thread moved to a supported out-of-band CPU,
-	 * which is not part of its original affinity mask, assume
-	 * user wants to extend this mask.
-	 */
-	if (!cpumask_test_cpu(cpu, &thread->affinity))
-		cpumask_set_cpu(cpu, &thread->affinity);
-
-	evl_migrate_thread(thread, rq);
-out:
-	xnlock_put(&nklock);
-	evl_spin_unlock(&thread->lock);
-
-	return ret;
-}
-
 #else /* !CONFIG_SMP */
 
 static void handle_migration_event(struct dovetail_migration_data *d)
 {
 }
 
-static inline bool affinity_ok(struct task_struct *p)
-{
-	return true;
-}
-
 #endif /* CONFIG_SMP */
 
-void resume_oob_task(struct task_struct *p) /* oob stage stalled */
-{
-	struct evl_thread *thread = evl_thread_from_task(p);
-
-	if (affinity_ok(p))
-		evl_release_thread(thread, T_INBAND, 0);
-
-	evl_schedule();
-}
-
 static void handle_schedule_event(struct task_struct *next_task)
 {
 	struct task_struct *prev_task;
@@ -1854,9 +1554,9 @@ static void handle_schedule_event(struct task_struct *next_task)
 				sigismember(&pending, SIGINT))
 				goto check;
 		}
-		xnlock_get(&nklock);
+		evl_spin_lock(&next->rq->lock);
 		next->state &= ~T_SSTEP;
-		xnlock_put(&nklock);
+		evl_spin_unlock(&next->rq->lock);
 		next->local_info |= T_HICCUP;
 	}
 	evl_spin_unlock_irqrestore(&next->lock, flags);
@@ -1912,9 +1612,9 @@ static void handle_sigwake_event(struct task_struct *p)
 		if (sigismember(&pending, SIGTRAP) ||
 			sigismember(&pending, SIGSTOP)
 			|| sigismember(&pending, SIGINT))
-			xnlock_get(&nklock);
+			evl_spin_lock(&thread->rq->lock);
 			thread->state &= ~T_SSTEP;
-			xnlock_put(&nklock);
+			evl_spin_unlock(&thread->rq->lock);
 	}
 
 	evl_spin_unlock_irqrestore(&thread->lock, flags);
@@ -1972,15 +1672,14 @@ void handle_inband_event(enum inband_event_type event, void *data)
 	}
 }
 
-/* thread->lock + nklock held, irqs off */
+/* thread->lock + thread->rq->lock held, irqs off */
 static int set_time_slice(struct evl_thread *thread, ktime_t quantum)
 {
-	struct evl_rq *rq;
+	struct evl_rq *rq = thread->rq;
 
 	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_evl_lock(&rq->lock);
 
-	rq = thread->rq;
 	thread->rrperiod = quantum;
 
 	if (!timeout_infinite(quantum)) {
@@ -2010,13 +1709,13 @@ static int set_sched_attrs(struct evl_thread *thread,
 	struct evl_sched_class *sched_class;
 	union evl_sched_param param;
 	unsigned long flags;
+	struct evl_rq *rq;
 	int ret = -EINVAL;
 	ktime_t tslice;
 
 	trace_evl_thread_setsched(thread, attrs);
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 
 	tslice = thread->rrperiod;
 	sched_class = evl_find_sched_class(&param, attrs, &tslice);
@@ -2029,8 +1728,7 @@ static int set_sched_attrs(struct evl_thread *thread,
 
 	ret = evl_set_thread_schedparam_locked(thread, sched_class, &param);
 out:
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 
 	evl_schedule();
 
@@ -2043,8 +1741,7 @@ static void __get_sched_attrs(struct evl_sched_class *sched_class,
 {
 	union evl_sched_param param;
 
-	assert_evl_lock(&thread->lock);
-	requires_ugly_lock();
+	assert_thread_pinned(thread);
 
 	attrs->sched_policy = sched_class->policy;
 
@@ -2081,23 +1778,22 @@ static void get_sched_attrs(struct evl_thread *thread,
 			struct evl_sched_attrs *attrs)
 {
 	unsigned long flags;
+	struct evl_rq *rq;
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 	/* Get the base scheduling attributes. */
 	attrs->sched_priority = thread->bprio;
 	__get_sched_attrs(thread->base_class, thread, attrs);
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 }
 
 void evl_get_thread_state(struct evl_thread *thread,
 			struct evl_thread_state *statebuf)
 {
 	unsigned long flags;
+	struct evl_rq *rq;
 
-	evl_spin_lock_irqsave(&thread->lock, flags);
-	xnlock_get(&nklock);
+	rq = evl_get_thread_rq(thread, flags);
 	/* Get the effective scheduling attributes. */
 	statebuf->eattrs.sched_priority = thread->cprio;
 	__get_sched_attrs(thread->sched_class, thread, &statebuf->eattrs);
@@ -2109,8 +1805,7 @@ void evl_get_thread_state(struct evl_thread *thread,
 	statebuf->rwa = evl_get_counter(&thread->stat.rwa);
 	statebuf->xtime = ktime_to_ns(evl_get_account_total(
 					&thread->stat.account));
-	xnlock_put(&nklock);
-	evl_spin_unlock_irqrestore(&thread->lock, flags);
+	evl_put_thread_rq(thread, rq, flags);
 }
 EXPORT_SYMBOL_GPL(evl_get_thread_state);
 
diff --git a/kernel/evl/tick.c b/kernel/evl/tick.c
index 625040277e1e..1a82d8d46b22 100644
--- a/kernel/evl/tick.c
+++ b/kernel/evl/tick.c
@@ -68,9 +68,9 @@ static int proxy_set_oneshot_stopped(struct clock_event_device *proxy_dev)
 
 	rq = this_evl_rq();
 	evl_stop_timer(&rq->inband_timer);
-	rq->lflags |= RQ_TSTOPPED;
+	rq->local_flags |= RQ_TSTOPPED;
 
-	if (rq->lflags & RQ_IDLE) {
+	if (rq->local_flags & RQ_IDLE) {
 		real_dev = dev->real_device;
 		real_dev->set_state_oneshot_stopped(real_dev);
 	}
@@ -178,13 +178,13 @@ void evl_program_proxy_tick(struct evl_clock *clock)
 	 * will be done on exit anyway. Also exit if there is no
 	 * pending timer.
 	 */
-	if (this_rq->lflags & RQ_TIMER)
+	if (this_rq->local_flags & RQ_TIMER)
 		return;
 
 	tmb = evl_this_cpu_timers(clock);
 	tn = evl_get_tqueue_head(&tmb->q);
 	if (tn == NULL) {
-		this_rq->lflags |= RQ_IDLE;
+		this_rq->local_flags |= RQ_IDLE;
 		return;
 	}
 
@@ -206,14 +206,14 @@ void evl_program_proxy_tick(struct evl_clock *clock)
 	 * __evl_schedule()), or a timer with an earlier timeout date
 	 * is scheduled, whichever comes first.
 	 */
-	this_rq->lflags &= ~(RQ_TDEFER|RQ_IDLE|RQ_TSTOPPED);
+	this_rq->local_flags &= ~(RQ_TDEFER|RQ_IDLE|RQ_TSTOPPED);
 	timer = container_of(tn, struct evl_timer, node);
 	if (timer == &this_rq->inband_timer) {
 		if (evl_need_resched(this_rq) ||
 			!(this_rq->curr->state & T_ROOT)) {
 			tn = evl_get_tqueue_next(&tmb->q, tn);
 			if (tn) {
-				this_rq->lflags |= RQ_TDEFER;
+				this_rq->local_flags |= RQ_TDEFER;
 				timer = container_of(tn, struct evl_timer, node);
 			}
 		}
diff --git a/kernel/evl/timer.c b/kernel/evl/timer.c
index 0e7afa891961..de545c180970 100644
--- a/kernel/evl/timer.c
+++ b/kernel/evl/timer.c
@@ -71,9 +71,11 @@ static inline void double_timer_base_lock(struct evl_timerbase *tb1,
 		raw_spin_lock(&tb1->lock);
 	else if (tb1 < tb2) {
 		raw_spin_lock(&tb1->lock);
+		/* FIXME: raw_spin_lock_nested for tb2? */
 		raw_spin_lock(&tb2->lock);
 	} else {
 		raw_spin_lock(&tb2->lock);
+		/* FIXME: raw_spin_lock_nested for tb1? */
 		raw_spin_lock(&tb1->lock);
 	}
 }
@@ -98,7 +100,7 @@ static bool timer_at_front(struct evl_timer *timer)
 	if (tn == &timer->node)
 		return true;
 
-	if (rq->lflags & RQ_TDEFER) {
+	if (rq->local_flags & RQ_TDEFER) {
 		tn = evl_get_tqueue_next(tq, tn);
 		if (tn == &timer->node)
 			return true;
@@ -116,7 +118,7 @@ static void program_timer(struct evl_timer *timer,
 	evl_enqueue_timer(timer, tq);
 
 	rq = evl_get_timer_rq(timer);
-	if (!(rq->lflags & RQ_TSTOPPED) && !timer_at_front(timer))
+	if (!(rq->local_flags & RQ_TSTOPPED) && !timer_at_front(timer))
 		return;
 
 	if (rq != this_evl_rq())
@@ -345,12 +347,14 @@ EXPORT_SYMBOL_GPL(evl_destroy_timer);
 
 /*
  * evl_move_timer - change the reference clock and/or the CPU
- *                     affinity of a timer
+ *                  affinity of a timer
  * @timer:      timer to modify
  * @clock:      reference clock
  * @rq:         runqueue to assign the timer to
+ *
+ * oob stage stalled on entry.
  */
-void evl_move_timer(struct evl_timer *timer, /* nklocked, IRQs off */
+void evl_move_timer(struct evl_timer *timer,
 		struct evl_clock *clock, struct evl_rq *rq)
 {
 	struct evl_timerbase *old_base, *new_base;
@@ -358,7 +362,7 @@ void evl_move_timer(struct evl_timer *timer, /* nklocked, IRQs off */
 	unsigned long flags;
 	int cpu;
 
-	requires_ugly_lock();	/* XXX: why that? */
+	EVL_WARN_ON_ONCE(CORE, !oob_irqs_disabled());
 
 	trace_evl_timer_move(timer, clock, evl_rq_cpu(rq));
 
-- 
2.16.4

