From 465ff1651439ce241a5a67d2b600255df7b3af6a Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Thu, 24 Oct 2019 10:46:01 +0200
Subject: [PATCH] evl/lock: add true nesting support to the ugly one

As it stands, xnlock_put() would re-enable interrupts unconditionally
even if nested inside a xnlock_get_irqsave() ->
xnlock_put_irqrestore() section. Fix this to provide true nesting
which does not rely on the in-flags marker.
---
 include/evl/lock.h | 77 ++++++++++++++++++++++++++++--------------------------
 1 file changed, 40 insertions(+), 37 deletions(-)

diff --git a/include/evl/lock.h b/include/evl/lock.h
index 878f069e3884..972f738171ce 100644
--- a/include/evl/lock.h
+++ b/include/evl/lock.h
@@ -15,21 +15,23 @@
 
 #include <linux/irq_pipeline.h>
 
-#define splhigh(x)  ((x) = oob_irq_save() & 1)
+#define splhigh(x)  ((x) = oob_irq_save())
 #ifdef CONFIG_SMP
-#define splexit(x)  oob_irq_restore(x & 1)
+#define splexit(x)  oob_irq_restore(x)
 #else /* !CONFIG_SMP */
 #define splexit(x)  oob_irq_restore(x)
 #endif /* !CONFIG_SMP */
 
 struct xnlock {
 	unsigned owner;
+	unsigned int nesting;
 	arch_spinlock_t alock;
 };
 
 #define XNARCH_LOCK_UNLOCKED			\
 	(struct xnlock) {			\
 		~0,				\
+		0,				\
 		__ARCH_SPIN_LOCK_UNLOCKED,	\
 	}
 
@@ -41,8 +43,6 @@ struct xnlock {
 	((x) = __xnlock_get_irqsave(lock))
 #define xnlock_put_irqrestore(lock,x) \
 	__xnlock_put_irqrestore(lock,x)
-#define xnlock_clear_irqoff(lock)	xnlock_put_irqrestore(lock, 1)
-#define xnlock_clear_irqon(lock)	xnlock_put_irqrestore(lock, 0)
 
 static inline void xnlock_init (struct xnlock *lock)
 {
@@ -54,23 +54,28 @@ static inline void xnlock_init (struct xnlock *lock)
 #define DEFINE_XNLOCK(lock)		struct xnlock lock = XNARCH_LOCK_UNLOCKED
 #define DEFINE_PRIVATE_XNLOCK(lock)	static DEFINE_XNLOCK(lock)
 
-static inline int ___xnlock_get(struct xnlock *lock)
+static inline void __xnlock_get(struct xnlock *lock)
 {
 	int cpu = raw_smp_processor_id();
 
-	if (lock->owner == cpu)
-		return 2;
+	if (lock->owner == cpu) {
+		lock->nesting++;
+		return;
+	}
 
 	arch_spin_lock(&lock->alock);
 	lock->owner = cpu;
-
-	return 0;
+	lock->nesting = 1;
 }
 
-static inline void ___xnlock_put(struct xnlock *lock)
+static inline void __xnlock_put(struct xnlock *lock)
 {
-	lock->owner = ~0U;
-	arch_spin_unlock(&lock->alock);
+	WARN_ON_ONCE(lock->nesting <= 0);
+
+	if (--lock->nesting == 0) {
+		lock->owner = ~0U;
+		arch_spin_unlock(&lock->alock);
+	}
 }
 
 static inline unsigned long
@@ -79,34 +84,27 @@ __xnlock_get_irqsave(struct xnlock *lock)
 	unsigned long flags;
 
 	splhigh(flags);
-
-	flags |= ___xnlock_get(lock);
-
+	__xnlock_get(lock);
 	return flags;
 }
 
-static inline void __xnlock_put_irqrestore(struct xnlock *lock, unsigned long flags)
-{
-	/* Only release the lock if we didn't take it recursively. */
-	if (!(flags & 2))
-		___xnlock_put(lock);
-
-	splexit(flags & 1);
-}
-
-static inline int xnlock_is_owner(struct xnlock *lock)
+static inline
+void __xnlock_put_irqrestore(struct xnlock *lock, unsigned long flags)
 {
-	return lock->owner == raw_smp_processor_id();
+	__xnlock_put(lock);
+	splexit(flags);
 }
 
-static inline int __xnlock_get(struct xnlock *lock)
+static inline void xnlock_clear_irqon(struct xnlock *lock)
 {
-	return ___xnlock_get(lock);
+	lock->nesting = 1;
+	__xnlock_put(lock);
+	splexit(0);
 }
 
-static inline void __xnlock_put(struct xnlock *lock)
+static inline int xnlock_is_owner(struct xnlock *lock)
 {
-	___xnlock_put(lock);
+	return lock->owner == raw_smp_processor_id();
 }
 
 #else /* !CONFIG_SMP */
@@ -116,7 +114,6 @@ static inline void __xnlock_put(struct xnlock *lock)
 #define xnlock_put(lock)		do { } while(0)
 #define xnlock_get_irqsave(lock,x)	splhigh(x)
 #define xnlock_put_irqrestore(lock,x)	splexit(x)
-#define xnlock_clear_irqoff(lock)	oob_irq_disable()
 #define xnlock_clear_irqon(lock)	oob_irq_enable()
 #define xnlock_is_owner(lock)		1
 
@@ -139,14 +136,20 @@ DECLARE_EXTERN_XNLOCK(nklock);
  *    xnlock_put--(&nklock[, flags_inner]);
  * evl_spin_unlock--(&lock[, flags_outer]);
  *
+ * In addition to preventing unwanted rescheduling, this construct
+ * makes sure xnlock_put_irq* will not re-enable irqs unexpectedly, by
+ * having evl_spin_lock_irq* update the OOB stall bit, which the
+ * xnlock* API tests and saves.
+ *
  * Conversely, when the protected section is guaranteed not to call
- * evl_schedule() either directly or indirectly, and the ugly nklock
- * is not nested, Dovetail's hard spinlock API may be used instead.
+ * evl_schedule() either directly or indirectly, does not traverse
+ * code altering the OOB stall bit (including as a consequence of
+ * nesting the ugly nklock), Dovetail's hard spinlock API may be used
+ * instead.
  *
- * Once the ugly lock is entirely dropped from the code, we may
- * manipulate pure hardware flags for interrupt masking instead of the
- * OOB stall bit, as we won't have to store any lock recursion marker
- * there anymore. IOW, splhigh/exit can be replaced by regular
+ * Once the ugly lock is dropped, we may manipulate hardware flags for
+ * interrupt masking directly instead of maintaining the consistency
+ * of the OOB stall bit, replacing splhigh/exit by regular
  * hard_local_irqsave/restore pairs.
  */
 
-- 
2.16.4

