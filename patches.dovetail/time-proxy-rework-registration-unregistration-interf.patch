From 2b7f5c98b1f303fcd30375fd6723b8851f7fe3b1 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Tue, 30 Apr 2019 18:13:14 +0200
Subject: [PATCH] time: proxy: rework registration/unregistration interface

Integrate the proxy logic deeper into the clock event framework, which
fixes some conflicts with the broadcast device, paving the way to the
x86 port.

Signed-off-by: Philippe Gerum <rpm@xenomai.org>
---
 include/linux/clockchips.h  |  48 +++--
 include/linux/tick.h        |  14 +-
 kernel/irq/irqptorture.c    |  55 ++---
 kernel/time/clockevents.c   |  70 +++++-
 kernel/time/tick-common.c   |  20 +-
 kernel/time/tick-internal.h |   5 +
 kernel/time/tick-proxy.c    | 512 ++++++++++++++++++--------------------------
 7 files changed, 358 insertions(+), 366 deletions(-)

diff --git a/include/linux/clockchips.h b/include/linux/clockchips.h
index 7b9f434a15fd..3ea6ccd30859 100644
--- a/include/linux/clockchips.h
+++ b/include/linux/clockchips.h
@@ -39,6 +39,7 @@ enum clock_event_state {
 	CLOCK_EVT_STATE_PERIODIC,
 	CLOCK_EVT_STATE_ONESHOT,
 	CLOCK_EVT_STATE_ONESHOT_STOPPED,
+	CLOCK_EVT_STATE_RESERVED,
 };
 
 /*
@@ -69,15 +70,15 @@ enum clock_event_state {
 # define CLOCK_EVT_FEAT_HRTIMER		0x000080
 
 /*
- * Clockevent device can work with pipelined timer events.
+ * Interrupt pipeline support:
+ *
+ * - Clockevent device can work with pipelined timer events (i.e. proxied).
+ * - Device currently delivers high-precision events via out-of-band interrupts.
+ * - Device acts as a proxy for timer interrupt pipelining.
  */
 # define CLOCK_EVT_FEAT_PIPELINE	0x000100
-
-/*
- * Clockevent device can deliver high-precision events via out-of-band
- * interrupts.
- */
 # define CLOCK_EVT_FEAT_OOB		0x000200
+# define CLOCK_EVT_FEAT_PROXY		0x000400
 
 /**
  * struct clock_event_device - clock event device descriptor
@@ -149,6 +150,11 @@ static inline bool clockevent_state_detached(struct clock_event_device *dev)
 	return dev->state_use_accessors == CLOCK_EVT_STATE_DETACHED;
 }
 
+static inline bool clockevent_state_reserved(struct clock_event_device *dev)
+{
+	return dev->state_use_accessors == CLOCK_EVT_STATE_RESERVED;
+}
+
 static inline bool clockevent_state_shutdown(struct clock_event_device *dev)
 {
 	return dev->state_use_accessors == CLOCK_EVT_STATE_SHUTDOWN;
@@ -235,35 +241,45 @@ static inline void tick_setup_hrtimer_broadcast(void) { }
 # endif
 
 #ifdef CONFIG_IRQ_PIPELINE
+
+struct clock_proxy_device {
+	struct clock_event_device proxy_device;
+	struct clock_event_device *real_device;
+	void (*handle_inband_event)(struct clock_event_device *dev);
+	void (*handle_oob_event)(struct clock_event_device *dev);
+};
+
 void tick_notify_proxy(void);
+
 static inline
 void clockevents_handle_event(struct clock_event_device *ced)
 {
 	/*
 	 * If called from the in-band stage, or for delivering a
-	 * high-precision timer event to the oob stage, call the event
-	 * handler immediately.
+	 * high-precision timer event to the out-of-band stage, call
+	 * the event handler immediately.
 	 *
-	 * Otherwise, ced is still the regular tick device for the
-	 * current CPU which does not handle high-precision events, so
-	 * just relay the incoming tick to the in-band stage via
-	 * tick_notify_proxy().  This situation can happen when all
-	 * CPUs receive the same out-of-band IRQ from a given clock
-	 * event device, but only a subset of the online CPUs is
-	 * actually interested in high-precision events from that
-	 * device.
+	 * Otherwise, ced is still the in-band tick device for the
+	 * current CPU, so just relay the incoming tick to the in-band
+	 * stage via tick_notify_proxy().  This situation can happen
+	 * when all CPUs receive the same out-of-band IRQ from a given
+	 * clock event device, but only a subset of the online CPUs has
+	 * enabled a proxy.
 	 */
 	if (clockevent_is_oob(ced) || running_inband())
 		ced->event_handler(ced);
 	else
 		tick_notify_proxy();
 }
+
 #else
+
 static inline
 void clockevents_handle_event(struct clock_event_device *ced)
 {
 	ced->event_handler(ced);
 }
+
 #endif	/* !CONFIG_IRQ_PIPELINE */
 
 #else /* !CONFIG_GENERIC_CLOCKEVENTS: */
diff --git a/include/linux/tick.h b/include/linux/tick.h
index 04cd3afe490a..08e3984a3844 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -22,19 +22,11 @@ extern void tick_handover_do_timer(void);
 extern void tick_cleanup_dead_cpu(int cpu);
 
 #ifdef CONFIG_IRQ_PIPELINE
-struct proxy_tick_ops {
-	void (*register_device)(struct clock_event_device *proxy_ced,
-				struct clock_event_device *real_ced);
-	void (*unregister_device)(struct clock_event_device *proxy_ced,
-				  struct clock_event_device *real_ced);
-	void (*handle_event)(struct clock_event_device *real_ced);
-};
-int tick_install_proxy(struct proxy_tick_ops *ops,
+int tick_install_proxy(struct clock_proxy_device *(*get_percpu_device)(void),
 		       const struct cpumask *cpumask);
-void tick_uninstall_proxy(struct proxy_tick_ops *ops,
-			  const struct cpumask *cpumask);
+void tick_uninstall_proxy(const struct cpumask *cpumask);
 void tick_notify_proxy(void);
-#endif /* !CONFIG_IRQ_PIPELINE */
+#endif
 
 #else /* CONFIG_GENERIC_CLOCKEVENTS */
 static inline void tick_init(void) { }
diff --git a/kernel/irq/irqptorture.c b/kernel/irq/irqptorture.c
index 9b410cebcbd5..1cf099709914 100644
--- a/kernel/irq/irqptorture.c
+++ b/kernel/irq/irqptorture.c
@@ -20,37 +20,9 @@
 #include <linux/slab.h>
 #include "settings.h"
 
-/*
- * Configure and register the proxy device.
- */
-static void torture_device_register(struct clock_event_device *proxy_ced,
-				    struct clock_event_device *real_ced)
-{
-	u32 freq = (1000000000ULL * real_ced->mult) >> real_ced->shift;
-
-	/*
-	 * Ensure the proxy device has a better rating than the real
-	 * one, so that it will be picked immediately as the system
-	 * tick device when registered.
-	 */
-	proxy_ced->rating = real_ced->rating + 1;
+static DEFINE_PER_CPU(struct clock_proxy_device, torture_tick_device);
 
-	/*
-	 * Configure the proxy as a transparent device, which passes
-	 * on timing requests to the real device unmodified. This is
-	 * basically the default configuration we received from
-	 * tick_install_proxy().
-	 */
-	clockevents_config_and_register(proxy_ced, freq,
-					real_ced->min_delta_ticks,
-					real_ced->max_delta_ticks);
-
-	pr_alert("irq_pipeline" TORTURE_FLAG
-		 " CPU%d: proxy tick registered (%u.%02uMHz)\n",
-		 raw_smp_processor_id(), freq / 1000000, (freq / 10000) % 100);
-}
-
-static void torture_event_handler(struct clock_event_device *real_ced)
+static void torture_event_handler(struct clock_event_device *dev)
 {
 	/*
 	 * We are running on the oob stage, in NMI-like mode. Schedule
@@ -60,19 +32,30 @@ static void torture_event_handler(struct clock_event_device *real_ced)
 	tick_notify_proxy();
 }
 
-static struct proxy_tick_ops proxy_ops = {
-	.register_device = torture_device_register,
-	.handle_event = torture_event_handler,
-};
+static struct clock_proxy_device *get_percpu_device(void)
+{
+	struct clock_proxy_device *dev = raw_cpu_ptr(&torture_tick_device);
+
+	/*
+	 * This test module is only activated once, so this is ok to
+	 * assume that torture_tick_device is zeroed since init. In
+	 * case of multiple activations, we'd need to zero @dev
+	 * manually to make sure not to inherit callbacks and
+	 * settings from a previous run.
+	 */
+	dev->handle_oob_event = torture_event_handler;
+
+	return dev;
+}
 
 static int start_tick_takeover_test(void)
 {
-	return tick_install_proxy(&proxy_ops, cpu_online_mask);
+	return tick_install_proxy(get_percpu_device, cpu_online_mask);
 }
 
 static void stop_tick_takeover_test(void)
 {
-	tick_uninstall_proxy(&proxy_ops, cpu_online_mask);
+	tick_uninstall_proxy(cpu_online_mask);
 }
 
 struct stop_machine_p_data {
diff --git a/kernel/time/clockevents.c b/kernel/time/clockevents.c
index f5490222e134..dc7b2fe45ce2 100644
--- a/kernel/time/clockevents.c
+++ b/kernel/time/clockevents.c
@@ -97,6 +97,7 @@ static int __clockevents_switch_state(struct clock_event_device *dev,
 	/* Transition with new state-specific callbacks */
 	switch (state) {
 	case CLOCK_EVT_STATE_DETACHED:
+	case CLOCK_EVT_STATE_RESERVED:
 		/* The clockevent device is getting replaced. Shut it down. */
 
 	case CLOCK_EVT_STATE_SHUTDOWN:
@@ -437,6 +438,67 @@ int clockevents_unbind_device(struct clock_event_device *ced, int cpu)
 }
 EXPORT_SYMBOL_GPL(clockevents_unbind_device);
 
+#ifdef CONFIG_IRQ_PIPELINE
+
+/**
+ * clockevents_register_proxy - register a proxy device on the current CPU
+ * @dev:	proxy to register
+ */
+int clockevents_register_proxy(struct clock_proxy_device *dev)
+{
+	struct clock_event_device *proxy_dev, *real_dev;
+	unsigned long flags;
+	u32 freq;
+
+	raw_spin_lock_irqsave(&clockevents_lock, flags);
+
+	proxy_dev = &dev->proxy_device;
+	clockevent_set_state(proxy_dev, CLOCK_EVT_STATE_DETACHED);
+
+	real_dev = tick_setup_proxy(dev);
+	if (real_dev == NULL) {
+		raw_spin_unlock_irqrestore(&clockevents_lock, flags);
+		return -ENODEV;
+	}
+
+	list_add(&proxy_dev->list, &clockevent_devices);
+	tick_check_new_device(proxy_dev);
+	clockevents_notify_released();
+
+	raw_spin_unlock_irqrestore(&clockevents_lock, flags);
+
+	freq = (1000000000ULL * real_dev->mult) >> real_dev->shift;
+	printk(KERN_INFO "CPU%d: proxy tick device registered (%u.%02uMHz)\n",
+		 smp_processor_id(), freq / 1000000, (freq / 10000) % 100);
+
+	return 0;
+}
+
+void clockevents_unregister_proxy(struct clock_proxy_device *dev)
+{
+	unsigned long flags;
+	int ret;
+
+	clockevents_register_device(dev->real_device);
+	clockevents_switch_state(dev->real_device, CLOCK_EVT_STATE_DETACHED);
+
+	/*
+	 *  Pop the proxy device, do not give it back to the
+	 *  framework.
+	 */
+	raw_spin_lock_irqsave(&clockevents_lock, flags);
+	ret = clockevents_replace(&dev->proxy_device);
+	raw_spin_unlock_irqrestore(&clockevents_lock, flags);
+
+	if (WARN_ON(ret))
+		return;
+
+	printk(KERN_INFO "CPU%d: proxy tick device unregistered\n",
+		smp_processor_id());
+}
+
+#endif
+
 /**
  * clockevents_register_device - register a clock event device
  * @dev:	device to register
@@ -575,9 +637,13 @@ void clockevents_exchange_device(struct clock_event_device *old,
 	 */
 	if (old) {
 		module_put(old->owner);
-		clockevents_switch_state(old, CLOCK_EVT_STATE_DETACHED);
 		list_del(&old->list);
-		list_add(&old->list, &clockevents_released);
+		if (new && new->features & CLOCK_EVT_FEAT_PROXY) {
+			clockevents_switch_state(old, CLOCK_EVT_STATE_RESERVED);
+		} else {
+			clockevents_switch_state(old, CLOCK_EVT_STATE_DETACHED);
+			list_add(&old->list, &clockevents_released);
+		}
 	}
 
 	if (new) {
diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 59225b484e4e..660e77431503 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -243,7 +243,8 @@ static void tick_setup_device(struct tick_device *td,
 	} else {
 		handler = td->evtdev->event_handler;
 		next_event = td->evtdev->next_event;
-		td->evtdev->event_handler = clockevents_handle_noop;
+		if (!clockevent_state_reserved(td->evtdev))
+			td->evtdev->event_handler = clockevents_handle_noop;
 	}
 
 	td->evtdev = newdev;
@@ -318,6 +319,17 @@ static bool tick_check_preferred(struct clock_event_device *curdev,
 	       !cpumask_equal(curdev->cpumask, newdev->cpumask);
 }
 
+static bool tick_check_is_proxy(struct clock_event_device *curdev)
+{
+	if (!irqs_pipelined())
+		return false;
+
+	/*
+	 * Never replace an active proxy except when unregistering it.
+	 */
+	return curdev && curdev->features & CLOCK_EVT_FEAT_PROXY;
+}
+
 /*
  * Check whether the new device is a better fit than curdev. curdev
  * can be NULL !
@@ -325,6 +337,9 @@ static bool tick_check_preferred(struct clock_event_device *curdev,
 bool tick_check_replacement(struct clock_event_device *curdev,
 			    struct clock_event_device *newdev)
 {
+	if (tick_check_is_proxy(curdev))
+		return false;
+
 	if (!tick_check_percpu(curdev, newdev, smp_processor_id()))
 		return false;
 
@@ -345,6 +360,9 @@ void tick_check_new_device(struct clock_event_device *newdev)
 	td = &per_cpu(tick_cpu_device, cpu);
 	curdev = td->evtdev;
 
+	if (tick_check_is_proxy(curdev))
+		goto out_bc;
+
 	/* cpu local device ? */
 	if (!tick_check_percpu(curdev, newdev, cpu))
 		goto out_bc;
diff --git a/kernel/time/tick-internal.h b/kernel/time/tick-internal.h
index 299bec837004..a213888b91e2 100644
--- a/kernel/time/tick-internal.h
+++ b/kernel/time/tick-internal.h
@@ -55,6 +55,11 @@ extern int clockevents_program_event(struct clock_event_device *dev,
 				     ktime_t expires, bool force);
 extern void clockevents_handle_noop(struct clock_event_device *dev);
 extern int __clockevents_update_freq(struct clock_event_device *dev, u32 freq);
+#ifdef CONFIG_IRQ_PIPELINE
+extern struct clock_event_device *tick_setup_proxy(struct clock_proxy_device *dev);
+extern int clockevents_register_proxy(struct clock_proxy_device *dev);
+extern void clockevents_unregister_proxy(struct clock_proxy_device *dev);
+#endif
 extern ssize_t sysfs_get_uname(const char *buf, char *dst, size_t cnt);
 
 /* Broadcasting support */
diff --git a/kernel/time/tick-proxy.c b/kernel/time/tick-proxy.c
index 50ddd635f2d7..3216d06a371b 100644
--- a/kernel/time/tick-proxy.c
+++ b/kernel/time/tick-proxy.c
@@ -17,142 +17,134 @@
 #include <linux/slab.h>
 #include "tick-internal.h"
 
-struct proxy_tick_device {
-	struct clock_event_device *real_device;
-	struct clock_event_device proxy_device;
-	void (*original_event_handler)(struct clock_event_device *ced);
-};
-
 static unsigned int proxy_tick_irq;
 
-static DEFINE_PER_CPU(struct proxy_tick_device, proxy_tick_device);
+static DEFINE_MUTEX(proxy_mutex);
 
-static inline struct clock_event_device *get_real_tick_device(void)
-{
-	return raw_cpu_ptr(&proxy_tick_device)->real_device;
-}
+static DEFINE_PER_CPU(struct clock_proxy_device *, proxy_tick_device);
 
-static inline struct clock_event_device *get_proxy_tick_device(void)
+static inline struct clock_event_device *
+get_real_tick_device(struct clock_event_device *proxy_dev)
 {
-	return &raw_cpu_ptr(&proxy_tick_device)->proxy_device;
+	return container_of(proxy_dev, struct clock_proxy_device, proxy_device)->real_device;
 }
 
-static void proxy_event_handler(struct clock_event_device *real_ced)
+static void proxy_event_handler(struct clock_event_device *real_dev)
 {
-	struct proxy_tick_device *ptd = this_cpu_ptr(&proxy_tick_device);
-	struct clock_event_device *ced = &ptd->proxy_device;
+	struct clock_proxy_device *dev = __this_cpu_read(proxy_tick_device);
+	struct clock_event_device *proxy_dev = &dev->proxy_device;
 
-	ced->event_handler(ced);
+	proxy_dev->event_handler(proxy_dev);
 }
 
-static int proxy_set_oneshot(struct clock_event_device *ced)
+static int proxy_set_state_oneshot(struct clock_event_device *dev)
 {
-	struct clock_event_device *real_ced = get_real_tick_device();
+	struct clock_event_device *real_dev = get_real_tick_device(dev);
 	unsigned long flags;
 	int ret;
 
 	flags = hard_local_irq_save();
-	ret = real_ced->set_state_oneshot(real_ced);
+	ret = real_dev->set_state_oneshot(real_dev);
 	hard_local_irq_restore(flags);
 
 	return ret;
 }
 
-static int proxy_set_periodic(struct clock_event_device *ced)
+static int proxy_set_state_periodic(struct clock_event_device *dev)
 {
-	struct clock_event_device *real_ced = get_real_tick_device();
+	struct clock_event_device *real_dev = get_real_tick_device(dev);
 	unsigned long flags;
 	int ret;
 
 	flags = hard_local_irq_save();
-	ret = real_ced->set_state_periodic(real_ced);
+	ret = real_dev->set_state_periodic(real_dev);
 	hard_local_irq_restore(flags);
 
 	return ret;
 }
 
-static int proxy_set_oneshot_stopped(struct clock_event_device *ced)
+static int proxy_set_state_oneshot_stopped(struct clock_event_device *dev)
 {
-        struct clock_event_device *real_ced = get_real_tick_device();
+        struct clock_event_device *real_dev = get_real_tick_device(dev);
 	unsigned long flags;
 	int ret;
 
 	flags = hard_local_irq_save();
-	ret = real_ced->set_state_oneshot_stopped(real_ced);
+	ret = real_dev->set_state_oneshot_stopped(real_dev);
 	hard_local_irq_restore(flags);
 
 	return ret;
 }
 
-static int proxy_shutdown(struct clock_event_device *ced)
+static int proxy_set_state_shutdown(struct clock_event_device *dev)
 {
-        struct clock_event_device *real_ced = get_real_tick_device();
+        struct clock_event_device *real_dev = get_real_tick_device(dev);
 	unsigned long flags;
 	int ret;
 
 	flags = hard_local_irq_save();
-	ret = real_ced->set_state_shutdown(real_ced);
+	ret = real_dev->set_state_shutdown(real_dev);
 	hard_local_irq_restore(flags);
 
 	return ret;
 }
 
-static void proxy_suspend(struct clock_event_device *ced)
+static void proxy_suspend(struct clock_event_device *dev)
 {
-        struct clock_event_device *real_ced = get_real_tick_device();
+        struct clock_event_device *real_dev = get_real_tick_device(dev);
 	unsigned long flags;
 
 	flags = hard_local_irq_save();
-	real_ced->suspend(real_ced);
+	real_dev->suspend(real_dev);
 	hard_local_irq_restore(flags);
 }
 
-static void proxy_resume(struct clock_event_device *ced)
+static void proxy_resume(struct clock_event_device *dev)
 {
-        struct clock_event_device *real_ced = get_real_tick_device();
+        struct clock_event_device *real_dev = get_real_tick_device(dev);
 	unsigned long flags;
 
 	flags = hard_local_irq_save();
-	real_ced->resume(real_ced);
+	real_dev->resume(real_dev);
 	hard_local_irq_restore(flags);
 }
 
-static int proxy_tick_resume(struct clock_event_device *ced)
+static int proxy_tick_resume(struct clock_event_device *dev)
 {
-        struct clock_event_device *real_ced = get_real_tick_device();
+        struct clock_event_device *real_dev = get_real_tick_device(dev);
 	unsigned long flags;
 	int ret;
 
 	flags = hard_local_irq_save();
-	ret = real_ced->tick_resume(real_ced);
+	ret = real_dev->tick_resume(real_dev);
 	hard_local_irq_restore(flags);
 
 	return ret;
 }
 
 static int proxy_set_next_event(unsigned long delay,
-				struct clock_event_device *ced)
+				struct clock_event_device *dev)
 {
-	struct clock_event_device *real_ced = get_real_tick_device();
+	struct clock_event_device *real_dev = get_real_tick_device(dev);
 	unsigned long flags;
 	int ret;
 
 	flags = hard_local_irq_save();
-	ret = real_ced->set_next_event(delay, real_ced);
+	ret = real_dev->set_next_event(delay, real_dev);
 	hard_local_irq_restore(flags);
 
 	return ret;
 }
 
 static int proxy_set_next_ktime(ktime_t expires,
-				struct clock_event_device *ced)
+				struct clock_event_device *dev)
 {
-	struct clock_event_device *real_ced = get_real_tick_device();
+	struct clock_event_device *real_dev = get_real_tick_device(dev);
 	unsigned long flags;
 	int ret;
 
 	flags = hard_local_irq_save();
-	ret = real_ced->set_next_ktime(expires, real_ced);
+	ret = real_dev->set_next_ktime(expires, real_dev);
 	hard_local_irq_restore(flags);
 
 	return ret;
@@ -166,25 +158,24 @@ static irqreturn_t proxy_irq_handler(int sirq, void *dev_id)
 	 * Tricky: we may end up running this in-band IRQ handler
 	 * because tick_notify_proxy() was posted either:
 	 *
-	 * - by the co-kernel from ops->handle_event() for emulating a
-	 * regular kernel tick, if the clock chip device on the local
-	 * CPU is managed in out-of-band mode (i.e. a proxy device was
-	 * fully enabled on the receiving CPU).  In this case, the
-	 * active tick device for the regular timing core is the proxy
-	 * device, whose event handler is identical to the real tick
+	 * - from the out-of-band stage via ->handle_oob_event() for
+	 * emulating an in-band tick.  In this case, the active tick
+	 * device for the in-band timing core is the proxy device,
+	 * whose event handler is still the same than the real tick
 	 * device's.
 	 *
-	 * - or directly by the clock chip driver on the local CPU via
+	 * - directly by the clock chip driver on the local CPU via
 	 * clockevents_handle_event(), for propagating a tick to the
-	 * regular kernel nobody from the oob stage is interested on
-	 * i.e. no proxy device was registered on the receiving CPU,
-	 * which was excluded from @cpumask in the call to
-	 * tick_install_proxy(). In this case, the active tick device
-	 * for the regular timing core is a real clock event device.
+	 * in-band stage nobody from the out-of-band stage is
+	 * interested on i.e. no proxy device was registered on the
+	 * receiving CPU, which was excluded from @cpumask in the call
+	 * to tick_install_proxy(). In this case, the active tick
+	 * device for the in-band timing core is a real clock event
+	 * device.
 	 *
 	 * In both cases, we are running on the in-band stage, and we
 	 * should fire the event handler of the currently active tick
-	 * device for the regular timing core.
+	 * device for the in-band timing core.
 	 */
 	evt = raw_cpu_ptr(&tick_cpu_device)->evtdev;
 	evt->event_handler(evt);
@@ -192,148 +183,91 @@ static irqreturn_t proxy_irq_handler(int sirq, void *dev_id)
 	return IRQ_HANDLED;
 }
 
-static void register_proxy_device(void *arg) /* irqs_disabled() */
-{
-	struct clock_event_device *proxy_ced, *real_ced;
-	const struct proxy_tick_ops *ops = arg;
-	struct proxy_tick_device *ptd;
-	int cpu = smp_processor_id();
+#define interpose_proxy_handler(__dev, __real, __h)		\
+	do {							\
+		if ((__dev)->__h == NULL) {			\
+			if ((__real)->__h)			\
+				(__dev)->__h = proxy_ ## __h;	\
+		}						\
+	} while (0)
 
-	real_ced = raw_cpu_ptr(&tick_cpu_device)->evtdev;
-	proxy_ced = get_proxy_tick_device();
-
-	/* Setup the percpu proxy device slots. */
-	ptd = this_cpu_ptr(&proxy_tick_device);
-	ptd->original_event_handler = real_ced->event_handler;
-	ptd->real_device = real_ced;
+/*
+ * Setup a proxy which is about to override the tick device on the
+ * current CPU. Called with clockevents_lock held so that the tick
+ * device does not change under our feet.
+ */
+struct clock_event_device *tick_setup_proxy(struct clock_proxy_device *dev)
+{
+	struct clock_event_device *proxy_dev, *real_dev;
+
+	real_dev = raw_cpu_ptr(&tick_cpu_device)->evtdev;
+	if ((real_dev->features &
+			(CLOCK_EVT_FEAT_PIPELINE|CLOCK_EVT_FEAT_ONESHOT))
+		!= (CLOCK_EVT_FEAT_PIPELINE|CLOCK_EVT_FEAT_ONESHOT)) {
+		WARN(1, "cannot use clockevent device %s in proxy mode!",
+			real_dev->name);
+		return NULL;
+	}
 
 	/*
-	 * Install a proxy clock event device on this CPU.  The proxy
-	 * device has the same characteristics as the real one
-	 * (esp. CLOCK_EVT_FEAT_C3STOP if present!), except the
-	 * broadcast capability SIRQs don't support, so add
-	 * CLOCK_EVT_FEAT_PERCPU.
+	 * The assumption is that clockevents_register_proxy() cannot
+	 * fail afterwards, so this is ok to advertise the new proxy
+	 * in proxy_tick_device.
 	 */
-	proxy_ced->features = real_ced->features | CLOCK_EVT_FEAT_PERCPU;
-	proxy_ced->name = "proxy";
-	proxy_ced->irq = real_ced->irq;
-	proxy_ced->cpumask = cpumask_of(cpu);
-	proxy_ced->rating = real_ced->rating;
-	proxy_ced->mult = real_ced->mult;
-	proxy_ced->shift = real_ced->shift;
-	proxy_ced->max_delta_ticks = real_ced->max_delta_ticks;
-	proxy_ced->min_delta_ticks = real_ced->min_delta_ticks;
-	proxy_ced->max_delta_ns = real_ced->max_delta_ns;
-	proxy_ced->min_delta_ns = real_ced->min_delta_ns;
-	/*
-	 * Interpose default handlers which are safe wrt preemption by
-	 * the oob stage.
-	 */
-	proxy_ced->set_state_oneshot = NULL;
-	if (real_ced->set_state_oneshot)
-		proxy_ced->set_state_oneshot = proxy_set_oneshot;
-	proxy_ced->set_state_periodic = NULL;
-	if (real_ced->set_state_periodic)
-		proxy_ced->set_state_periodic = proxy_set_periodic;
-	proxy_ced->set_state_oneshot_stopped = NULL;
-	if (real_ced->set_state_oneshot_stopped)
-		proxy_ced->set_state_oneshot_stopped = proxy_set_oneshot_stopped;
-	proxy_ced->suspend = NULL;
-	if (real_ced->suspend)
-		proxy_ced->suspend = proxy_suspend;
-	proxy_ced->resume = NULL;
-	if (real_ced->resume)
-		proxy_ced->resume = proxy_resume;
-	proxy_ced->tick_resume = NULL;
-	if (real_ced->tick_resume)
-		proxy_ced->tick_resume = proxy_tick_resume;
-	proxy_ced->set_next_event = NULL;
-	if (real_ced->set_next_event)
-		proxy_ced->set_next_event = proxy_set_next_event;
-	proxy_ced->set_next_ktime = NULL;
-	if (real_ced->set_next_ktime)
-		proxy_ced->set_next_ktime = proxy_set_next_ktime;
-	proxy_ced->set_state_shutdown = NULL;
-	if (real_ced->set_state_shutdown)
-		proxy_ced->set_state_shutdown = proxy_shutdown;
+	__this_cpu_write(proxy_tick_device, dev);
+	dev->handle_inband_event = real_dev->event_handler;
+	dev->real_device = real_dev;
+
 	/*
-	 * ops->register_device() must fill in the
-	 * set_next_event/set_next_ktime() handler and the rating of
-	 * the proxy device, before proceeding to its registration on
-	 * the clockevent framework. The features bits may be altered
-	 * for the purpose of adding CLOCK_EVT_FEAT_KTIME if
-	 * set_next_ktime() is preferred over set_next_event().
+	 * Inherit the feature bits since the proxy device has the
+	 * same capabilities than the real one we are overriding
+	 * (including CLOCK_EVT_FEAT_C3STOP if present).
 	 */
-	ops->register_device(proxy_ced, real_ced);
+	proxy_dev = &dev->proxy_device;
+	proxy_dev->features |= real_dev->features |
+		CLOCK_EVT_FEAT_PERCPU | CLOCK_EVT_FEAT_PROXY;
+	proxy_dev->name = "proxy";
+	proxy_dev->irq = real_dev->irq;
+	proxy_dev->cpumask = cpumask_of(smp_processor_id());
+	proxy_dev->rating = real_dev->rating + 1;
+	proxy_dev->mult = real_dev->mult;
+	proxy_dev->shift = real_dev->shift;
+	proxy_dev->max_delta_ticks = real_dev->max_delta_ticks;
+	proxy_dev->min_delta_ticks = real_dev->min_delta_ticks;
+	proxy_dev->max_delta_ns = real_dev->max_delta_ns;
+	proxy_dev->min_delta_ns = real_dev->min_delta_ns;
 
 	/*
-	 * If the proxy replaced the current (real) tick device, we
-	 * have two issues to handle:
-	 *
-	 * 1. the event handler of the real device was nop'ed during
-	 * the transition.  We need to restore a valid handler for
-	 * routing ticks to the regular timer core as if they came
-	 * from the proxy device, until the timer IRQ is switched to
-	 * out-of-band mode. Once this happens, ticks are routed to
-	 * the overlay handler instead.
-	 *
-	 * 2. the clock event layer decides to transition the device
-	 * overlaid by the proxy from detached->shutdown, which makes
-	 * it unusable anew (see clockevent_replace) when the proxy is
-	 * unbound.
-	 *
-	 * This might happen as follows:
-	 *
-	 * - proxy is registered on CPUx from ->register_device()
-	 *   - real device is released, set to detached state
-	 *     - tick notifier runs on released devices
-	 *       - real device is picked for bc, old bc is released
-	 *         - real device is installed, set to shutdown state
-	 * ...
-	 *
-	 * 1. now that we have the real device switched to a shutdown
-	 *    state, the clockchip handler may have turned off the
-	 *    hardware.
-	 *
-	 * 2. if/when the proxy is unregistered from CPUx, the real
-	 *    device is not considered as it is not in detached state
-	 *    (clockevent_replace), so the dummy device is picked
-	 *    instead.
-	 *
-	 * In both cases, the CPU gets no tick anymore. What we need
-	 * to do to fix the situation is two-fold:
-	 *
-	 * - switch the real device back to detached state.
-	 *
-	 * - trigger a tick immediately on the proxy device, which
-	 *   causes the real device's set_next_event() handler to be
-	 *   called, turning it on again before scheduling the event.
+	 * Interpose default handlers which are safe wrt preemption by
+	 * the out-of-band stage.
 	 */
-	if (raw_cpu_ptr(&tick_cpu_device)->evtdev == proxy_ced) {
-		real_ced->event_handler = proxy_event_handler;
-		clockevents_switch_state(real_ced, CLOCK_EVT_STATE_DETACHED);
-		if (clockevent_state_oneshot(proxy_ced))
-			clockevents_program_event(proxy_ced, ktime_get(), true);
-	}
+	interpose_proxy_handler(proxy_dev, real_dev, set_state_oneshot);
+	interpose_proxy_handler(proxy_dev, real_dev, set_state_oneshot_stopped);
+	interpose_proxy_handler(proxy_dev, real_dev, set_state_periodic);
+	interpose_proxy_handler(proxy_dev, real_dev, set_state_shutdown);
+	interpose_proxy_handler(proxy_dev, real_dev, suspend);
+	interpose_proxy_handler(proxy_dev, real_dev, resume);
+	interpose_proxy_handler(proxy_dev, real_dev, tick_resume);
+	interpose_proxy_handler(proxy_dev, real_dev, set_next_event);
+	interpose_proxy_handler(proxy_dev, real_dev, set_next_ktime);
+
+	return real_dev;
 }
 
 static int enable_oob_timer(void *arg) /* hard_irqs_disabled() */
 {
-	const struct proxy_tick_ops *ops = arg;
-	struct clock_event_device *real_ced;
+	struct clock_proxy_device *dev = __this_cpu_read(proxy_tick_device);
+	struct clock_event_device *real_dev;
 
 	/*
-	 * Install the overlay handler on this CPU's real clock
+	 * Install the out-of-band handler on this CPU's real clock
 	 * device, then turn on out-of-band mode for the associated
 	 * IRQ (duplicates are silently ignored if the IRQ is common
 	 * to multiple CPUs).
 	 */
-	real_ced = get_real_tick_device();
-	if (WARN_ON(real_ced == NULL))
-		return -ENODEV;
-
-	real_ced->event_handler = ops->handle_event;
-	real_ced->features |= CLOCK_EVT_FEAT_OOB;
+	real_dev = dev->real_device;
+	real_dev->event_handler = dev->handle_oob_event;
+	real_dev->features |= CLOCK_EVT_FEAT_OOB;
 	barrier();
 
 	/*
@@ -341,40 +275,46 @@ static int enable_oob_timer(void *arg) /* hard_irqs_disabled() */
 	 * mutable, so that is fine to invoke this routine with hard
 	 * IRQs off.
 	 */
-	irq_switch_oob(real_ced->irq, true);
+	irq_switch_oob(real_dev->irq, true);
 
 	return 0;
 }
 
-int tick_install_proxy(struct proxy_tick_ops *ops,
-		       const struct cpumask *cpumask)
+struct proxy_install_arg {
+	struct clock_proxy_device *(*get_device)(void);
+	int result;
+};
+
+static void register_proxy_device(void *arg) /* irqs_disabled() */
 {
-	struct clock_event_device *real_ced;
-	int ret, cpu, sirq;
-
-	cpus_read_lock();
-
-	for_each_cpu(cpu, cpumask) {
-		real_ced = per_cpu(tick_cpu_device, cpu).evtdev;
-		ret = -EINVAL;
-		if (real_ced == NULL) {
-			WARN(1, "no clockevent device on CPU%d!", cpu);
-			goto fail;
-		}
-		if ((real_ced->features &
-		     (CLOCK_EVT_FEAT_PIPELINE|CLOCK_EVT_FEAT_ONESHOT))
-		    != (CLOCK_EVT_FEAT_PIPELINE|CLOCK_EVT_FEAT_ONESHOT)) {
-			WARN(1, "cannot use clockevent device %s in pipelined mode!",
-			     real_ced->name);
-			goto fail;
-		}
+	struct proxy_install_arg *req = arg;
+	struct clock_proxy_device *dev = req->get_device();
+	int ret;
+
+	ret = clockevents_register_proxy(dev);
+	if (ret) {
+		if (!req->result)
+			req->result = ret;
+	} else {
+		dev->real_device->event_handler = proxy_event_handler;
 	}
+}
+
+int tick_install_proxy(struct clock_proxy_device *(*get_percpu_device)(void),
+		       const struct cpumask *cpumask)
+{
+	struct proxy_install_arg arg;
+	int ret, sirq;
+
+	mutex_lock(&proxy_mutex);
+
+	ret = -EAGAIN;
+	if (proxy_tick_irq)
+		goto out;
 
 	sirq = irq_create_direct_mapping(synthetic_irq_domain);
-	if (WARN_ON(sirq == 0)) {
-		ret = -EAGAIN;
-		goto fail;
-	}
+	if (WARN_ON(sirq == 0))
+		goto out;
 
 	ret = __request_percpu_irq(sirq, proxy_irq_handler,
 				   IRQF_NO_THREAD, /* no IRQF_TIMER here. */
@@ -382,7 +322,7 @@ int tick_install_proxy(struct proxy_tick_ops *ops,
 				   &proxy_tick_device);
 	if (WARN_ON(ret)) {
 		irq_dispose_mapping(sirq);
-		goto fail;
+		goto out;
 	}
 
 	proxy_tick_irq = sirq;
@@ -390,148 +330,120 @@ int tick_install_proxy(struct proxy_tick_ops *ops,
 
 	/*
 	 * Install a proxy tick device on each CPU. As the proxy
-	 * device is picked, the previous (real) tick device is shut
-	 * down by the clockevent core.  Immediately after, the proxy
-	 * device starts controlling the real device under the hood to
-	 * carry out timing requests from the co-kernel.  From that
-	 * point, the co-kernel is also in charge of emulating host
-	 * ticks, as requested by the host kernel through calls to the
-	 * ->set_next_event()/set_next_ktime() handler of the proxy
-	 * device.
+	 * device is picked, the previous (real) tick device is
+	 * switched to reserved state by the clockevent core.
+	 * Immediately after, the proxy device starts controlling the
+	 * real device under the hood to carry out the timing requests
+	 * it receives.
 	 *
 	 * For a short period of time, after the proxy device is
 	 * installed, and until the real device IRQ is switched to
-	 * pipelined mode, the flow is as follows:
+	 * out-of-band mode, the flow is as follows:
 	 *
-	 *    [kernel timing request]
+	 *    [inband timing request]
 	 *        proxy_dev->set_next_event(proxy_dev)
-	 *            overlay_program_event(proxy_dev)
-	 *                original_clockevent_set_next_event(real_dev)
+	 *            oob_program_event(proxy_dev)
+	 *                real_dev->set_next_event(real_dev)
 	 *        ...
 	 *        <tick event>
-	 *        original_timer_handler() [ROOT STAGE]
+	 *        original_timer_handler() [in-band stage]
 	 *            clockevents_handle_event(real_dev)
 	 *               proxy_event_handler(real_dev)
-	 *                  original_clockevent_handler(proxy_dev)
+	 *                  handle_inband_event(proxy_dev)
 	 *
-	 * Eventually, we substitute the original clock event handler
-	 * with the overlay handler for the real clock event device,
-	 * then turn on out-of-band mode for the timer IRQ associated
-	 * to the latter. The last two steps are performed over a
-	 * stop_machine() context, so that no tick can race with this
-	 * code while we swap handlers.
+	 * Eventually, we substitute the original (in-band) clock
+	 * event handler with the out-of-band handler for the real
+	 * clock event device, then turn on out-of-band mode for the
+	 * timer IRQ associated to the latter. These two steps are
+	 * performed over a stop_machine() context, so that no tick
+	 * can race with this code while we swap handlers.
 	 *
 	 * Once the hand over is complete, the flow is as follows:
 	 *
-	 *    [kernel timing request]
+	 *    [inband timing request]
 	 *        proxy_dev->set_next_event(proxy_dev)
-	 *            overlay_program_event(proxy_dev)
-	 *                original_clockevent_set_next_event(real_dev)
+	 *            oob_program_event(proxy_dev)
+	 *                real_dev->set_next_event(real_dev)
 	 *        ...
 	 *        <tick event>
-	 *        original_timer_handler() [HEAD STAGE]
+	 *        handle_inband_event() [out-of-band stage]
 	 *            clockevents_handle_event(real_dev)
-	 *                overlay_handle_event(proxy_dev)
-	 *                    ...(host tick emulation)...
-	 *                    tick_kick_proxy()
+	 *                handle_oob_event(proxy_dev)
+	 *                    ...(inband tick emulation)...
+	 *                         tick_notify_proxy()
 	 *        ...
-	 *        proxy_irq_handler(proxy_dev) [ROOT stage]
+	 *        proxy_irq_handler(proxy_dev) [in-band stage]
 	 *            clockevents_handle_event(proxy_dev)
-	 *                original_clockevent_handler(proxy_dev)
+	 *                handle_inband_event(proxy_dev)
 	 */
-
-	on_each_cpu_mask(cpumask, register_proxy_device, ops, true);
-
-	cpus_read_unlock();
+	arg.get_device = get_percpu_device;
+	arg.result = 0;
+	on_each_cpu_mask(cpumask, register_proxy_device, &arg, true);
+	if (arg.result) {
+		tick_uninstall_proxy(cpumask);
+		return arg.result;
+	}
 
 	/*
-	 * Start ticking from the oob interrupt stage via out-of-band
-	 * events.
+	 * Start ticking from the out-of-band interrupt stage upon
+	 * receipt of out-of-band timer events.
 	 */
-	stop_machine(enable_oob_timer, ops, cpumask);
-
-	return 0;
-fail:
-	cpus_read_unlock();
+	stop_machine(enable_oob_timer, NULL, cpumask);
+out:
+	mutex_unlock(&proxy_mutex);
 
 	return ret;
 }
 EXPORT_SYMBOL_GPL(tick_install_proxy);
 
-static void unregister_proxy_device(void *arg) /* irqs_disabled() */
-{
-	struct clock_event_device *real_ced, *proxy_ced;
-	const struct proxy_tick_ops *ops = arg;
-	struct proxy_tick_device *ptd;
-
-	ptd = raw_cpu_ptr(&proxy_tick_device);
-	real_ced = ptd->real_device;
-	ptd->real_device = NULL;
-
-	if (ops->unregister_device) {
-		proxy_ced = &ptd->proxy_device;
-		ops->unregister_device(proxy_ced, real_ced);
-	}
-}
-
 static int disable_oob_timer(void *arg) /* hard_irqs_disabled() */
 {
-	struct clock_event_device *real_ced, *proxy_ced;
-	struct proxy_tick_device *ptd;
-
-	ptd = raw_cpu_ptr(&proxy_tick_device);
-	real_ced = ptd->real_device;
-	if (real_ced == NULL)
-		return 0;
+	struct clock_event_device *real_dev;
+	struct clock_proxy_device *dev;
 
-	real_ced->event_handler = ptd->original_event_handler;
-	real_ced->features &= ~CLOCK_EVT_FEAT_OOB;
+	dev = __this_cpu_read(proxy_tick_device);
+	real_dev = dev->real_device;
+	real_dev->event_handler = dev->handle_inband_event;
+	real_dev->features &= ~CLOCK_EVT_FEAT_OOB;
 	barrier();
-	proxy_ced = get_proxy_tick_device();
-	proxy_ced->set_next_event = real_ced->set_next_event;
-	irq_switch_oob(real_ced->irq, false);
+
+	dev->proxy_device.set_next_event = real_dev->set_next_event;
+	irq_switch_oob(real_dev->irq, false);
 
 	return 0;
 }
 
-void tick_uninstall_proxy(struct proxy_tick_ops *ops,
-			  const struct cpumask *cpumask)
+static void unregister_proxy_device(void *arg) /* irqs_disabled() */
 {
-	struct clock_event_device *proxy_ced;
-	struct proxy_tick_device *ptd;
-	int cpu;
-
-	/*
-	 * Undo all we did in tick_install_proxy(), handing over
-	 * control of the tick device back to the host kernel, then
-	 * removing the proxy device on each CPU.
-	 */
-	stop_machine(disable_oob_timer, NULL, cpu_online_mask);
+	struct clock_proxy_device *dev = __this_cpu_read(proxy_tick_device);
 
-	cpus_read_lock();
-
-	for_each_cpu(cpu, cpumask) {
-		ptd = &per_cpu(proxy_tick_device, cpu);
-		proxy_ced = &ptd->proxy_device;
-		if (!clockevent_state_detached(proxy_ced))
-			clockevents_unbind_device(proxy_ced, cpu);
+	if (dev) {
+		clockevents_unregister_proxy(dev);
+		__this_cpu_write(proxy_tick_device, NULL);
 	}
+}
 
-	on_each_cpu_mask(cpumask, unregister_proxy_device, ops, true);
-
-	cpus_read_unlock();
-
+void tick_uninstall_proxy(const struct cpumask *cpumask)
+{
 	/*
-	 * Remove the synthetic IRQ we used for emulating ticks from
-	 * the proxy device.
+	 * Undo all we did in tick_install_proxy(), handing over
+	 * control of the tick device back to the inband code.
 	 */
+	mutex_lock(&proxy_mutex);
+	stop_machine(disable_oob_timer, NULL, cpu_online_mask);
+	on_each_cpu_mask(cpumask, unregister_proxy_device, NULL, true);
 	free_percpu_irq(proxy_tick_irq, &proxy_tick_device);
 	irq_dispose_mapping(proxy_tick_irq);
+	proxy_tick_irq = 0;
+	mutex_unlock(&proxy_mutex);
 }
 EXPORT_SYMBOL_GPL(tick_uninstall_proxy);
 
 void tick_notify_proxy(void)
 {
+	if (WARN_ON_ONCE(irq_pipeline_debug() && running_inband()))
+		return;
+
 	/*
 	 * Schedule a tick on the proxy device to occur from the
 	 * in-band stage, which will trigger proxy_irq_handler() at
-- 
2.16.4

