From 83c75a1ffaf9a56d4eb12173c0b0c1e6ca33ae6c Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Sat, 27 Jul 2019 19:15:21 +0200
Subject: [PATCH] arm64: dovetail: do local TLB invalidation unlocked on mm
 switch

TLB invalidation is a potentially long operation on some hardware when
caches are under pressure. We may safely run such invalidation for the
current CPU unlocked before leaving the mm context switching code,
reducing the spinning time of other CPUs proportionally.

Signed-off-by: Philippe Gerum <rpm@xenomai.org>
---
 arch/arm64/mm/context.c | 7 +++++--
 1 file changed, 5 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/mm/context.c b/arch/arm64/mm/context.c
index 8e11bfdd76cf..addb7d14e589 100644
--- a/arch/arm64/mm/context.c
+++ b/arch/arm64/mm/context.c
@@ -184,6 +184,7 @@ void check_and_switch_context(struct mm_struct *mm, unsigned int cpu)
 {
 	unsigned long flags;
 	u64 asid, old_active_asid;
+	bool need_flush;
 
 	WARN_ON_ONCE(dovetail_debug() && !hard_irqs_disabled());
 
@@ -221,12 +222,14 @@ void check_and_switch_context(struct mm_struct *mm, unsigned int cpu)
 		atomic64_set(&mm->context.id, asid);
 	}
 
-	if (cpumask_test_and_clear_cpu(cpu, &tlb_flush_pending))
-		local_flush_tlb_all();
+	need_flush = cpumask_test_and_clear_cpu(cpu, &tlb_flush_pending);
 
 	atomic64_set(&per_cpu(active_asids, cpu), asid);
 	raw_spin_unlock_irqrestore(&cpu_asid_lock, flags);
 
+	if (need_flush)
+		local_flush_tlb_all();
+
 switch_mm_fastpath:
 
 	arm64_apply_bp_hardening();
-- 
2.16.4

