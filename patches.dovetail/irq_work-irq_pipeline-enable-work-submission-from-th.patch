From 6639419636dc39923c1f966a84b20d7a38f8bfb1 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Sun, 19 Mar 2017 12:18:51 +0100
Subject: [PATCH] irq_work: irq_pipeline: enable work submission from the oob
 stage

The lockless algorithm of irq_work_queue() is NMI-safe by design,
which means that calling it from the oob stage should be possible,
provided that the code raising the IRQ work interrupt allows it too.

Instead of fixing up multiple implementations of arch_irq_work_raise()
to support this, and with the interrupt pipeline in place, we can use
a pseudo-interrupt from the synthetic domain for scheduling a work on
the local CPU, instead of a real interrupt.

Doing so is cheaper when pipelining IRQs since this entirely bypasses
the hw logic; this also guarantees immediate delivery after the
interrupts are enabled back for the in-band stage regardless of the
architecture, including for those which do not provide for hw-assisted
IRQ_WORK support (which would otherwise postpone the handling until
the next timer tick comes in).

The net result makes irq_work_queue() available for scheduling work
requests in the in-band stage from the oob stage.
---
 kernel/irq/pipeline.c | 37 +++++++++++++++++++++++++++++++++++++
 kernel/irq_work.c     |  9 +++++++--
 2 files changed, 44 insertions(+), 2 deletions(-)

diff --git a/kernel/irq/pipeline.c b/kernel/irq/pipeline.c
index 293d897930f..efc5596c6ab 100644
--- a/kernel/irq/pipeline.c
+++ b/kernel/irq/pipeline.c
@@ -9,6 +9,7 @@
 #include <linux/irq.h>
 #include <linux/irqdomain.h>
 #include <linux/irq_pipeline.h>
+#include <linux/irq_work.h>
 #include <trace/events/irq.h>
 #include "internals.h"
 
@@ -1332,6 +1333,39 @@ bool irq_cpuidle_enter(struct cpuidle_device *dev,
 	return !stage_irqs_pending(p) && irq_cpuidle_control(dev, state);
 }
 
+static unsigned int inband_work_sirq;
+
+static irqreturn_t inband_work_interrupt(int sirq, void *dev_id)
+{
+	irq_work_run();
+
+	return IRQ_HANDLED;
+}
+
+static struct irqaction inband_work = {
+	.handler = inband_work_interrupt,
+	.name = "in-band work",
+	.flags = IRQF_NO_THREAD,
+};
+
+void irq_local_work_raise(void)
+{
+	unsigned long flags;
+
+	/*
+	 * irq_work_queue() may be called from the in-band stage too
+	 * in case we want to delay a work until the hard irqs are on
+	 * again, so we may only sync the in-band log when unstalled,
+	 * with hard irqs on.
+	 */
+	flags = hard_local_irq_save();
+	irq_post_inband(inband_work_sirq);
+	if (running_inband() &&
+	    !hard_irqs_disabled_flags(flags) && !irqs_disabled())
+		sync_current_stage();
+	hard_local_irq_restore(flags);
+}
+
 #ifdef CONFIG_DEBUG_IRQ_PIPELINE
 
 notrace void check_inband_stage(void)
@@ -1435,6 +1469,9 @@ void __init irq_pipeline_init(void)
 	synthetic_irq_domain = irq_domain_add_nomap(NULL, ~0,
 						    &sirq_domain_ops,
 						    NULL);
+	inband_work_sirq = irq_create_direct_mapping(synthetic_irq_domain);
+	setup_percpu_irq(inband_work_sirq, &inband_work);
+
 	/*
 	 * We are running on the boot CPU, hw interrupts are off, and
 	 * secondary CPUs are still lost in space. Now we may run
diff --git a/kernel/irq_work.c b/kernel/irq_work.c
index 6b7cdf17ccf..618bcaefc65 100644
--- a/kernel/irq_work.c
+++ b/kernel/irq_work.c
@@ -56,6 +56,11 @@ void __weak arch_irq_work_raise(void)
 	 */
 }
 
+void __weak irq_local_work_raise(void)
+{
+	arch_irq_work_raise();
+}
+
 /*
  * Enqueue the irq_work @work on @cpu unless it's already pending
  * somewhere.
@@ -100,10 +105,10 @@ bool irq_work_queue(struct irq_work *work)
 	if (work->flags & IRQ_WORK_LAZY) {
 		if (llist_add(&work->llnode, this_cpu_ptr(&lazy_list)) &&
 		    tick_nohz_tick_stopped())
-			arch_irq_work_raise();
+			irq_local_work_raise();
 	} else {
 		if (llist_add(&work->llnode, this_cpu_ptr(&raised_list)))
-			arch_irq_work_raise();
+			irq_local_work_raise();
 	}
 
 	preempt_enable();
-- 
2.16.4

