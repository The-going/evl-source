From b5461c8dbc789a3d6e0339dadf9ef4a388552854 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Sun, 19 Mar 2017 12:18:51 +0100
Subject: [PATCH] irq_work: irq_pipeline: enable work submission from the oob
 stage

The lockless algorithm of irq_work_queue() is NMI-safe by design,
which means that calling it from the oob stage should be possible,
provided that the code raising the IRQ work interrupt allows it too.

Instead of fixing up multiple implementations of arch_irq_work_raise()
to support this, and with the interrupt pipeline in place, we can use
a pseudo-interrupt from the synthetic domain for scheduling a work on
the local CPU, instead of a real interrupt.

Doing so is cheaper when pipelining IRQs since this entirely bypasses
the hw logic; this also guarantees immediate delivery after the
interrupts are enabled back for the in-band stage regardless of the
architecture, including for those which do not provide for hw-assisted
IRQ_WORK support (which would otherwise postpone the handling until
the next timer tick comes in).

The net result makes irq_work_queue() available for scheduling work
requests in the in-band stage from the oob stage.
---
 kernel/irq/pipeline.c | 37 +++++++++++++++++++++++++++++++++++++
 kernel/irq_work.c     |  9 +++++++--
 2 files changed, 44 insertions(+), 2 deletions(-)

diff --git a/kernel/irq/pipeline.c b/kernel/irq/pipeline.c
index 293d897930f5..efc5596c6ab0 100644
--- a/kernel/irq/pipeline.c
+++ b/kernel/irq/pipeline.c
@@ -9,6 +9,7 @@
 #include <linux/irq.h>
 #include <linux/irqdomain.h>
 #include <linux/irq_pipeline.h>
+#include <linux/irq_work.h>
 #include <trace/events/irq.h>
 #include "internals.h"
 
@@ -1332,6 +1333,39 @@ bool irq_cpuidle_enter(struct cpuidle_device *dev,
 	return !stage_irqs_pending(p) && irq_cpuidle_control(dev, state);
 }
 
+static unsigned int inband_work_sirq;
+
+static irqreturn_t inband_work_interrupt(int sirq, void *dev_id)
+{
+	irq_work_run();
+
+	return IRQ_HANDLED;
+}
+
+static struct irqaction inband_work = {
+	.handler = inband_work_interrupt,
+	.name = "in-band work",
+	.flags = IRQF_NO_THREAD,
+};
+
+void irq_local_work_raise(void)
+{
+	unsigned long flags;
+
+	/*
+	 * irq_work_queue() may be called from the in-band stage too
+	 * in case we want to delay a work until the hard irqs are on
+	 * again, so we may only sync the in-band log when unstalled,
+	 * with hard irqs on.
+	 */
+	flags = hard_local_irq_save();
+	irq_post_inband(inband_work_sirq);
+	if (running_inband() &&
+	    !hard_irqs_disabled_flags(flags) && !irqs_disabled())
+		sync_current_stage();
+	hard_local_irq_restore(flags);
+}
+
 #ifdef CONFIG_DEBUG_IRQ_PIPELINE
 
 notrace void check_inband_stage(void)
@@ -1435,6 +1469,9 @@ void __init irq_pipeline_init(void)
 	synthetic_irq_domain = irq_domain_add_nomap(NULL, ~0,
 						    &sirq_domain_ops,
 						    NULL);
+	inband_work_sirq = irq_create_direct_mapping(synthetic_irq_domain);
+	setup_percpu_irq(inband_work_sirq, &inband_work);
+
 	/*
 	 * We are running on the boot CPU, hw interrupts are off, and
 	 * secondary CPUs are still lost in space. Now we may run
diff --git a/kernel/irq_work.c b/kernel/irq_work.c
index 73288914ed5e..6a403bbde35e 100644
--- a/kernel/irq_work.c
+++ b/kernel/irq_work.c
@@ -56,6 +56,11 @@ void __weak arch_irq_work_raise(void)
 	 */
 }
 
+void __weak irq_local_work_raise(void)
+{
+	arch_irq_work_raise();
+}
+
 /* Enqueue on current CPU, work must already be claimed and preempt disabled */
 static void __irq_work_queue_local(struct irq_work *work)
 {
@@ -63,10 +68,10 @@ static void __irq_work_queue_local(struct irq_work *work)
 	if (work->flags & IRQ_WORK_LAZY) {
 		if (llist_add(&work->llnode, this_cpu_ptr(&lazy_list)) &&
 		    tick_nohz_tick_stopped())
-			arch_irq_work_raise();
+			irq_local_work_raise();
 	} else {
 		if (llist_add(&work->llnode, this_cpu_ptr(&raised_list)))
-			arch_irq_work_raise();
+			irq_local_work_raise();
 	}
 }
 
-- 
2.16.4

