From c94e0379be8091a22d258fedb6d834fff9ca831d Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Wed, 24 Apr 2019 16:40:19 +0200
Subject: [PATCH] x86: irq_pipeline: add IRQ pipeline core

Signed-off-by: Philippe Gerum <rpm@xenomai.org>
---
 arch/x86/Kconfig                      |   1 +
 arch/x86/entry/common.c               |  34 ++++-
 arch/x86/entry/entry_64.S             |  65 +++++++-
 arch/x86/entry/thunk_64.S             |   1 +
 arch/x86/include/asm/apic.h           |   8 +-
 arch/x86/include/asm/hw_irq.h         |   5 +
 arch/x86/include/asm/i8259.h          |   2 +-
 arch/x86/include/asm/irq_pipeline.h   | 139 +++++++++++++++++
 arch/x86/include/asm/irq_vectors.h    |  11 +-
 arch/x86/include/asm/irqflags.h       |  77 ++++++----
 arch/x86/include/asm/mmu_context.h    |   7 +
 arch/x86/include/asm/processor.h      |   3 +
 arch/x86/include/asm/tlbflush.h       |  29 ++--
 arch/x86/kernel/Makefile              |   1 +
 arch/x86/kernel/apic/apic.c           |  22 ++-
 arch/x86/kernel/apic/apic_flat_64.c   |   4 +-
 arch/x86/kernel/apic/apic_numachip.c  |  10 +-
 arch/x86/kernel/apic/io_apic.c        |  11 +-
 arch/x86/kernel/apic/ipi.c            |  32 ++--
 arch/x86/kernel/apic/msi.c            |   8 +-
 arch/x86/kernel/apic/vector.c         |  29 ++--
 arch/x86/kernel/apic/x2apic_cluster.c |   4 +-
 arch/x86/kernel/apic/x2apic_phys.c    |   4 +-
 arch/x86/kernel/cpu/mce/core.c        |   4 +-
 arch/x86/kernel/dumpstack.c           |   8 +-
 arch/x86/kernel/i8259.c               |   3 +-
 arch/x86/kernel/idt.c                 |   6 +
 arch/x86/kernel/irq.c                 |   2 +-
 arch/x86/kernel/irq_pipeline.c        | 273 ++++++++++++++++++++++++++++++++++
 arch/x86/kernel/process.c             |  18 ++-
 arch/x86/kernel/smp.c                 |   4 +-
 arch/x86/kernel/smpboot.c             |   9 +-
 arch/x86/kernel/traps.c               | 113 +++++++++++---
 arch/x86/kernel/tsc.c                 |  17 ++-
 arch/x86/kernel/tsc_sync.c            |   4 +
 arch/x86/mm/fault.c                   |  12 +-
 36 files changed, 829 insertions(+), 151 deletions(-)
 create mode 100644 arch/x86/include/asm/irq_pipeline.h
 create mode 100644 arch/x86/kernel/irq_pipeline.c

diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 5e8949953660..56841d3d2b72 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -27,6 +27,7 @@ config X86_64
 	select ARCH_SUPPORTS_INT128 if CC_HAS_INT128
 	select ARCH_USE_CMPXCHG_LOCKREF
 	select HAVE_ARCH_SOFT_DIRTY
+	select HAVE_IRQ_PIPELINE
 	select MODULES_USE_ELF_RELA
 	select NEED_DMA_MAP_STATE
 	select SWIOTLB
diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c
index 9747876980b5..edc94f366037 100644
--- a/arch/x86/entry/common.c
+++ b/arch/x86/entry/common.c
@@ -42,6 +42,8 @@
 /* Called on entry from user mode with IRQs off. */
 __visible inline void enter_from_user_mode(void)
 {
+	if (irqs_pipelined() && (!running_inband() || irqs_disabled()))
+		return;
 	CT_WARN_ON(ct_state() != CONTEXT_USER);
 	user_exit_irqoff();
 }
@@ -49,6 +51,22 @@ __visible inline void enter_from_user_mode(void)
 static inline void enter_from_user_mode(void) {}
 #endif
 
+#ifdef CONFIG_IRQ_PIPELINE
+#define disable_local_irqs()	do {	\
+	hard_local_irq_disable();	\
+	trace_hardirqs_off();		\
+} while (0)
+#define enable_local_irqs()	do {	\
+	trace_hardirqs_on();		\
+	hard_local_irq_enable();	\
+} while (0)
+#define check_irqs_disabled()	hard_irqs_disabled()
+#else
+#define disable_local_irqs()	local_irq_disable()
+#define enable_local_irqs()	local_irq_enable()
+#define check_irqs_disabled()	irqs_disabled()
+#endif
+
 static void do_audit_syscall_entry(struct pt_regs *regs, u32 arch)
 {
 #ifdef CONFIG_X86_64
@@ -144,7 +162,7 @@ static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)
 	 */
 	while (true) {
 		/* We have work to do. */
-		local_irq_enable();
+		enable_local_irqs();
 
 		if (cached_flags & _TIF_NEED_RESCHED)
 			schedule();
@@ -169,7 +187,7 @@ static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)
 			fire_user_return_notifiers();
 
 		/* Disable IRQs and retry */
-		local_irq_disable();
+		disable_local_irqs();
 
 		cached_flags = READ_ONCE(current_thread_info()->flags);
 
@@ -263,7 +281,7 @@ __visible inline void syscall_return_slowpath(struct pt_regs *regs)
 
 	if (IS_ENABLED(CONFIG_PROVE_LOCKING) &&
 	    WARN(irqs_disabled(), "syscall %ld left IRQs disabled", regs->orig_ax))
-		local_irq_enable();
+		enable_local_irqs();
 
 	rseq_syscall(regs);
 
@@ -274,7 +292,7 @@ __visible inline void syscall_return_slowpath(struct pt_regs *regs)
 	if (unlikely(cached_flags & SYSCALL_EXIT_WORK_FLAGS))
 		syscall_slow_exit_work(regs, cached_flags);
 
-	local_irq_disable();
+	disable_local_irqs();
 	prepare_exit_to_usermode(regs);
 }
 
@@ -284,7 +302,7 @@ __visible void do_syscall_64(unsigned long nr, struct pt_regs *regs)
 	struct thread_info *ti;
 
 	enter_from_user_mode();
-	local_irq_enable();
+	enable_local_irqs();
 	ti = current_thread_info();
 	if (READ_ONCE(ti->flags) & _TIF_WORK_SYSCALL_ENTRY)
 		nr = syscall_trace_enter(regs);
@@ -356,7 +374,7 @@ static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)
 __visible void do_int80_syscall_32(struct pt_regs *regs)
 {
 	enter_from_user_mode();
-	local_irq_enable();
+	enable_local_irqs();
 	do_syscall_32_irqs_on(regs);
 }
 
@@ -380,7 +398,7 @@ __visible long do_fast_syscall_32(struct pt_regs *regs)
 
 	enter_from_user_mode();
 
-	local_irq_enable();
+	enable_local_irqs();
 
 	/* Fetch EBP from where the vDSO stashed it. */
 	if (
@@ -398,7 +416,7 @@ __visible long do_fast_syscall_32(struct pt_regs *regs)
 		) {
 
 		/* User code screwed up. */
-		local_irq_disable();
+		disable_local_irqs();
 		regs->ax = -EFAULT;
 		prepare_exit_to_usermode(regs);
 		return 0;	/* Keep it simple: use IRET. */
diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index 76942cbd95a1..eca914f5019c 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -57,7 +57,7 @@ SYM_CODE_END(native_usergs_sysret64)
 #ifdef CONFIG_TRACE_IRQFLAGS
 	btl	$9, \flags		/* interrupts off? */
 	jnc	1f
-	TRACE_IRQS_ON
+	TRACE_IRQS_ON_PIPELINED
 1:
 #endif
 .endm
@@ -332,6 +332,7 @@ SYM_CODE_END(__switch_to_asm)
  */
 SYM_CODE_START(ret_from_fork)
 	UNWIND_HINT_EMPTY
+	ENABLE_INTERRUPTS_IF_PIPELINED
 	movq	%rax, %rdi
 	call	schedule_tail			/* rdi: 'prev' task parameter */
 
@@ -405,6 +406,9 @@ SYM_CODE_END(spurious_entries_start)
  * Requires kernel GSBASE.
  *
  * The invariant is that, if irq_count != -1, then the IRQ stack is in use.
+ *
+ * irq_pipeline: caller must ensure that hard irqs are off upon
+ * ENTER/LEAVE_IRQ_STACK so that irq_count is always consistent.
  */
 .macro ENTER_IRQ_STACK regs=1 old_rsp save_ret=0
 	DEBUG_ENTRY_ASSERT_IRQS_OFF
@@ -575,8 +579,13 @@ SYM_CODE_START(interrupt_entry)
 
 1:
 	ENTER_IRQ_STACK old_rsp=%rdi save_ret=1
-	/* We entered an interrupt context - irqs are off: */
+	/* We entered an interrupt context - irqs are off unless
+	   pipelining is enabled, in which case we defer tracing until
+	   __ipipe_do_sync_stage() where the virtual IRQ state is
+	   updated for the root stage. */
+#ifndef CONFIG_IRQ_PIPELINE
 	TRACE_IRQS_OFF
+#endif
 
 	ret
 SYM_CODE_END(interrupt_entry)
@@ -604,7 +613,17 @@ SYM_CODE_START_LOCAL(common_interrupt)
 	addq	$-0x80, (%rsp)			/* Adjust vector to [-256, -1] range */
 	call	interrupt_entry
 	UNWIND_HINT_REGS indirect=1
+#ifdef CONFIG_IRQ_PIPELINE
+	call	handle_arch_irq_pipelined
+	testl	%eax, %eax
+	jnz	ret_from_intr
+	LEAVE_IRQ_STACK
+	testb	$3, CS(%rsp)
+	jz	retint_kernel_early
+	jmp	retint_user_early
+#else	
 	call	do_IRQ	/* rdi points to pt_regs */
+#endif	
 	/* 0(%rsp): old RSP */
 ret_from_intr:
 	DISABLE_INTERRUPTS(CLBR_ANY)
@@ -619,6 +638,7 @@ ret_from_intr:
 .Lretint_user:
 	mov	%rsp,%rdi
 	call	prepare_exit_to_usermode
+retint_user_early:
 	TRACE_IRQS_IRETQ
 
 SYM_INNER_LABEL(swapgs_restore_regs_and_return_to_usermode, SYM_L_GLOBAL)
@@ -676,7 +696,8 @@ retint_kernel:
 #endif
 	/*
 	 * The iretq could re-enable interrupts:
-	 */
+	*/
+retint_kernel_early:
 	TRACE_IRQS_IRETQ
 
 SYM_INNER_LABEL(restore_regs_and_return_to_kernel, SYM_L_GLOBAL)
@@ -793,7 +814,29 @@ _ASM_NOKPROBE(common_interrupt)
 
 /*
  * APIC interrupts.
- */
+*/
+#ifdef CONFIG_IRQ_PIPELINE
+.macro apicinterrupt2 num sym
+SYM_CODE_START(\sym)
+	UNWIND_HINT_IRET_REGS
+	pushq	$~(\num)
+.Lcommon_\sym:
+	call	interrupt_entry
+	UNWIND_HINT_REGS indirect=1
+	call	handle_arch_irq_pipelined /* rdi points to pt_regs */
+	testl	%eax, %eax
+	jnz	ret_from_intr
+	LEAVE_IRQ_STACK
+	testb	$3, CS(%rsp)
+	jz	retint_kernel_early
+	jmp	retint_user_early
+SYM_CODE_END(\sym)
+_ASM_NOKPROBE(\sym)
+.endm
+.macro apicinterrupt3 num sym do_sym
+apicinterrupt2 \num \sym
+.endm
+#else	
 .macro apicinterrupt3 num sym do_sym
 SYM_CODE_START(\sym)
 	UNWIND_HINT_IRET_REGS
@@ -806,7 +849,8 @@ SYM_CODE_START(\sym)
 SYM_CODE_END(\sym)
 _ASM_NOKPROBE(\sym)
 .endm
-
+#endif
+	
 /* Make sure APIC interrupt handlers end up in the irqentry section: */
 #define PUSH_SECTION_IRQENTRY	.pushsection .irqentry.text, "ax"
 #define POP_SECTION_IRQENTRY	.popsection
@@ -851,8 +895,15 @@ apicinterrupt THERMAL_APIC_VECTOR		thermal_interrupt		smp_thermal_interrupt
 apicinterrupt CALL_FUNCTION_SINGLE_VECTOR	call_function_single_interrupt	smp_call_function_single_interrupt
 apicinterrupt CALL_FUNCTION_VECTOR		call_function_interrupt		smp_call_function_interrupt
 apicinterrupt RESCHEDULE_VECTOR			reschedule_interrupt		smp_reschedule_interrupt
+#ifdef CONFIG_IRQ_PIPELINE
+apicinterrupt2 RESCHEDULE_OOB_VECTOR		reschedule_oob_interrupt
+#endif
 #endif
 
+#ifdef CONFIG_IRQ_PIPELINE
+apicinterrupt2 TIMER_OOB_VECTOR			timer_oob_interrupt
+#endif
+	
 apicinterrupt ERROR_APIC_VECTOR			error_interrupt			smp_error_interrupt
 apicinterrupt SPURIOUS_APIC_VECTOR		spurious_interrupt		smp_spurious_interrupt
 
@@ -1078,9 +1129,13 @@ SYM_CODE_END(.Lbad_gs)
 SYM_FUNC_START(do_softirq_own_stack)
 	pushq	%rbp
 	mov	%rsp, %rbp
+	DISABLE_INTERRUPTS_IF_PIPELINED
 	ENTER_IRQ_STACK regs=0 old_rsp=%r11
+	ENABLE_INTERRUPTS_IF_PIPELINED
 	call	__do_softirq
+	DISABLE_INTERRUPTS_IF_PIPELINED
 	LEAVE_IRQ_STACK regs=0
+	ENABLE_INTERRUPTS_IF_PIPELINED
 	leaveq
 	ret
 SYM_FUNC_END(do_softirq_own_stack)
diff --git a/arch/x86/entry/thunk_64.S b/arch/x86/entry/thunk_64.S
index c5c3b6e86e62..e47c2391ed78 100644
--- a/arch/x86/entry/thunk_64.S
+++ b/arch/x86/entry/thunk_64.S
@@ -39,6 +39,7 @@ SYM_FUNC_END(\name)
 
 #ifdef CONFIG_TRACE_IRQFLAGS
 	THUNK trace_hardirqs_on_thunk,trace_hardirqs_on_caller,1
+	THUNK trace_hardirqs_on_pipelined_thunk,trace_hardirqs_on_pipelined,1
 	THUNK trace_hardirqs_off_thunk,trace_hardirqs_off_caller,1
 #endif
 
diff --git a/arch/x86/include/asm/apic.h b/arch/x86/include/asm/apic.h
index 2ebc17d9c72c..9094b787b525 100644
--- a/arch/x86/include/asm/apic.h
+++ b/arch/x86/include/asm/apic.h
@@ -443,7 +443,7 @@ static inline void apic_set_eoi_write(void (*eoi_write)(u32 reg, u32 v)) {}
 
 extern void apic_ack_irq(struct irq_data *data);
 
-static inline void ack_APIC_irq(void)
+static inline void __ack_APIC_irq(void)
 {
 	/*
 	 * ack_APIC_irq() actually gets compiled as a single instruction
@@ -452,6 +452,12 @@ static inline void ack_APIC_irq(void)
 	apic_eoi();
 }
 
+static inline void ack_APIC_irq(void)
+{
+	if (!irqs_pipelined())
+		__ack_APIC_irq();
+}
+
 static inline unsigned default_get_apic_id(unsigned long x)
 {
 	unsigned int ver = GET_APIC_VERSION(apic_read(APIC_LVR));
diff --git a/arch/x86/include/asm/hw_irq.h b/arch/x86/include/asm/hw_irq.h
index 4154bc5f6a4e..beae9ae335ac 100644
--- a/arch/x86/include/asm/hw_irq.h
+++ b/arch/x86/include/asm/hw_irq.h
@@ -50,6 +50,11 @@ extern asmlinkage void deferred_error_interrupt(void);
 extern asmlinkage void call_function_interrupt(void);
 extern asmlinkage void call_function_single_interrupt(void);
 
+#ifdef CONFIG_IRQ_PIPELINE
+extern asmlinkage void reschedule_oob_interrupt(void);
+extern asmlinkage void timer_oob_interrupt(void);
+#endif
+
 #ifdef	CONFIG_X86_LOCAL_APIC
 struct irq_data;
 struct pci_dev;
diff --git a/arch/x86/include/asm/i8259.h b/arch/x86/include/asm/i8259.h
index 89789e8c80f6..facf1bc68de6 100644
--- a/arch/x86/include/asm/i8259.h
+++ b/arch/x86/include/asm/i8259.h
@@ -26,7 +26,7 @@ extern unsigned int cached_irq_mask;
 #define SLAVE_ICW4_DEFAULT	0x01
 #define PIC_ICW4_AEOI		2
 
-extern raw_spinlock_t i8259A_lock;
+extern hard_spinlock_t i8259A_lock;
 
 /* the PIC may need a careful delay on some platforms, hence specific calls */
 static inline unsigned char inb_pic(unsigned int port)
diff --git a/arch/x86/include/asm/irq_pipeline.h b/arch/x86/include/asm/irq_pipeline.h
new file mode 100644
index 000000000000..314704565141
--- /dev/null
+++ b/arch/x86/include/asm/irq_pipeline.h
@@ -0,0 +1,139 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2019 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#ifndef _ASM_X86_IRQ_PIPELINE_H
+#define _ASM_X86_IRQ_PIPELINE_H
+
+#include <asm-generic/irq_pipeline.h>
+
+#ifdef CONFIG_IRQ_PIPELINE
+
+#include <asm/ptrace.h>
+
+#define FIRST_SYSTEM_IRQ	NR_IRQS
+#define TIMER_OOB_IPI		apicm_vector_irq(TIMER_OOB_VECTOR)
+#define RESCHEDULE_OOB_IPI	apicm_vector_irq(RESCHEDULE_OOB_VECTOR)
+#define apicm_irq_vector(__irq) ((__irq) - FIRST_SYSTEM_IRQ + FIRST_SYSTEM_VECTOR)
+#define apicm_vector_irq(__vec) ((__vec) - FIRST_SYSTEM_VECTOR + FIRST_SYSTEM_IRQ)
+
+#define X86_EFLAGS_SS_BIT	31
+
+static inline notrace
+unsigned long arch_irqs_virtual_to_native_flags(int stalled)
+{
+	return (!stalled) << X86_EFLAGS_IF_BIT;
+}
+
+static inline notrace
+unsigned long arch_irqs_native_to_virtual_flags(unsigned long flags)
+{
+	return hard_irqs_disabled_flags(flags) << X86_EFLAGS_SS_BIT;
+}
+
+static inline notrace unsigned long arch_local_irq_save(void)
+{
+	int stalled = inband_irq_save();
+	barrier();
+	return arch_irqs_virtual_to_native_flags(stalled);
+}
+
+static inline notrace void arch_local_irq_enable(void)
+{
+	barrier();
+	inband_irq_enable();
+}
+
+static inline notrace void arch_local_irq_disable(void)
+{
+	inband_irq_disable();
+	barrier();
+}
+
+static inline notrace unsigned long arch_local_save_flags(void)
+{
+	int stalled = inband_irqs_disabled();
+	barrier();
+	return arch_irqs_virtual_to_native_flags(stalled);
+}
+
+static inline notrace void arch_local_irq_restore(unsigned long flags)
+{
+	inband_irq_restore(native_irqs_disabled_flags(flags));
+	barrier();
+}
+
+static inline
+void arch_save_timer_regs(struct pt_regs *dst,
+			  struct pt_regs *src, bool oob_context)
+{
+	dst->flags = src->flags;
+	dst->cs = src->cs;
+	dst->ip = src->ip;
+	dst->bp = src->bp;
+	dst->ss = src->ss;
+	dst->sp = src->sp;
+	if (oob_context)
+		dst->flags &= ~X86_EFLAGS_IF;
+}
+
+static inline bool arch_steal_pipelined_tick(struct pt_regs *regs)
+{
+	return !(regs->flags & X86_EFLAGS_IF);
+}
+
+static inline int arch_enable_oob_stage(void)
+{
+	return 0;
+}
+
+unsigned long pipelined_fault_entry(struct pt_regs *regs);
+
+void pipelined_fault_exit(unsigned long combo);
+
+#else /* !CONFIG_IRQ_PIPELINE */
+
+struct pt_regs;
+
+static inline notrace unsigned long arch_local_save_flags(void)
+{
+	return native_save_fl();
+}
+
+static inline notrace void arch_local_irq_restore(unsigned long flags)
+{
+	native_restore_fl(flags);
+}
+
+static inline notrace void arch_local_irq_disable(void)
+{
+	native_irq_disable();
+}
+
+static inline notrace void arch_local_irq_enable(void)
+{
+	native_irq_enable();
+}
+
+/*
+ * For spinlocks, etc:
+ */
+static inline notrace unsigned long arch_local_irq_save(void)
+{
+	unsigned long flags = arch_local_save_flags();
+	arch_local_irq_disable();
+	return flags;
+}
+
+static inline
+unsigned long pipelined_fault_entry(struct pt_regs *regs)
+{
+	return 0;
+}
+
+static inline void pipelined_fault_exit(unsigned long combo) { }
+
+#endif /* !CONFIG_IRQ_PIPELINE */
+
+#endif /* _ASM_X86_IRQ_PIPELINE_H */
diff --git a/arch/x86/include/asm/irq_vectors.h b/arch/x86/include/asm/irq_vectors.h
index 889f8b1b5b7f..1e51dc4850a6 100644
--- a/arch/x86/include/asm/irq_vectors.h
+++ b/arch/x86/include/asm/irq_vectors.h
@@ -106,10 +106,19 @@
 
 #define LOCAL_TIMER_VECTOR		0xec
 
+#ifdef CONFIG_IRQ_PIPELINE
+#define TIMER_OOB_VECTOR		0xeb
+#define RESCHEDULE_OOB_VECTOR		0xea
+#define FIRST_SYSTEM_APIC_VECTOR	RESCHEDULE_OOB_VECTOR
+#define NR_APIC_VECTORS	        	(NR_VECTORS - FIRST_SYSTEM_VECTOR)
+#else
+#define FIRST_SYSTEM_APIC_VECTOR	LOCAL_TIMER_VECTOR
+#endif
+
 #define NR_VECTORS			 256
 
 #ifdef CONFIG_X86_LOCAL_APIC
-#define FIRST_SYSTEM_VECTOR		LOCAL_TIMER_VECTOR
+#define FIRST_SYSTEM_VECTOR		FIRST_SYSTEM_APIC_VECTOR
 #else
 #define FIRST_SYSTEM_VECTOR		NR_VECTORS
 #endif
diff --git a/arch/x86/include/asm/irqflags.h b/arch/x86/include/asm/irqflags.h
index 8a0e56e1dcc9..a5fb034e804a 100644
--- a/arch/x86/include/asm/irqflags.h
+++ b/arch/x86/include/asm/irqflags.h
@@ -35,6 +35,11 @@ extern inline unsigned long native_save_fl(void)
 	return flags;
 }
 
+static inline unsigned long native_save_flags(void)
+{
+	return native_save_fl();
+}
+
 extern inline void native_restore_fl(unsigned long flags);
 extern inline void native_restore_fl(unsigned long flags)
 {
@@ -54,6 +59,33 @@ static inline void native_irq_enable(void)
 	asm volatile("sti": : :"memory");
 }
 
+static inline unsigned long native_irq_save(void)
+{
+	unsigned long flags;
+
+	flags = native_save_flags();
+
+	native_irq_disable();
+
+	return flags;
+}
+
+static inline void native_irq_restore(unsigned long flags)
+{
+	return native_restore_fl(flags);
+}
+
+static inline int native_irqs_disabled_flags(unsigned long flags)
+{
+	return !(flags & X86_EFLAGS_IF);
+}
+
+static inline bool native_irqs_disabled(void)
+{
+	unsigned long flags = native_save_flags();
+	return native_irqs_disabled_flags(flags);
+}
+
 static inline __cpuidle void native_safe_halt(void)
 {
 	mds_idle_clear_cpu_buffers();
@@ -73,26 +105,7 @@ static inline __cpuidle void native_halt(void)
 #else
 #ifndef __ASSEMBLY__
 #include <linux/types.h>
-
-static inline notrace unsigned long arch_local_save_flags(void)
-{
-	return native_save_fl();
-}
-
-static inline notrace void arch_local_irq_restore(unsigned long flags)
-{
-	native_restore_fl(flags);
-}
-
-static inline notrace void arch_local_irq_disable(void)
-{
-	native_irq_disable();
-}
-
-static inline notrace void arch_local_irq_enable(void)
-{
-	native_irq_enable();
-}
+#include <asm/irq_pipeline.h>
 
 /*
  * Used in the idle loop; sti takes one instruction cycle
@@ -112,15 +125,6 @@ static inline __cpuidle void halt(void)
 	native_halt();
 }
 
-/*
- * For spinlocks, etc:
- */
-static inline notrace unsigned long arch_local_irq_save(void)
-{
-	unsigned long flags = arch_local_save_flags();
-	arch_local_irq_disable();
-	return flags;
-}
 #else
 
 #define ENABLE_INTERRUPTS(x)	sti
@@ -161,7 +165,7 @@ static inline notrace unsigned long arch_local_irq_save(void)
 #ifndef __ASSEMBLY__
 static inline int arch_irqs_disabled_flags(unsigned long flags)
 {
-	return !(flags & X86_EFLAGS_IF);
+	return native_irqs_disabled_flags(flags);
 }
 
 static inline int arch_irqs_disabled(void)
@@ -176,8 +180,14 @@ static inline int arch_irqs_disabled(void)
 #ifdef CONFIG_TRACE_IRQFLAGS
 #  define TRACE_IRQS_ON		call trace_hardirqs_on_thunk;
 #  define TRACE_IRQS_OFF	call trace_hardirqs_off_thunk;
+#ifdef CONFIG_IRQ_PIPELINE
+#  define TRACE_IRQS_ON_PIPELINED	call trace_hardirqs_on_pipelined_thunk;
+#else
+#  define TRACE_IRQS_ON_PIPELINED	TRACE_IRQS_ON
+#endif
 #else
 #  define TRACE_IRQS_ON
+#  define TRACE_IRQS_ON_PIPELINED
 #  define TRACE_IRQS_OFF
 #endif
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
@@ -204,6 +214,13 @@ static inline int arch_irqs_disabled(void)
 #  define LOCKDEP_SYS_EXIT
 #  define LOCKDEP_SYS_EXIT_IRQ
 #endif
+#ifdef CONFIG_IRQ_PIPELINE
+#define ENABLE_INTERRUPTS_IF_PIPELINED	ENABLE_INTERRUPTS(CLBR_ANY)
+#define DISABLE_INTERRUPTS_IF_PIPELINED	DISABLE_INTERRUPTS(CLBR_ANY)
+#else
+#define ENABLE_INTERRUPTS_IF_PIPELINED
+#define DISABLE_INTERRUPTS_IF_PIPELINED
+#endif
 #endif /* __ASSEMBLY__ */
 
 #endif
diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h
index 5f33924e200f..3de1f73c371e 100644
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@ -379,10 +379,13 @@ typedef struct {
 static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 {
 	temp_mm_state_t temp_state;
+	unsigned long flags;
 
 	lockdep_assert_irqs_disabled();
 	temp_state.mm = this_cpu_read(cpu_tlbstate.loaded_mm);
+	protect_inband_mm(flags);
 	switch_mm_irqs_off(NULL, mm, current);
+	unprotect_inband_mm(flags);
 
 	/*
 	 * If breakpoints are enabled, disable them while the temporary mm is
@@ -403,8 +406,12 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 
 static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 {
+	unsigned long flags;
+
 	lockdep_assert_irqs_disabled();
+	protect_inband_mm(flags);
 	switch_mm_irqs_off(NULL, prev_state.mm, current);
+	unprotect_inband_mm(flags);
 
 	/*
 	 * Restore the breakpoints if they were disabled before the temporary mm
diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 0340aad3f2fc..ff9cf98c4634 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -563,6 +563,9 @@ static inline void arch_thread_struct_whitelist(unsigned long *offset,
  * have to worry about atomic accesses.
  */
 #define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/
+#define TS_OOB			0x0004	/* Thread is running out-of-band */
+
+#define _TLF_OOB		TS_OOB
 
 static inline void
 native_load_sp0(unsigned long sp0)
diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h
index 6f66d841262d..585b18ff6d68 100644
--- a/arch/x86/include/asm/tlbflush.h
+++ b/arch/x86/include/asm/tlbflush.h
@@ -284,7 +284,10 @@ static inline void cr4_init_shadow(void)
 
 static inline void __cr4_set(unsigned long cr4)
 {
-	lockdep_assert_irqs_disabled();
+	if (irqs_pipelined())
+		check_hard_irqs_disabled();
+	else
+		lockdep_assert_irqs_disabled();
 	this_cpu_write(cpu_tlbstate.cr4, cr4);
 	__write_cr4(cr4);
 }
@@ -292,21 +295,25 @@ static inline void __cr4_set(unsigned long cr4)
 /* Set in this cpu's CR4. */
 static inline void cr4_set_bits_irqsoff(unsigned long mask)
 {
-	unsigned long cr4;
+	unsigned long cr4, flags
 
+	flags = hard_local_irq_save();
 	cr4 = this_cpu_read(cpu_tlbstate.cr4);
 	if ((cr4 | mask) != cr4)
 		__cr4_set(cr4 | mask);
+	hard_local_irq_restore(flags);
 }
 
 /* Clear in this cpu's CR4. */
 static inline void cr4_clear_bits_irqsoff(unsigned long mask)
 {
-	unsigned long cr4;
+	unsigned long cr4, flags;
 
+	flags = hard_local_irq_save();
 	cr4 = this_cpu_read(cpu_tlbstate.cr4);
 	if ((cr4 & ~mask) != cr4)
 		__cr4_set(cr4 & ~mask);
+	hard_local_irq_restore(flags);
 }
 
 /* Set in this cpu's CR4. */
@@ -314,9 +321,9 @@ static inline void cr4_set_bits(unsigned long mask)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	cr4_set_bits_irqsoff(mask);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /* Clear in this cpu's CR4. */
@@ -324,17 +331,19 @@ static inline void cr4_clear_bits(unsigned long mask)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	cr4_clear_bits_irqsoff(mask);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static inline void cr4_toggle_bits_irqsoff(unsigned long mask)
 {
-	unsigned long cr4;
+	unsigned long cr4, flags;
 
+	flags = hard_local_irq_save();
 	cr4 = this_cpu_read(cpu_tlbstate.cr4);
 	__cr4_set(cr4 ^ mask);
+	hard_local_irq_restore(flags);
 }
 
 /* Read the CR4 shadow. */
@@ -437,7 +446,7 @@ static inline void __native_flush_tlb_global(void)
 	 * from interrupts. (Use the raw variant because this code can
 	 * be called from deep inside debugging code.)
 	 */
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 
 	cr4 = this_cpu_read(cpu_tlbstate.cr4);
 	/* toggle PGE */
@@ -445,7 +454,7 @@ static inline void __native_flush_tlb_global(void)
 	/* write old PGE again and flush TLBs */
 	native_write_cr4(cr4);
 
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /*
diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
index 6175e370ee4a..5293dd97ac3a 100644
--- a/arch/x86/kernel/Makefile
+++ b/arch/x86/kernel/Makefile
@@ -120,6 +120,7 @@ obj-$(CONFIG_PARAVIRT_CLOCK)	+= pvclock.o
 obj-$(CONFIG_X86_PMEM_LEGACY_DEVICE) += pmem.o
 
 obj-$(CONFIG_JAILHOUSE_GUEST)	+= jailhouse.o
+obj-$(CONFIG_IRQ_PIPELINE)	+= irq_pipeline.o
 
 obj-$(CONFIG_EISA)		+= eisa.o
 obj-$(CONFIG_PCSPKR_PLATFORM)	+= pcspeaker.o
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index 28446fa6bf18..d7daf1766eb2 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -271,10 +271,10 @@ void native_apic_icr_write(u32 low, u32 id)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	apic_write(APIC_ICR2, SET_APIC_DEST_FIELD(id));
 	apic_write(APIC_ICR, low);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 u64 native_apic_icr_read(void)
@@ -330,6 +330,9 @@ int lapic_get_maxlvt(void)
 static void __setup_APIC_LVTT(unsigned int clocks, int oneshot, int irqen)
 {
 	unsigned int lvtt_value, tmp_value;
+	unsigned long flags;
+
+	flags = hard_cond_local_irq_save();
 
 	lvtt_value = LOCAL_TIMER_VECTOR;
 	if (!oneshot)
@@ -354,6 +357,7 @@ static void __setup_APIC_LVTT(unsigned int clocks, int oneshot, int irqen)
 		asm volatile("mfence" : : : "memory");
 
 		printk_once(KERN_DEBUG "TSC deadline timer enabled\n");
+		hard_cond_local_irq_restore(flags);
 		return;
 	}
 
@@ -367,6 +371,8 @@ static void __setup_APIC_LVTT(unsigned int clocks, int oneshot, int irqen)
 
 	if (!oneshot)
 		apic_write(APIC_TMICT, clocks / APIC_DIVISOR);
+
+	hard_cond_local_irq_restore(flags);
 }
 
 /*
@@ -472,25 +478,31 @@ static int lapic_next_event(unsigned long delta,
 static int lapic_next_deadline(unsigned long delta,
 			       struct clock_event_device *evt)
 {
+	unsigned long flags;
 	u64 tsc;
 
+	flags = hard_local_irq_save();
 	tsc = rdtsc();
 	wrmsrl(MSR_IA32_TSC_DEADLINE, tsc + (((u64) delta) * TSC_DIVISOR));
+	hard_local_irq_restore(flags);
 	return 0;
 }
 
 static int lapic_timer_shutdown(struct clock_event_device *evt)
 {
+	unsigned long flags;
 	unsigned int v;
 
 	/* Lapic used as dummy for broadcast ? */
 	if (evt->features & CLOCK_EVT_FEAT_DUMMY)
 		return 0;
 
+	flags = hard_local_irq_save();
 	v = apic_read(APIC_LVTT);
 	v |= (APIC_LVT_MASKED | LOCAL_TIMER_VECTOR);
 	apic_write(APIC_LVTT, v);
 	apic_write(APIC_TMICT, 0);
+	hard_local_irq_restore(flags);
 	return 0;
 }
 
@@ -1541,7 +1553,7 @@ static bool apic_check_and_ack(union apic_ir *irr, union apic_ir *isr)
 		 * per set bit.
 		 */
 		for_each_set_bit(bit, isr->map, APIC_IR_BITS)
-			ack_APIC_irq();
+			__ack_APIC_irq();
 		return true;
 	}
 
@@ -2176,7 +2188,7 @@ __visible void __irq_entry smp_spurious_interrupt(struct pt_regs *regs)
 	if (v & (1 << (vector & 0x1f))) {
 		pr_info("Spurious interrupt (vector 0x%02x) on CPU#%d. Acked\n",
 			vector, smp_processor_id());
-		ack_APIC_irq();
+		__ack_APIC_irq();
 	} else {
 		pr_info("Spurious interrupt (vector 0x%02x) on CPU#%d. Not pending!\n",
 			vector, smp_processor_id());
@@ -2210,7 +2222,7 @@ __visible void __irq_entry smp_error_interrupt(struct pt_regs *regs)
 	if (lapic_get_maxlvt() > 3)	/* Due to the Pentium erratum 3AP. */
 		apic_write(APIC_ESR, 0);
 	v = apic_read(APIC_ESR);
-	ack_APIC_irq();
+	__ack_APIC_irq();
 	atomic_inc(&irq_err_count);
 
 	apic_printk(APIC_DEBUG, KERN_DEBUG "APIC error on CPU%d: %02x",
diff --git a/arch/x86/kernel/apic/apic_flat_64.c b/arch/x86/kernel/apic/apic_flat_64.c
index 7862b152a052..d3762181070f 100644
--- a/arch/x86/kernel/apic/apic_flat_64.c
+++ b/arch/x86/kernel/apic/apic_flat_64.c
@@ -52,9 +52,9 @@ static void _flat_send_IPI_mask(unsigned long mask, int vector)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	__default_send_IPI_dest_field(mask, vector, apic->dest_logical);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static void flat_send_IPI_mask(const struct cpumask *cpumask, int vector)
diff --git a/arch/x86/kernel/apic/apic_numachip.c b/arch/x86/kernel/apic/apic_numachip.c
index cdf45b4700f2..7cd977356605 100644
--- a/arch/x86/kernel/apic/apic_numachip.c
+++ b/arch/x86/kernel/apic/apic_numachip.c
@@ -96,21 +96,21 @@ static void numachip_send_IPI_one(int cpu, int vector)
 	int local_apicid, apicid = per_cpu(x86_cpu_to_apicid, cpu);
 	unsigned int dmode;
 
-	preempt_disable();
+	hard_preempt_disable();
 	local_apicid = __this_cpu_read(x86_cpu_to_apicid);
 
 	/* Send via local APIC where non-local part matches */
 	if (!((apicid ^ local_apicid) >> NUMACHIP_LAPIC_BITS)) {
 		unsigned long flags;
 
-		local_irq_save(flags);
+		flags = hard_local_irq_save();
 		__default_send_IPI_dest_field(apicid, vector,
 			APIC_DEST_PHYSICAL);
-		local_irq_restore(flags);
-		preempt_enable();
+		hard_local_irq_restore(flags);
+		hard_preempt_enable();
 		return;
 	}
-	preempt_enable();
+	hard_preempt_enable();
 
 	dmode = (vector == NMI_VECTOR) ? APIC_DM_NMI : APIC_DM_FIXED;
 	numachip_apic_icr_write(apicid, dmode | vector);
diff --git a/arch/x86/kernel/apic/io_apic.c b/arch/x86/kernel/apic/io_apic.c
index 913c88617848..ddff70c9cbb3 100644
--- a/arch/x86/kernel/apic/io_apic.c
+++ b/arch/x86/kernel/apic/io_apic.c
@@ -78,7 +78,7 @@
 #define for_each_irq_pin(entry, head) \
 	list_for_each_entry(entry, &head, list)
 
-static DEFINE_RAW_SPINLOCK(ioapic_lock);
+static DEFINE_HARD_SPINLOCK(ioapic_lock);
 static DEFINE_MUTEX(ioapic_mutex);
 static unsigned int ioapic_dynirq_base;
 static int ioapic_initialized;
@@ -1831,7 +1831,7 @@ static void ioapic_ack_level(struct irq_data *irq_data)
 	 * We must acknowledge the irq before we move it or the acknowledge will
 	 * not propagate properly.
 	 */
-	ack_APIC_irq();
+	__ack_APIC_irq();
 
 	/*
 	 * Tail end of clearing remote IRR bit (either by delivering the EOI
@@ -1951,7 +1951,7 @@ static struct irq_chip ioapic_chip __read_mostly = {
 	.irq_set_affinity	= ioapic_set_affinity,
 	.irq_retrigger		= irq_chip_retrigger_hierarchy,
 	.irq_get_irqchip_state	= ioapic_irq_get_chip_state,
-	.flags			= IRQCHIP_SKIP_SET_WAKE,
+	.flags			= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 static struct irq_chip ioapic_ir_chip __read_mostly = {
@@ -1964,7 +1964,7 @@ static struct irq_chip ioapic_ir_chip __read_mostly = {
 	.irq_set_affinity	= ioapic_set_affinity,
 	.irq_retrigger		= irq_chip_retrigger_hierarchy,
 	.irq_get_irqchip_state	= ioapic_irq_get_chip_state,
-	.flags			= IRQCHIP_SKIP_SET_WAKE,
+	.flags			= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 static inline void init_IO_APIC_traps(void)
@@ -2011,7 +2011,7 @@ static void unmask_lapic_irq(struct irq_data *data)
 
 static void ack_lapic_irq(struct irq_data *data)
 {
-	ack_APIC_irq();
+	__ack_APIC_irq();
 }
 
 static struct irq_chip lapic_chip __read_mostly = {
@@ -2019,6 +2019,7 @@ static struct irq_chip lapic_chip __read_mostly = {
 	.irq_mask	= mask_lapic_irq,
 	.irq_unmask	= unmask_lapic_irq,
 	.irq_ack	= ack_lapic_irq,
+	.flags		= IRQCHIP_PIPELINE_SAFE,
 };
 
 static void lapic_register_intr(int irq)
diff --git a/arch/x86/kernel/apic/ipi.c b/arch/x86/kernel/apic/ipi.c
index 6ca0f91372fd..d501cb4a9678 100644
--- a/arch/x86/kernel/apic/ipi.c
+++ b/arch/x86/kernel/apic/ipi.c
@@ -116,8 +116,10 @@ void __default_send_IPI_shortcut(unsigned int shortcut, int vector)
 	 * cli/sti.  Otherwise we use an even cheaper single atomic write
 	 * to the APIC.
 	 */
+	unsigned long flags;
 	unsigned int cfg;
 
+	flags = hard_cond_local_irq_save();
 	/*
 	 * Wait for idle.
 	 */
@@ -136,6 +138,8 @@ void __default_send_IPI_shortcut(unsigned int shortcut, int vector)
 	 * Send the IPI. The write to APIC_ICR fires this off.
 	 */
 	native_apic_mem_write(APIC_ICR, cfg);
+
+	hard_cond_local_irq_restore(flags);
 }
 
 /*
@@ -144,8 +148,10 @@ void __default_send_IPI_shortcut(unsigned int shortcut, int vector)
  */
 void __default_send_IPI_dest_field(unsigned int mask, int vector, unsigned int dest)
 {
+	unsigned long flags;
 	unsigned long cfg;
 
+	flags = hard_cond_local_irq_save();
 	/*
 	 * Wait for idle.
 	 */
@@ -169,16 +175,18 @@ void __default_send_IPI_dest_field(unsigned int mask, int vector, unsigned int d
 	 * Send the IPI. The write to APIC_ICR fires this off.
 	 */
 	native_apic_mem_write(APIC_ICR, cfg);
+
+	hard_cond_local_irq_restore(flags);
 }
 
 void default_send_IPI_single_phys(int cpu, int vector)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	__default_send_IPI_dest_field(per_cpu(x86_cpu_to_apicid, cpu),
 				      vector, APIC_DEST_PHYSICAL);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 void default_send_IPI_mask_sequence_phys(const struct cpumask *mask, int vector)
@@ -191,12 +199,12 @@ void default_send_IPI_mask_sequence_phys(const struct cpumask *mask, int vector)
 	 * to an arbitrary mask, so I do a unicast to each CPU instead.
 	 * - mbligh
 	 */
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	for_each_cpu(query_cpu, mask) {
 		__default_send_IPI_dest_field(per_cpu(x86_cpu_to_apicid,
 				query_cpu), vector, APIC_DEST_PHYSICAL);
 	}
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 void default_send_IPI_mask_allbutself_phys(const struct cpumask *mask,
@@ -208,14 +216,14 @@ void default_send_IPI_mask_allbutself_phys(const struct cpumask *mask,
 
 	/* See Hack comment above */
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	for_each_cpu(query_cpu, mask) {
 		if (query_cpu == this_cpu)
 			continue;
 		__default_send_IPI_dest_field(per_cpu(x86_cpu_to_apicid,
 				 query_cpu), vector, APIC_DEST_PHYSICAL);
 	}
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /*
@@ -255,12 +263,12 @@ void default_send_IPI_mask_sequence_logical(const struct cpumask *mask,
 	 * should be modified to do 1 message per cluster ID - mbligh
 	 */
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	for_each_cpu(query_cpu, mask)
 		__default_send_IPI_dest_field(
 			early_per_cpu(x86_cpu_to_logical_apicid, query_cpu),
 			vector, apic->dest_logical);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 void default_send_IPI_mask_allbutself_logical(const struct cpumask *mask,
@@ -272,7 +280,7 @@ void default_send_IPI_mask_allbutself_logical(const struct cpumask *mask,
 
 	/* See Hack comment above */
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	for_each_cpu(query_cpu, mask) {
 		if (query_cpu == this_cpu)
 			continue;
@@ -280,7 +288,7 @@ void default_send_IPI_mask_allbutself_logical(const struct cpumask *mask,
 			early_per_cpu(x86_cpu_to_logical_apicid, query_cpu),
 			vector, apic->dest_logical);
 		}
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /*
@@ -294,10 +302,10 @@ void default_send_IPI_mask_logical(const struct cpumask *cpumask, int vector)
 	if (!mask)
 		return;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	WARN_ON(mask & ~cpumask_bits(cpu_online_mask)[0]);
 	__default_send_IPI_dest_field(mask, vector, apic->dest_logical);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /* must come after the send_IPI functions above for inlining */
diff --git a/arch/x86/kernel/apic/msi.c b/arch/x86/kernel/apic/msi.c
index 7f7533462474..997040f37a69 100644
--- a/arch/x86/kernel/apic/msi.c
+++ b/arch/x86/kernel/apic/msi.c
@@ -58,7 +58,7 @@ static struct irq_chip pci_msi_controller = {
 	.irq_ack		= irq_chip_ack_parent,
 	.irq_retrigger		= irq_chip_retrigger_hierarchy,
 	.irq_compose_msi_msg	= irq_msi_compose_msg,
-	.flags			= IRQCHIP_SKIP_SET_WAKE,
+	.flags			= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 int native_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)
@@ -156,7 +156,7 @@ static struct irq_chip pci_msi_ir_controller = {
 	.irq_ack		= irq_chip_ack_parent,
 	.irq_retrigger		= irq_chip_retrigger_hierarchy,
 	.irq_set_vcpu_affinity	= irq_chip_set_vcpu_affinity_parent,
-	.flags			= IRQCHIP_SKIP_SET_WAKE,
+	.flags			= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 static struct msi_domain_info pci_msi_ir_domain_info = {
@@ -198,7 +198,7 @@ static struct irq_chip dmar_msi_controller = {
 	.irq_retrigger		= irq_chip_retrigger_hierarchy,
 	.irq_compose_msi_msg	= irq_msi_compose_msg,
 	.irq_write_msi_msg	= dmar_msi_write_msg,
-	.flags			= IRQCHIP_SKIP_SET_WAKE,
+	.flags			= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 static irq_hw_number_t dmar_msi_get_hwirq(struct msi_domain_info *info,
@@ -295,7 +295,7 @@ static struct irq_chip hpet_msi_controller __ro_after_init = {
 	.irq_retrigger = irq_chip_retrigger_hierarchy,
 	.irq_compose_msi_msg = irq_msi_compose_msg,
 	.irq_write_msi_msg = hpet_msi_write_msg,
-	.flags = IRQCHIP_SKIP_SET_WAKE,
+	.flags = IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 static irq_hw_number_t hpet_msi_get_hwirq(struct msi_domain_info *info,
diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c
index 2c5676b0a6e7..c8ab1608177b 100644
--- a/arch/x86/kernel/apic/vector.c
+++ b/arch/x86/kernel/apic/vector.c
@@ -39,7 +39,7 @@ struct apic_chip_data {
 
 struct irq_domain *x86_vector_domain;
 EXPORT_SYMBOL_GPL(x86_vector_domain);
-static DEFINE_RAW_SPINLOCK(vector_lock);
+static DEFINE_HARD_SPINLOCK(vector_lock);
 static cpumask_var_t vector_searchmask;
 static struct irq_chip lapic_controller;
 static struct irq_matrix *vector_matrix;
@@ -761,9 +761,11 @@ void lapic_online(void)
 
 void lapic_offline(void)
 {
-	lock_vector_lock();
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&vector_lock, flags);
 	irq_matrix_offline(vector_matrix);
-	unlock_vector_lock();
+	raw_spin_unlock_irqrestore(&vector_lock, flags);
 }
 
 static int apic_set_affinity(struct irq_data *irqd,
@@ -772,6 +774,8 @@ static int apic_set_affinity(struct irq_data *irqd,
 	struct apic_chip_data *apicd = apic_chip_data(irqd);
 	int err;
 
+	WARN_ON_ONCE(irqs_pipelined() && !hard_irqs_disabled());
+
 	/*
 	 * Core code can call here for inactive interrupts. For inactive
 	 * interrupts which use managed or reservation mode there is no
@@ -813,7 +817,7 @@ static int apic_retrigger_irq(struct irq_data *irqd)
 void apic_ack_irq(struct irq_data *irqd)
 {
 	irq_move_irq(irqd);
-	ack_APIC_irq();
+	__ack_APIC_irq();
 }
 
 void apic_ack_edge(struct irq_data *irqd)
@@ -858,10 +862,11 @@ asmlinkage __visible void __irq_entry smp_irq_move_cleanup_interrupt(void)
 	struct hlist_head *clhead = this_cpu_ptr(&cleanup_list);
 	struct apic_chip_data *apicd;
 	struct hlist_node *tmp;
+	unsigned long flags;
 
 	entering_ack_irq();
 	/* Prevent vectors vanishing under us */
-	raw_spin_lock(&vector_lock);
+	raw_spin_lock_irqsave(&vector_lock, flags);
 
 	hlist_for_each_entry_safe(apicd, tmp, clhead, clist) {
 		unsigned int irr, vector = apicd->prev_vector;
@@ -883,15 +888,16 @@ asmlinkage __visible void __irq_entry smp_irq_move_cleanup_interrupt(void)
 		free_moved_vector(apicd);
 	}
 
-	raw_spin_unlock(&vector_lock);
+	raw_spin_unlock_irqrestore(&vector_lock, flags);
 	exiting_irq();
 }
 
 static void __send_cleanup_vector(struct apic_chip_data *apicd)
 {
+	unsigned long flags;
 	unsigned int cpu;
 
-	raw_spin_lock(&vector_lock);
+	raw_spin_lock_irqsave(&vector_lock, flags);
 	apicd->move_in_progress = 0;
 	cpu = apicd->prev_cpu;
 	if (cpu_online(cpu)) {
@@ -900,7 +906,7 @@ static void __send_cleanup_vector(struct apic_chip_data *apicd)
 	} else {
 		apicd->prev_vector = 0;
 	}
-	raw_spin_unlock(&vector_lock);
+	raw_spin_unlock_irqrestore(&vector_lock, flags);
 }
 
 void send_cleanup_vector(struct irq_cfg *cfg)
@@ -938,6 +944,8 @@ void irq_force_complete_move(struct irq_desc *desc)
 	struct irq_data *irqd;
 	unsigned int vector;
 
+	WARN_ON_ONCE(irqs_pipelined() && !hard_irqs_disabled());
+
 	/*
 	 * The function is called for all descriptors regardless of which
 	 * irqdomain they belong to. For example if an IRQ is provided by
@@ -1028,9 +1036,10 @@ void irq_force_complete_move(struct irq_desc *desc)
 int lapic_can_unplug_cpu(void)
 {
 	unsigned int rsvd, avl, tomove, cpu = smp_processor_id();
+	unsigned long flags;
 	int ret = 0;
 
-	raw_spin_lock(&vector_lock);
+	raw_spin_lock_irqsave(&vector_lock, flags);
 	tomove = irq_matrix_allocated(vector_matrix);
 	avl = irq_matrix_available(vector_matrix, true);
 	if (avl < tomove) {
@@ -1045,7 +1054,7 @@ int lapic_can_unplug_cpu(void)
 			rsvd, avl);
 	}
 out:
-	raw_spin_unlock(&vector_lock);
+	raw_spin_unlock_irqrestore(&vector_lock, flags);
 	return ret;
 }
 #endif /* HOTPLUG_CPU */
diff --git a/arch/x86/kernel/apic/x2apic_cluster.c b/arch/x86/kernel/apic/x2apic_cluster.c
index b0889c48a2ac..0e1126a7320c 100644
--- a/arch/x86/kernel/apic/x2apic_cluster.c
+++ b/arch/x86/kernel/apic/x2apic_cluster.c
@@ -42,7 +42,7 @@ __x2apic_send_IPI_mask(const struct cpumask *mask, int vector, int apic_dest)
 	u32 dest;
 
 	x2apic_wrmsr_fence();
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 
 	tmpmsk = this_cpu_cpumask_var_ptr(ipi_mask);
 	cpumask_copy(tmpmsk, mask);
@@ -66,7 +66,7 @@ __x2apic_send_IPI_mask(const struct cpumask *mask, int vector, int apic_dest)
 		cpumask_andnot(tmpmsk, tmpmsk, &cmsk->mask);
 	}
 
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static void x2apic_send_IPI_mask(const struct cpumask *mask, int vector)
diff --git a/arch/x86/kernel/apic/x2apic_phys.c b/arch/x86/kernel/apic/x2apic_phys.c
index bc9693841353..2d6c4e33ba33 100644
--- a/arch/x86/kernel/apic/x2apic_phys.c
+++ b/arch/x86/kernel/apic/x2apic_phys.c
@@ -50,7 +50,7 @@ __x2apic_send_IPI_mask(const struct cpumask *mask, int vector, int apic_dest)
 
 	x2apic_wrmsr_fence();
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 
 	this_cpu = smp_processor_id();
 	for_each_cpu(query_cpu, mask) {
@@ -59,7 +59,7 @@ __x2apic_send_IPI_mask(const struct cpumask *mask, int vector, int apic_dest)
 		__x2apic_send_IPI_dest(per_cpu(x86_cpu_to_apicid, query_cpu),
 				       vector, APIC_DEST_PHYSICAL);
 	}
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static void x2apic_send_IPI_mask(const struct cpumask *mask, int vector)
diff --git a/arch/x86/kernel/cpu/mce/core.c b/arch/x86/kernel/cpu/mce/core.c
index 2e2a421c8528..cfa91af6f9b0 100644
--- a/arch/x86/kernel/cpu/mce/core.c
+++ b/arch/x86/kernel/cpu/mce/core.c
@@ -1357,11 +1357,11 @@ void do_machine_check(struct pt_regs *regs, long error_code)
 	/* Fault was in user mode and we need to take some action */
 	if ((m.cs & 3) == 3) {
 		ist_begin_non_atomic(regs);
-		local_irq_enable();
+		hard_local_irq_enable();
 
 		if (kill_it || do_memory_failure(&m))
 			force_sig(SIGBUS);
-		local_irq_disable();
+		hard_local_irq_disable();
 		ist_end_non_atomic();
 	} else {
 		if (!fixup_exception(regs, X86_TRAP_MC, error_code, 0))
diff --git a/arch/x86/kernel/dumpstack.c b/arch/x86/kernel/dumpstack.c
index e07424e19274..223791c55ed5 100644
--- a/arch/x86/kernel/dumpstack.c
+++ b/arch/x86/kernel/dumpstack.c
@@ -7,6 +7,7 @@
 #include <linux/uaccess.h>
 #include <linux/utsname.h>
 #include <linux/hardirq.h>
+#include <linux/irq_pipeline.h>
 #include <linux/kdebug.h>
 #include <linux/module.h>
 #include <linux/ptrace.h>
@@ -369,6 +370,8 @@ int __die(const char *str, struct pt_regs *regs, long err)
 {
 	const char *pr = "";
 
+	irq_pipeline_oops();
+
 	/* Save the regs of the first oops for the executive summary later. */
 	if (!die_counter)
 		exec_summary_regs = *regs;
@@ -377,13 +380,14 @@ int __die(const char *str, struct pt_regs *regs, long err)
 		pr = IS_ENABLED(CONFIG_PREEMPT_RT) ? " PREEMPT_RT" : " PREEMPT";
 
 	printk(KERN_DEFAULT
-	       "%s: %04lx [#%d]%s%s%s%s%s\n", str, err & 0xffff, ++die_counter,
+	       "%s: %04lx [#%d]%s%s%s%s%s%s\n", str, err & 0xffff, ++die_counter,
 	       pr,
 	       IS_ENABLED(CONFIG_SMP)     ? " SMP"             : "",
 	       debug_pagealloc_enabled()  ? " DEBUG_PAGEALLOC" : "",
 	       IS_ENABLED(CONFIG_KASAN)   ? " KASAN"           : "",
 	       IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION) ?
-	       (boot_cpu_has(X86_FEATURE_PTI) ? " PTI" : " NOPTI") : "");
+	       (boot_cpu_has(X86_FEATURE_PTI) ? " PTI" : " NOPTI") : "",
+	       irqs_pipelined()           ? " IRQ_PIPELINE"    : "");
 
 	show_regs(regs);
 	print_modules();
diff --git a/arch/x86/kernel/i8259.c b/arch/x86/kernel/i8259.c
index 519649ddf100..a2d8a2589d23 100644
--- a/arch/x86/kernel/i8259.c
+++ b/arch/x86/kernel/i8259.c
@@ -33,7 +33,7 @@
 static void init_8259A(int auto_eoi);
 
 static int i8259A_auto_eoi;
-DEFINE_RAW_SPINLOCK(i8259A_lock);
+DEFINE_HARD_SPINLOCK(i8259A_lock);
 
 /*
  * 8259A PIC functions to handle ISA devices:
@@ -227,6 +227,7 @@ struct irq_chip i8259A_chip = {
 	.irq_disable	= disable_8259A_irq,
 	.irq_unmask	= enable_8259A_irq,
 	.irq_mask_ack	= mask_and_ack_8259A,
+	.flags		= IRQCHIP_PIPELINE_SAFE,
 };
 
 static char irq_trigger[2];
diff --git a/arch/x86/kernel/idt.c b/arch/x86/kernel/idt.c
index 87ef69a72c52..7158c35785e5 100644
--- a/arch/x86/kernel/idt.c
+++ b/arch/x86/kernel/idt.c
@@ -114,6 +114,9 @@ static const __initconst struct idt_data apic_idts[] = {
 	INTG(CALL_FUNCTION_SINGLE_VECTOR, call_function_single_interrupt),
 	INTG(IRQ_MOVE_CLEANUP_VECTOR,	irq_move_cleanup_interrupt),
 	INTG(REBOOT_VECTOR,		reboot_interrupt),
+#ifdef CONFIG_IRQ_PIPELINE
+	INTG(RESCHEDULE_OOB_VECTOR,	reschedule_oob_interrupt),
+#endif
 #endif
 
 #ifdef CONFIG_X86_THERMAL_VECTOR
@@ -145,6 +148,9 @@ static const __initconst struct idt_data apic_idts[] = {
 	INTG(SPURIOUS_APIC_VECTOR,	spurious_interrupt),
 	INTG(ERROR_APIC_VECTOR,		error_interrupt),
 #endif
+#ifdef CONFIG_IRQ_PIPELINE
+	INTG(TIMER_OOB_VECTOR,		timer_oob_interrupt),
+#endif
 };
 
 #ifdef CONFIG_X86_64
diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c
index 21efee32e2b1..ed0b75d37bb4 100644
--- a/arch/x86/kernel/irq.c
+++ b/arch/x86/kernel/irq.c
@@ -49,7 +49,7 @@ void ack_bad_irq(unsigned int irq)
 	 * completely.
 	 * But only ack when the APIC is enabled -AK
 	 */
-	ack_APIC_irq();
+	__ack_APIC_irq();
 }
 
 #define irq_stats(x)		(&per_cpu(irq_stat, x))
diff --git a/arch/x86/kernel/irq_pipeline.c b/arch/x86/kernel/irq_pipeline.c
new file mode 100644
index 000000000000..f71eae8eee02
--- /dev/null
+++ b/arch/x86/kernel/irq_pipeline.c
@@ -0,0 +1,273 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2019 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <linux/irq.h>
+#include <linux/irq_pipeline.h>
+#include <asm/irqdomain.h>
+#include <asm/apic.h>
+#include <asm/traps.h>
+#include <asm/irq_work.h>
+#include <asm/mshyperv.h>
+
+static struct irq_domain *sipic_domain;
+
+static void sipic_irq_noop(struct irq_data *data) { }
+
+static unsigned int sipic_irq_noop_ret(struct irq_data *data)
+{
+	return 0;
+}
+
+static struct irq_chip sipic_chip = {
+	.name		= "SIPIC",
+	.irq_startup	= sipic_irq_noop_ret,
+	.irq_shutdown	= sipic_irq_noop,
+	.irq_enable	= sipic_irq_noop,
+	.irq_disable	= sipic_irq_noop,
+	.irq_ack	= sipic_irq_noop,
+	.irq_mask	= sipic_irq_noop,
+	.irq_unmask	= sipic_irq_noop,
+	.flags		= IRQCHIP_PIPELINE_SAFE | IRQCHIP_SKIP_SET_WAKE,
+};
+
+void handle_apic_irq(struct irq_desc *desc)
+{
+	unsigned int irq = irq_desc_get_irq(desc);
+	struct pt_regs *regs = get_irq_regs(); /* from generic_pipeline_irq() */
+
+	if (WARN_ON_ONCE(irq_pipeline_debug() && !on_pipeline_entry()))
+		return;
+
+	switch (apicm_irq_vector(irq)) {
+	case SPURIOUS_APIC_VECTOR:
+	case ERROR_APIC_VECTOR:
+		/*
+		 * No ack for error events, which should never
+		 * happen. If they do, the situation is messy, leave
+		 * the decision to acknowledge or not to the in-band
+		 * handler.
+		 */
+		break;
+	case THERMAL_APIC_VECTOR:
+		/*
+		 * MCE events are non-maskable, their in-band handlers
+		 * have to be OOB-compatible by construction, so we
+		 * can run them immediately.
+		 */
+		smp_thermal_interrupt(regs);
+		__ack_APIC_irq();
+		break;
+	case THRESHOLD_APIC_VECTOR:
+		smp_threshold_interrupt(regs);
+		__ack_APIC_irq();
+		break;
+	default:
+		__ack_APIC_irq();
+	}
+
+	handle_oob_irq(desc);
+}
+
+void irq_pipeline_send_remote(unsigned int ipi,
+			      const struct cpumask *cpumask)
+{
+	apic->send_IPI_mask_allbutself(cpumask,	apicm_irq_vector(ipi));
+}
+EXPORT_SYMBOL_GPL(irq_pipeline_send_remote);
+
+static void do_apic_irq(unsigned int irq, struct pt_regs *regs)
+{
+	int vector = apicm_irq_vector(irq);
+
+	switch (vector) {
+	case SPURIOUS_APIC_VECTOR:
+		smp_spurious_interrupt(regs);
+		break;
+	case ERROR_APIC_VECTOR:
+		smp_error_interrupt(regs);
+		break;
+#ifdef CONFIG_SMP
+	case RESCHEDULE_VECTOR:
+		smp_reschedule_interrupt(regs);
+		break;
+	case CALL_FUNCTION_VECTOR:
+		smp_call_function_interrupt(regs);
+		break;
+	case CALL_FUNCTION_SINGLE_VECTOR:
+		smp_call_function_single_interrupt(regs);
+		break;
+	case REBOOT_VECTOR:
+		smp_reboot_interrupt();
+		break;
+#endif
+	case X86_PLATFORM_IPI_VECTOR:
+		smp_x86_platform_ipi(regs);
+		break;
+	case IRQ_WORK_VECTOR:
+		smp_irq_work_interrupt(regs);
+		break;
+#ifdef CONFIG_X86_UV
+	case UV_BAU_MESSAGE:
+		uv_bau_message_interrupt(regs);
+		break;
+#endif
+#ifdef CONFIG_X86_MCE_AMD
+	case DEFERRED_ERROR_VECTOR:
+		smp_deferred_error_interrupt(regs);
+		break;
+#endif
+#ifdef CONFIG_HAVE_KVM
+	case POSTED_INTR_VECTOR:
+		smp_kvm_posted_intr_ipi(regs);
+		break;
+	case POSTED_INTR_WAKEUP_VECTOR:
+		smp_kvm_posted_intr_wakeup_ipi(regs);
+		break;
+	case POSTED_INTR_NESTED_VECTOR:
+		smp_kvm_posted_intr_nested_ipi(regs);
+		break;
+#endif
+#ifdef CONFIG_HYPERV
+	case HYPERVISOR_CALLBACK_VECTOR:
+		hyperv_vector_handler(regs);
+		break;
+	case HYPERV_REENLIGHTENMENT_VECTOR:
+		hyperv_reenlightenment_intr(regs);
+		break;
+	case HYPERV_STIMER0_VECTOR
+		hv_stimer0_vector_handler(regs);
+		break;
+#endif
+	case LOCAL_TIMER_VECTOR:
+		smp_apic_timer_interrupt(regs);
+		break;
+	case THERMAL_APIC_VECTOR:
+	case THRESHOLD_APIC_VECTOR:
+		/*
+		 * MCE have been dealt with immediatly on entry to the
+		 * pipeline (see handle_apic_irq()).
+		 */
+		break;
+	default:
+		printk_once(KERN_ERR "irq_pipeline: unexpected event"
+			" on vector #%.2x (irq=%u)", vector, irq);
+	}
+}
+
+void arch_do_IRQ_pipelined(struct irq_desc *desc)
+{
+	struct pt_regs *regs = raw_cpu_ptr(&irq_pipeline.tick_regs);
+	struct pt_regs *old_regs = set_irq_regs(regs);
+	unsigned int irq = irq_desc_get_irq(desc);
+
+	if (desc->irq_data.domain == sipic_domain) {
+		do_apic_irq(irq, regs);
+		return;
+	}
+
+	entering_irq();
+	generic_handle_irq_desc(desc);
+	exiting_irq();
+
+	set_irq_regs(old_regs);
+}
+
+__visible unsigned int __irq_entry handle_arch_irq_pipelined(struct pt_regs *regs)
+{
+	unsigned int irq, vector = ~regs->orig_ax;
+	struct irq_desc *desc;
+
+	if (vector >= FIRST_SYSTEM_VECTOR)
+		irq = apicm_vector_irq(vector);
+	else {
+		desc = __this_cpu_read(vector_irq[vector]);
+		if (unlikely(desc == NULL)) {
+			pr_err("IRQ pipeline: unhandled vector %#.2x\n", vector);
+			return 0;
+		}
+		irq = irq_desc_get_irq(desc);
+	}
+
+	generic_pipeline_irq(irq, regs);
+
+	return running_inband() && !irqs_disabled();
+}
+
+static int sipic_irq_map(struct irq_domain *d, unsigned int irq,
+			irq_hw_number_t hwirq)
+{
+	irq_set_percpu_devid(irq);
+	irq_set_chip_and_handler(irq, &sipic_chip, handle_apic_irq);
+
+	return 0;
+}
+
+static struct irq_domain_ops sipic_domain_ops = {
+	.map	= sipic_irq_map,
+};
+
+static void create_x86_apic_domain(void)
+{
+	sipic_domain = irq_domain_add_simple(NULL, NR_APIC_VECTORS,
+					     FIRST_SYSTEM_IRQ,
+					     &sipic_domain_ops, NULL);
+}
+
+#ifdef CONFIG_SMP
+
+static irqreturn_t irq_move_cleanup_handler(int irq, void *dev_id)
+{
+	smp_irq_move_cleanup_interrupt();
+
+	return IRQ_HANDLED;
+}
+
+static struct irqaction irq_move_cleanup = {
+	.handler = irq_move_cleanup_handler,
+	.name = "irq move cleanup",
+	.flags = IRQF_NO_THREAD,
+};
+
+static void smp_setup(void)
+{
+	struct irq_desc *desc;
+	unsigned int irq;
+	int cpu;
+
+	/*
+	 * The IRQ cleanup event must be pipelined to the inband
+	 * stage, so we need a valid IRQ descriptor for it. Since
+	 * IRQ_MOVE_CLEANUP_VECTOR does not belong to the APIC mapping
+	 * range, get a descriptor from the x86 vector domain instead.
+	 */
+	irq = irq_create_mapping(x86_vector_domain, IRQ_MOVE_CLEANUP_VECTOR);
+	BUG_ON(irq == 0);
+	desc = irq_to_desc(irq);
+	for_each_possible_cpu(cpu)
+		per_cpu(vector_irq, cpu)[IRQ_MOVE_CLEANUP_VECTOR] = desc;
+
+	setup_irq(irq, &irq_move_cleanup);
+}
+
+#else
+
+static void smp_setup(void) { }
+
+#endif
+
+void __init arch_irq_pipeline_init(void)
+{
+	/*
+	 * Create an IRQ domain for mapping APIC system interrupts
+	 * (in-band and out-of-band), with fixed sirq numbers starting
+	 * from FIRST_SYSTEM_IRQ. Upon receipt of a system interrupt,
+	 * the corresponding sirq is injected into the pipeline.
+	 */
+	create_x86_apic_domain();
+
+	smp_setup();
+}
diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 61e93a318983..39efdf3f96f6 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -600,9 +600,9 @@ void speculation_ctrl_update(unsigned long tif)
 	unsigned long flags;
 
 	/* Forced update. Make sure all relevant TIF flags are different */
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	__speculation_ctrl_update(~tif, tif);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /* Called from seccomp/prctl update */
@@ -697,6 +697,8 @@ void __cpuidle default_idle(void)
 {
 	trace_cpu_idle_rcuidle(1, smp_processor_id());
 	safe_halt();
+	if (irqs_pipelined())
+		local_irq_enable();
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
 #if defined(CONFIG_APM_MODULE) || defined(CONFIG_HALTPOLL_CPUIDLE_MODULE)
@@ -716,7 +718,7 @@ bool xen_set_default_idle(void)
 
 void stop_this_cpu(void *dummy)
 {
-	local_irq_disable();
+	hard_local_irq_disable();
 	/*
 	 * Remove this CPU:
 	 */
@@ -811,13 +813,15 @@ static __cpuidle void mwait_idle(void)
 		}
 
 		__monitor((void *)&current_thread_info()->flags, 0, 0);
-		if (!need_resched())
+		if (!need_resched()) {
 			__sti_mwait(0, 0);
-		else
-			local_irq_enable();
+			if (irqs_pipelined())
+				local_irq_enable();
+		} else
+			local_irq_enable_full();
 		trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 	} else {
-		local_irq_enable();
+		local_irq_enable_full();
 	}
 	__current_clr_polling();
 }
diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index b8d4e9c3c070..5a75d586e5cb 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -213,10 +213,10 @@ static void native_stop_other_cpus(int wait)
 			udelay(1);
 	}
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	disable_local_APIC();
 	mcheck_cpu_clear(this_cpu_ptr(&cpu_info));
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /*
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 69881b2d446c..6dd327510268 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -253,7 +253,7 @@ static void notrace start_secondary(void *unused)
 	x86_platform.nmi_init();
 
 	/* enable local interrupts */
-	local_irq_enable();
+	local_irq_enable_full();
 
 	/* to prevent fake stack check failure in clock setup */
 	boot_init_stack_canary();
@@ -1128,7 +1128,6 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
 	int apicid = apic->cpu_present_to_apicid(cpu);
 	int cpu0_nmi_registered = 0;
-	unsigned long flags;
 	int err, ret = 0;
 
 	lockdep_assert_irqs_enabled();
@@ -1179,9 +1178,9 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 	 * Check TSC synchronization with the AP (keep irqs disabled
 	 * while doing so):
 	 */
-	local_irq_save(flags);
+	local_irq_disable_full();
 	check_tsc_sync_source(cpu);
-	local_irq_restore(flags);
+	local_irq_enable_full();
 
 	while (!cpu_online(cpu)) {
 		cpu_relax();
@@ -1635,7 +1634,7 @@ void play_dead_common(void)
 	/*
 	 * With physical CPU hotplug, we should halt the cpu
 	 */
-	local_irq_disable();
+	local_irq_disable_full();
 }
 
 static bool wakeup_cpu0(void)
diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 05da6b5b167b..5f5855ff78f0 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -69,16 +69,54 @@
 
 DECLARE_BITMAP(system_vectors, NR_VECTORS);
 
+#ifdef CONFIG_IRQ_PIPELINE
+
+unsigned long pipelined_fault_entry(struct pt_regs *regs)
+{
+	unsigned long flags;
+	int nosync = 1;
+
+	flags = hard_local_irq_save();
+
+	if (hard_irqs_disabled_flags(flags))
+		nosync = test_and_set_stage_bit(STAGE_STALL_BIT,
+					this_inband_staged());
+	if (oob_stage_present())
+		hard_local_irq_enable();
+
+	return irqs_merge_flags(flags, nosync);
+}
+
+void pipelined_fault_exit(unsigned long combo)
+{
+	unsigned long flags;
+	int nosync;
+
+	WARN_ON_ONCE(irq_pipeline_debug() &&
+		oob_stage_present() && hard_irqs_disabled());
+
+	flags = irqs_split_flags(combo, &nosync);
+	if (!nosync) {
+		hard_local_irq_disable();
+		clear_stage_bit(STAGE_STALL_BIT, this_inband_staged());
+		if (!hard_irqs_disabled_flags(flags))
+			hard_local_irq_enable();
+	} else if (hard_irqs_disabled_flags(flags))
+		hard_local_irq_disable();
+}
+
+#endif	/* CONFIG_IRQ_PIPELINE */
+
 static inline void cond_local_irq_enable(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
-		local_irq_enable();
+		hard_local_irq_enable();
 }
 
 static inline void cond_local_irq_disable(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
-		local_irq_disable();
+		hard_local_irq_disable();
 }
 
 /*
@@ -258,6 +296,10 @@ NOKPROBE_SYMBOL(do_trap);
 static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 	unsigned long trapnr, int signr, int sicode, void __user *addr)
 {
+	unsigned long flags;
+
+	flags = pipelined_fault_entry(regs);
+
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 
 	/*
@@ -265,13 +307,16 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 	 * notifier chain.
 	 */
 	if (!user_mode(regs) && fixup_bug(regs, trapnr))
-		return;
+		goto out;
 
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
 			NOTIFY_STOP) {
 		cond_local_irq_enable(regs);
 		do_trap(trapnr, signr, str, regs, error_code, sicode, addr);
 	}
+
+out:
+	pipelined_fault_exit(flags);
 }
 
 #define IP ((void __user *)uprobe_get_trap_addr(regs))
@@ -435,11 +480,14 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 {
 	const struct mpx_bndcsr *bndcsr;
+	unsigned long flags;
+
+	flags = pipelined_fault_entry(regs);
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	if (notify_die(DIE_TRAP, "bounds", regs, error_code,
 			X86_TRAP_BR, SIGSEGV) == NOTIFY_STOP)
-		return;
+		goto out;
 	cond_local_irq_enable(regs);
 
 	if (!user_mode(regs))
@@ -505,7 +553,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 		die("bounds", regs, error_code);
 	}
 
-	return;
+	goto out;
 
 exit_trap:
 	/*
@@ -516,6 +564,8 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	 * time..
 	 */
 	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, 0, NULL);
+out:
+	pipelined_fault_exit(flags);
 }
 
 dotraplinkage void
@@ -523,25 +573,28 @@ do_general_protection(struct pt_regs *regs, long error_code)
 {
 	const char *desc = "general protection fault";
 	struct task_struct *tsk;
+	unsigned long flags;
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	cond_local_irq_enable(regs);
 
+	flags = pipelined_fault_entry(regs);
+
 	if (static_cpu_has(X86_FEATURE_UMIP)) {
 		if (user_mode(regs) && fixup_umip_exception(regs))
-			return;
+			goto out;
 	}
 
 	if (v8086_mode(regs)) {
 		local_irq_enable();
 		handle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);
-		return;
+		goto out;
 	}
 
 	tsk = current;
 	if (!user_mode(regs)) {
 		if (fixup_exception(regs, X86_TRAP_GP, error_code, 0))
-			return;
+			goto out;
 
 		tsk->thread.error_code = error_code;
 		tsk->thread.trap_nr = X86_TRAP_GP;
@@ -553,12 +606,12 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		 */
 		if (!preemptible() && kprobe_running() &&
 		    kprobe_fault_handler(regs, X86_TRAP_GP))
-			return;
+			goto out;
 
 		if (notify_die(DIE_GPF, desc, regs, error_code,
 			       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)
 			die(desc, regs, error_code);
-		return;
+		goto out;
 	}
 
 	tsk->thread.error_code = error_code;
@@ -567,11 +620,17 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	show_signal(tsk, SIGSEGV, "", desc, regs, error_code);
 
 	force_sig(SIGSEGV);
+out:
+	pipelined_fault_exit(flags);
 }
 NOKPROBE_SYMBOL(do_general_protection);
 
 dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
+	unsigned long flags;
+
+	flags = pipelined_fault_entry(regs);
+
 #ifdef CONFIG_DYNAMIC_FTRACE
 	/*
 	 * ftrace must be first, everything else may cause a recursive crash.
@@ -579,10 +638,10 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	 */
 	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
 	    ftrace_int3_handler(regs))
-		return;
+		goto out;
 #endif
 	if (poke_int3_handler(regs))
-		return;
+		goto out;
 
 	/*
 	 * Use ist_enter despite the fact that we don't use an IST stack.
@@ -614,6 +673,8 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 
 exit:
 	ist_exit(regs);
+out:
+	pipelined_fault_exit(flags);
 }
 NOKPROBE_SYMBOL(do_int3);
 
@@ -713,8 +774,8 @@ static bool is_sysenter_singlestep(struct pt_regs *regs)
 dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk = current;
+	unsigned long dr6, flags;
 	int user_icebp = 0;
-	unsigned long dr6;
 	int si_code;
 
 	ist_enter(regs);
@@ -771,9 +832,11 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 		goto exit;
 #endif
 
+	flags = pipelined_fault_entry(regs);
+
 	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, error_code,
 							SIGTRAP) == NOTIFY_STOP)
-		goto exit;
+		goto exit_fault;
 
 	/*
 	 * Let others (NMI) know that the debug stack is in use
@@ -789,7 +852,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 					X86_TRAP_DB);
 		cond_local_irq_disable(regs);
 		debug_stack_usage_dec();
-		goto exit;
+		goto exit_fault;
 	}
 
 	if (WARN_ON_ONCE((dr6 & DR_STEP) && !user_mode(regs))) {
@@ -809,6 +872,8 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	cond_local_irq_disable(regs);
 	debug_stack_usage_dec();
 
+exit_fault:
+	pipelined_fault_exit(flags);
 exit:
 	ist_exit(regs);
 }
@@ -823,15 +888,18 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 {
 	struct task_struct *task = current;
 	struct fpu *fpu = &task->thread.fpu;
+	unsigned long flags;
 	int si_code;
 	char *str = (trapnr == X86_TRAP_MF) ? "fpu exception" :
 						"simd exception";
 
+	flags = pipelined_fault_entry(regs);
+
 	cond_local_irq_enable(regs);
 
 	if (!user_mode(regs)) {
 		if (fixup_exception(regs, trapnr, error_code, 0))
-			return;
+			goto out;
 
 		task->thread.error_code = error_code;
 		task->thread.trap_nr = trapnr;
@@ -839,7 +907,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 		if (notify_die(DIE_TRAP, str, regs, error_code,
 					trapnr, SIGFPE) != NOTIFY_STOP)
 			die(str, regs, error_code);
-		return;
+		goto out;
 	}
 
 	/*
@@ -853,10 +921,12 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	si_code = fpu__exception_code(fpu, trapnr);
 	/* Retry when we get spurious exceptions: */
 	if (!si_code)
-		return;
+		goto out;
 
 	force_sig_fault(SIGFPE, si_code,
 			(void __user *)uprobe_get_trap_addr(regs));
+out:
+	pipelined_fault_exit(flags);
 }
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
@@ -881,7 +951,7 @@ do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
-	unsigned long cr0 = read_cr0();
+	unsigned long cr0 = read_cr0(), flags __maybe_unused;
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 
@@ -889,10 +959,14 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 	if (!boot_cpu_has(X86_FEATURE_FPU) && (cr0 & X86_CR0_EM)) {
 		struct math_emu_info info = { };
 
+		flags = pipelined_fault_entry(regs);
+
 		cond_local_irq_enable(regs);
 
 		info.regs = regs;
 		math_emulate(&info);
+
+		pipelined_fault_exit(flags);
 		return;
 	}
 #endif
@@ -916,6 +990,7 @@ NOKPROBE_SYMBOL(do_device_not_available);
 dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 {
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
+
 	local_irq_enable();
 
 	if (notify_die(DIE_TRAP, "iret exception", regs, error_code,
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 7e322e2daaf5..07ee274ba8a9 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -124,8 +124,11 @@ static void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long ts
 {
 	unsigned long long ns_now;
 	struct cyc2ns_data data;
+	unsigned long flags;
 	struct cyc2ns *c2n;
 
+	flags = hard_cond_local_irq_save();
+
 	ns_now = cycles_2_ns(tsc_now);
 
 	/*
@@ -156,6 +159,8 @@ static void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long ts
 	c2n->data[0] = data;
 	raw_write_seqcount_latch(&c2n->seq);
 	c2n->data[1] = data;
+
+	hard_cond_local_irq_restore(flags);
 }
 
 static void set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)
@@ -752,11 +757,11 @@ static unsigned long pit_hpet_ptimer_calibrate_cpu(void)
 		 * calibration, which will take at least 50ms, and
 		 * read the end value.
 		 */
-		local_irq_save(flags);
+		flags = hard_local_irq_save();
 		tsc1 = tsc_read_refs(&ref1, hpet);
 		tsc_pit_khz = pit_calibrate_tsc(latch, ms, loopmin);
 		tsc2 = tsc_read_refs(&ref2, hpet);
-		local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 
 		/* Pick the lowest PIT TSC calibration so far */
 		tsc_pit_min = min(tsc_pit_min, tsc_pit_khz);
@@ -865,9 +870,9 @@ unsigned long native_calibrate_cpu_early(void)
 	if (!fast_calibrate)
 		fast_calibrate = cpu_khz_from_msr();
 	if (!fast_calibrate) {
-		local_irq_save(flags);
+		flags = hard_local_irq_save();
 		fast_calibrate = quick_pit_calibrate();
-		local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 	}
 	return fast_calibrate;
 }
@@ -935,7 +940,7 @@ void tsc_restore_sched_clock_state(void)
 	if (!sched_clock_stable())
 		return;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 
 	/*
 	 * We're coming out of suspend, there's no concurrency yet; don't
@@ -953,7 +958,7 @@ void tsc_restore_sched_clock_state(void)
 		per_cpu(cyc2ns.data[1].cyc2ns_offset, cpu) = offset;
 	}
 
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 #ifdef CONFIG_CPU_FREQ
diff --git a/arch/x86/kernel/tsc_sync.c b/arch/x86/kernel/tsc_sync.c
index b8acf639abd1..0cf7e22a84cc 100644
--- a/arch/x86/kernel/tsc_sync.c
+++ b/arch/x86/kernel/tsc_sync.c
@@ -327,6 +327,8 @@ void check_tsc_sync_source(int cpu)
 		atomic_set(&test_runs, 1);
 	else
 		atomic_set(&test_runs, 3);
+
+	hard_cond_local_irq_disable();
 retry:
 	/*
 	 * Wait for the target to start or to skip the test:
@@ -408,6 +410,8 @@ void check_tsc_sync_target(void)
 	if (unsynchronized_tsc())
 		return;
 
+	hard_cond_local_irq_disable();
+
 	/*
 	 * Store, verify and sanitize the TSC adjust register. If
 	 * successful skip the test.
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 304d31d8cbbc..84ace5c3c477 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -15,6 +15,7 @@
 #include <linux/hugetlb.h>		/* hstate_index_to_shift	*/
 #include <linux/prefetch.h>		/* prefetchw			*/
 #include <linux/context_tracking.h>	/* exception_enter(), ...	*/
+#include <linux/irq_pipeline.h>		/* pipelined_fault_entry(), ...	*/
 #include <linux/uaccess.h>		/* faulthandler_disabled()	*/
 #include <linux/efi.h>			/* efi_recover_from_page_fault()*/
 #include <linux/mm_types.h>
@@ -873,7 +874,7 @@ __bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,
 		/*
 		 * It's possible to have interrupts off here:
 		 */
-		local_irq_enable();
+		hard_local_irq_enable();
 
 		/*
 		 * Valid to do another page fault here because this one came
@@ -1335,11 +1336,11 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 * potential system fault or CPU buglet:
 	 */
 	if (user_mode(regs)) {
-		local_irq_enable();
+		hard_local_irq_enable();
 		flags |= FAULT_FLAG_USER;
 	} else {
 		if (regs->flags & X86_EFLAGS_IF)
-			local_irq_enable();
+			hard_local_irq_enable();
 	}
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
@@ -1524,10 +1525,15 @@ dotraplinkage void
 do_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned long address)
 {
 	enum ctx_state prev_state;
+	unsigned long flags;
+
+	flags = pipelined_fault_entry(regs);
 
 	prev_state = exception_enter();
 	trace_page_fault_entries(regs, error_code, address);
 	__do_page_fault(regs, error_code, address);
 	exception_exit(prev_state);
+
+	pipelined_fault_exit(flags);
 }
 NOKPROBE_SYMBOL(do_page_fault);
-- 
2.16.4

