From aa0211969820e2c1766edba2a6a4123d656945ef Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Mon, 10 Jun 2019 12:12:22 +0200
Subject: [PATCH] x86: irq_pipeline: handle IRQ cleanup events properly

IRQ_MOVE_CLEANUP_VECTOR is not part of the APIC range for which we
have a virtual interrupt domain, but this is actually the first
external IRQ vector which makes it a low-priority event.

Cleanup events are triggered by the CPU the IRQ was moved to when it
receives the first notification of such interrupt, which implicitly
confirms that the migration is complete hardware-wise.  We need to
handle events on this vector specifically in order to hand them to the
interrupt cleanup code.

NOTE: It is currently assumed that smp_irq_move_cleanup_interrupt()
won't cause latency to skyrocket due to a large number of vectors to
clean up in a single pass while holding the hard vector_lock. The good
news is that there is no reason for other CPUs to grab the vector_lock
as part of out-of-band processing. However, the current CPU handling
the cleanup event might run with IRQs hard disabled for too long if
that happens.

Signed-off-by: Philippe Gerum <rpm@xenomai.org>
---
 arch/x86/kernel/apic/vector.c  |  4 ++++
 arch/x86/kernel/irq_pipeline.c | 44 +++++++++++++++++++++---------------------
 2 files changed, 26 insertions(+), 22 deletions(-)

diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c
index c8ab1608177b..2af0ebd4222b 100644
--- a/arch/x86/kernel/apic/vector.c
+++ b/arch/x86/kernel/apic/vector.c
@@ -727,6 +727,10 @@ static struct irq_desc *__setup_vector_irq(int vector)
 {
 	int isairq = vector - ISA_IRQ_VECTOR(0);
 
+	/* Copy the cleanup vector if irqs are pipelined. */
+	if (IS_ENABLED(CONFIG_SMP) &&
+		vector == IRQ_MOVE_CLEANUP_VECTOR)
+		return irq_to_desc(IRQ_MOVE_CLEANUP_VECTOR); /* 1:1 mapping */
 	/* Check whether the irq is in the legacy space */
 	if (isairq < 0 || isairq >= nr_legacy_irqs())
 		return VECTOR_UNUSED;
diff --git a/arch/x86/kernel/irq_pipeline.c b/arch/x86/kernel/irq_pipeline.c
index 391e76bc2e33..eefe68f8a357 100644
--- a/arch/x86/kernel/irq_pipeline.c
+++ b/arch/x86/kernel/irq_pipeline.c
@@ -221,38 +221,38 @@ static void create_x86_apic_domain(void)
 
 #ifdef CONFIG_SMP
 
-static irqreturn_t irq_move_cleanup_handler(int irq, void *dev_id)
+void handle_irq_move_cleanup(struct irq_desc *desc)
 {
-	smp_irq_move_cleanup_interrupt();
-
-	return IRQ_HANDLED;
+	if (on_pipeline_entry()) {
+		/* First there on receipt from hardware. */
+		__ack_APIC_irq();
+		handle_oob_irq(desc);
+	} else /* Next there on inband delivery. */
+		smp_irq_move_cleanup_interrupt();
 }
 
-static struct irqaction irq_move_cleanup = {
-	.handler = irq_move_cleanup_handler,
-	.name = "irq move cleanup",
-	.flags = IRQF_NO_THREAD,
-};
-
 static void smp_setup(void)
 {
-	struct irq_desc *desc;
-	unsigned int irq;
-	int cpu;
+	int irq;
 
 	/*
 	 * The IRQ cleanup event must be pipelined to the inband
-	 * stage, so we need a valid IRQ descriptor for it. Since
-	 * IRQ_MOVE_CLEANUP_VECTOR does not belong to the APIC mapping
-	 * range, get a descriptor from the x86 vector domain instead.
+	 * stage, so we need a valid IRQ descriptor for it. Since we
+	 * still are in the early boot stage on CPU0, we ask for a 1:1
+	 * mapping between the vector number and IRQ number, to make
+	 * things easier for us later on.
+	 */
+	irq = irq_alloc_desc_at(IRQ_MOVE_CLEANUP_VECTOR, 0);
+	WARN_ON(IRQ_MOVE_CLEANUP_VECTOR != irq);
+	/*
+	 * Set up the vector_irq[] mapping array for the boot CPU,
+	 * other CPUs will copy this entry when their APIC is going
+	 * online (see lapic_online()).
 	 */
-	irq = irq_create_mapping(x86_vector_domain, IRQ_MOVE_CLEANUP_VECTOR);
-	BUG_ON(irq == 0);
-	desc = irq_to_desc(irq);
-	for_each_possible_cpu(cpu)
-		per_cpu(vector_irq, cpu)[IRQ_MOVE_CLEANUP_VECTOR] = desc;
+	per_cpu(vector_irq, 0)[irq] = irq_to_desc(irq);
 
-	setup_irq(irq, &irq_move_cleanup);
+	irq_set_chip_and_handler(irq, &dummy_irq_chip,
+				handle_irq_move_cleanup);
 }
 
 #else
-- 
2.16.4

