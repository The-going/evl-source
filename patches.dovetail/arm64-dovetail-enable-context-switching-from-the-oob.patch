From d02e6a7a6f24f005f378d328a70f8e3b647f9fb8 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Fri, 14 Sep 2018 09:57:59 +0200
Subject: [PATCH] arm64: dovetail: enable context switching from the oob stage

Signed-off-by: Philippe Gerum <rpm@xenomai.org>
---
 arch/arm64/include/asm/efi.h         |  6 ++++++
 arch/arm64/include/asm/mmu_context.h | 22 ++++++++++++++++++++--
 arch/arm64/kernel/irq_pipeline.c     |  3 +++
 arch/arm64/mm/context.c              |  4 +++-
 4 files changed, 32 insertions(+), 3 deletions(-)

diff --git a/arch/arm64/include/asm/efi.h b/arch/arm64/include/asm/efi.h
index b54d3a86c444..0bdb392861ca 100644
--- a/arch/arm64/include/asm/efi.h
+++ b/arch/arm64/include/asm/efi.h
@@ -135,6 +135,10 @@ static inline void efifb_setup_from_dmi(struct screen_info *si, const char *opt)
 
 static inline void efi_set_pgd(struct mm_struct *mm)
 {
+	unsigned long flags;
+
+	protect_inband_mm(flags);
+
 	__switch_mm(mm);
 
 	if (system_uses_ttbr0_pan()) {
@@ -159,6 +163,8 @@ static inline void efi_set_pgd(struct mm_struct *mm)
 			update_saved_ttbr0(current, current->active_mm);
 		}
 	}
+
+	unprotect_inband_mm(flags);
 }
 
 void efi_virtmap_load(void);
diff --git a/arch/arm64/include/asm/mmu_context.h b/arch/arm64/include/asm/mmu_context.h
index d0937a1d8a58..35c659fc9c1e 100644
--- a/arch/arm64/include/asm/mmu_context.h
+++ b/arch/arm64/include/asm/mmu_context.h
@@ -238,7 +238,7 @@ static inline void __switch_mm(struct mm_struct *next)
 }
 
 static inline void
-switch_mm(struct mm_struct *prev, struct mm_struct *next,
+do_switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	  struct task_struct *tsk)
 {
 	if (prev != next)
@@ -253,8 +253,26 @@ switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	update_saved_ttbr0(tsk, next);
 }
 
+static inline void
+switch_mm(struct mm_struct *prev, struct mm_struct *next,
+	  struct task_struct *tsk)
+{
+	unsigned long flags;
+
+	protect_inband_mm(flags);
+	do_switch_mm(prev, next, tsk);
+	unprotect_inband_mm(flags);
+}
+
 #define deactivate_mm(tsk,mm)	do { } while (0)
-#define activate_mm(prev,next)	switch_mm(prev, next, current)
+#define activate_mm(prev,next)	do_switch_mm(prev, next, current)
+
+static inline void
+switch_oob_mm(struct mm_struct *prev, struct mm_struct *next,
+	      struct task_struct *tsk)
+{
+	do_switch_mm(prev, next, tsk);
+}
 
 void verify_cpu_asid_bits(void);
 void post_ttbr_update_workaround(void);
diff --git a/arch/arm64/kernel/irq_pipeline.c b/arch/arm64/kernel/irq_pipeline.c
index b41b5ff52ac4..5afe8cca0432 100644
--- a/arch/arm64/kernel/irq_pipeline.c
+++ b/arch/arm64/kernel/irq_pipeline.c
@@ -8,6 +8,9 @@
 #include <linux/irq.h>
 #include <linux/irq_pipeline.h>
 
+/* irq_nesting tracks the interrupt nesting level for a CPU. */
+DEFINE_PER_CPU(int, irq_nesting);
+
 #ifdef CONFIG_SMP
 
 static struct irq_domain *sipic_domain;
diff --git a/arch/arm64/mm/context.c b/arch/arm64/mm/context.c
index b5e329fde2dd..8e11bfdd76cf 100644
--- a/arch/arm64/mm/context.c
+++ b/arch/arm64/mm/context.c
@@ -17,7 +17,7 @@
 #include <asm/tlbflush.h>
 
 static u32 asid_bits;
-static DEFINE_RAW_SPINLOCK(cpu_asid_lock);
+static DEFINE_HARD_SPINLOCK(cpu_asid_lock);
 
 static atomic64_t asid_generation;
 static unsigned long *asid_map;
@@ -185,6 +185,8 @@ void check_and_switch_context(struct mm_struct *mm, unsigned int cpu)
 	unsigned long flags;
 	u64 asid, old_active_asid;
 
+	WARN_ON_ONCE(dovetail_debug() && !hard_irqs_disabled());
+
 	if (system_supports_cnp())
 		cpu_set_reserved_ttbr0();
 
-- 
2.16.4

