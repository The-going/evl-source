From 01a35a136d18379476a44e23db8ddc4caed99562 Mon Sep 17 00:00:00 2001
From: Philippe Gerum <rpm@xenomai.org>
Date: Fri, 22 Jul 2016 17:15:36 +0200
Subject: [PATCH] sched: irq_pipeline: handle preemption on IRQ

---
 kernel/sched/core.c | 41 +++++++++++++++++++++++++++++++++++++++--
 1 file changed, 39 insertions(+), 2 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 010d578118d6..1f6c529e1e81 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4048,7 +4048,7 @@ asmlinkage __visible void __sched notrace preempt_schedule(void)
 	 * If there is a non-zero preempt_count or interrupts are disabled,
 	 * we do not want to preempt the current task. Just return..
 	 */
-	if (likely(!preemptible()))
+	if (likely(!running_inband() || !preemptible()))
 		return;
 
 	preempt_schedule_common();
@@ -4074,7 +4074,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
 {
 	enum ctx_state prev_ctx;
 
-	if (likely(!preemptible()))
+	if (likely(!running_inband() || !preemptible()))
 		return;
 
 	do {
@@ -4110,6 +4110,27 @@ EXPORT_SYMBOL_GPL(preempt_schedule_notrace);
 
 #endif /* CONFIG_PREEMPT */
 
+#ifdef CONFIG_IRQ_PIPELINE
+static inline void preempt_sync_inband_irqs(unsigned long flags)
+{
+	struct irq_stage_data *p;
+
+	hard_local_irq_disable();
+	p = this_inband_staged();
+	if (unlikely(stage_irqs_pending(p))) {
+		preempt_disable();
+		trace_hardirqs_on();
+		clear_stage_bit(STAGE_STALL_BIT, p);
+		sync_current_stage();
+		preempt_enable_no_resched_notrace();
+	}
+	/* We leave IRQs hard disabled. */
+	inband_irq_restore_nosync(flags);
+}
+#else
+static inline void preempt_sync_inband_irqs(unsigned long flags) { }
+#endif
+
 /*
  * this is the entry point to schedule() from kernel preemption
  * off of irq context.
@@ -4119,6 +4140,13 @@ EXPORT_SYMBOL_GPL(preempt_schedule_notrace);
 asmlinkage __visible void __sched preempt_schedule_irq(void)
 {
 	enum ctx_state prev_state;
+	unsigned long flags;
+
+	if (irqs_pipelined()) {
+		WARN_ON_ONCE(!hard_irqs_disabled());
+		local_irq_save(flags);
+		hard_local_irq_enable();
+	}
 
 	/* Catch callers which need to be fixed */
 	BUG_ON(preempt_count() || !irqs_disabled());
@@ -4133,6 +4161,15 @@ asmlinkage __visible void __sched preempt_schedule_irq(void)
 		sched_preempt_enable_no_resched();
 	} while (need_resched());
 
+	/*
+	 * If pipelining interrupts, flush any pending IRQ that might
+	 * have been logged since the last time we stalled the in-band
+	 * stage. The caller is expected to call us back again until
+	 * need_resched is clear, so we just need to synchronize the
+	 * in-band stage log.
+	 */
+	preempt_sync_inband_irqs(flags);
+
 	exception_exit(prev_state);
 }
 
-- 
2.16.4

